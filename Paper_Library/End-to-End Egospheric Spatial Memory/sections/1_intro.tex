\section{Introduction}\label{sec:introduction}

Egocentric spatial memory is central to our understanding of spatial reasoning in biology \citep{klatzky1998allocentric, burgess2006spatial}, where an embodied agent constantly carries with it a local map of its surrounding geometry. Such representations have particular significance for action selection and motor control \citep{hinman2019neuronal}. For robotics and embodied AI, the benefits of a persistent local spatial memory are also clear. Such a system has the potential to run for long periods, and bypass both the memory and runtime complexities of large scale world-centric mapping. \cite{peters2001sensory} propose an EgoSphere as being a particularly suitable representation for robotics, and more recent works have utilized ego-centric formulations for planar robot mapping \citep{fankhauser2014robot}, drone obstacle avoidance \citep{fragoso2018dynamically} and mono-to-depth \citep{liu2019neural}.

In parallel with these ego-centric mapping systems, a new paradigm of differentiable memory architectures has arisen, where a memory bank is augmented to a neural network, which can then learn read and write operations \citep{weston2014memory, graves2014neural, sukhbaatar2015end}. When compared to Recurrent Neural Networks (RNNs), the persistent memory circumvents issues of vanishing or exploding gradients, enabling solutions to long-horizon tasks. These have also been applied to visuomotor control and navigation tasks \citep{wayne2018unsupervised}, surpassing baselines such as the ubiquitous Long Short-Term Memory (LSTM) \citep{hochreiter1997long}.

We focus on the intersection of these two branches of research, and propose Egospheric Spatial Memory (ESM), a parameter-free module which encodes geometric and semantic information about the scene in an ego-sphere around the agent. To the best of our knowledge, ESM is the first end-to-end trainable egocentric memory with a full panoramic representation, enabling direct encoding of the surrounding scene in a 2.5D image.

We also show that by propagating gradients through the ESM computation graph we can learn features to be stored in the memory. We demonstrate the superiority of learning features through the ESM module on both target shape reaching and object segmentation tasks. For other visuomotor control tasks, we show that even without learning features through the module, and instead directly projecting image color values into memory, ESM consistently outperforms other memory baselines.

Through these experiments, we show that the applications of our parameter-free ESM module are widespread, where it can either be dropped into existing pipelines as a non-learned module, or end-to-end trained in a larger computation graph, depending on the task requirements.

% \begin{wrapfigure}[18]{R}{0.5\textwidth}
%     \vspace{-20pt}
%     \centering
%     \includegraphics[scale=0.17]{figures/memory_mapping_analogy.png}
%     \caption{Computation graphs of (a) ESM and (b) neural memory (MemNN, MemN2N, NTM)}
%     \label{fig:esm_vs_ntm}
% \end{wrapfigure}