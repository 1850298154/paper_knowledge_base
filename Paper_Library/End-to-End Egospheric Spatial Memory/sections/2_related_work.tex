\section{Related Work}\label{sec:related_work}

\subsection{Mapping}

Geometric mapping is a mature field, with many solutions available for constructing high quality maps. Such systems typically maintain an allocentric map, either by projecting points into a global world co-ordinate system \citep{newcombe2011kinectfusion, whelan2015elasticfusion}, or by maintaining a certain number of keyframes in the trajectory history \citep{zhou2018deeptam,bloesch2018codeslam}. If these systems are to be applied to life-long embodied AI, then strategies are required to effectively select the parts of the map which are useful, and discard the rest from memory \citep{cadena2016past}.

For robotics applications, prioritizing geometry in the immediate vicinity is a sensible prior. Rather than taking a world-view to map construction, such systems often formulate the mapping problem in a purely ego-centric manner, performing continual re-projection to the newest frame and pose with fixed-sized storage. Unlike allocentric formulations, the memory indexing is then fully coupled to the agent pose, resulting in an ordered representation particularly well suited for downstream egocentric tasks, such as action selection. \cite{peters2001sensory} outline an EgoSphere memory structure as being suitable for humanoid robotics, with indexing via polar and azimuthal angles. \cite{fankhauser2014robot} use ego-centric height maps, and demonstrate on a quadrupedal robot walking over obstacles. \cite{cigla2017gaussian} use per-pixel depth Gaussian Mixture Models (GMMs) to maintain an ego-cylinder of belief around a drone, with applications to collision avoidance \citep{fragoso2018dynamically}. In a different application, \cite{liu2019neural} learn to predict depth images from a sequence of RGB images, again using ego reprojections. These systems are all designed to represent only at the level of depth and RGB features. For mapping more expressive implicit features via end-to-end training, a fully differentiable long-horizon computation graph is required. Any computation graph which satisfies this requirement is generally referred to as memory in the neural network literature.

\subsection{Memory}

The concept of memory in neural networks is deeply coupled with recurrence. Naive recurrent networks have vanishing and exploding gradient problems \citep{hochreiter1998vanishing}, which LSTMs \citep{hochreiter1997long} and Gated Recurrent Units (GRUs) \citep{cho2014properties} mediate using additive gated structures. More recently, dedicated differentiable memory blocks have become a popular alternative. \cite{weston2014memory} applied Memory Networks (MemNN) to question answering, using hard read-writes and separate training of components. \cite{graves2014neural} and \cite{sukhbaatar2015end} instead made the read and writes `soft' with the proposal of Neural Turing Machines (NTM) and End-to-End Memory Networks (MemN2N) respectively, enabling joint training with the controller. Other works have since conditioned dynamic memory on images, for tasks such as visual question answering \citep{xiong2016dynamic} and object segmentation \citep{oh2019video}. Another distinct but closely related approach is self attention \citep{vaswani2017attention}. These approaches also use key-based content retrieval, but do so on a history of previous observations with adjacent connectivity. Despite the lack of geometric inductive bias, recent results demonstrate the amenability of general memory \citep{wayne2018unsupervised} and attention \citep{parisotto2019stabilizing} to visuomotor control and navigation tasks.

Other authors have explored the intersection of network memory and spatial mapping for navigation, but have generally been limited to 2D aerial-view maps, focusing on planar navigation tasks. \cite{gupta2017cognitive} used an implicit ego-centric memory which was updated with warping and confidence maps for discrete action navigation problems. \cite{parisotto2017neural} proposed a similar setup, but used dedicated learned read and write operations for updates, and tested on simulated Doom environments. Without consideration for action selection, \cite{henriques2018mapnet} proposed a similar system, but instead used an allocentric formulation, and tested on free-form trajectories of real images. \cite{zhang2018egocentric} also propose a similar system, but with the inclusion of loop closure. Our memory instead focuses on local perception, with the ability to represent detailed 3D geometry in all directions around the agent. The benefits of our module are complementary to existing 2D methods, which instead focus on occlusion-aware planar understanding suitable for navigation.

% \begin{figure}[]
% \centering
% \includegraphics[scale=0.22]{figures/FullScanNetImage.png}
%   \caption{Left: point cloud representation of the ego-centric memory around the camera after full rotation in a scene from ScanNet, with RGB features. Mid: (top) Equivalent omni-directional RGB image, (bottom) equivalent omni-directional depth image, both without smoothing to better demonstrate the quantization holes. Right: (top) A single RGB frame, (bottom) a single depth frame}
%   \label{fig:point_cloud}
% \end{figure}