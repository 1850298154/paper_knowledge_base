\subsection{Object Segmentation}
\label{sec:obj_seg}

We now explore the suitability of the ESM module for object segmentation. One approach is to perform image-level segmentation in individual monocular frames, and then perform probabilistic fusion when projecting into the map \citep{mccormac2017semanticfusion}. We refer to this approach as Mono. Another approach is to first construct an RGB map, and then pass this map as input to a network. This has the benefit of wider context, but lower resolution is necessary to store a large map in memory, meaning details can be lost. ESMN-RGB adopts this approach. Another approach is to combine monocular predictions with multi-view optimization to gain the benefits of wider surrounding context as in \citep{zhi2019scenecode}. Similarly, the ESMN architecture is able to combine monocular inference with the wider map context, but does so by constructing a network with both image-level and map-level convolutions. These ESM variants adopt the same broad architectures as shown in Fig \ref{fig:simplified_networks}, with the full networks specified in Appendix \ref{app:obj_seg}. We do not attempt to quote state-of-the-art results, but rather to further demonstrate the wide applications of the ESM module, and to explore the effect of placing the ESM module at different locations in the convolutional stack. We evaluate segmentation accuracy based on the predictions projected to the ego-centric map. With fixed network capacity between methods, we see that ESMN outperforms both baselines, see Table \ref{table:obj_seg} for the results, and Figure \ref{fig:obj_seg} for example predictions in a ScanNet reconstruction. Further details are given in Appendix \ref{app:obj_seg}.

%Our ESMN approach is conceptually comparable to SceneCode \cite{zhi2019scenecode}, which makes monocular segmentation predictions and then optimizes these using multi-view optimization and the full network Jacobians, enabling wider context. Our ESMN method is implementationally much simpler, with predictions made directly in the network forward pass.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{figures/obj_seg.png}
  \caption{Top: Segmentation predictions in ESM memory for Mono and ESMN, and ground truth. Middle: Point cloud rendering of the predictions. Bottom: Monocular input image sequence.}
  \label{fig:obj_seg}
\end{figure}

\begin{table}[h] %{l}{0.5\linewidth}
\centering
 \begin{tabular}{|| c || c | c || c | c || c | c ||}
 \cline{2-7}
 \multicolumn{1}{c||}{} & \multicolumn{2}{c||}{Images $ 60\times80$} & \multicolumn{2}{c||}{Images $ 120\times160$} & \multicolumn{2}{c||}{Images $ 60\times80$} \\
 \cline{2-7}
 \multicolumn{1}{c||}{} & \multicolumn{2}{c||}{Memory $ 90\times180$} & \multicolumn{2}{c||}{Memory $ 90\times180$} & \multicolumn{2}{c||}{Memory $ 180\times360$} \\
 \cline{2-7}
 \multicolumn{1}{c||}{} & 1-frame & 16-frame & 1-frame & 16-frame & 1-frame & 16-frame \\
 \hline
 \hline
 Mono & \textbf{54.5}(19.0) & 55.1(13.6) & \textbf{54.9}(19.3) & 55.6(13.9) & \textbf{54.6}(19.2) & 55.3(13.7) \\ 
 \hline
 ESMN-RGB & 51.4(19.6) & 54.1(13.8) & 51.7(19.3) & 54.4(13.2) & \textbf{54.0}(19.2) & 57.1(13.5) \\
 \hline
 ESMN & \textbf{55.0}(19.0) & \textbf{59.4}(12.7) & \textbf{55.3}(19.1) & \textbf{59.8}(13.0) & \textbf{55.2}(19.4) & \textbf{59.7}(12.9) \\ 
 \hline
\end{tabular}
\caption{Object segmentation segmentation accuracies on the ScanNet test set for a monocular fusion baseline (Mono), as well as ESMN-RGB and ESMN.}
\label{table:obj_seg}
\end{table}