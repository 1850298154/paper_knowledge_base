\section{Experiments}\label{sec:5}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
  \centering
  \includegraphics[width=0.85\linewidth]{figures_update/_legend.pdf}
   \subcaptionbox{Episode Return(AntCircle)\label{fig:circle-epret}}
    {\includegraphics[width=0.24\linewidth,trim=10 15 0 0,clip]{figures_update/antcircle_averageepret_nolegend.pdf}}
      \subcaptionbox{Episode Cost(AntCircle)\label{fig:circle-epcost}}
    {\includegraphics[width=0.24\linewidth,trim=10 15 0 0,clip]{figures_update/antcircle_averageepcost_nolegend.pdf}}
    \subcaptionbox{Episode Return(PointGather)\label{fig:gather-epret}}
    {\includegraphics[width=0.24\linewidth,trim=10 15 0 0,clip]{figures_update/pointgather_averageepret_nolegend.pdf}}
      \subcaptionbox{Episode Cost(PointGather)\label{fig:gather-epcost}}
    {\includegraphics[width=0.24\linewidth,trim=10 15 0 0,clip]{figures_update/pointgather_averageepcost_nolegend.pdf}}

  \caption{Average episode return and cost in the single-constraint scenario. The x-axis is the number of interactions with the emulator. The y-axis is the average reward/cost-return. The solid line is the mean and the shaded area is the standard deviation. Each tested algorithm runs over five different seeds. The dashed line in the cost plot is the constraint threshold which is 50 for Circle and 0.5 for Gather.}
  \label{fig:single_result}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section, we empirically demonstrate the efficacy of the P3O algorithm from the following four perspectives:
\begin{itemize}
    \item P3O outperforms the state-of-the-art algorithms, e.g. CPO~\cite{achiam2017constrained}, PPO-Lagrangian~\cite{ray2019benchmarking} and FOCOPS~\cite{zhang2020first} in safe RL benchmarks with a single constraint.
    
    
    
    \item P3O is robust to more stochastic and complex environments~\cite{ray2019benchmarking} where previous methods fail to simultaneously improve the return effectively and satisfy the constraint strictly.
    
    \item P3O introduces only one additional hyper-parameter (i.e., the penalty factor $\kappa$), which is easy to tune and insensitive to different settings and constraint thresholds.
    
    \item P3O can be extended to multi-constraint and multi-agent scenarios that are barely tested in the previous work.
\end{itemize}

\begin{table*}
\centering
\small
\begin{tabular}{cccccc}
\hline
\!\!\!Task \!\!\!&  & P3O (Ours) &\!\!\! CPO~\cite{achiam2017constrained} & \!\!\! PPO-L~\cite{ray2019benchmarking} & \!\!\! FOCOPS~\cite{zhang2020first}\!\!\\
\hline
\multirow{2}*{\!\!\!AntCircle} & Reward & $253.92\pm 11.67$& $214.45\pm 26.32$& $189.82\pm52.12$ & $232.08\pm 19.87$\\
~ &\!\!\! Cost($\le50$) & $46.18\pm7.49$ & $57.66\pm9.42$ &$27.71\pm7.33$ & $34.50\pm11.21$\\
\hline
\multirow{2}*{\!\!\!PointGather} & Reward & $4.57\pm0.54$& $3.11\pm0.29$ &$3.49\pm0.33$ & $3.14\pm0.79$ \\
~ &\!\!\! Cost($\le 0.5$) & $0.51\pm0.10$& $0.73\pm0.17$ & $0.48\pm0.09$& $0.51\pm0.14$\\
\hline
\end{tabular}
\caption{Mean performance with normal 95\% confidence interval on single-constraint tasks.}
\label{tab:perf}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
      \centering
      ~~~~~\includegraphics[width=0.95\linewidth]{figures_update/sa_legend.pdf}
      \subcaptionbox{Episode Return\label{fig:sa-epret}}
        {\includegraphics[width=0.45\linewidth,trim=10 15 0 0,clip]{figures_update/sa_averageepret_nolegend.pdf}}
      \subcaptionbox{Episode Return\label{fig:sa-epcost}}
        {\includegraphics[width=0.45\linewidth,trim=10 15 0 0,clip]{figures_update/sa_averageepcost_nolegend.pdf}}
      \caption{Performance of P3O for different $\kappa$ settings on AntCircle.}
      \label{fig:sa_result}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\paragraph{Experiment Details.}
We design and conduct experiments in 2 single-constraint (\textit{Circle} and \textit{Gather}), 1 multi-constraint (\textit{Navigation}) and 1 multi-agent (\textit{Simple Spread}) safe RL environments respectively, as illustrated in Figure \ref{fig:exp_benchmarks}.

To be fair in comparison, the proposed P3O algorithm and FOCOPS~\cite{zhang2020first} are implemented with same rules and tricks on the code-base of \citeauthor{ray2019benchmarking}\shortcite{ray2019benchmarking}\footnote{https://github.com/openai/safety-starter-agents} for benchmarking safe RL algorithms. Also, we take standard PPO as the reference which ignores any constraint and serves as an upper bound baseline on the reward performance. 

More information about experiment environments and detailed parameters are provided in the supplementary material. 




\paragraph{Single-Constraint Scenario.} 

% Learning curves of average episode return and cost for single-constraint scenarios are shown in Figure \ref{fig:single_result}. 
The numerical results of different algorithms are listed in Table \ref{tab:perf}.
The proposed P3O algorithm has +15\%, +24\%, +9\% higher reward improvement over CPO, PPO-Lagrangian, FOCOPS on the AntCircle task, and +34\%, +23\%, +31\% on the PointGather task respectively. Meanwhile, P3O converges to the upper limit of the safety constraint more tightly which enlarges the parametric search space, as well as satisfies the hard constraint.

As illustrated in Figure \ref{fig:single_result}, P3O is the best for improving the policy while satisfying the given constraint. Conversely, CPO has more constraint violations and even fails to satisfy the constraint after convergence possibly due to Taylor's approximation. This is especially prominent in complex environments like SafetyGym~\cite{ray2019benchmarking}  shown in Figure~\ref{fig:safetygym_result}. The non-negligible approximate error prevents the CPO agent from fully ensuring constraints on virtually over these environments whereas it enjoys a meaningless high return. 
As for PPO-Lagrangian, it is too conservative for the trade-off between reward improvement and constraint satisfaction. Besides, it fluctuates mostly on the learning curves which means it is less steady on the learning process. FOCOPS is better-performed than the two baselines above. However, the hyper-parameter $\lambda$ takes a great effect on the final performance. At last, both PPO-Lagrangian and FOCOPS are more or less sensitive to the initial value as well as the learning rate of the Lagrange multiplier. Improper hyper-parameters would cause catastrophic performance on reward and constraint value.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
      \centering
      \includegraphics[width=0.8\linewidth]{figures_update/costlimit_legend.pdf}
      \subcaptionbox{Episode Return\label{fig:sac-epret}}
        {\includegraphics[width=0.45\linewidth,trim=10 15 0 0,clip]{figures_update/costlimit_averageepret_nolegend.pdf}}
      \subcaptionbox{Episode Return\label{fig:sac-epcost}}
        {\includegraphics[width=0.45\linewidth,trim=10 15 0 0,clip]{figures_update/costlimit_averageepcost_nolegend.pdf}}
      \caption{Performance of P3O for different cost limit $d$ on AntCircle.}
      \label{fig:sac_result}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}
  \centering
  \includegraphics[width=0.76\linewidth]{figures_update/multi_legend.pdf}
  \subcaptionbox{Episode Return\label{fig:multi-epret}}
    {\includegraphics[width=0.24\linewidth,trim=10 15 0 0,clip]{figures_update/multi_averageepret_nolegend.pdf}}\hspace{5mm}
  \subcaptionbox{Episode Cost for hazards\label{fig:multi-epcost1}}
    {\includegraphics[width=0.24\linewidth,trim=10 15 0 0,clip]{figures_update/multi_averageepcost1_nolegend.pdf}}\hspace{5mm}
  \subcaptionbox{Episode Cost for pillars\label{fig:multi-epcost2}}
    {\includegraphics[width=0.24\linewidth,trim=10 15 0 0,clip]{figures_update/multi_averageepcost2_nolegend.pdf}}
  \caption{Average episode return(left), cost1(center, for hazards) and cost2(right, for pillars) in the multiple-constraint scenario. The dashed line in the cost plot is the constraint threshold which is 25 for cost1 and  20 for cost2. Hazard/Pillar constrained means only taking cost1/cost2 into P3O loss function whereas ignoring the other one. }
  \label{fig:multi_result}
\end{figure*}
\paragraph{Sensitivity Analysis.}
The penalty factor $\kappa$ plays a critical role in P3O, 
 which is supposed to vary across different tasks depending on the specific value of $A_R$ and each $A_{c_i}$. As shown in Algorithm ~\ref{algo:exact}, we can increase $\kappa$ at every time steps. Otherwise we can map the advantage estimation to an approximate standard normal distribution by applying advantage normalizing tricks and use a fixed $\kappa$ for general good results. The experimental results are stable in a wide range of $\kappa$, as illustrated in Figure~\ref{fig:sa_result}.

We further verify that P3O works effectively for different threshold levels by changing the cost limit $d$. Figure~\ref{fig:sac_result} shows P3O can effectively learn constraint-satisfying policies for all the cases. Notably, P3O precisely converges to the cost upper limit. By contrast, the baseline algorithms are too conservative for policy updates and have more overshoot in the process of cost violation reduction.

\begin{figure}
      \centering
      ~~~~~~\includegraphics[width=0.9\linewidth]{figures_update/mpe_legend.pdf}
      \subcaptionbox{Episode Return\label{fig:mpe-epret}}
        {\includegraphics[width=0.45\linewidth,trim=10 15 0 0,clip]{figures_update/mpe_averageepret_nolegend.pdf}}
      \subcaptionbox{Episode Return\label{fig:mpe-epcost}}
        {\includegraphics[width=0.45\linewidth,trim=10 15 0 0,clip]{figures_update/mpe_averageepcost_nolegend.pdf}}
      \caption{Average episode return and cost in the multi-agent scenario. The algorithm is decentralized with notation (dec).}
      \label{fig:mpe_result}
\end{figure}


\paragraph{Multi-Constraint Scenario.}

It is convenient to extend P3O to multi-constraint scenarios by simply adding exact penalty functions with the minor modification on the loss function. 

Learning curves of average episode return and two independent costs for the navigation task are shown in Figure \ref{fig:multi_result}. The unconstrained algorithm is the original PPO. The hazard (pillar) constrained algorithm only takes the cost of hazard (pillar) into P3O loss function whereas ignoring the other one. It can be seen that those single-constrained algorithms maintain the specified constraint under its upper limit but fail to control the other one. When both constraints are incorporated into our algorithm, each one is well satisfied at the convergence. The episode return is lower than unconstrained and single-constrained settings, which is reasonable because the safety constraints are more strict.

It is remarkable that at the beginning of training, the agent violates the hazard constraint but barely hits pillars because there are more small hazards around the starting point and the pillars are nearly unreachable by random moving due to the initialization mechanism. However, with the increasing velocity, the agent is less likely to satisfy the pillar constraint. Satisfaction on the two types of constraints shows that P3O admits both feasible and infeasible initial policies and finds a constraint-satisfying solution in the end.

\paragraph{Multi-Agent Scenario.}
We also study Multi-Agent Penalized Policy Optimization    (MAP3O) in the collaborative task and take unconstrained Multi-Agent PPO (MAPPO)~\cite{chao2021surprising} as the reference. To tackle the issue of partial observability, recurrent networks are used in both algorithms.


The given task is a decentralized partially observable Markov decision process (DEC-POMDP) in which each agent has an exclusive observation and a shared global reward/cost. We replace the MAPPO loss function with Eq.~(\ref{cppo5}) and conduct a fully decentralized training and
execution. The experimental results show that MAP3O can improve the total reward while ensuring the mutual collision constraint. Inspired by \citeauthor{chao2021surprising}\shortcite{chao2021surprising}, we enhance MAP3O by learning a centralized critic for both global reward and cost that takes in the concatenation of each agent's local observation. The centralized version of MAP3O has better reward performance and satisfies the constraint more strictly. The learning curves of average episode return and cost are shown in Figure \ref{fig:mpe_result}.


\begin{figure}
      \centering
      \includegraphics[width=0.8\linewidth]{figures_update/safetygym_legend.pdf}
      %\vspace{-0.12cm}
      \subcaptionbox{Episode Return\label{fig:sg-epret}}
        {\includegraphics[width=0.45\linewidth,trim=10 15 0 0,clip]{figures_update/safetygym_averageepret_nolegend.pdf}}
      \subcaptionbox{Episode Return\label{fig:sg-epcost}}
        {\includegraphics[width=0.45\linewidth,trim=10 15 0 0,clip]{figures_update/safetygym_averageepcost_nolegend.pdf}}
      \caption{Average performance for P3O, CPO and PPO-Lagrangian on Safetygym PointGoal.}
      \label{fig:safetygym_result}
\end{figure}
