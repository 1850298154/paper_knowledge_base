\section{Introduction}

Reinforcement Learning (RL) has achieved significant successes in playing video games~\cite{vinyals2019grandmaster}, robotic manipulation~\cite{levine2016end}, mastering Go~\cite{silver2017mastering}, etc. 
However, standard RL merely maximizes cumulative rewards, which may lead to undesirable behaviors in real-world applications especially for safety-critical tasks.

%, for example, service robots would not avoid harm to humans deliberately when completing their tasks.

It is intractable to learn reasonable policies by directly penalizing unsafe interactions onto the reward function, since varied intensities of the punishment result in different Markov decision processes. There is still a lack of theories revealing the explicit relationship between policy improvement and safety satisfaction via reward shaping.


Constrained Markov decision process (CMDP)~\cite{altman1999constrained} is a more practical and popular formulation that requires the agent to perform actions under given constraints. A common approach to solving such constrained sequential optimization is approximating the non-convex constraint function to a quadratic optimization problem via Taylor's formulation~\cite{achiam2017constrained,yang2020projection}.
However, those algorithms still have the following drawbacks: (1) The convex approximation to a non-convex policy optimization results in non-negligible approximate errors and only learns near constraint-satisfying policies;
 (2) The closed-form solution to the primal problem involves the inversion of Hessian matrix, which is computationally expensive in large CMDPs with deep neural networks; (3) If there is more than one constraint, it is cumbersome to obtain the analytical solution without an extra inner-loop optimization, which limits their applications in multi-constraint and multi-agent scenarios; (4) If the primal problem is infeasible under certain initial policies, above algorithms require additional interactions for feasible recovery, which reduces their sample-efficiency.

To address above issues, we propose \textbf{P}enalized \textbf{P}roximal \textbf{P}olicy \textbf{O}ptimization (P3O) algorithm that contains two key techniques. Firstly, we employ exact penalty functions to derive an  equivalent unconstrained optimization problem that is naturally compatible with multiple constraints and arbitrary initial policies. Secondly, we extend the clipped surrogate objective in Proximal Policy Optimization (PPO)~\cite{schulman2017proximal} to CMDPs, which eliminates the trust-region constraint on both reward term and cost term. As a consequence, our method removes the need for quadratic approximation and Hessian matrix inversion. By minimizing the unconstrained objective instead, the solution to the primal constrained problem can be obtained.


Conclusively, the proposed P3O algorithm has three main strengths:
(1) \textit{accuracy}: P3O algorithm is accurate and sample-efficient with first-order optimization over a theoretically equivalent unconstrained objective instead of solving an approximate quadratic optimization;
(2) \textit{feasibility}: P3O algorithm could admit arbitrary initialization with a consistent objective function and doesn't need additional recovery methods for infeasible policies;
(3) \textit{scalability}: P3O is naturally scalable to multiple constraints or agents without increasing complexity due to the exact penalty reformulation.

In the end, we summarize our contributions as four-fold:
\begin{itemize}
\item  We propose the Penalized Proximal Policy Optimization (P3O) algorithm for safe RL, which employs first-order optimization over an equivalent  unconstrained objective.
\item  We theoretically prove the exactness of the penalized method when the penalty factor is sufficiently large (doesn't have to go towards positive infinity) and propose an adaptive penalty factor tuning algorithm.
\item We extend clipped surrogate objectives and advantage normalization tricks to CMDPs, which are easy to implement and enable a fixed penalty factor for general good results across different tasks.
\item We conduct extensive experiments to show that P3O outperforms several state-of-the-art algorithms with respect to both reward improvement and constraint satisfaction and demonstrate its efficacy in more difficult multi-constraint and multi-agent scenarios which are less studied in previous safe RL algorithms.
\end{itemize}



