\section{Related Work}

Safety in reinforcement learning is a challenging topic formally raised by \citeauthor{garcia2015comprehensive} \shortcite{garcia2015comprehensive}. Readers can  refer to the survey~\cite{liu2021policy} for recent advances in safe RL. In this section, we only summarize the most related studies to our algorithm. 

\paragraph{Primal-Dual solution.}

Considering the success of Lagrangian relaxation in solving constrained optimization problems, Primal-Dual Optimization (PDO)~\cite{chow2017risk} and its variants~\cite{tessler2018reward,stooke2020responsive} leverage the Lagrangian Duality in constrained reinforcement learning. Although plenty of work has provided rigorous analysis on duality gap~\cite{paternain2019constrained}, non-asymptotic convergence rate~\cite{ding2020natural} and regret bound~\cite{ding2021provably}, Primal-dual methods are still hard to be implemented and applied in practical use due to their sensitivity to the initialization as well as the learning rate of Lagrangian multipliers.

\paragraph{Primal solution.}
Constrained Policy Optimization (CPO)~\cite{achiam2017constrained} directly searches the feasible policy in the trust region and guarantees a monotonic performance improvement while ensuring constraint satisfaction by solving a quadratic optimization problem with appropriate approximations. Projection-based CPO~\cite{yang2020projection} updates the policy in two stages by firstly performing regular TRPO~\cite{schulman2015trust} and secondly projecting the policy back into the constraint set. Those methods based on local policy search mainly suffer from approximate errors, the inversion of high-dimensional Hessian matrices, and the poor scalability to multiple constraints. To address drawbacks of the quadratic approximation, FOCOPS~\cite{zhang2020first} solves the optimum of constrained policy optimization within the non-parametric space and then derives the first-order gradients of the $\ell_2$ loss function within the parameter space. Nevertheless, FOCOPS has more auxiliary variables to learn than our first-order optimization objective, and the analytical solutions of FOCOPS are not straightforward with the increasing number of constraints.

The most similar work to our proposed algorithm is Interior-point Policy Optimization (IPO)~\cite{liu2020ipo} which uses log-barrier functions as penalty terms to restrict policies into the feasible set. However, the interior-point method requires a feasible policy upon initialization which is not necessarily fulfilled and needs a further recovery. Moreover, the log-barrier function possibly leads to numerical issues when the penalty term goes towards infinity, or the solution is not exactly the same as the primal problem. By contrast, we employ the exact penalty function to derive an equivalent unconstrained objective and restrict policy updates in the trust region by clipping the important sampling ratio on both reward and cost terms for the approximate error reduction, which is different from the prior work.





