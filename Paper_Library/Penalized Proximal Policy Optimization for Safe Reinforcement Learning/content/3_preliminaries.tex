\section{Preliminaries}
A Markov decision process (MDP)~\cite{sutton2018reinforcement} is a tuple $(S,A,R,P,\mu)$ ,where $S$ is the state space, $A$ is the action space, $R:  S \times  A \times  S  \mapsto \mathbb{R}$ is the reward function, $P: S \times  A \times  S \mapsto [0,1]$ is the transition  probability  function to describe the dynamics of the environment, and $\mu : S  \mapsto [0,1] $ is the initial state distribution. A stationary policy $\pi : S \mapsto \cal{P}(A)$ maps the given states to probability distributions over action space and  $\Pi$ denotes the set of all stationary policies $\pi$.
The optimal policy $\pi^*$ maximizes the expected discounted return
$J_R(\pi) = \mathop{\mathbb{E}}_{\tau\sim \pi}\big [ \sum^\infty_{t=0}\gamma^t R(s_t,a_t,s_{t+1})\big ]$ where $\tau=\{(s_t,a_t)\}_{t\ge0}$ is a sample trajectory and $\tau\sim\pi$ indicates the distribution over trajectories depending on $s_0 \sim \mu, a_t \sim \pi(\cdot | s_t), s_{t+1} \sim P(\cdot | s_t,a_t)$.
The value function is defined as $V_R^\pi (s) = \mathop{\mathbb{E}}_{\tau\sim \pi}\big [ \sum^\infty_{t=0}\gamma^t R(s_t,a_t,s_{t+1}) | s_0 = s\big ]$, the action-value function is defined as $Q_R^\pi (s,a) = \mathop{\mathbb{E}}_{\tau\sim \pi}\big [ \sum^\infty_{t=0}\gamma^t R(s_t,a_t,s_{t+1}) | s_0 = s,a_0 =a \big]$ and the advantage function is defined as $A_R^\pi(s,a) = Q_R^\pi(s,a) - V_R^\pi(s)$.


A constrained Markov decision process (CMDP)~\cite{altman1999constrained} extends MDP with a constraint set $C$ consisting of cost functions $C_i : S \times A\times S\mapsto \mathbb{R},\ i=1,2,...,m$. We define the expected discounted cost-return $J_{C_i}(\pi) = \mathop{\mathbb{E}}_{\tau\sim \pi}\big [ \Sigma^\infty_{t=0}\gamma^t C_i(s_t,a_t,s_{t+1})\big ]$ and $V_{C_i}^\pi, Q_{C_i}^\pi,A_{C_i}^\pi$ similarly in MDPs. The set of feasible stationary policies for a CMDP is denoted as $\Pi_C = \{ \pi \in \Pi \ | J_{C_i}(\pi) \leq d_i,\ \forall i\}$, where $d_i$ is the upper limit of the corresponding safety or cost constraint. The goal of safe RL is to find an optimal policy over the hard safe constraint, i.e.,  $\pi^* = \mathop{\arg\max}_{\pi \in \Pi_C} J_R(\pi)$.