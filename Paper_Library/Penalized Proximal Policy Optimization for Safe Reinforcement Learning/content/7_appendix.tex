\setcounter{section}{0}
\setcounter{subsection}{0}
\section*{Supplementary Material}
To make this work self-contained and easy to read, the supplementary material is organized as follows:  In the first 3 sections, we provide the detailed proof of proposition and theorems claimed in the main paper. In Section 4, we propose the Multi-agent P3O algorithm in detail. In Section 5, we give the implementation details of experiment environments, agents, and algorithms.

\section{Proof of Proposition 1.}
The following lemma adapts Performance Difference Lemma~\cite{kakade2002approximately} to more general settings.

\begin{lemma}
For any function $f:  S \times  A \times  S  \mapsto \mathbb{R}$ and any policies $\pi$ and $\pi'$, define
$J_f(\pi) = \mathop{\mathbb{E}}_{\tau\sim \pi}\big [ \Sigma^\infty_{t=0}\gamma^tf(s_t,a_t,s_{t+1})\big ]$ and $d^\pi(s)  =  (1-\gamma) \sum^\infty_{t=0} \gamma^t P(s_t=s | \pi)$. Then the following equation holds:
\[
J_f(\pi')-J_f(\pi)=\frac{1}{1-\gamma}\mathop{\mathbb{E}}_{\substack{s\sim d^{\pi'}\\  a\sim \pi'}} [ A_f^{\pi} (s,a)]
\]
\label{performance_lemma}
\end{lemma}
\begin{proof}
First, $J_f(\pi')$ can be reformulated as:
\begin{equation*}
\resizebox{.97\linewidth}{!}{$
\begin{aligned}
& J_f(\pi') = \mathop{\mathbb{E}}_{\tau\sim \pi'}\big [ \sum^\infty_{t=0}\gamma^tf(s_t,a_t,s_{t+1})\big ]\\ &=\mathop{\mathbb{E}}_{\tau\sim \pi'}\big [ \sum^\infty_{t=0}\gamma^tf(s_t,a_t,s_{t+1}) -\sum^\infty_{t=0}\gamma^tV_f^\pi(s_t)  +\sum^\infty_{t=0}\gamma^tV_f^\pi(s_t) \big ]\\ &=\mathop{\mathbb{E}}_{\tau\sim \pi'}\big [ \sum^\infty_{t=0}\gamma^t\big (f(s_t,a_t,s_{t+1}) -V_f^\pi(s_t) \\
& \qquad\qquad\qquad\qquad\qquad\qquad+\gamma V_f^\pi(s_{t+1}) \big )\big ]+\mathop{\mathbb{E}}_{\tau\sim \pi'}\big [V_f^\pi(s_0)\big ]\\ & =\mathop{\mathbb{E}}_{\tau\sim \pi'}\big [ \sum^\infty_{t=0}\gamma^t A_f^\pi(s_t,a_t)\big ]+\mathop{\mathbb{E}}_{s_0\sim \mu}\big [V_f^\pi(s_0)\big ]\\ & = \mathop{\mathbb{E}}_{\tau\sim \pi'}\big [ \sum^\infty_{t=0}\gamma^t A_f^\pi(s_t,a_t)\big ]+ J_f(\pi)
\end{aligned}
$}
\end{equation*}
Next, replace $\tau\sim\pi'$ with $s\sim d^{\pi'}$ and $a\sim\pi'$:
\begin{equation*}
\resizebox{.97\linewidth}{!}{$
\begin{aligned}
& \quad \mathop{\mathbb{E}}_{\tau\sim \pi'}\big [ \sum^\infty_{t=0}\gamma^t A_f^\pi(s_t,a_t)\big ]\\ &=\mathbb{E}\big [ \sum^\infty_{t=0} \sum_s P(s_t=s)\sum_a \pi(a_t=a|s_t=s)\gamma^t A_f^\pi(s_t,a_t)\big ] \\ &=\mathbb{E}\big [ \big ( \sum_s \sum^\infty_{t=0}\gamma^tP(s_t=s)\big)\sum_a \pi(a_t=a|s_t=s)A_f^\pi(s_t,a_t)\big ] \\ &=\frac{1}{1-\gamma}\mathop{\mathbb{E}}_{\substack{s\sim d^{\pi'} \\a\sim \pi'}}\big [  A_f^\pi(s_t,a_t)\big ]
\end{aligned}
$}
\end{equation*}
Thus, the following equation holds: \[J_f(\pi')-J_f(\pi)=\frac{1}{1-\gamma}\mathop{\mathbb{E}}_{\substack{s\sim d^{\pi'}\\  a\sim \pi'}} [ A_f^{\pi} (s,a)].\] 
The proof of Lemma~\ref{performance_lemma} is completed. 
\end{proof}


For a better readability, we rewrite Proposition 1 as below.
\begin{aproposition}
The new policy $\pi_{k+1}$  obtained from the current policy $\pi_k$ via the following problem:
\begin{equation}
%\resizebox{.91\linewidth}{!}{$
\begin{aligned}
&\pi_{k+1}  =  \mathop{\arg\max}_{\pi}\mathop{\mathbb{E}}_{\substack{s\sim d^{\pi} \\a\sim \pi}}[ A^{\pi_k}_R (s,a)]\\
& \mathrm{s.t.}  \quad J_{C_i}(\pi_{k})+ \frac{1}{1-\gamma}\mathop{\mathbb{E}}_{\substack{s\sim d^{\pi} \\a\sim \pi}} \big  [A_{C_i}^{\pi_ k} (s,a) \big ] \leq d_i,\ \forall i.\\
\end{aligned}
%$}
\label{appendixP1}
\end{equation}
yields a monotonic return improvement and hard constraint satisfaction.


\label{th:appendix1}
\end{aproposition}
\begin{proof}[Proof of Proposition 1]
The goal is to obtain new policy $\pi_{k+1}$ which is the solution of the following problem:
\begin{equation}
%\resizebox{.91\linewidth}{!}{$
\begin{aligned}
&\pi_{k+1}  =  \mathop{\arg\max}_{\pi} J_R(\pi)\\
& \mathrm{s.t.}  \quad J_{C_i}(\pi) \leq d_i,\ \forall i.\\
\end{aligned}
%$}
\end{equation}
Replace $f$ with reward function $R$ and each cost function $C_i$ respectively in Lemma~\ref{performance_lemma}, we have:
\begin{equation}
J_R(\pi)-J_R(\pi_k)=\frac{1}{1-\gamma}\mathop{\mathbb{E}}_{\substack{s\sim d^{\pi}\\  a\sim \pi}} [ A_R^{\pi_k} (s,a)]
\end{equation}
\begin{equation}
J_{C_i}(\pi)-J_{C_i}(\pi_k)=\frac{1}{1-\gamma}\mathop{\mathbb{E}}_{\substack{s\sim d^{\pi'}\\  a\sim \pi'}} [ A_{C_i}^{\pi_k} (s,a)], \ \forall i
\end{equation}
Thus, the above problem (\ref{appendixP1}) can be reformulated as:
\begin{equation}
%\resizebox{.91\linewidth}{!}{$
\begin{aligned}
&\pi_{k+1}  =  \mathop{\arg\max}_{\pi}\big( J_R(\pi_k) + \frac{1}{1-\gamma}\mathop{\mathbb{E}}_{\substack{s\sim d^{\pi} \\a\sim \pi}}[ A^{\pi_k}_R (s,a)] \big)\\
& \mathrm{s.t.}  \quad J_{C_i}(\pi_{k})+ \frac{1}{1-\gamma}\mathop{\mathbb{E}}_{\substack{s\sim d^{\pi} \\a\sim \pi}} \big  [A_{C_i}^{\pi_ k} (s,a) \big ] \leq d_i,\ \forall i.\\
\end{aligned}
%$}
\end{equation}
The proof is completed by omitting the constant term in the optimization objective.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proof of Theorem 2.}
We rewrite problem (\ref{cppo2}) as:
\begin{equation}
\resizebox{.385\linewidth}{!}{$
\begin{aligned}
 &\theta_{k+1} = \mathop{\arg\min}_{\theta} \mathcal{L}_R(\theta)\\
& \mathrm{s.t.} \quad \mathcal{L}_{C_i}(\theta) \leq 0, \ \forall i\\
\end{aligned}
$}
 \tag{P}
 \label{P}
\end{equation}
and rewrite problem (\ref{cppo3}) as:
\begin{equation}
\theta_{k+1} = \mathop{\arg\min}_{\theta}  \mathcal{L}_R(\theta) + \kappa\sum_i \max\{0,\mathcal{L}_{C_i}(\theta)\}
\tag{Q}
\label{Q}
\end{equation}
Here we shorthand $\mathcal{L}_R(\theta) =  \mathop{\mathbb{E}}_{\substack{s\sim d^{\pi} \\a\sim \pi_{k}}}[-r(\theta)A^{\pi_k}_R (s,a)]$ and 
$\mathcal{L}_{C_i}=\mathop{\mathbb{E}}_{\substack{s\sim d^{\pi} \\a\sim \pi_{k}}} \big  [r(\theta)A_{C_i}^{\pi_ k} (s,a)\big ] + (1-\gamma)(J_{C_i}(\pi_{k})-d_i), \ \forall i$.
\begin{lemma}
Suppose $\bar{\theta}$ is the optimum of the constrained problem (\ref{P}). Let $\bar\lambda$ be the corresponding Lagrange multiplier vector for its dual problem. Then for $\kappa \geq ||\bar\lambda||_\infty$, $\bar{\theta}$ is also a minimizer of its penalized optimization problem (\ref{Q}).
\label{exact_penalty_function_lemma1}
\end{lemma}
\begin{proof}
Since $\kappa \geq ||\bar\lambda||_\infty$, by taking $[\cdot]^+$ as shorthand for $\max\{0,\cdot\}, $it follows that:
\begin{align}\label{th1:1}
    \mathcal{L}_R(\theta) + \kappa\sum_i \mathcal{L}^+_{C_i}(\theta) 
    &\geq \mathcal{L}_R(\theta) + \sum_i \bar{\lambda_i}\mathcal{L}^+_{C_i}(\theta)\\ 
    &\geq \mathcal{L}_R(\theta) + \sum_i \bar{\lambda_i}\mathcal{L}_{C_i}(\theta)\label{th1:2}
\end{align}

By assumption, $\bar\theta$ is a Karush-Kuhn-Tucker point in the constrained problem (\ref{P}), at which KKT conditions are satisfied with the Lagrange multiplier vector  $\bar{\lambda}$. It gives:
\begin{align}\label{th1:3}
    \mathcal{L}_R(\theta) + \sum_i \bar{\lambda_i}\mathcal{L}_{C_i}(\theta)
    &\geq \mathcal{L}_R(\bar\theta) + \sum_i \bar{\lambda_i}\mathcal{L}_{C_i}(\bar\theta)\\ \label{th1:4}
    &=  \mathcal{L}_R(\bar\theta) + \sum_i \bar{\lambda_i}\mathcal{L}^+_{C_i}(\bar\theta)\\\label{th1:5}
     &= \mathcal{L}_R(\bar\theta) + \kappa \sum_i \mathcal{L}^+_{C_i}(\bar\theta)
\end{align}
(\ref{th1:3}) holds because $\bar\theta$ minimizes the Lagrange function. (\ref{th1:4}) is derived from the complementary slackness.



Then, by (\ref{th1:1})-(\ref{th1:5}), we conclude that $\mathcal{L}(\theta) \geq \mathcal{L}(\bar\theta)$ holds for all $\theta \in \Theta$, which means $\bar\theta$ is a minimizer of  the penalized optimization problem (\ref{Q}). 

The proof of Lemma~\ref{exact_penalty_function_lemma1} is completed.
\end{proof}



\begin{lemma}
Let $\widetilde{\theta}$ be a minimizer of the penalized optimization problem (\ref{Q}). $\bar\theta$ and $\bar\lambda$ are the same as they are defined in Theorem~\ref{exact_penalty_function_lemma1}. Then for $\kappa \geq ||\bar\lambda||_\infty$, $\widetilde{\theta}$ is also optimal in the constrained problem (\ref{P}).
\label{exact_penalty_function_lemma2}
\end{lemma}
\begin{proof}
If $\widetilde\theta$ is in the set of feasible solutions $D=\{\theta | \mathcal{L}_{C_i}(\theta) \leq 0, \ \forall i\}$, it gives:
\begin{align}
\mathcal{L}_R(\widetilde\theta) &= \mathcal{L}_R(\widetilde\theta) + \kappa\sum_i \mathcal{L}^+_{C_i}(\widetilde\theta)\\
&\leq \mathcal{L}_R(\theta) + \kappa\sum_i \mathcal{L}^+_{C_i}(\theta) =\mathcal{L}_R(\theta)
\end{align}
The inequality above indicates  $\widetilde{\theta}$ is also optimal in the constrained problem (\ref{P}). 

If  $\widetilde\theta$ is not feasible, we have:
\begin{align}
 &\mathcal{L}_R(\bar\theta) + \kappa \sum_i \mathcal{L}^+_{C_i}(\bar\theta)
    =  \mathcal{L}_R(\bar\theta) + \sum_i \bar{\lambda_i}\mathcal{L}^+_{C_i}(\bar\theta) \\
    & = \mathcal{L}_R(\bar\theta) + \sum_i \bar{\lambda_i}\mathcal{L}_{C_i}(\bar\theta)
    \leq \mathcal{L}_R(\widetilde\theta) + \sum_i \bar{\lambda_i}\mathcal{L}_{C_i}(\widetilde\theta) \\
   & \leq \mathcal{L}_R(\widetilde\theta) + \sum_i \bar{\lambda_i}\mathcal{L}^+_{C_i}(\widetilde\theta)
   \leq \mathcal{L}_R(\widetilde\theta) + \kappa\sum_i \mathcal{L}^+_{C_i}(\widetilde\theta) 
\end{align}which is a contradiction to the assumption that  $\widetilde{\theta}$ is a minimizer of the penalized optimization problem (\ref{Q}).Thus, $\widetilde\theta$ can only be the feasible optimal solution for problem (\ref{P}).

The proof of Lemma~\ref{exact_penalty_function_lemma2} is completed.
\end{proof}

For a better readability, we rewrite Theorem 2 as below.
\begin{atheorem}
Suppose $\bar\lambda$ is the corresponding Lagrange multiplier vector for the optimum of problem (\ref{P}). Let the penalty factor $\kappa$ be a sufficiently large constant ($\kappa \geq ||\bar\lambda||_\infty$), the two problems (\ref{P}) and  (\ref{Q}) share the same optimal solution set.
\end{atheorem}
\begin{proof}[Proof of Theorem 2]
The proof is completed with Lemma~\ref{exact_penalty_function_lemma1} and Lemma~\ref{exact_penalty_function_lemma2}.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem 3.}
We rewrite problem (\ref{cppo3}) as:
\begin{equation}
\mathcal{L}^\pi(\theta)  = \mathcal{L}^\pi_R(\theta) +\kappa\sum_i \max\{0,\mathcal{L}^\pi_{C_i}(\theta)\}
\tag{M}
\label{M}
\end{equation}
Here we shorthand $\mathcal{L}^\pi_R(\theta) =  \mathop{\mathbb{E}}_{\substack{s\sim d^{\pi} \\a\sim \pi_{k}}}[-r(\theta)A^{\pi_k}_R (s,a)]$ and 
$\mathcal{L}^\pi_{C_i}(\theta)=\mathop{\mathbb{E}}_{\substack{s\sim d^{\pi} \\a\sim \pi_{k}}} \big  [r(\theta)A_{C_i}^{\pi_ k} (s,a)\big ] + (1-\gamma)(J_{C_i}(\pi_{k})-d_i), \ \forall i$.

We rewrite problem (\ref{cppo4}) as:
\begin{equation}
\mathcal{L}^\pi_k(\theta)  = \mathcal{L}^{\pi_k}_R(\theta) +\kappa\sum_i \max\{0,\mathcal{L}^{\pi-K}_{C_i}(\theta)\}
\tag{N}
\label{N}
\end{equation}
Here we shorthand $\mathcal{L}^{\pi_k}_R(\theta) =  \mathop{\mathbb{E}}_{\substack{s\sim d^{\pi_k} \\a\sim \pi_{k}}}[-r(\theta)A^{\pi_k}_R (s,a)]$ and 
$\mathcal{L}^{\pi_k}_{C_i}(\theta)=\mathop{\mathbb{E}}_{\substack{s\sim d^{\pi_k} \\a\sim \pi_{k}}} \big  [r(\theta)A_{C_i}^{\pi_ k} (s,a)\big ] + (1-\gamma)(J_{C_i}(\pi_{k})-d_i), \ \forall i$.


\begin{lemma}For any function $f:  S \times  A \times  S  \mapsto \mathbb{R}$ and any policies $\pi$ and $\pi'$, define
\begin{equation*}
\begin{aligned}
\delta& = \mathop{\mathbb{E}}_{s\sim d^{\pi}}\big[\mathrm{D}_{KL}(\pi' || \pi)[s]\big]\\
\varepsilon^{\pi'}_f &= \mathop{\max}_{s} |\mathop{\mathbb{E}}_{a\sim \pi'} A_f^\pi(s,a)| \\
D^{\pm}_{\pi,f}(\pi') &= \frac{1}{1-\gamma}\mathop{\mathbb{E}}_{\substack{s\sim d^{\pi}\\  a\sim \pi}} \bigg[\frac{\pi'(s,a)}{\pi(s,a)} A^{\pi}_f (s,a) \pm \frac{\gamma\varepsilon^{\pi'}_f\sqrt{2\delta}}{1-\gamma} \bigg]
\end{aligned}
\end{equation*}
The following bounds hold:
\[D^{+}_{\pi,f}(\pi') \geq J_f(\pi') - J_f(\pi) \geq D^{-}_{\pi,f}(\pi')\]
\label{approxiamte_error1}
\end{lemma}
\begin{proof}
See \cite{achiam2017constrained} for details.
\end{proof}


For a better readability, we rewrite Theorem 3 as below.
\begin{atheorem}
For any policies $\pi$, define \[\delta = \mathop{\mathbb{E}}_{s\sim d^{\pi_k}}\big[\mathrm{D}_{KL}(\pi || \pi_k)[s]\big].\]
The worst-case approximate error is $\mathcal{O}(\kappa m\sqrt{\delta})$  if we replace problem (\ref{M}) with problem (\ref{N}).
\label{th:appendix3}
\end{atheorem}
\begin{proof}[Proof of Theorem 3]
According to Lemma~\ref{performance_lemma} and Lemma~\ref{approxiamte_error1}, we have:
\small
\begin{align}
&\quad\ \ |\mathcal{L}^\pi(\theta) - \mathcal{L}^\pi_k(\theta)|\\
&\leq |\mathcal{L}^\pi_R(\theta)- \mathcal{L}^{\pi_k}_R(\theta)|  +\kappa\sum_i |[\mathcal{L}^\pi_{C_i}(\theta)]^+ - [\mathcal{L}^{\pi_k}_{C_i}(\theta)]^+|\\
&\leq |\mathcal{L}^\pi_R(\theta)- \mathcal{L}^{\pi_k}_R(\theta)|  +\kappa\sum_i |\mathcal{L}^\pi_{C_i}(\theta) - \mathcal{L}^{\pi_k}_{C_i}(\theta)|\\
&\leq \frac{\gamma\varepsilon^{\pi'}_R\sqrt{2\delta}}{1-\gamma} +\kappa\sum_i \frac{\gamma\varepsilon^{\pi'}_{C_i}\sqrt{2\delta}}{1-\gamma} \\
&\leq  (1+\kappa m) \frac{\gamma||\varepsilon^{\pi'}||_{\infty}\sqrt{2\delta}}{1-\gamma}.
\end{align}
Then, the proof is completed.
\end{proof}


\section{Multi-Agent P3O}
In this section, we extend P3O to the decentralized multi-agent refinement learning (MARL) scenarios and denote it as MAP3O, inspired by MAPPO~\cite{chao2021surprising}. 


A decentralized partially observed constrained Markov decision process (DEC-POCMDP) is defined by a 7-tuple $(S,\{A_i\},T,\{\Omega_i\},O,R,C)$,
where $S$ is the state space, $A_i$ is the action space for agent i, $A = \times_iA_i$ is the joint action space, $\Omega_i$ is the observation space for agent i, $\Omega = \times_i\Omega_i$ is the joint observation space, $T(s,a,s') = P(s'|s,a)$ and $O(s,a,o) = P(o|s',a)$ are transition probabilities, $R$ is the global reward function and $C$ is a set of global constraint functions.
$J_R(\pi) = \mathop{\mathbb{E}}_{\tau\sim \pi}\big [ \sum^\infty_{t=0}\gamma^t R(s_t,\boldsymbol{a_t},s_{t+1})\big ]$
and $J_{C}(\pi) = \mathop{\mathbb{E}}_{\tau\sim \pi}\big [ \Sigma^\infty_{t=0}\gamma^t C(s_t,\boldsymbol{a_t},s_{t+1})\big ]$.
The goal of Multi-agent safe RL is to optimize the joint policy, i.e., $\pi^* = \mathop{\arg\max}_{\pi \in \Pi_C} J_R(\pi)$ where $\Pi_C = \{ \pi \in \Pi \ | J_{C}(\pi) \leq d\}$.


The detailed iterates scheme of fully decentralized MAP3O for agent $i$ is summarized in Algorithm \ref{MAP30-alg1}.

\begin{algorithm}[H]
\caption{MAP3O (dec): Decentralized Multi-Agent Penalized Proximal Policy Optimization} 
\label{MAP30-alg1}
\small
\textbf{Input}: initial policy $\pi(\theta^i_0)$, value function $V_R(\phi^i_0)$ and cost-value function $V_C(\psi^i_0)$.
\begin{algorithmic}[1] %[1] enables line numbers
\FOR{$ k\mathrm{\  in\ } 0,1,2,...K$ }
            \STATE Empty data buffer $\mathcal{D}_k$.
            \WHILE{$\mathcal{D}_k$ is not full}
            \STATE Reset a new trajectory $\tau$.
            \STATE Initialize actor RNN state $h^i_{0,\pi}$.
            \STATE Initialize critic RNN states $h^i_{0,V_R}$ and $h^i_{0,V_C}$.
            \FOR{$ t\mathrm{\  in\ } 1,2,...T$ }
            \STATE $p^i_t, h^i_{t,\pi} = \pi(o^i_t,h^i_{t-1,\pi}; \theta^i_{t-1})$.
            \STATE Sample $a^i_t$ over $p^i_t$.
            \STATE $v^i_{t,R},h^i_{t,V_R} = V_R(o^i_t,h^i_{t-1,V_R}; \phi^i_{t-1})$.
            \STATE $v^i_{t,C},h^i_{t,V_C} = V_C(o^i_t,h^i_{t-1,V_C}; \psi^i_{t-1})$.
            
            \STATE Execute $a^i_t$ and record $r_t, c_t, o^i_{t+1}$.
            \STATE $\tau += [o^i_t,a^i_t,h^i_{t,\pi},v^i_{t,R},h^i_{t,V_R},v^i_{t,C},h^i_{t,V_C},r_t,c_t,o^i_{t+1}]$.
            \ENDFOR
        	\STATE Compute every $\hat{R}_t = \sum^{T-t}_{k=0} \gamma^k r_{t+k}$ and $\hat{A}^i_R(o^i_t,a^i_t)$.
        	\STATE Compute every  $\hat{C}_t = \sum^{T-t}_{k=0} \gamma^k c_{t+k}$ and $\hat{A}^i_C(o^i_t,a^i_t)$.
        	\STATE Add data into $\mathcal{D}_k$.
        	\ENDWHILE
        	\FOR{$ n \mathrm{\  in\ } 0,1,2,...N$ }
        	\STATE Update the RNN hidden states in data buffer $\mathcal{D}_k$.
            \STATE Compute $\mathcal{L}^{\mathrm{CLIP}}_{R}(\theta^i_k)$ in Eq.~(\ref{L_R_CLIP}).
            \STATE Compute $\mathcal{L}^{\mathrm{CLIP}}_{C}(\theta^i_k)$ in Eq.~(\ref{L_C_CLIP}).
            \STATE $\theta^i_k \leftarrow \theta^i_k  - \eta \nabla  \mathcal{L}^{\mathrm{P3O}}(\theta^i_k)$ in Eq.~(\ref{cppo5}).
            \ENDFOR
            \STATE $\theta^i_{k+1} \leftarrow \theta^i_{k}$.
            \STATE Update $\phi^i_{k+1}$ and $\psi^i_{k+1}$ with value regression objective.
          \ENDFOR
\end{algorithmic}
\textbf{Output}: Optimal policy $\pi(\theta^i_K)$.
\end{algorithm}

Centralized training and decentralized execution (CTDE) is a popular framework to learn a single policy and avoid the joint action as inputs at the same time. The key technique is to learn a centralized critic that takes in the concatenation of each agentâ€™s local observation, as shown in Algorithm \ref{MAP30-alg2}.
\begin{algorithm}[H]
\caption{MAP3O: Multi-Agent Penalized Proximal Policy Optimization} 
\label{MAP30-alg2}
\small
\textbf{Input}: initial policy $\pi(\theta_0)$, value function $V_R(\phi_0)$ and cost-value function $V_C(\psi_0)$.
\begin{algorithmic}[1] %[1] enables line numbers
\FOR{$ k\mathrm{\  in\ } 0,1,2,...K$ }
            \STATE Empty data buffer $\mathcal{D}_k$.
            \WHILE{$\mathcal{D}_k$ is not full}
            \STATE Reset a new trajectory $\tau$.
            \STATE Initialize each actor RNN state $h^i_{0,\pi}$.
            \STATE Initialize each critic RNN states $h^i_{0,V_R}$ and $h^i_{0,V_C}$.
            \FOR{$ t\mathrm{\  in\ } 1,2,...T$ }
            \FOR{each agent $i$}
            \STATE $p^i_t, h^i_{t,\pi} = \pi(o^i_t,h^i_{t-1,\pi}; \theta_{t-1})$.
            \STATE Sample $a^i_t$ over $p^i_t$.
            \STATE $v^i_{t,R},h^i_{t,V_R} = V_R(\boldsymbol{o}_t,h^i_{t-1,V_R}; \phi_{t-1})$.
            \STATE $v^i_{t,C},h^i_{t,V_C} = V_C(\boldsymbol{o}_t,h^i_{t-1,V_C}; \psi_{t-1})$.
            \ENDFOR
            
            \STATE Execute $\boldsymbol{a}_t$ and record $r_t, c_t, \boldsymbol{o}_{t+1}$.
            \STATE $\tau += [\boldsymbol{o}_t,\boldsymbol{a}_t,\boldsymbol{h}_{t,\pi},\boldsymbol{v}_{t,R},\boldsymbol{h}_{t,V_R},\boldsymbol{v}_{t,C},\boldsymbol{h}_{t,V_C},r_t,c_t,\boldsymbol{o}_{t+1}]$.
            \ENDFOR
        	\STATE Compute every $\hat{R}_t = \sum^{T-t}_{k=0} \gamma^k r_{t+k}$ and $\hat{A}_R(\boldsymbol{o}_t,\boldsymbol{a}_t)$.
        	\STATE Compute every  $\hat{C}_t = \sum^{T-t}_{k=0} \gamma^k c_{t+k}$ and $\hat{A}_C(\boldsymbol{o}_t,\boldsymbol{a}_t)$.
        	\STATE Add data into $\mathcal{D}_k$.
        	\ENDWHILE
        	\FOR{$ n \mathrm{\  in\ } 0,1,2,...N$ }
        	\STATE Update the RNN hidden states in data buffer $\mathcal{D}_k$.
            \STATE Compute $\mathcal{L}^{\mathrm{CLIP}}_{R}(\theta_k)$ in Eq.~(\ref{L_R_CLIP}).
            \STATE Compute $\mathcal{L}^{\mathrm{CLIP}}_{C}(\theta_k)$ in Eq.~(\ref{L_C_CLIP}).
            \STATE $\theta_k \leftarrow \theta_k  - \eta \nabla  \mathcal{L}^{\mathrm{P3O}}(\theta_k)$ in Eq.~(\ref{cppo5}).
            \ENDFOR
            \STATE $\theta_{k+1} \leftarrow \theta_{k}$.
            \STATE Update $\phi_{k+1}$ and $\psi_{k+1}$ with value regression objective.
          \ENDFOR
\end{algorithmic}
\textbf{Output}: Optimal policy $\pi(\theta_K)$.
\end{algorithm}

\section{Implementation Details.}


\begin{table*}
\centering
\small
\caption{Algorithm-specific parameters.}
\setlength{\tabcolsep}{6mm}{
\begin{tabular}{lccccc}
\hline
Parameter & P3O & CPO & PPO-L & FOCOPS & PPO\\
\hline
Number of Hidden layers & 2 & 2 & 2 & 2 & 2 \\
Number of Hidden nodes & 255 & 255& 255& 255& 255\\
Activation Function  & $\tanh$ & $\tanh$ & $\tanh$ & $\tanh$ & $\tanh$ \\
% Rollout Length $T$ & 1e3 & 1e3& 1e3& 1e3& 1e3 \\
% Buffer Size $|\mathcal{D}|$ & 3e4 & 3e4& 3e4& 3e4& 3e4 \\
% Total Interactions & 1e7 & 1e7& 1e7& 1e7& 1e7 \\
Discount Factor $\gamma$ & 0.99 & 0.99 & 0.99 & 0.99& 0.99\\
GAE parameter $\lambda^{\mathrm{GAE}}$ & 0.97 & 0.97& 0.97& 0.97& 0.97\\
Actor Learning Rate $\eta_\pi$    & 3e-4  & N/A & 3e-4& 3e-4& 3e-4  \\
Critic Learning Rate $\eta_{V_R}$  & 1e-3  & 1e-3& 1e-3& 1e-3& 1e-3     \\
Critic Learning Rate $\eta_{V_C}$  & 1e-3  & 1e-3& 1e-3& 1e-3& N/A    \\
Penalty Factor $\kappa$   & 20  & N/A & N/A  & N/A & N/A      \\
Clip Ratio $\epsilon$ & 0.2  & N/A & 0.2  & N/A & 0.2 \\
Trust Region $[\delta^-,\delta^+]$ & [0,1e-2] & [0,1e-2]& [0,1e-2]& [0,1e-2]& [0,1e-2]   \\
Damping Coefficient & N/A & 0.1& N/A& N/A& N/A\\
Backtrack Learning Rate& N/A & 0.8& N/A& N/A& N/A\\
Backtrack Iterations& N/A & 10& N/A& N/A& N/A\\
Penalty Initial Value $\nu_0$ & N/A & N/A & 1& 1& N/A\\
Penalty Learning Rate $\eta_\nu$ & N/A & N/A & 0.05& 0.01& N/A\\
Temperature Factor $\lambda$ & N/A & N/A & N/A & 1.5 & N/A\\
\hline
\end{tabular}}
\label{tab:algo}
\end{table*}

\begin{table*}
\centering
\small
\caption{Experiment-specific parameters.}
\setlength{\tabcolsep}{6mm}{
\begin{tabular}{lccccc}
\hline
Parameter & Circle & Gather & Navigation &  Simple Spread & PointGoal\\
\hline
State Space $|S|$       & $\mathbb{R}^{113}$  & $\mathbb{R}^{86}$& $\mathbb{R}^{60}$& $\mathbb{R}^{54}$  &$\mathbb{R}^{60}$    \\
Action Space $|A|$    & $\mathbb{R}^8$     & $\mathbb{R}^2$  & $\mathbb{R}^2$  & 5  & $\mathbb{R}^2$  \\
Cost Limit $d$ & 50 & 0.5 & 25,20 & 0.5  & 25\\
Rollout Length $T$ & 1e3 & 1e2 & 1e3 & 25 & 1e3 \\
Buffer Size $|\mathcal{D}|$ & 3e4 & 3e3 & 3e4 & 1e3 & 3e4  \\
Total Interactions & 1e7 & 1e7& 1e7 & 5e6 & 1e7\\
\hline
\end{tabular}}
\label{tab:exp}
\end{table*}

\subsection{Environments}
\paragraph{Circle}
This environment is inspired by \citeauthor{achiam2017constrained}~\shortcite{achiam2017constrained}. Reward is maximized by moving along a circle of radius $d$:
\begin{equation*}
    R = \frac{v^\mathrm{T}[-y,x]}{1 + \big| \sqrt{x^2+y^2} - d\big|},
\end{equation*}
but the safety region $x_{\mathrm{lim}}$ is smaller than the radius $d$:
\begin{equation*}
    C = \mathbf{1}[x > x_{\mathrm{lim}}].
\end{equation*}

In our setting, $d=10, x_{\mathrm{lim}}=3$ and the constraint threshold is 50 by default, which indicates the agent is only allowed to move in a narrow space for safety requirements.

\paragraph{Gather} 
This environment is inspired by \citeauthor{achiam2017constrained}~\shortcite{achiam2017constrained}. The agent receives a reward of +1 for collecting an apple (green ball) and a reward of -1 for collecting a bomb (red ball). It also receives a cost of +1 when encountering the bomb. 8 apples and bombs spawn respectively on the map at the start of each episode.

In our settings, we set the constraint threshold to be 0.5. Notably, the punishment in reward function, to some extent, keeps the agent from collecting bombs even trained by the unconstrained PPO. But it still has many safety violations and fails to satisfy the constraint threshold.

\paragraph{Navigation}
This environment is inspired by \citeauthor{ray2019benchmarking}~\shortcite{ray2019benchmarking}. Reward is maximized by getting close to the destination:
\begin{equation*}
    R = \mathrm{Dist}(target,s_{t-1}) - \mathrm{Dist}(target,s_{t}),
\end{equation*}
but it yields a cost of +1 when the agent hits the hazard or the pillar. The two different types of cost functions are returned separately and have different thresholds. In out setting, $d_1 = 25$ for the hazard constraint and $d_2 = 20$ for the pillar constraint.  

\paragraph{Simple Spread} 
This environment is inspired by \citeauthor{lowe2017multi}~\shortcite{lowe2017multi}. In our experiment, 3 agents and 3 endpoints spawn respectively on the 2D arena at the start of each episode.
The agents are collaborative and rewarded for moving towards corresponding endpoints:
\begin{equation*}
    R = -\sum_{i=1}^{3} \mathrm{Dist}(endpoint_i,s_{it}),
\end{equation*}
but it yields a cost of +1 if any pair of agents collide. We set $d = 0.5$.

\subsection{Agents}
For single-constraint scenarios, Point agent is a 2D mass point($A \subseteq  \mathbb{R}^2$) and Ant agent is an quadruped robot($A \subseteq  \mathbb{R}^8$). For the multi-constraint scenario which is modified from OpenAI SafetyGym~\cite{ray2019benchmarking}, $S \subseteq \mathbb{R}^{28 + 16\cdot m}$  where $m$ is the number of pseudo-radars(one for each type of obstacles and we set two different types of obstacles in the Navigation task) and $A \subseteq \mathbb{R}^2$ for a mass point or a wheeled car.
For the multi-agent scenario which is inspired by OpenAI Multi-Agent Particle Environment~\cite{lowe2017multi}, $O \subseteq \mathbb{R}^{18}$ each (the state is partially observable) and $|A|$ is 5 for any particle moving up/down/left/right and remaining still.
 

\subsection{Parameters}
We summarize the algorithm-specific parameters and experiment-specific parameters in Table \ref{tab:algo} and Table \ref{tab:exp} , respectively.
























