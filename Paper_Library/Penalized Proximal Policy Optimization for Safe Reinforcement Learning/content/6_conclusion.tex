\section{Conclusion}
In this paper, we propose Penalized Proximal Policy Optimization (P3O), an effective and easy-to-implement algorithm for safe reinforcement learning. P3O has an unconstrained optimization objective due to the exact penalty reformulation and guarantees better performance on reward and constraint value empirically. Meanwhile, it is naturally compatible with multi-constraint and multi-agent scenarios. P3O adopts first-order minimization and avoids the inversion of high-dimensional Hessian Matrix, which is especially beneficial to large CMDPs with deep neural networks. Thus we consider the future work as end-to-end visual tasks which are less studied in safe reinforcement learning.
 