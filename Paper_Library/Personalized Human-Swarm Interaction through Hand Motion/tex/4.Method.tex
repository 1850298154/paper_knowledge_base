\section{Pilot Study}\label{sec:pilots}

For the preliminary experimental stage, N=20 human subjects were recruited to participate in 2 experimental sessions.
Informed consent was obtained from everyone before the experiment and the study was conducted while adhering to standard ethical principles\footnote{The experiments were approved by the Human Research Ethics Committee of the École Polytechnique Fédérale de Lausanne.}.
The results of our studies consist of statistical analysis and observations of human factors in human-robot interaction activities, mainly the participant's body motion.
Due to the limited number of subjects per condition, we chose non-parametric methods to assess the significance of our results: we used the Kruskal-Wallis test to assess the equality of the medians of different groups \cite{kruskal_use_1952}.

% In this section, we discuss the system hardware and algorithm components of our pipeline.
% \mm{Ludovic, you can start writing one paragraph for each of these themes:}
% Here an intro on the motivation for this part (why are we doing this - to provide a novel method for wearable-based swarm teleoperation).
% \begin{itemize}
%     \item Simulator (what framework (Unity), which environment, what kind of robots, what kind of dynamics, controls...)
%     \item Swarming algorithm (concept, equations)
%     \item Experimental protocol  - what happens during the experiment (show simulator, drones perform maneuvers - describe, ask people to move instinctively). 
%     Use a figure like Fig. \ref{f:protocol} to explain pipeline. Add Optitrack for this stage to the figure. this figure explains our whole experimental setup. It would be nice if the hand can be a CAD figure where the tracked parts are highlighted.
%     \item Data acquisition (what is recorded - motion + swarm commands, with what kind of hardware - Optitrack + LEAP
%     \item Algorithm overview (remember: you don't need to claim that this method is new, refer to the old one before your start this part and point out differences)
%     \begin{itemize}
%         \item Data preprocessing (what kind of data we have, sampling, filtering, everything you do before feature selection
%         \item Feature selection and velocity/position control discrimination (explain problem and novelty)
%         \item Regression
%         \item ...
%     \end{itemize}
% \end{itemize}
\begin{figure}[h]
    \begin{center}
        %\includegraphics[width=0.5\columnwidth]{./simulator}
        \includegraphics[width=0.9\columnwidth]{./protocol1.png}
        \caption{
        Experimental scenario for the identification of spontaneous motion patterns for aerial swarm teleoperation. The user sees a set of predefined actions performed autonomously by the robotic swarm in a simulated environment  (in the figure: left motion, the blue dot is used as reference). 
        The user moves as if they were controlling the motion of the drones and their body movements are tracked.}
        \label{f:protocol1}
    \end{center}
\end{figure}
\textbf{\textit{Acquisition of spontaneous body motion:}}
%  simulator
we implemented a swarm simulator, based on an existing quadrotor model\footnote{\href{https://github.com/UAVs-at-Berkeley/UnityDroneSim}{https://github.com/UAVs-at-Berkeley/UnityDroneSim}}. We configured the swarm to move according to a list of predefined maneuvers.
A master drone is controlled by the algorithm (or the operator), while the collective behavior of the slave agents is based on the Reynolds' algorithm~\cite{reynolds1987flocks,soria2019influence}. 
This potential-field based control algorithm consists of three rules: 1) a repulsive separation term to steer nearby drones away from each other, 2) a cohesion term to keep the drones close to each other, and 3) an alignment term that aligns the velocity vector of the agents. The respective acceleration terms for cohesion, separation, and alignment can be formalized as:
\begin{align*}
    \boldsymbol{a}_{i} &=  \boldsymbol{a}^{coh}_i + \boldsymbol{a}_{i}^{sep} + \boldsymbol{a}_i^{ali}\\
    \boldsymbol{a}_{i}^{coh} &= k^{coh}\frac{1}{|\mathcal{A}_i|}\sum_{j\in \mathcal{A}_i}\boldsymbol{r}_{ij}\\
    \boldsymbol{a}_{i}^{sep} &= k^{sep}\frac{1}{|\mathcal{A}_i|}\sum_{j\in \mathcal{A}_i}\frac{\boldsymbol{-r}_{ij}}{||\boldsymbol{r}_{ij}||}\\
    \boldsymbol{a}_{i}^{ali} &= k^{ali}\frac{1}{|\mathcal{A}_i|}\sum_{j\in \mathcal{A}_i} \boldsymbol{v}_i-\boldsymbol{v}_j
\end{align*}

where $\mathcal{A}_i$ represents the set of neighbors of the $i$-th agent, $\boldsymbol{r}_{ij}$ denotes the relative distance between of the $j$-th and the $i$-th agent, and $\boldsymbol{v}_{i}$ and $\boldsymbol{v}_{j}$ are the velocities of the $i$-th and the $j$-th agents, respectively.

The swarm was visualized in third-person view through a Head-Mounted Display (HMD) and performed 8 distinct maneuvers: right-left motion, up-down motion, front-back motion, and expansion-contraction, for a total of 4 DoF.
% \red{HOW DID YOU TUNE K?} \ld{I first used the coefficient Hugo had before me and slightly changed them so I had a swarm that is less compact (reducing the cohesion). I also slightly changed the alignment (increased) so the swarm moves faster}
We showed the simulation to N=10 subjects and asked them to move as they were controlling the drone swarm with their body motion.
We used a motion capture system\footnote{\href{https://optitrack.com}{https://optitrack.com}} to track the kinematics of the user's upper body during the experiment (\reffig{f:protocol1}).
The human upper body was modeled as a kinematic chain consisting of 9 rigid bodies interconnected by spherical joints.
\\
% \begin{figure}[th]
%     \begin{center}
%         \includegraphics[width=1\columnwidth]{./protocol}
%         \caption{
%         Experimental protocol. Explain : what is in the figure? (A) Experiment 1: describe acquisition of body motion using MoCap (B) Experiment 2: describe use of LEAP to track hands, definition of personalized mapping, control stage \red{NOTE: for now in the figure we have only Experiment 2!} \mm{this can be a page width figure}
%         }
%         \label{f:protocol}
%     \end{center}
% \end{figure}

\begin{figure}[t]
\begin{center}
  \includegraphics[width=1\columnwidth]{./opti_motion_complete1}
\caption{
Results of the spontaneous motion acquisition experiment. The total motion of the center of mass of the different body segments shows a preponderant use of hands and forearms. The color code represents the statistical significance of equality of medians in the distributions.}

\label{f:motion}
\end{center}
\end{figure}

\textbf{\textit{Pilot Study 1 - Hand motion is the most relevant motion feature for the task:}}
we studied the participants' motion to identify the body segments which are most relevant for the task.
To do so, we analyzed the total displacement of the different body segments.
We found that higher displacement was associated with forearms and hands positions than with the rest of the upper body segments, demonstrating that users spontaneously prefer to use their hand motion to control the swarm (\reffig{f:motion}).
Specifically, most of the participants mimicked the motion of the drone using one or both hands, resulting in high displacement values for the right ($2.34 \pm 1.47m$) and left hand ($1.31 \pm 1.10m$).
Consequently, we observed that the forearms moved on average more than the rest of the body: both the right  ($1.67 \pm 1.03m$) and the left one ($0.68 \pm 0.41m$). The unbalance between right and left body segments is due to most of the participants being right-handed.
We discovered that the displacement of these 4 body segments was significantly higher than the motion of the torso and shoulders (all below $0.3m$ on average, $p<0.01$).
Also, hand motion was higher than upper arms motion, at a less significant level ($0.05>p>0.01$).


\textbf{\textit{Acquisition of hand motion:}}
according to this result, we decided to track only the hands of the users and we used as an acquisition system the LEAP Motion controller\footnote{\href{https://www.ultraleap.com/product/leap-motion-controller/}{https://www.ultraleap.com/product/leap-motion-controller/}}. With this choice we moved to a more portable solution and added finger tracking.
The LEAP Motion controller models the human hand as a kinematic chain composed of 21 rigid bodies, 1 for the palm and 4 for each finger, and provides their poses (positions and rotations). 
As we observed the participants mainly used hand motion, (\reffig{f:motion}), we decided to retain the positions of palms and fingertips relative to the palm, for a total of 6 rigid bodies per hand.
We computed the relative positions of the fingers as follows:
\begin{align*}
     \boldsymbol{p}_F' =  \boldsymbol{q}_P( \boldsymbol{p}_F -  \boldsymbol{p}_P)
\end{align*}
Where $ \boldsymbol{q}_P$ is the quaternion describing the rotation of the palm, $ \boldsymbol{p}_P$ the position of the palm, and $ \boldsymbol{p}_F$ the position of the finger's bone in the LEAP Motion's frame.
The original position $ \boldsymbol{p}_F$ is replaced by the new position $\boldsymbol{p}_F'$.
Additionally, we included a grasp factor to account for this common motion synergy. 
It was represented as:
\begin{align*}
    \boldsymbol{g} = \frac{\sum_{i=1}^5 \sum_{j=i+1}^{5} \boldsymbol{r}_{Fij}}{10}
\end{align*}
where $\boldsymbol{r}_{Fij}$ is the distance between the tip of the $i$-th and the $j$-th finger of the hand.
In total, we acquire 22 kinematic variables for each hand consisting of the 3D coordinates of palm and fingertips, the 3D rotation of the palm, and the grasp factor.
Fingertip rotations were removed from our variable set as they highly correlate to their positions.

\begin{figure*}[h!]
\begin{center}
\includegraphics[width=\textwidth]{./fig3}
	\caption{Variability of the observed motion strategies across experimental subjects. (A) Kinematic variables with the highest correlation between human and robot motion. Additional ones are added for a subject if their correlation score is at least $90\%$ of the first. Most subjects display individual motion traits, while only three of them (subj 3,4,5) show a full agreement. (B) Example of motion strategies for the different DoF of the swarm, front motion. From left to right, double-handed velocity control through palm rotation, double-handed position control through palm position, single-handed position control through palm position. (C) Example of motion strategies for the different DoF of the swarm, expansion of the swarm. From left to right, double-handed velocity control through palm proximity, single-handed position control through grasp synergy, asymmetric double-handed position control through palm proximity - asymmetric.}
	\label{f:variab}
\end{center}
\end{figure*}

\textbf{\textit{Pilot Study 2 -  Spontaneous hand motion is variable across users:}}
in presence of high agreement between the spontaneous motion of a population, it is possible to derive general interfaces that would fit all users.
We assessed the similarity of spontaneous hand motion among a set of users, to quantify the need for a personalized interface.
We asked 10 subjects to perform the same task as in the previous experiment, this time acquiring hand motion data from the LEAP Motion Controller.

% To investigate the relation between hand motion and swarm actions, we 
% \subsection{Dataset extension} We saw from the user study that some users instinctively try to control swarm velocity while others tend to control swarm position. \grey{Actually, it can also happen that a user wants to control the position for some of the motions and the velocity for others.} The purpose of the mapping is to associate hand motion features to swarm commands with as less assumptions as possible. Therefore our algorithm should always be able to find efficient features to control the swarm position, independently of the nature of the hand motion. More specifically, even if the operator controls the velocity of the swarm, our algorithm should translate the hand motion to swarm position commands.\\
% To do that, we extended the hand motion dataset by adding the integral of the hand motion signals as new features. \grey{Indeed, mapping a feature to the y-velocity of the swarm is equivalent to mapping the integral of this feature to the y-position of the swarm.} Therefore, we add new features defined by the integral of the signals:
% \begin{align}
%     \label{equ:integral}
%     x_{int}[0] &= 0\\
%     x_{integral}[k+1] &= x_{integral}[k] + (x[k+1] - x[k])\times \delta t\\
% \end{align}

We computed for each kinematic variable the Pearson's correlation coefficient $\alpha$ with respect to the swarm command, separately for each maneuver:
\begin{align*}
\alpha_{ij} = \frac{cov(X_i,Y_j)}{\sigma_{X_i}\sigma_{Y_j}}
\end{align*}
where $cov$ is the covariance of the $i$-th kinematic variable with the $j$-th robot maneuver, and $\sigma$ their standard deviation.
We normalized the correlation values on the features so that $\sum^i{\alpha_{ij}} = 1$.
In the case of velocity control, however, the participant's motion does not correlate with the swarm position (which we acquire as swarm action) but with its velocity.
For this reason, the Pearson's $\alpha$ coefficient was computed also between the swarm actions and the integral of the kinematic variables.

We found that, for each maneuver, the body segment mostly correlated with the robot motion was different for most subjects  (\reffig{f:variab}).
We retained body segments for which the correlation was at least $90\%$ of the maximally correlated one, showing a high variability.
Our data show that while some subjects spontaneously moved according to the position of the swarm, others tended to move their hands according to its velocity.

We identified 4 common motion patterns: right hand position control for the control of the 3D position of the drone, and right hand grasp for contraction/expansion.
% However, each DoF presents at least $40\%$ of subjects using different motion strategies, excluding the vertical displacement, where $80\%$ of the participants agreed on the motion.
In total, only $30\%$ of the participants agreed in using these patterns, while the others presented individual variants.
Specifically, for front/back motion, $70\%$ of the participants implemented a position control, $60\%$ using the position of the right hand and one using both hands, and $30\%$ a velocity control, $10\%$ using the position of the right hand and the others using the rotation of right or left hand.
We refer the reader to \reffig{f:variab}A for more details on the remaining DoFs.

Based on these results, we decided to implement personalized HRIs for each user and to allow them to choose with their motion between position and velocity control for each degree of freedom of the robot.


\section{Method}\label{sec:method}

The framework used to create the personalized HRIs is based on our previous work \cite{macchini2019personalized}.
The framework has been extended to accept inputs from the LEAP Motion controller and modified to allow both position and velocity control.
Here, we summarize the main algorithmic steps, pointing out the novelties introduced for this study.

\textbf{\textit{Data acquisition and preprocessing:}}
a first extension of our previous implementation is the acquisition of data from a new device and the definition of a new list of kinematic variables based on human hand biomechanics.
The simulation environment and the data acquired during the imitation phase are described in Sec. \ref{sec:pilots}.
Data are collected while the subject imitates the swarm motion with their hand movements and preprocessed to obtain the relative fingertip position and the grasp coefficient.
We compute the integral throughout the imitation phase and add it to the hand motion dataset.
The integral values are reset at the beginning of each new maneuver to prevent minor displacements from being accumulated over time and affect the dataset.
Since we consider a miscellaneous set of 3D coordinates and angular data, for which normalization is essential, all the motion variables were normalized to zero mean and unit variance.

\textbf{\textit{Feature selection:}}
in order to regularize the regression step, we reduce the dimensionality of the dataset by extracting the most informative kinematic variables.
We rank the variables based on a quality factor and select the most informative ones based on a threshold.
Let $X_i$ be the $i-th$ kinematic variable, and $Y_j$ be the $j-th$ degree of freedom of the robot.
The quality factor of $X_i$ with respect to $Y_j$ is given by:
\begin{align*}
    \lambda_{ij} = \alpha_{ij} \times SNR_i^k
\end{align*}
Where $\alpha_{ij}$ is the Pearson' correlation coefficient computed between $X_i$ and $Y_j$, and $SNR_i$ the signal-to-noise ratio of the kinematic variable $X_i$.
$k$ is a coefficient used to compensate the lower SNR associated with the integral terms, which are inherently low-passed. We set $k=2$.

We noticed that the number of variables selected changed substantially depending on the user's motion strategy. 
As this aspect made selecting an optimal threshold more challenging, we modified the algorithm pipeline to select a reduced set of variables for each of the robot's DoF (motion on the x, y, and z axes, expansion/contraction).
We normalized the $\lambda$ values to have $\sum_i \lambda_{ij} = 1$, and ranked the variables based on the associated $\lambda_{ij}$. 
Subsequently, we choose the $M$ first variables, so that $\sum_{1<i<M} \lambda_{ij} \geq \tau$, and set the threshold $\tau=0.7$.
This modification makes the feature selection process more interpretable and meaningful for human supervision and provides a lower-dimensional set of variables for each DOF, simplifying the regression step.

\textbf{\textit{Regression:}}
we finally train a linear model to define the mapping function between the user's motion and the robot's actions. We use ridge regression with BIC-optimized ridge parameter to maximize the model's fit to the data while preventing overfitting \cite{Ghadban_ridge, Neath_BIC}. 
This improvement, together with the previously described separation of the feature selection for each DOF, allowed us to remove the regularization step in the original algorithm based on CCA and simplify our pipeline. 

% \grey{Eventually, we \textbf{compute the Euler angles} from the quaternions in order to have signals that directly reflect the rotation of the hand. Indeed, quaternions allow faster and easier computation but the signals do not reflect the rotation of the hand as the user wants it. Therefore, for each of the bones of the hand we compute the roll, pitch and yaw based on the quaternion describing them.}\\


% \grey{Please note that before the preprocessing step the signals are splitted into four parts, each corresponding to the time where the swarm is doing a specific motion. The consequence is that the integral of the signal is set back to 0 at the beginning of each maneuver. Without this the integrals would not really reflect the expected motion since a small drift on the first motions will affect the following.}

% \subsection{Signals normalization}
% After the preprocessing steps we need to normalize the signals before studying the correlation. Indeed, we want all the signals to have similar range of motion, otherwise the quality metric defined in the following paragraph could depend on the range of motion of the signal and therefore be higher for positions signals than for orientation signals.

% \subsection{Feature selection}
% % \ld{All this already existed in the original framework so it may not be relevant. The difference relies on the fact that have scores for each of the swarm commands which I believe wasn't the case before.}
% This step consists in a dimensionality reduction on the hand motion signals.\\
% Since we make no assumptions on the user’s instinctive motion, an automatic feature selection functionality has been implemented to extract the most relevant hand motion variables. For this purpose, we compute a quality metric for each hand motion variable $x_i$. The metric is given by two elements: the cross-correlation of each signal with the swarm commands and its signal-to-noise ratio (SNR). Firstly, we want to choose the hand features that correlate as much as possible with the given command. We compute the correlation of the hand motion signals $X_i$ with each swarm command signal $Y_j$ through the cross-correlation coefficient $K_{X_i Y_j}$ defined as:
% \begin{align*}
%     K_{X_i Y_j} = \frac{\sum_m(X_i[m]-\mu_{X_i})(Y_j[m]-\mu_{Y_j})}{\sigma_{X_i}\sigma_{Y_j}}
% \end{align*}
% where $X_i[m]$ is the $m-th$ sample of $X_i$. Note that $K_{X_i Y_j}$ is computed at the origin since we assumed that the time-shift was negligible (the user moves nearly at the moment the swarm moves). We also normalized the signals before, yielding:
% \begin{align*}
%     K_{X_i Y_j} = \sum_m X_i[m] Y_j[m]
% \end{align*}

% Secondly, to take into account noise in the quality metric, we assume that $X_i$ can be expressed as $X_i = S_i + N_i$ where $N_i$ represents the noise, assumed to be fully caused by quantization, thus uniform and $S_i$ is computed with a moving average Low-Pass FIR filter on 50 samples on $X_i$. Therefore, we denote with $SNR_i$ the signal-to-noise ratio of the time series $X_i$. Then, the desired quality metric $\lambda_[{ij}$ associated with a feature $x_i$ and one of the swarm commands $y_j$ can be expressed as:
% \begin{align*}
%     \lambda_{ij} = K_{X_iY_j} \times SNR_i^k
% \end{align*}
% Where $k$ is a variable coefficient allowing us to increase or reduce the importance of the SNR.\\

% In summary, the quality metric $\lambda_i$ is proportional to the amount of information relevant for the mapping definition which is carried by each kinematic variable $x_i$ and with respect to a swarm command $y_j$. The selection is thus done independently for each swarm motion based on the best scores over all $\lambda_{ij}$.\\

% \grey{The number of features selected for each swarm motion is variable and depends on a threshold. Specifically, we select the features that have the highest scores while the sum of the scores is smaller than an adjustable value. This way, we make sure to have enough information for each swarm motion.\\
% Note that the algorithm also includes the possibility of selecting all the features whose scores are greater than a threshold.}

% \subsection{Motion synergy identification and regression} Finally, we ran an additional dimensionality reduction step and a regression step as in the original framework. We then got the mapping between hand motion and swarm command for a specific user.

% \subsection{Results}
% \ld{If you want to show results of the mapping}\\
% Some results of the mapping are shown Figure~\ref{fig:mapping_real_predict}.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{././result_contract.PNG}
%     \caption{Contraction / expansion command: Comparison between the predicted swarm commands (red) and the real swarm commands (blue). The red signal presents the swarm commands predicted by the personalized mapping based on the acquired hand motion. It uses the hand motion data that we acquired to create the mapping and feed it to the personalized mapping to predict commands. Therefore, if the user do the exact same movements than he did to create the mapping, the swarm will move according to the red line }
%     \label{fig:mapping_real_predict}
% \end{figure}

% \grey{Note that the steps (jumps) visible on some of the figures (not on this one) are due to the use of the integrals as features. The visual feedback the user has during the teleoperation phase allows him to make sure the swarm is at the correct position at each time and thus the integrals won't be put to 0 at the beginning of each maneuver.}

% \subsection{Control algorithm}
% The same modifications have been applied to the original control algorithm.
% At the end of this part of the framework we have commands that are sent to Unity at each frame of the simulator. The swarm then moves according to the received commands.