\section{Discussion}\label{sec:conclusion}

This study investigates the motion strategies arising from the spontaneous interaction between a human and an aerial drone swarm and propose a solution to automatically derive a human-swarm interface from a user's spontaneous motion data.
%We considered a drone swarm as an example of multi-agent robotic system, as despite their advantages with respect to single quadrotors are becoming \red{XXX} in areas such as \red{XXX}, there are still no standard interfaces to let humans control them.
We ran two pilot studies (N=20) to identify the most relevant body segments for this interaction and their motion characteristics.
Based on the results from this phase, we designed a machine learning-based algorithm to map a user's spontaneous motion into commands for the swarm and characterized it in a final user study (N=10). 
Here, we discuss our main findings.

Our pilot studies produced two major results.
First, by observing the full-body motion of a set of participants imitating the swarm behavior, we realized that the most engaged body segment was the users' hands.
On average, our participants moved their right hand $920\%$ and their left hand $515\%$ more than the torso, reflecting the statistical predominance of right-handedness in the population (Fig. \ref{f:motion}).
This result is particularly relevant in the field of motion-based HRIs, as it contrasts with findings that emerged in prior studies on different robots. 
When asked to mimic the flight of a fixed-wing drone, subjects tend to imitate its roll and pitch angles using their torso, with a minor engagement of their hands \cite{macchini_personalized_2020}. In our case of teleoperation of a drone swarm, we observed the opposite. 
We suggest two possible explanations for this fact. First, the interaction with a different robot might affect a user's spontaneous motion patterns. Also, the higher complexity of a drone swarm in terms of controllable DoFs might induce the necessity to use more articulated body segments, such as the arms. Second, switching the viewpoint impact on a user's spontaneous motion in VR imitation tasks \cite{macchini_impact_2021}.
Ground-view viewpoints, particularly, are associated with a higher inter-subject motion variability.
% However, such viewpoint is the only option for the teleoperation of a swarm, as first-person view from multiple cameras might overload the operator's senses.
However, we chose this viewpoint since it is the easiest to adopt in a drone swarm teleoperation scenario. The alternatives would be to either switch from an onboard camera to another single robot or try to give the operator a first-person view from multiple cameras, which might overload the operator's senses. 

The second finding related to our pilot study concerns the variability of the users' hand motion.
By observing the human-robot motion correlation, we found that only $30\%$ of our participants exhibited the same motion patterns to control all the 4 DoF of the swarm (Fig. \ref{f:variab}).
This poor agreement is related to the hand segments interested in the motion patterns and the type of controller that the participants imagined for the drone swarm. 
While some users moved their hand as if they were controlling the position of the swarm (for instance, placing their hand in the same position as the center of the virtual agents), others moved as if they were controlling its velocity. We proved this by correlating the hands' kinematic variables and their integrals with the swarm actions.

Based on these results, we decided to define personalized mapping functions for each subject.
We implemented a set of modifications to an existing algorithm to allow the use of the new sensor (the LEAP Motion controller), and to allow the user to choose the control mode (position/velocity) through their motion.
The results relative to the teleoperation user study provide new insights in motion-based teleoperation of robotics swarms. First, we found that the use of remote controllers leads to a shorter time needed to complete the task prior to training (Fig. \ref{f:res_alt}A). 
Due to the widespread popularity of remote controllers for several applications, from gaming to teleoperation, it is today nearly impossible to recruit participants who are naive to their use.
However, group H improved their performance almost twice as fast as group R. This result might be due to the lower initial performance, and was perceived by our participants and reported as subjective feedback in our survey.
We observed similar results regarding the number of collisions that occurred during the task (Fig. \ref{f:res_alt}B).
While group R never had more than 7 accidents during the 5 repetitions of the task, group H managed to improve from 19 collisions in the first run to 8, after training ($-58\%$).
We also found that this effect was only true for some sections of the proposed path: user's time need to cross 2 out of 4 gates were similar with both interfaces, while significantly changed for the remaining two gates (Fig. \ref{f:res_alt}B)
This result suggests that some maneuvers (like, in this case, crossing a horizontal and vertical gate) can be more challenging using body motion, while others are equally hard to perform with different interfaces.
Possible explanations of such effect could be related to the human's perturbed depth perception, and proprioceptive capabilities in virtual environments \cite{ingram_proprio,chen_human_2007}.
Finally, the subjective feedback survey demonstrated that users did not prefer one of the two interfaces and that the personalization was accurate.

Despite the lower performance for untrained users, our study provides encouraging results: all the participants were able to navigate the drone swarm through the path, using a partly position-based, partly velocity-based control paradigm adapted to their preferences. To our knowledge, this work provides the first algorithm capable of such a level of personalization. 
Moreover, the simplicity of the calibration procedure  allows a user to change their strategy in a matter of minutes, without redesigning the interface.
As one of our subjects observed, it is known that humans are not able to determine the easiest way to control robots based on their spontaneous motion \cite{macchini_does_2020}, and so a different control strategy might seem more effective in hindsight.
With our method, it is possible to generate a new interface in less than two minutes.

This work opens to new intriguing options for future research. 
First, it would be a valuable addition to test the transferability of the proposed approach on a real drone swarm and to study if our results hold in the real world. 
Moreover, the conception of a method to perform first-person view control of robotics swarms could extend this work to this common viewpoint for teleoperation.
Also, as we observed that a maneuver-related effect on the interface performance, it would be interesting to verify the consistency of performance with a set of different paths.
Finally, given the high variability, we observed in the motion of our subjects, increasing the participants pool to include a more varied population would undoubtedly add value to our work.

\section{Conclusions}

In this paper, we applied a methodology to identify the relevant motion patterns for human-swarm interactions through body motion and assessed their variability among different individuals. 
Our results showed that the hand body segment was the most commonly adopted one and that most people tend to use their hands in significantly different ways. We extended an HRI learning framework to allow users to control a drone swarm through their preferred hand strategy.
The proposed interfaces showed promising performance and provided a convincing user experience. The possibility to develop quickly new interfaces through an imitation task is a significant contribution, which could impact the future design of human-swarm interaction systems.


% Since personalized interfaces have been demonstrated to be more effective than generic ones for the control of fixed-wing drones, the application of these results could facilitate the design and the implementation of new \offtwo{wearable,}motion-based telerobotic systems.


\section{Acknowledgements}

This work was partially funded by the European Unionâ€™s Horizon 2020 research and innovation  programme under grant agreement ID: 871479  AERIAL-CORE, the Swiss National Science Foundation (SNSF) with grant number 200021-155907, and the National Centre of Competence in Research (NCCR) Robotics.