\section{Introduction}\label{sec:introduction}

Recent advancements in robotic swarms and collective behavior are opening to new perspectives in many fields, such as collective transportation, surveillance, and mapping~\cite{cortes2017coordinated,hunt2020checklist}.
However, despite the concrete advantages of human capabilities with respect to autonomous navigation algorithms, no standard interfaces exist to date for such systems, and letting humans intervene on their behavior is still a challenge.
% In these applications, operators could implement high-level actions by supervisory control or carry out other tasks which require higher perception or dexterity.
Here, we study the spontaneous motion patterns arising for naive users for human-swarm interaction, and leverage this knowledge to design and validate a machine learning algorithm capable of generating personalized human-swarm interfaces based on a user's preferred motion strategy.

The term \textit{telerobotics} identifies the branch of robotics involving a human operator controlling a robot situated in a different environment \cite{niemeyer_telerobotics_2008}.
Telerobotics is needed in all the tasks for which robotic autonomy is still incapable of achieving a sufficient level of performance \cite{abbink_topology_2018}.
Relevant examples include, but are not limited to, search-and-rescue, exploration of challenging environments, minimally invasive surgery \cite{bodner_first_2004, diftler_robonaut_2011, murphy_search_2008}.
As the robots dedicated to these applications rise in complexity and performance, a complementary effort is required to implement interfaces that are at the same time effective and comfortable for most users \cite{casper_human-robot_2003}.
However, standard interfaces like remote controllers still fail to achieve this goal and require substantial time and effort to be proficiently mastered by inexperienced users \cite{chen_human_2007, peschel_humanmachine_2013}.

Modern Human-Computer Interfaces (HCIs) tend to leverage the innate control capabilities that humans can exert over their body motion to provide more effective teleoperation systems for both Virtual Reality (VR) applications and telerobotics \cite{casadio_body-machine_2012}.
% Interfaces based on body motion are a subset of Body-Machine Interfaces (BoMIs), which more generally consider biometric signals to interface the human body with external devices \cite{casadio_body-machine_2012}. 
BoMIs have shown the potential to be more effective than standard interfaces both in terms of performance and of user experience, measured as a combination of cognitive workload necessary to control the robot and user engagement \cite{toet_toward_2020}.
% Moreover, covering a larger body area than joysticks, motion-based interfaces open to new possibilities regarding the possible feedback channels to the user, especially by means of haptic devices [].

% \textbf{PARAGRAPH ON DRONES + SWARMS} \red{FABRIZIO something like "specifically, drones..." and then "swarms could facilitate tasks as..." and close with interfaces for swarms}
For few decades, drones have attracted much attention both from researchers and industrial players~\cite{floreano2015science}. These robots achieved stunning levels of autonomy~\cite{LoiannoRAL2017,kaufmann2020deep} and we are now witnessing an effort in scaling the autonomy of single-drone systems to the so-called aerial swarms~\cite{chung2018survey,tahir2019swarms,coppola2020survey}. Indeed, groups of drones could unlock applications that are too complex, time-consuming, or even impossible for a single drone (e.g., collective transportation~\cite{villa2019survey}). However, a drone swarm, like other robotic entities, can still benefit from the high-level teleoperation and decision-making of a human operator.  
Different solutions allowed the user to control a single robot through hands, torso, or full-body motion \cite{macchini_hand-worn_2020, rognon_flyjacket:_2018, sanna_kinect-based_2013}. 
Recent work is dealing with the implementation of novel paradigms for swarm control \cite{schiano_rigidity_maintenance_2017,schiano_dynamic_2018,schilling2019learning}. In particular, we believe that the use of BoMIs for the operation of drone swarms is only at its dawn.
Most of these interfaces, though, rely on high-level commands such as "take off", "go right/left", and are not sufficiently sensitive for accurate navigation. 
Moreover, the implementation of a single, generic interface does not allow individuals to control the drone using their preferred motion strategy.
Based on an individual calibration procedure, personalized interfaces have shown superior results to generic ones in terms of learning time and performance \cite{macchini_personalized_2020, khurshid_data-driven_2015}.
However, research in motion-based HRIs for collective systems is not as developed, and few solutions have been proposed for this topic to date \cite{tsykunov_swarmtouch:_2018,aggravi2020connectivity}.
These HRIs are fixed, and thus do not account for individual motor preferences. Moreover, they rely on fixed mapping functions and directly map the user's hand position into the drone swarm center, possibly limiting the available workspace.  On the other hand, controlling the swarm velocity extends the workspace but can be less intuitive for naive subjects.



% An open problem in the field of HRIs is the difference between position and velocity control.
% While some users might prefer to control the position of the robot with their motion, others might rather find more intuitive to control its velocity.
% For all the existing interfaces, this paradigm is defined a-priori, thus not allowing users to choose their preferred option.

In this study, we propose a motion-based HRI to control of a drone swarm, which allows the user to both define their preferred strategy and choose between position and velocity control based on a calibration procedure.
The calibration consists of a physical demonstration of the preferred motion patterns used to control each Degree of Freedom (DoF) of the swarm~(Fig. \ref{f:protocol1}).
Briefly, we first observed the spontaneous motion patterns of participants when interacting with the swarm. 
After reducing the sensor coverage to the user's hands, since we found it to be the most relevant body segment for the task, we run a second experiment to assess their motion variability.
As hand movements varied from subject to subject and correlated partly with the robots' positions and partly with their velocity, we decided to let this option free. Therefore, we extended our framework to define the control methodology (i.e., position or velocity) from the subject's motion demonstration \cite{macchini_personalized_2020}.
Finally, we evaluated our system in a teleoperation task in a simulation with a swarm with 4 drones.

% This paper is structured as follows: Sec. \ref{sec:pilots} describes the two preliminary studies leading to our design choices. Sec. \ref{sec:method} summarizes the proposed machine learning algorithm used to derive personalized BoMIs for swarm teleoperation. Sec. \ref{sec:experiments} describes our experimental protocols and results, and in Sec. \ref{sec:conclusion} we summarize and discuss our findings.