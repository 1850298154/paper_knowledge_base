
\section{Related works}

\subsection{Complex Reasoning and Planning by LLMs}
Despite the remarkable progress made by large language models (LLMs) in recent years, current LLMs still struggle significantly when tackling complex reasoning and planning tasks. For instance, Xie et al. introduced a benchmark dataset called TravelPlanner~\cite{xie2024travelplanner}, which is designed to evaluate performance on realistic travel planning tasks. 
Although research by Wei et al.~\cite{wei2022chain} has demonstrated that generating Chain of Thought (i.e., sequences of intermediate reasoning steps) can substantially improve LLMs’ ability to solve complex reasoning tasks, such improvements remain quite limited: even when prompts are carefully crafted and include some prior knowledge in advance, the performance of LLMs is still far from satisfactory. For example, in the sole-planning mode of the TravelPlanner dataset, even with carefully designed prompts and the injection of prior knowledge, GPT-4o achieves only a 7\% Final Pass Rate; Even in thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B achieve only 5.9\% and 40\% Final Pass Rates, respectively.
%
Subsequent work by Hao et al.~\cite{hao2024large} proposed integrating LLMs with specialized tools such as the Z3 SMT solver~\cite{de2008z3}. In this setup, LLMs can invoke the solver during travel planning, which improves performance on the TravelPlanner dataset. Although such approaches that involve the invocation of specialized tools can improve performance, this approach does not focus on enhancing the LLM's own capabilities in solving such complex reasoning problems. 
%

A series of subsequent works have explored Multi-Agent System based approaches. Zhang et al.~\cite{zhang2024planning} proposed a method called Planning with Multi-Constraints (PMC), a framework for LLM-based collaborative Multi-Agent System. PMC decomposes constraint-rich tasks into hierarchical sub-tasks and maps each sub-task to executable actions, thereby simplifying the planning process. 
Chen et al.~\cite{chen2024reprompt} introduced REPROMPT, which optimizes “step-by-step instructions” in the prompts provided to LLM agents by leveraging conversation history derived from multi-agent interaction and reflection. 
Guo et al.~\cite{guo2025mirror} proposed a Multi-Agent System that integrates intra-reflection and inter-reflection mechanisms, achieving a Final Pass Rate of 2.2\% on the TravelPlanner benchmark.
Yang et al.~\cite{yang2025plan} introduced Multiple Aspects of Planning (MAoP), which employs a strategist to generate planning blueprints from multiple dimensions. 
Zhang et al.~\cite{zhang2025swarmagentic} built a Multi-Agent System from scratch, inspired by Particle Swarm Optimization (PSO). 
Ou et al.~\cite{ou2025analyzing} developed an LLM-based Multi-Agent System incorporating an orchestrator agent to enhance coordination among agents. 
Choi et al.~\cite{choi2025atlas} proposed a multi-agent framework called ATLAS, designed to handle the constraint-aware complexity of real-world travel planning. 

However, all the aforementioned methods share critical limitations: such LLM-based Multi-Agent Systems face challenges such as high interaction overhead, long single-response latency, difficulties in end-to-end training, substantial resource and cost demands, and communication redundancy. These issues hinder the scalability, operational efficiency, and performance of the systems, particularly as system complexity increases and reliance on external APIs becomes more pronounced.

\subsection{LLM-based Multi-Agent Systems (LLM-MAS)}
In recent years, with the rapid development of large language models (LLMs), LLM-based Multi-Agent Systems have emerged as a research hotspot. These systems construct multiple LLM agents with distinct roles and functionalities to collaboratively accomplish complex tasks. Through multi-turn interactions, role specialization, and reflective mechanisms, LLM-based Multi-Agent Systems exhibit human-like collaboration and the potential for collective intelligence.
%
Recently, the Reflective LLM Agent System ~\cite{shinn2023reflexion} simulates human-like reflection mechanisms to detect and correct reasoning errors in a timely manner, thereby improving the quality of final outputs. MIRROR ~\cite{guo2025mirror} is a multi-agent system that leverages the reflective capabilities of LLMs by combining internal reflection before actions with interactive reflection after actions to enhance reasoning performance. MetaGPT ~\cite{hong2024metagpt} transforms Standard Operating Procedures (SOPs) into structured prompt chains and assigns diverse roles to different LLM agents in order to optimize workflow efficiency. CAMEL ~\cite{li2023camel}, or Communicative Agent Framework, introduces a prompting strategy known as inception prompting, which guides LLM agents to complete tasks aligned with human objectives.

Despite growing interest in LLM-MAS, critical limitations hinder their deployment and scalability. These include the need for complex manual prompt design and workflows, rapidly escalating inference costs with more agents, excessive storage and API costs, difficulty in optimizing through end-to-end training, and inefficient computation due to redundant multi-turn interactions.
% Despite the growing interest in LLM-MAS, several critical limitations still hinder their practical deployment and scalability. First, these systems often require manually designing complex prompts for each agent and constructing intricate workflows by hand, which significantly increases development difficulty and human labor costs. Second, as these systems rely on multi-turn interactions between agents, the communication overhead escalates rapidly with the number of agents, resulting in substantially increased response times for even single-task queries. This interaction cost grows exponentially with the number of agents, fundamentally limiting the system's scalability and real-world applicability. Third, the presence of multiple LLMs within the system leads to excessive storage demands. Moreover, if the system relies on paid external LLM APIs, the frequent multi-round interactions among agents can result in prohibitively high costs. Fourth, due to the complexity and decentralized nature of these systems, it is extremely difficult to improve performance through end-to-end training, posing substantial challenges for optimization. Finally, the necessity of repeated multi-turn communications among agents introduces a large amount of redundant information exchange, leading to inefficient computation and excessive overhead.

\subsection{Reinforcement Learning for Reasoning Model}
% \noindent\textbf{Policy Optimization}
The use of reinforcement learning (RL) to enhance reasoning in large language models (LLMs) has garnered significant attention~\citep{cheng2025revisiting,zhang2025critique,xiong2025minimalist}, primarily due to its ability to foster self-improvement without relying on human-annotated datasets. This is typically achieved by fine-tuning the models on complex reasoning tasks, aiming to cultivate diverse reasoning strategies~\citep{gandhi2025cognitive,yue2025does}. Key advancements, such as OpenAI o1~\citep{jaech2024openai} and DeepSeek-R1~\citep{guo2025deepseek}, demonstrate that RL techniques can be successfully deployed in large-scale commercial applications, significantly enhancing reasoning capabilities and revealing new emergent skills, such as extended reasoning chains.
%
In recent developments, reinforcement learning has been guided by scalar feedback signals~\citep{jaech2024openai,guo2025deepseek,liu2025understanding,yu2025dapo}. 
%
Among the commonly utilized algorithms in this field are online policy optimization methods, such as REINFORCE~\citep{10.1007/BF00992696}, Proximal Policy Optimization (PPO)~\citep{schulman2017proximal}, Group Relative Policy Optimization (GRPO)~\citep{shao2024deepseekmath}, and Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO)~\citep{yu2025dapo}.

However, despite their success in specialized domains, research into the application of RL algorithms to more complex tasks—such as planning—remains limited. Furthermore, there is a lack of sufficient empirical evidence and well-grounded reasoning to support the adaptability and effectiveness of existing RL methods for such planning tasks.
% \singlespacing
% \noindent\textbf{Planning Reasoning}


% \textbf{Planning Reasoning}
% Despite the remarkable progress made by large language models (LLMs) in recent years, current LLMs still struggle significantly when tackling complex reasoning and planning tasks. For instance, Xie et al. introduced a benchmark dataset called TravelPlanner~\cite{xie2024travelplanner}, which is designed to evaluate performance on realistic travel planning tasks. These tasks involve generating comprehensive plans that satisfy user requirements across multiple dimensions, including transportation, accommodation, dining, attractions, and budget, based on both user input and reference information. This type of problem necessitates complex reasoning and planning, and is also prevalent in real-world applications, making it of considerable practical importance.

% Although research by Wei et al.~\cite{wei2022chain} has demonstrated that generating Chain of Thought (i.e., sequences of intermediate reasoning steps) can substantially improve LLMs’ ability to solve complex reasoning tasks, such improvements remain quite limited: even when prompts are carefully crafted for the TravelPlanner dataset, and include some prior knowledge in advance, the performance of LLMs is still far from satisfactory. For example, under the sole-planning mode (where LLMs are provided with complete reference information to generate the final travel plan), current models such as GPT-4o-2024-11-20 achieve only a 7\% Final Pass Rate. Similarly, the Qwen-3-8B-Instruct model in the think mode achieves only 5.9\%, DeepSeek-R1-671B reaches only 40\%.

% Subsequent work by Hao et al.~\cite{hao2024large} proposed integrating LLMs with specialized tools such as the Z3 SMT solver~\cite{de2008z3}. In this setup, LLMs can invoke the solver during travel planning, which improves performance on the TravelPlanner dataset. Although such approaches that involve the invocation of specialized tools can improve performance, this approach does not focus on enhancing the LLM's own capabilities in solving such complex reasoning problems. In real-world scenarios, not all complex reasoning and planning tasks can be transformed into problems solvable by external tools. Therefore, it remains crucial to improve the LLM’s inherent ability to handle such complex reasoning and planning challenges.

% A series of subsequent works have explored Multi-Agent System based approaches. Zhang et al.~\cite{zhang2024planning} proposed a method called Planning with Multi-Constraints (PMC), a framework for LLM-based collaborative Multi-Agent System. PMC decomposes constraint-rich tasks into hierarchical sub-tasks and maps each sub-task to executable actions, thereby simplifying the planning process. This method improved the Final Pass Rate on the TravelPlanner benchmark to 42.68\%.
% Chen et al.~\cite{chen2024reprompt} introduced REPROMPT, which optimizes “step-by-step instructions” in the prompts provided to LLM agents by leveraging conversation history derived from multi-agent interaction and reflection. This method achieved a Final Pass Rate of 3.89\% on TravelPlanner.
% Guo et al.~\cite{guo2025mirror} proposed a Multi-Agent System that integrates intra-reflection and inter-reflection mechanisms, achieving a Final Pass Rate of 2.2\% on the TravelPlanner benchmark.
% Yang et al.~\cite{yang2025plan} introduced Multiple Aspects of Planning (MAoP), which employs a strategist to generate planning blueprints from multiple dimensions. These blueprints guide planning agents within the LLM-based Multi-Agent System to facilitate broad and strategic thinking for solving complex planning tasks.
% Zhang et al.~\cite{zhang2025swarmagentic} built a Multi-Agent System from scratch, inspired by Particle Swarm Optimization (PSO). This system maintains a population of candidate solutions and evolves them using feedback-driven updates, reaching a Final Pass Rate of 32.2\% on TravelPlanner.
% Ou et al.~\cite{ou2025analyzing} developed an LLM-based Multi-Agent System incorporating an orchestrator agent to enhance coordination among agents. This orchestrator helps guide the system to focus on specific sub-domains, ultimately achieving a Final Pass Rate of 25\% on TravelPlanner.
% Choi et al.~\cite{choi2025atlas} proposed a multi-agent framework called ATLAS, designed to handle the constraint-aware complexity of real-world travel planning. ATLAS incorporates a formalized methodology with dedicated mechanisms for constraint handling, iterative plan evaluation, and adaptive interleaved search, achieving a Final Pass Rate of 35\% on the test set of TravelPlanner.

% However, all the aforementioned approaches share several critical limitations:

% Manual Prompt Engineering and Workflow Design: These Multi-Agent System approaches often require extensive manual effort to design sophisticated prompts for each agent and to construct complex system workflows, increasing development costs and human effort;

% {\color{blue}Excessive Interaction Overhead: Due to multi-turn interactions among agents, the overall response time for a single query can become significantly prolonged. As the number of agents increases, the pairwise interactions can grow exponentially, resulting in high communication costs that severely limit scalability and practical deployment;

% Lack of End-to-End Solvability: These systems are generally unable to generate final answers in an end-to-end fashion. The complexity and opacity of interactions among multiple agents make end-to-end training extremely difficult, thus hindering performance improvements through direct learning;

% High Resource and Cost Overhead: Since multiple LLMs are involved, such systems typically require substantial memory and computational resources. If the system relies on commercial LLM APIs, the multi-turn, multi-agent interactions can incur extremely high operational costs;

% Redundant Communication: The necessity of repeated interactions among agents leads to considerable redundant communication, which further increases computational overhead and reduces system efficiency.}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 这些是已经翻译过的 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%【2402】尽管大型语言模型在近年来取得了十分瞩目的进展，但是，当前的LLM在回答一些需复杂推理与规划的困难问题时，仍然面临很大的困难。例如，Xie等人提出了一个名为TravelPlanner的旅行规划数据集【xie2024travelplanner】，这是一个基于现实世界中旅行规划任务的评测数据集，涉及到根据用户要求以及参考信息，规划出符合要求的交通、住宿、餐食、景点、预算等多方面的内容。这类问题就属于需要进行复杂推理与规划的困难问题，而且在实际应用当中，这类问题是非常普遍的,具有广泛的实际应用意义。虽然【wei2022chain】的研究发现，生成思维链（即一系列中间推理步骤）能显著提升LLM完成复杂推理任务的能力，但是这种提升依然十分有限：即便我们针对TravelPlanner数据集精心设计了Prompt，甚至在Prompt中事先加入了一部分先验信息，在sole-planning模式下（即在预先提供完备参考信息的条件下让LLM做推理），当前的LLM，如GPT-4o-2024-11-20，在该数据集上仅能达到7\%的Final Pass Rate; 带think模式的Qwen-3-8B-Instruct模型仅能达到5.9\%的Final Pass Rate；DeepSeek-R1-671B也仅能达到40\%的Final Pass Rate
%【2404】后续hao等人的研究【hao2024large】提出将LLM与专门的Z3 SMT solver【de2008z3】相结合，在进行旅行规划时，LLM通过调用求解器，可以提升在TravelPlanner数据集上的性能。虽然这类涉及专门工具调用的Multi-Agent System能够提升在TravelPlanner数据集上的分数，但是这并未专注于提升LLM本身在解决此类需复杂推理的困难问题上的性能与表现。因为在现实世界中，并非所有的复杂推理规划问题都能够被转化为可被求解器解决的问题，提升LLM自身解决这类复杂推理与规划问题的能力仍然是相当有必要的。
% 后续有一系列基于Multi-Agent System的工作，例如：
%【2405】【zhang2024planning】提出了一种名为 “多约束规划（PMC，Planning with Multi-Constraints）” 的方法，这是一种适用于基于LLM的协作式多智能体系统的零样本方法。该方法通过将含约束的复杂任务分解为层级化的子任务，进而将每个子任务映射为可执行动作，从而简化复杂任务的规划过程, 在TravelPlanner基准测试中，将Final pass rate提升至42.68%
%【2406】【chen2024reprompt】提出了一种名为 REPROMPT 的方法，该方法基于Multi-Agent System中多个LLM的交互与反思过程中获取的对话历史，对提供给 LLM Agent的 Prompt 中的 “逐步指令”（step-by-step instructions）进行优化，在TravelPlanner基准测试中，将Final pass rate提升至3.89%
%【2505】MIRROR【guo2025mirror】利用一个专门的Multi-Agent System，将intra-reflection和inter-reflection机制相结合，在TravelPlanner基准测试中将Final pass rate提升至2.2%
%【2506】【yang2025plan】将TravelPlanner这类旅行规划问题定义为L^{3} planning problem,即long context, long instruction, and long output. 该研究提出了 Multiple Aspects of Planning (MAoP)，MAoP借助strategist从多个维度进行预规划，并为规划模型提供规划蓝图，基于LLM-based Multi-Agent System 展开广域思维，以解决此类复杂的规划问题。
%【2506】 SwarmAgentic【zhang2025swarmagentic】通过从零开始构建Multi-Agent System，并借鉴Particle Swarm Optimization (PSO) 的思想，维持一组候选系统，通过反馈引导的更新方式推动其逐步进化，在TravelPlanner 基准测试中将Final pass rate提升至32.2%。
%【2508】【ou2025analyzing】构建了一个LLM-based Multi-Agent System，通过 “协调智能体”（orchestrator agent）改善了智能体间自由对话协同的作用。“协调智能体” 可引导多智能体系统聚焦于特定子领域，最终在 TravelPlanner 基准测试中的Final pass rate达到 25%
%【2509】【choi2025atlas】提出了一种多智能体框架ATLAS，旨在有效应对真实世界旅行规划任务中约束感知的复杂特性。ATLAS 引入了一套规范化方法，并通过专用机制实现了约束管理、迭代式方案评估以及自适应交错搜索。在 TravelPlanner 基准测试中，ATLAS在test集上将Final pass rate提升至 35%
% 但是这些工作存在多个关键局限：
% 1.这类Multi Agent System往往需要人工为系统中的每个Agent设计大量复杂的Prompt，并人工搭建复杂的工作流，这加大了人工开发难度和人工成本；
% 2.由于系统内存在多轮交互，随着智能体间两两交互增加，开销会越来越大，造成单个问题回答时间被极大延长。这种情况会随着智能体数量的增多，造成交互成本无限增长，从根本上限制了其实际部署和应用。
% 3.这类系统无法端到端地给出最终回答，且由于整个系统内包含多个Agent且交互复杂，因此难以通过端到端的训练来提升性能，训练难度高；
% 4.由于系统内存在多个LLM，导致整个系统占用存储空间过大。如果是调用外部需付费的LLM的API接口，又会因系统内的多轮交互式LLM API调用，造成高额的费用；
% 5.由于需要多轮交互，系统中的多个智能体间存在大量冗余通信，导致计算开销过高；

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 这些是已经翻译过的 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 这些是还没加的 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%【2407】【miin2024smart】具备综合规划能力的代理一直是人工智能领域的长期目标。随着大型语言模型（LLMs）的出现，自然语言处理领域的最新创新已取得显著成效。本研究旨在基于先前TravelPlanner一文（谢等人，文献 1）的研究成果，进一步提升此类大型语言模型的旅行规划能力。我们的目标是探索一种利用大型语言模型改善旅行规划体验的新方法，并重点关注旅行规划中的 sole-planning mode—— 即向代理提供必要的参考信息，其目标是基于这些参考信息制定一份全面的规划方案。尽管这种模式并未完全模拟现实场景，但我们认为，优化旅行规划代理的独立规划能力仍能从整体上提升用户体验。
%为此，我们提出了一个半自动化提示生成框架，该框架将大型语言模型自动生成提示与 “人类参与循环”（human-in-the-loop）相结合，通过迭代优化提示内容来提升大型语言模型的性能。研究结果表明，仅依靠大型语言模型自动生成提示存在局限性，而加入 “人类参与循环” 后，仅经过一次迭代，模型性能便显著提升了 139%

%【2502】【zhang2025narrative】 为提升游客的旅行体验与沉浸感，本文提出一种名为 “叙事向导”（NarrativeGuide）的叙事驱动型旅行规划框架。该框架可为旅行者生成基于地理文化的叙事脚本，为其旅程提供新颖的角色扮演体验。在初始阶段，“叙事向导” 会构建某一城市内景点的知识图谱，随后基于该知识图谱设定世界观、角色背景与故事开端。以此为基础，结合知识图谱为每个景点生成独立的场景单元。在行程规划阶段，“叙事向导” 将叙事驱动型旅行规划建模为一个优化问题，并利用遗传算法（Genetic Algorithm, GA）对行程进行优化。在评估候选行程前，会为每对相邻景点生成过渡脚本，过渡脚本与场景单元共同构成完整脚本。之后，将脚本连贯性、旅行时间与景点评分的加权和作为适应度值，对候选解集进行更新。在实验中，本文融入 TravelPlanner 基准数据集，系统评估了 “叙事向导” 在复杂约束条件下的规划能力；此外，还从叙事连贯性与文化适配性两个维度对其性能进行了评估。结果表明，“叙事向导” 在行程规划与脚本生成两方面均展现出较强的能力


%【2505】【gui2025hypertree】近年来的技术进展大幅提升了大语言模型（LLMs）在处理复杂推理任务时的性能，使其在数学推理、逻辑推理等领域取得了显著成果。然而，这些方法在应对复杂规划任务时仍面临挑战，主要原因包括推理步骤冗长、约束条件多样，以及难以处理多个不同的子任务。为解决这些问题，我们提出了超树规划（HyperTree Planning，HTP）—— 一种新颖的推理范式，该范式通过构建超树结构的规划框架来实现高效规划。超树结构能够让大语言模型灵活运用分治策略进行层级化思考，从而有效拆解复杂推理步骤、适配多样约束条件，并以条理清晰的方式管理多个不同子任务。我们进一步引入了一个自主规划框架，通过迭代优化和扩展超树结构规划框架来完成整个规划过程。实验结果验证了超树规划的有效性：在 TravelPlanner 基准测试中，使用 Gemini-1.5-Pro 模型时，该方法实现了当前最优（state-of-the-art）的准确率，相较于 o1-preview 模型，性能提升了 3.6 倍


%【2508】【fang2025memp】基于大型语言模型（LLMs）的智能体在各类任务中表现出色，但它们的程序性记忆存在明显缺陷 —— 这类记忆要么需人工设计构建，要么嵌入静态参数中，灵活性极差。本研究旨在探索相关策略，为智能体赋予可学习、可更新且能终身持续优化的程序性记忆。研究提出Memp 框架，该框架可将智能体过往的行为轨迹提炼为两种形式：一是细粒度的逐步操作指令，二是更高层次、类似脚本的抽象知识。同时，研究还探究了程序性记忆在 “构建（Build）”、“检索（Retrieval）” 与 “更新（Update）” 三个环节中，不同策略所产生的影响。结合一套能持续对内容进行更新、修正与淘汰的动态机制，Memp 的记忆库可与智能体的新经验同步演进。在 TravelPlanner（旅行规划）与 ALFWorld（交互式模拟环境）两个场景下的实证评估显示：随着记忆库的不断完善，智能体在同类任务中的成功率稳步提升，执行效率也显著提高。此外，由性能更强的模型构建的程序性记忆具备良好的价值留存性 —— 将其迁移至性能较弱的模型中，可使后者的任务表现获得大幅提升。


% 这说明，当前的LLM在此类问题上还存在巨大的提升空间。

% 我们认为，在这类需复杂推理的困难问题上，模型在给出最终回答前的think过程对模型的最终回答质量有很大影响。


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 这些是还没加的 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%