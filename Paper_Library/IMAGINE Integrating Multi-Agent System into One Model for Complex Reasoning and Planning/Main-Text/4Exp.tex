% \section{Experiments}
% \subsection{Experimental Setup}

\newcommand{\uparrowtext}[1]{\textcolor{green!30!black}{\raisebox{0.1em}{\fontsize{7pt}{7pt}\selectfont ↑}\raisebox{0.05em}{\fontsize{7pt}{7pt}\selectfont#1}}}
\newcommand{\downarrowtext}[1]{\textcolor{red}{\raisebox{0.1em}{\fontsize{7pt}{7pt}\selectfont ↓}\raisebox{0.05em}{\fontsize{7pt}{7pt}\selectfont#1}}}
\definecolor{sigmaBG}{HTML}{D9FFDD}
\definecolor{baselineBG}{HTML}{EDEDED}
\definecolor{SequentialBG}{HTML}{D3F3FE}

\section{Experiments}
\subsection{Experimental Setup}
\noindent\textbf{Datasets.}
During the Agentic SFT stage, we utilize the newly generated 4,105 queries (an example of which is shown in Figure \ref{new_query_example}), along with the training set (45 queries) from the original TravelPlanner \cite{xie2024travelplanner} dataset, to construct the training set for the Agentic SFT stage with the method described in the section "Multi-Agent System based Inference Data Generation";
In the Agentic RL stage, we use the training set (45 queries) and the validation set (180 queries) from the original TravelPlanner dataset, totaling 225 queries, as the training set for the Agentic RL stage;
During testing, we consistently select the test set (1,000 queries in total) from the original TravelPlanner dataset as our test set. For testing, we first have the model under evaluation perform inference on the test set, and then use the rule-based evaluation code provided in TravelPlanner to assess the model's inference results.


% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\linewidth]{Main-Text/Figures/queriesv1.pdf}
%     \caption{Caption}
%     \label{fig:placeholder}
% \end{figure}
 \singlespacing

\noindent\textbf{Criteria.}
We use the following six evaluation criteria from \cite{xie2024travelplanner} to assess the performance of different models:
1) \textit{Delivery Rate}: The ratio of successfully generating the final travel plan (Regardless of the quality of the travel plan, as long as it can be successfully generated). 2) \textit{Commonsense Constraint Micro Pass Rate}: The ratio of the number of commonsense constraints passed to the total number of commonsense constraints. 3) \textit{Commonsense Constraint Macro Pass Rate}: The proportion of solutions that completely pass all commonsense constraints among all test solutions. 4) \textit{Hard Constraint Micro Pass Rate}: The ratio of the number of hard constraints passed to the total number of hard constraints.5) \textit{Hard Constraint Macro Pass Rate}: The proportion of solutions that completely pass all hard constraints among all test solutions. 6) \textit{Final Pass Rate}: The ratio of successfully generating the final travel plan that satisfies all constraint conditions. This metric is used to evaluate the model’s ability to produce a plan that complies with practical standards. This is our core evaluation metric.

%     2) \textit{Commonsense Constraint Micro Pass Rate}: The ratio of the number of commonsense constraints passed to the total number of commonsense constraints;
%     3) \textit{Commonsense Constraint Macro Pass Rate}: The proportion of solutions that completely pass all commonsense constraints among all test solutions;
%     4) \textit{Hard Constraint Micro Pass Rate}: The ratio of the number of hard constraints passed to the total number of hard constraints;
%     5) \textit{Hard Constraint Macro Pass Rate}: The proportion of solutions that completely pass all hard constraints among all test solutions;


\singlespacing

\noindent\textbf{Baselines.} Our baselines are as follows:
\begin{itemize}[leftmargin=0.5cm]
    \item \noindent\textbf{Greedy Search}:
To evaluate the performance of traditional search algorithms in TravelPlanner, we adopt the greedy search strategy as one of the baselines. Greedy Search focuses on cost minimization as its core objective. Among transportation options, it selects the one with the lowest cost; for dining, it chooses restaurants with the lowest average expenditure; for accommodation, it selects the cheapest option; and for sightseeing, it arranges attractions by randomly selecting them each day. For a 5-day or 7-day travel plan, select the top 1 to 2 cities as destinations from the returned city search results.
\item  \noindent\textbf{Sole-Planning Mode}:
We focus on the sole-planning mode of the TravelPlan task. In this mode, the model is provided in advance with sufficient and necessary reference information required for reasoning and planning. This setting is used to evaluate the model's ability to perform complex reasoning and planning directly based on the given information. The baselines under the sole-planning mode include the following models and strategies:
\begin{itemize}[leftmargin=0.35cm]
    \item \textit{Models:} 
    GPT-3.5-Turbo, GPT-4-Turbo, GPT-4o (version: 2024-11-20), Mixtral-8×7B-MoE, Gemini Pro, Qwen3-8B-Instruct, DeepSeek-R1, our self-built Multi-Agent System mentioned above, and the qwen3-8b-instruct model trained only with Agentic SFT mentioned above.
    
    \item \textit{Strategies}: Our baselines include the following strategies: Direct, CoT, ReAct, Reflexion, and prompt reflect. Specifically, 
"Direct" refers to prompting the model to directly generate the final travel plan;
"CoT" refers to prompting the model to reason step by step before producing the final travel plan;
"ReAct" refers to prompting the model to solve the task by alternating between Thought, Action, and Observation steps;
"Reflexion" refers to prompting the model to perform self-reflection before generating the final travel plan;
"Prompt reflect" refers to prompting the model to reflect on its previous reasoning before producing the final travel plan.
\end{itemize}
% \begin{itemize}
    % \item 
    
    % \item  

% \end{itemize}
\end{itemize}

% 此外，还有我们设计的带有反思能力的Multi-Agent System;

% one agent

% {\color{blue}We use a single LLM, specifically DeepSeek-R1. First, we generate intermediate reasoning processes and final answers on a total of 4,150 queries, which consist of 4,105 newly produced queries and 45 queries from the original TravelPlanner dataset. Then, we concatenate the query (which includes necessary reference information), the reasoning content generated by DeepSeek-R1, and the final plan generated by DeepSeek-R1 into a single data entry for full supervised fine-tuning (SFT) of the Qwen3-8B-Instruct model. After observing performance improvements following the full SFT training of the Qwen3-8B-Instruct model, we further fine-tuned the SFT model with GRPO (Generalized Reward-based Policy Optimization) training on 225 queries from the original TravelPlanner dataset, consisting of the 45 training queries and 180 validation queries. After training, we used the model to perform inference on the test set of 1,000 queries from the original TravelPlanner dataset and evaluated the model’s reasoning performance.


% The reward function we designed for executing the GRPO training is as follows:We only evaluate the model's final travel plan, disregarding its reasoning process. Specifically, we perform two stages of checks on the model's output:}


\input{Main-Text/4Exp-mainTable}



\noindent\textbf{Models.}
% 我们采用Qwen3-8B-Instruct模型作为训练的基础模型。
We adopt the Qwen3-8B-Instruct model as the base model for training.
% All experiments are conducted on eight H800 GPUs  for efficient reasoning Agentic SFT and Agentic GRPO. The training was conducted on 8 H800 GPUs (80GB each), with a learning rate set to 1.0e-6, a group size of 16, and a batch size of 32.

% Our proposed Multi-Agent System with reflective reasoning capabilities.
% {\color{blue}In our primary setup, \texttt{Qwen2.5-7B-Instruct}~\citep{yang2025qwen3} serves as the foundational generator model, with inference conducted using vLLM~\citep{kwon2023efficient}. The executor is \texttt{GPT-4o-mini-2024-07-18}, accessible via API with a temperature set to 0. For ablation experiments, we substitute the generator with \texttt{Qwen2.5-3B-Instruct} and \texttt{Qwen2.5-14B-Instruct}, while \texttt{GPT-4o-mini-2024-07-18} is  still keeping  as the executor. All experiments are conducted on four H20 GPUs employing LoRA~\citep{hu2022lora} for efficient fine-tuning.}

   % Our proposed Multi-Agent System with reflective reasoning capabilities



\subsection{Main Results}
The main resutls are shown in Table \ref{tab:main-results}.
The experimental results clearly demonstrate that our proposed method, IMAGINE, is highly effective in enhancing the model’s capabilities in complex reasoning and planning. Specifically, compared to the strongest baseline we evaluated against (i.e., the Multi-Agent System), IMAGINE achieved significant improvements across all criteria. On the Commonsense Constraint Micro Pass Rate, IMAGINE reached a pass rate of 99.0375\%, which is nearly perfect and surpasses the strongest baseline by 5.375 percentage points; On the Commonsense Constraint Macro Pass Rate, IMAGINE achieved a pass rate of 92.5\%, outperforming the strongest baseline by a substantial margin of 32.2 percentage points; On the Hard Constraint Micro Pass Rate, IMAGINE achieved a pass rate of 92.271\%, exceeding the strongest baseline by 14.542 percentage points; On the Hard Constraint Macro Pass Rate, IMAGINE achieved a pass rate of 86.9\%, surpassing the strongest baseline by 19.1 percentage points; On the Final Pass Rate, IMAGINE achieved a pass rate of 82.7\%, significantly outperforming the strongest baseline by 36.9 percentage points.

In addition, our model is a single end-to-end small model with a model size of only 8B. While significantly outperforming all baseline models in terms of performance, it also demonstrates considerable advantages in inference efficiency and cost. This further highlights the practical value and strong competitiveness of the IMAGINE method.

\begin{figure}[t]
\centering
\subfigure[Model performance during Agentic SFT training]{\includegraphics[width=0.43\linewidth]{Main-Text/Figures/SFT_on_multi_agentv3.pdf}
\label{SFT}
}
\hspace{0.4cm}
\subfigure[Model Performance during Agentic GRPO training]{\includegraphics[width=0.47\linewidth]{Main-Text/Figures/grpo_from_sft_checkpoint_800_multi_agentv2.pdf}
\label{GRPO}
}
\caption{
The performance during Agentic SFT training and Agentic RL training
}
\end{figure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.5\linewidth]{Main-Text/Figures/SFT_on_multi_agentv3.pdf}
%     \caption{Model performance during full SFT training}
%     \label{SFT_on_multi_agent}
% \end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{Main-Text/Figures/MAS_vs_DS_R1v1.pdf}
    \caption{Comparison of the final travel plan produced}
    \label{Comparison_final_plan_MAS_DS-R1}
\end{figure}



\subsection{In-depth Analysis}
\paragraph{Analysis of SFT and GRPO.}
% 【总起段】：
% 考虑到SFT作为强化学习前的冷启动阶段，需要为RL阶段训练出一个既有一定探索能力，又有一定指令遵循能力的模型。因此，SFT阶段既不能训练得过少，也不能训练得过满。SFT阶段训练得过少时，模型还未对训练数据形成基本的行为对齐，那么在后续的RL阶段，模型将会难以获得正向的奖励信号，很难把模型训练好；反之，若SFT阶段训练得过满，则会导致模型内部的概率分布逐渐坍缩到一种或几种固定的模式上，使模型在RL阶段丧失足够多的探索空间，使模型难以探索到更优的action，这对强化学习来说也是极为不利的。
% 因此，我们选取训练中间的第800个step的checkpoint（checkpoint 800），作为RL训练的起点模型。
%
Considering that Supervised Fine-Tuning (SFT) serves as the warm-up stage before Reinforcement Learning (RL), the model trained during SFT needs to possess both a certain level of exploration capability and instruction-following ability to facilitate effective RL training. Therefore, the SFT stage should be neither under-trained nor over-trained.
%
If the model is under-trained during SFT, it will not have developed a basic alignment with the training data, making it difficult to obtain positive reward signals in the subsequent RL stage, and thus hindering the overall training effectiveness. Conversely, if the model is over-trained during SFT, its internal probability distribution may gradually collapse into one or a few fixed patterns. This results in the loss of sufficient exploration space during the RL stage, making it difficult for the model to discover more optimal actions—an outcome that is highly detrimental to reinforcement learning.
%
Figure \ref{full_sft_loss} shows the loss curve during our Agentic SFT training of the Qwen3-8B-Instruct model. Based on the consideration above, we selected the checkpoint at step 800 (checkpoint 800) during the intermediate phase of Agentic SFT training as the starting model for our Agentic RL training. 
% As shown in Table \ref{tab:main-results}, the performance of GRPO is better than that of SFT.

% 【Analysis of SFT】：

% 我们利用前述我们设计的Multi-Agent System 所产生的 Inference Data ，在8卡H800（80G）上，对Qwen-3-8B-Instruct模型进行full sft训练，learning_rate设为1.0e-5
% 下图为在full SFT训练过程中，每隔200个checkpoint在TravelPlanner的test数据集（1000条query）上测得的结果

% \noindent\textbf{Model performance during full SFT training}
\singlespacing

\noindent\textbf{Model performance during Agentic SFT training.}
Utilizing the inference data generated by the Multi-Agent System as described in the previous section on "Multi-Agent System based Inference Data Generation", we performed full SFT on the Qwen-3-8B-Instruct model using eight H800 GPUs (each with 80GB memory), with the learning rate set to 1.0e-5. Figure \ref{SFT} presents the test results on the TravelPlanner test dataset (1,000 queries) for every 200 checkpoints during the full SFT training process.
%

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.5\linewidth]{Main-Text/Figures/grpo_from_sft_checkpoint_800_multi_agentv2.pdf}
%     \caption{Model Performance during GRPO training}
%     \label{grpo_from_sft_checkpoint_800_multi_agent}
% \end{figure}







% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.5\linewidth]{Main-Text/Figures/full_sft_lossv2.pdf}
%     \caption{Performance of loss in full SFT training}
%     \label{full_sft_loss}
% \end{figure}

% 【Analysis of GRPO】：

% 我们在TravelPlanner原数据集的train集（45条query data）和validation集（180条query data），共计225条query data上，在8卡H800（80G）上，对上面full sft后的模型继续进行我们的Agentic GRPO训练。训练时，learning_rate设为1.0e-6，组大小设为16，batch size设为32

% 从图中可以观察到，模型在TravelPlanner的test dataset（1000条query）上，的Commonsense Constraint Micro Pass Rate、Commonsense Constraint Macro Pass Rate、Hard Constraint Micro Pass Rate、Hard Constraint Macro Pass Rate 以及 Final Pass Rate 均随着我们 Agentic GRPO训练的进行而逐渐提升。
\singlespacing

\noindent\textbf{Model Performance during Agentic GRPO training.}
We continued reinforcement learning training on the checkpoint at step 800 (checkpoint 800), which had been previously trained using the Agentic SFT approach, by applying the GRPO algorithm. In this stage, we used our custom-designed rule-based reward function as the reward signal. The training data for the Agentic RL stage consisted of 225 queries in total, comprising the training set (45 queries) and validation set (180 queries) from the original TravelPlanner dataset. The training was conducted on 8 H800 GPUs (each with 80GB of memory), with the learning rate set to 1.0e-6.
Figure \ref{GRPO} shows the evaluation results on the TravelPlanner test dataset (1,000 queries), conducted every 10 steps during the reinforcement learning process. As illustrated in the figure, during Agentic GRPO training, the model demonstrated consistent improvements across all evaluation metrics, including Commonsense Constraint Micro Pass Rate, Commonsense Constraint Macro Pass Rate, Hard Constraint Micro Pass Rate, Hard Constraint Macro Pass Rate, and Final Pass Rate.


% 


% \textbf{Analysis of data with different levels of difficulty}



% \textbf{Analysis of One-Agent and Multi-Agent}

% 为了验证我们设计的Multi-Agent System生产出的final travel plan是否优于单独的DeepSeek-R1-671B模型所产生的final travel plan，我们根据模型给出的final travel plan在格式、Commonsense Constraint以及Hard Constraint上的满足程度，对这两者所产生的final travel plan进行了严格对比，结果如图所示：

% 图中红色数据表示我们设计的Multi-Agent System所产生出的final travel plan优于单独的DeepSeek-R1-671B模型所产生的final travel plan的个数；蓝色数据表示我们设计的Multi-Agent System 所产生出的final travel plan比单独的DeepSeek-R1-671B模型所产生出的final travel plan更差的个数；
% 从图中我们可以观察到，除3day-easy数据以外，我们设计的Multi-Agent System生产出的final travel plan整体上均优于单独的DeepSeek-R1-671B模型所产生的final travel plan
\singlespacing

\noindent\textbf{Comparison of the final travel plan.}
To verify whether the final travel plan generated by our designed Multi-Agent System outperforms that generated by the standalone DeepSeek-R1-671B model, we conducted a rigorous comparison based on format compliance, commonsense constraint satisfaction, and hard constraint satisfaction. The detailed results are shown in Figure \ref{Comparison_final_plan_MAS_DS-R1}.
In the figure, the red bars represent the number of cases where the final travel plan generated by the Multi-Agent System is strictly superior to that generated by the standalone DeepSeek-R1-671B model, while the blue bars indicate the number of cases where it is strictly inferior. As observed from the figure, except for the 3day easy dataset, the final travel plan produced by the Multi-Agent System generally outperforms that of the standalone DeepSeek-R1-671B model. This demonstrates the effectiveness of our designed Multi-Agent System for this task.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{Main-Text/Figures/full_sft_lossv3.pdf}
    \caption{The loss during Agentic SFT training}
    \label{full_sft_loss}
\end{figure}
%
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{Main-Text/Figures/Self-Reflection_No_Error_Detected_Ratev1.pdf}
    \caption{Proportion of "no error" in reflections}
    \label{Self-Reflection_No_Error_Detected}
\end{figure}


\singlespacing

\noindent\textbf{Reflection Analysis.}
We also evaluated the proportion of instances where the model determined that no errors existed in its previous reasoning during the Agentic GRPO training process after Agentic SFT, assessed at every 10 steps. Our evaluation was conducted on the original TravelPlanner test set, which consists of 1,000 queries. The specific results are shown in Figure \ref{Self-Reflection_No_Error_Detected}.
Combining Figure \ref{Self-Reflection_No_Error_Detected} and Figure \ref{GRPO}, it can be observed that as GRPO training progresses, the model increasingly tends to believe that there are no errors in its previous reasoning during the reflection phase, and the quality of the final travel plan generated by the model also gradually improves.
