\begin{abstract}
\quad \quad Although large language models (LLMs) have made significant strides across various tasks, they still face significant challenges in complex reasoning and planning. For example, even with carefully designed prompts and prior information explicitly provided, GPT-4o achieves only a 7\% Final Pass Rate on the TravelPlanner dataset in the sole-planning mode. Similarly, even in the thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass Rates of 5.9\% and 40\%, respectively. Although well-organized Multi-Agent Systems (MAS) can offer improved collective reasoning, they often suffer from high reasoning costs due to multi-round internal interactions, long per-response latency, and difficulties in end-to-end training.
To address these challenges, we propose a general and scalable framework called IMAGINE, short for Integrating Multi-Agent System into One Model. 
This framework not only integrates the reasoning and planning capabilities of MAS into a single, compact model, but also significantly surpass the capabilities of the MAS through a simple end-to-end training.
% Our method consists of three key stages: New Query Generation, Multi-Agent System based Inference Data Generation, and Agentic Reasoning Training. New Query Generation expands the diversity of query data in the training set by generating novel queries; Multi-Agent System based Inference Data Generation using MAS to generate inference data for queries produced in the previous stage; Agentic Reasoning comprises two components: Agentic SFT, which fine-tunes a single model using the MAS-based inference data to inject the reasoning abilities and behaviors of a MAS into a single model; Agentic RL, which further enhances the modelâ€™s agentic reasoning abilities through end-to-end reinforcement learning, ultimately enabling the single model to surpass the performance of the original MAS.
Through this pipeline, a single small-scale model is not only able to acquire the structured reasoning and planning capabilities of a well-organized MAS but can also significantly outperform it. Experimental results demonstrate that, when using Qwen3-8B-Instruct as the base model and training it with our method, the model achieves an 82.7\% Final Pass Rate on the TravelPlanner benchmark, far exceeding the 40\% of DeepSeek-R1-671B, while maintaining a much smaller model size.
\end{abstract}