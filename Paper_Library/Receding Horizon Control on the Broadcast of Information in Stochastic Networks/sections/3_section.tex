
\subsection{Control Strategy}
\label{sec:control_strategy}
To focus on the transmission of information over the
network and understand the ability to 
shape the broadcast of information, we make the following
assumption:
\begin{assumption}
The communication graph has a directed
spanning tree with a root corresponding
to the robot that is the origin
of the information to be transmitted.
\end{assumption}
This assumption implies that each edge
of the spanning tree has a positive transmission 
rate (\textit{i.e.,} the information is propagated with 
some
positive baseline transmission).
In this scenario, it is clear that the
broadcast of information will eventually reach 
every robot on the network with probability $1$.
%\thales{Explain how this scenario can be similar
%to other works, gyro like, Xi work, task allocation.}
Hence, the main objective of 
our control problem can be viewed as an attempt
to shape the statistics of the broadcast of 
information on the network to satisfy a given
convergence criterion.
It is worth noticing that
most works on contact processes are concerned
with regular lattices or mean-field models \cite{Philip2018,Preciado2014}.
Also, usually, the desired convergence rate is 
performed through static strategies, targeting
central nodes, and using heuristics \cite{Preciado2014,shen2022topology}.
With our control strategy, the network can
adapt to the stochastic realization of the propagation
of information and only expend resources to actuate during
realizations that might interfere with the team's desired
behavior.

With the broadcast model in \eqref{eq:markov_2n},
we control the transmission rates $\omega_{ij}(t)$
by applying allowable actions based on the current
state of the network and on predicted future states,
sampled on predefined times, following
a sampled-data predictive controller \cite{Pantelis2014}.
The controller we study in this paper
is defined by the following optimization problem:
\begin{align}
\label{eq:pred_ctrl}
& \min_{{\omega_{ij}}\in~U} C(X_i(t))
\\
&~s.t.~ 
\big(1-E\big[X_i(t+\Delta t):X(t)\big]\big)
-\big(1-X_i(t)\big)e^{-r\Delta t}\leq 0,
\nonumber
\end{align}
where $E\big[X_i(t+\Delta t):X(t)\big]$ is the
prediction stage which is 
computed using \eqref{eq:markov_2n}
(later we utilize a moment-closure to compute its
upper-bounds),
$C(X_i(t))$ is the cost of applying the
control action ${\omega_{ij}}$ on the transmission
between nodes $i$ and $j$, $\Delta t>0$ defines a
window of prediction from the current instant
$t$, the positive constant $r$ defines a
desired exponential convergence rate, and $U$ represents
the set of admissible control actions.
In practice, the set of admissible control
actions should represent the operations that robots 
can perform to adjust their
transmission rate among neighbors.
For example, if the team of
robots operate in
geographically disjoint locations and travel back
and forward regularly to neighbors' sites and carry
information with them,
one could define an increase in the transmission
rate as an increase in the frequency the agents should
visit each other. Another example is in the
case of periodic rendezvous, in which 
admissible actions could map the range of possible
changes in the agent's period.

We draw attention to the fact
that ensuring the constraints in 
\eqref{eq:pred_ctrl} plays a central role in guaranteeing
the fulfillment of the desired
performance. We notice that for the general case
it might be difficult to guarantee 
feasibility of problem
\eqref{eq:pred_ctrl}. Therefore, in this
work we {\it assume} that there is an auxiliary
control law that provides feasibility and that the 
controller has access to it. 
This assumption is not uncommon in the nonlinear
model predictive literature (see \cite{Ellis2014} and references
therein), typically such an auxiliary controller is 
provided {\it ad hoc}.
Nonetheless, in the problems we are interested in,
we notice that in several 
applications
it is possible to find feasible, but high-priced,
solutions for the problem. For example, in tasks where 
robots carry the message from site to site intermittently
we might satisfy the constraints by bringing all robots
closer to each other; in the case of robots performing
tasks on cyclic paths
we could make all robots cover overlaying paths.
Hence, even though it is necessary to provide an
auxiliary control strategy according to each task, we notice
that in the problem of broadcasting information simply
defining a feasible strategy is not necessarily
interesting. Therefore, we seek to apply
admissible control actions that do not lead to
trivial connectivity on the network and preserve
a satisfactory performance.

\subsection{Expected Time to Broadcast Information}
The feasibility of the optimization 
in \eqref{eq:pred_ctrl}
guarantees that the information is transmitted 
throughout the
network exponentially fast.
However, a remaining open question
is how can we infer
how long it will take
until all robots in the networks
receive the message of interest.
In other words, what is the hitting time for
the stochastic process to reach the stationary
state.
In this section, we analyze this question and provide
the expectation for the propagation time.

Given that we have the {\it exact}
representation of
the Markov chain that describes the
underlying stochastic
process we could, in principle,
compute the hitting times directly. However, since
we solve the receding optimization problem 
\eqref{eq:pred_ctrl}, our Markov chain is
non-homogeneous and its transition rates
might
change according to each realization.
Therefore, we take advantage of the
constraint in 
problem \eqref{eq:pred_ctrl}
relying on the assumption 
previously discussed that
a feasible solution can always be found and
compute the expected time to broadcast
information based on Watkins {\it et al.}
(Please, note that the method shown below
is similar to Theorem 5 in \cite{Watkins2020}.)

Summing the constraint in \eqref{eq:pred_ctrl}
over the $n$ nodes gives,
\begin{align*}
    n-E\big[\Scal(\boldsymbol{X}(t+\Delta t))\big]
    \leq (n-\Scal(\boldsymbol{X}(t))e^{-r\Delta t},
\end{align*}
where $\Scal(\boldsymbol{X}(t))=\sum_i^n X_i(t)$
is the number of informed robots 
at time $t$. 
The feasibility of the problem ensures that
\begin{align}
    n-E\big[\Scal(\boldsymbol{X}(t))\big]
    \leq (n-\Scal(\boldsymbol{X}(0))e^{-r t}.
    \label{eq:identity_from_constraint}
\end{align}
Define the broadcast time as 
$t_b=\inf\{t\geq 0:\Scal(\boldsymbol{X}(t))=n\}$,
then
its expectation can be computed as
\begin{align}
    E[t_b:\boldsymbol{X}(0)]
    =\int^{\infty}_0 1-{\rm Pr}(t_b\leq \sigma)d\sigma.
    \label{eq:expt_tb}
\end{align}
Note that ${\rm Pr}\{t_b\leq t\}={\rm Pr}\{\Scal(\boldsymbol{X}(t))=n\}$, therefore the
equivalence
$1-{\rm Pr}\{t_b\leq t\}
={\rm Pr}\{\Scal(\boldsymbol{X}(t))<n\}$ holds,
where ${\rm Pr}\{\Scal(\boldsymbol{X}(t))<n\}$ is
the probability of having any robot 
non-informed at
time $t$. We use this equality and
\eqref{eq:identity_from_constraint}
to
compute the expectation time for
the broadcast of information.
Note that the right-hand side of 
\eqref{eq:identity_from_constraint}
bounds the expected number of the existence
of non-informed robots, therefore
\begin{align*}
    {\rm Pr}\big\{\Scal(\boldsymbol{X}(t))<n
    :(\boldsymbol{X}(0))\big\}\leq
    \min \big\{1,(n-\Scal(\boldsymbol{X}(0))e^{-r t}\big\},
\end{align*}
hence, \eqref{eq:expt_tb} yields
\begin{align*}
    E[t_b:\boldsymbol{X}(0)]
    \leq & {~}
    \int^{\infty}_0 
    \min \big\{1,(n-\Scal(\boldsymbol{X}(0))
    e^{-r \lfloor \frac{\sigma}{\Delta t}
    \rfloor\Delta t}\big\}
    d\sigma
    \nonumber
    \\
    \leq & {~}
    \tau_1 + 
    \sum_{i=\frac{\tau_1}{\Delta t}}^\infty
    (n-\Scal(\boldsymbol{X}(0))\Delta t
    e^{-r \lfloor \frac{\sigma}{\Delta t}
    \rfloor\Delta t}
\end{align*}
\begin{align}
    \leq & {~}
    \tau_1 + 
    \frac{e^{-r\tau_1}}{1-e^{-r\Delta t}}
    (n-\Scal(\boldsymbol{X}(0))\Delta t,
\end{align}
with $\tau_1=\inf\{t\geq
0:(n-\Scal(\boldsymbol{X}(t)))e^{-r\lfloor \frac{t}{\Delta t}
    \rfloor\Delta t}\leq 1\}$.


\subsection{Robust Moment Closure}
\label{sec:robust_moment_closure}
In this section we consider a moment closure technique to
solve Problem $1$ with a computational amenable
method. We reiterate that, while \eqref{eq:markov_2n} 
is a theoretically right representation for our problem, it is not
computationally tractable for relatively medium-size networks due to
the necessity to use $2^n$ states to capture the evolution of the
probabilities in the network.
The technique considered in this section was introduced 
in \cite{Watikins2018} for the problem of continuous-time epidemics.
The approach is based on the Fr\'echet inequalities,
defined as
\begin{align}
    \FF_{\rm lwr}({\rm Pr}\{A\},{\rm Pr}\{B\})=&{~}\max \{0,{\rm Pr}\{A\}+{\rm Pr}\{B\}-1\},
    \\
    \FF_{\rm upr}({\rm Pr}\{A\},{\rm Pr}\{B\})
    =&{~}\min \{{\rm Pr}\{A\},{\rm Pr}\{B\}\},
\end{align}
which are lower- and upper-bounds for the joint probability ${\rm Pr}\{A,B\}$, respectively.
Its main
advantage over mean-field approximations
is that it gives meaningful bounds for the evolution of the
real mean of the underlying stochastic process (see \cite{Watikins2018} for a detailed discussion).

Initially, we need to reformulate our representation to
consider the marginal 
probabilities of each robot receiving 
the information, since
we are interested in drawing bounds on their evolution.
For our problem,
the marginal probabilities can be written as the expectation of
a robot becoming informed as a function of its neighbors as,
\begin{align}
    \frac{dE[X_i(t)]}{dt}=\sum_{j\in \NN_i}E[\bar X_i(t)X_j(t)]\omega_{ij}(t),
    \label{eq:marginal_de}
\end{align}
for all $i\in \VV$, where $\bar X_i(t)$ is
 the complement of $X_i(t)$ ({\it i.e.,} 
 $\bar X_i(t)=1$ if $X_i(t)=0$).
 This equation maps an infinitesimal
probability of $X_i$ receiving the information
given that some neighbors $X_j$, $j\in\NN_i$, have the message.
Notice that \eqref{eq:marginal_de} is not closed, therefore
we cannot solve it for $E[X_i(t)]$ without equations for 
$E[\bar X_i(t)X_j(t)]$.
Central to the result in \cite{Watikins2018}
is to bound the joint probabilities of the form
$E[\bar X_iX_j]$.
Next, we show 
the optimal approximations for the
expectations we employ,
which can be expressed as closed-form functions.
\begin{lemma}[Watkins {{\bf et al.,}} \cite{Watikins2018}]
Let $\underbar{x}(0)=\boldsymbol{X}(0) = \overline{x}(0)$,
define $\Scal(\boldsymbol{X})$ as the number of nodes
informed in ${\boldsymbol X}$. It holds
that
\begin{align}
    E[\Scal(\boldsymbol{X}(t)):\boldsymbol{X}(0)]
    \leq & {} \sum_{i\in \VV}\min \Big\{
    \overline{x}_i^{\rm inf}(t),
    1-\underbar{x}_i^{\rm non}(t)\Big\}, \text{ and}
    \\
    E[\Scal(\boldsymbol{X}(t)):\boldsymbol{X}(0)]
    \geq & {} \sum_{i\in \VV} \max \Big\{
    \underbar{x}_i^{\rm inf}(t),
    1-\overline{x}_i^{\rm non}(t)\Big\},
\end{align}
where the variables $\overline{x}^{\ell}_i(t)$ and
$\underbar{x}^{\ell}_i(t)$, for each
$\ell\in\mathcal{L}=\{{\rm inf},
{\rm non}\}$ are the solutions of the following 
ordinary differential equations,
\begin{align}
    \label{eq:momentcls_upper}
   \dot{\overline{x}}_i^\ell=&{}\sum_{j\in\NN_i} 
   \sum_{\ell'\in\mathcal{L}}\FF_{\rm upr}(\BB_{\overline{x}_i^{\ell}}
   (\overline{x}^{\ell'}_i),\overline{x}_j^\ell)\omega_{ij}(t),
   \\
    \label{eq:momentcls_lower}
   \dot{\underbar{x}}_i^\ell=&{}\sum_{j\in\NN_i} 
   \sum_{\ell'\in\mathcal{L}}\FF_{\rm lwr}(\underbar{x}^{\ell'}_i,\underbar{x}_j^\ell)\omega_{ij}(t),
\end{align}
and $\BB_{\overline{x}_i^{\ell}} (\overline{x}^{\ell'}_i)
=\min\big\{1-\overline{x}_i^{\ell}, \overline{x}^{\ell'}_i\}$
is the upper-bounding operator.
In addition, the following inclusion is always satisfied
\begin{align*}
    E[{X}^\ell_i(t):\boldsymbol{X}(0)]
    \in \big[
    \underbar{x}_i^{\ell}(t),
    \overline{x}_i^{\ell}(t)\big]\subseteq\big[0,1\big],
\end{align*}
for all $t\geq0$.
\label{lemma:moment_cls}
\end{lemma}
The result in Lemma \ref{lemma:moment_cls} allows us to
compute bounds for the evolution of the probabilities 
of the broadcast of information in the network using
$4n$ differential equations (equations \eqref{eq:momentcls_upper}
and \eqref{eq:momentcls_lower} for each informed
and non-informed node), instead of $2^n$.
This enables us to apply the receding horizon
strategy in equation \eqref{eq:pred_ctrl} in larger networks.
