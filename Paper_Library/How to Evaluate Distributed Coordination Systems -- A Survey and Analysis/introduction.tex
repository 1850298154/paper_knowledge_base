Cloud and web-based big-data applications such as search engines, social networks, video streaming platforms, file-sharing tools, and the Internet of Things (IoT) are implemented as distributed systems, where a collection of nodes cooperate to achieve a common task for increased performance, availability, and scalability purposes. Developing, debugging, and evaluating such distributed systems have been a challenging task due to the coordination needed between the participating nodes of such systems. 

Distributed coordination is required for various purposes, including synchronization, locking, group membership, ownership, and reaching consensus. For the last three or four decades, the distributed systems community has developed protocols and services to achieve robust and scalable coordination in distributed applications. 
%
As one of the earliest efforts in this area, Paxos~\cite{Paxos} protocol was introduced by Lamport. The basic Paxos protocol gave rise to many variations and follow-up work, such as Disk Paxos~\cite{Disk-Paxos}, Cheap Paxos~\cite{Cheap-Paxos}, Fast Paxos~\cite{Fast-Paxos}, Generalized Paxos~\cite{Generalized-Paxos}, and Raft~\cite{RAFT}. To decrease the latency in Paxos communication and scale it to wide-area network (WAN) settings, many extensions were proposed, such as Mencius~\cite{Mencius}, Flexible Paxos (FPaxos)~\cite{Flexible-Paxos}, Egalitarian Paxos (E-Paxos)~\cite{E-paxos}, WPaxos~\cite{WPaxos}, and SwiftPaxos~\cite{ryabinin2024swiftpaxos}.

The difficulties in dealing with low-level Paxos and its variants in application development led to systems like ZooKeeper~\cite{Zookeeper}, Chubby~\cite{Chubby}, Tango~\cite{Tango}, and WanKeeper~\cite{Wankeeper}, which abstracted away distributed coordination and provided frequently used coordination primitives as a service to the application developers. Many distributed applications and especially high-demand distributed data stores such as Google's Spanner~\cite{Spanner}, Yahoo!'s PNUTS~\cite{PNUTS}, Apache's Mesos~\cite{Mesos} and Kafka~\cite{KAFKA}, and Twitter's Manhattan~\cite{Manhattan} were built on top of such coordination services, each requiring different levels of synchronization, consistency, and availability guarantees. 

Whether it is the development of a distributed coordination protocol or service or the development of an application employing these protocols/services, one of the significant challenges the developers face
is the lack of standard benchmarking tools that could provide a comprehensive evaluation of the coordination framework -- in terms of its performance, availability, scalability, and consistency guarantees. Currently, most developers use ad-hoc limited microbenchmarks to evaluate only a fraction of the functionality and use customized metrics and techniques. This results in a lack of comparability between coordination mechanisms and may lead to unfair claims of advantages over the competition.
% As an example, ZooKeeper uses throughput and latency as the main evaluation metrics, whereas Chubby focuses on availability and consistency. Although we know that ZooKeeper trades off consistency for improved performance (what??? elaborate?), there is no way to evaluate how much consistency is sacrificed for how much improvement in performance. 

In this paper, we study, analyze, and compare how different coordination systems are evaluated. More specifically, we try to answer the following questions: {\em What are the different evaluation metrics and parameters used by popular consensus algorithms, coordination services, and distributed applications? 
What are the major challenges and general requirements for achieving end-to-end benchmarking in distributed coordination systems? What functionality do the existing benchmarking systems provide, and where do they fall short in meeting the evaluation requirements of the distributed coordination systems?} 

The rest of this paper is organized as follows: Section 2 provides an analysis of the current evaluation practices for distributed coordination systems; Section 3 discusses the general requirements of a comprehensive benchmarking suite for distributed coordination; Section 4 presents the capabilities of existing benchmarking/testing frameworks in this area; Section 5 discusses the related work; and Section 6 concludes the paper.  