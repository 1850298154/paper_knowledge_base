Increasing demand in web-based big-data applications brought out the need for efficient use of distributed systems, which highly depend on the adequate implementation of distributed coordination. The distributed systems community has developed different protocols, coordination services, and distributed applications built on top of these services to satisfy this rapid growth in big data applications. However, due to the lack of a standard benchmarking tool, developers generally opt to use ad-hoc evaluation mechanisms and microbenchmarks. Hence, the evaluation of these systems has been very limited, resulting in inadequate and misleading measurements and an unfair comparison of the competing systems. In this paper, we have analyzed and compared well-known and widely used distributed coordination services, their evaluation mechanisms, and the tools used to benchmark those systems. We have identified the essential requirements of distributed coordination service benchmarking, such as the metrics and parameters for the evaluation of the performance, scalability, availability, and consistency of these systems.

Despite the commonalities in tuning parameters like read/write ratio, data access overlap, or size of data objects for performance measurements, access locality and number of data objects are not extensively evaluated by most of the systems. These two parameters, on the other hand, may have a significant impact on the system performance for systems that allow concurrent writes, since they are the main factors in producing conflicting or colliding commands. Similarly, they are critical for managing ownership of data objects for systems using ownership mechanisms to resolve conflicts. While most of the systems are evaluated with a single value of read/write ratio and size of data objects, tuning these parameters for different values is essential to understand the system performance extensively for different types of workloads. Depending on the type of applications using these services, the percentage of update operations or the size of data objects may change dramatically. Likewise, data access overlap is evaluated only for systems sharing the entire data space, but it is also a system-specific parameter that needs to be analyzed for different scenarios. 

% Overall, the ideal benchmark generates workloads from multiple clients on the WAN scale while exposing control over objects, read/write ratio, access overlap, access locality, and request conflicts.

The benchmarks discussed in this work do consider some of these tuning parameters, but often do not prioritize them when evaluating coordination algorithms and services. Benchmarks on performance often neglect scalability, data access overlap, and request conflicts as seen in YCSB++ \cite{ycsb++}, BG \cite{bg}, and BenchFoundry \cite{benchFoundry}. Some benchmarks on availability neglect to test for node failures, clock drift, and network partition altogether, as seen with UPB \cite{upb} and Chaos Monkey \cite{chaosmonkey}. Linearizability is often challenging to evaluate and needs more attention. To overcome these limitations with the existing benchmarks, some systems (such as ScalarDB \cite{ScalarDB} and CockroachDB \cite{cockroach})
combines multiple existing benchmarks to achieve the desired functionality. Most of the other systems considered in this study prefer using their own specially tailored microbenchmarks instead.