\hfill\\
Performance evaluation is the most common type of system benchmarking. Studying the performance typically involves pushing some significant workload through the system and measuring the latency and the throughput the system was able to sustain. 

\begin{table*}[h]
\centering
\begin{tabular}{l|l| l|l| l| l| l|}
\cline{2-7}
& \multirow{2}{*}{\textbf{System}} 
&\textbf{\Longunderstack{Write \\Ratio \%}}
& \textbf{\Longunderstack{Data \\Access\\Overlap \%}} & \textbf{\Longunderstack{Access \\Locality \%}}& \textbf{\Longunderstack{\# of\\ Objects}}& \textbf{\Longunderstack{Size of\\ Objects}}\\
\hline \hline
\multirow{13}{*}{\textbf{\begin{turn}{90}Algorithms\end{turn}}}
& Mencius~\cite{Mencius} & 50 &100 &NS  &16,128,1024 &6B,4KB \\
& FPaxos ~\cite{Flexible-Paxos} & NS &\xmark &\xmark &NS &64B\\
& Raft ~\cite{RAFT} & \xmark &\xmark &\xmark &\xmark &\xmark\\
& Multi-Paxos ~\cite{Multi-Paxos} &100 &100 &100 &NS &5B,8KB, 32KB\\
&Hybrid Paxos ~\cite{Hybrid-Paxos} &100 &100 &\xmark &1 &NS \\
&E-Paxos~\cite{E-paxos} &100 &100 &NS &NS &16B,1KB \\
&$M^2$ Paxos ~\cite{M2-Paxos} &NS &2-100 &0,100 &1,100,1000 &16B \\ 
&Bizur ~\cite{Bizur} &100 &NS &\xmark &1-$2^{13}$ &50B \\
&ZAB ~\cite{ZAB} &NS &100 &\xmark &NS &1KB \\
&WPaxos ~\cite{WPaxos} &100 &NS &70, 90 &1000 &NS \\
&SwiftPaxos ~\cite{ryabinin2024swiftpaxos} &0,5,20 &0-100 &\xmark &$10^6$ &1KB-8KB \\
&Omni-Paxos ~\cite{omnipaxos} &100 &100 &\xmark &500, 5K, 50K &8B \\
&Hydra ~\cite{Hydra}&100, 50 &0-100 &NS &NS &NS \\
 \hline
\multirow{11}{*}{\textbf{\begin{turn}{90}\Longunderstack{Coordination \\Services}\end{turn}}}
&ZooKeeper ~\cite{Zookeeper} &0-100 &100 &\xmark &NS &1KB \\
&Tango ~\cite{Tango} &\Longunderstack[l]{0,10,50\\90,100} &100 &0-100 &10-10M &4KB \\
&Calvin ~\cite{Calvin} &0-34~\cite{TPCC_char} &NS &90 &NS &NS  \\
&WanKeeper ~\cite{Wankeeper} &0-100 &0-100 &0,100 &NS &NS \\
&ZooNet ~\cite{Zoonet} &\Longunderstack{75,50,10,1,0} &NS  &NS &NS &1KB \\ 
&Boki ~\cite{Boki} &0-100 &0-100 &NS &NS &1KB \\
&FlexLog ~\cite{Flexlog} &5-95 &0-100 &NS &NS &1KB, 64B - 8KB \\
&SplitFT ~\cite{SplitFT} &0-100 &NS &NS & $10^7$-$10^8$ &128B - 8KB \\
&Fabric ~\cite{HyperledgerFabric} &50-100 &100 &NS & $2^{19}$ &0.5MB-4MB \\
&Narwhal ~\cite{Narwhal} &100 &NS &NS & NS &0.5MB \\
 \hline
\multirow{7}{*}{\textbf{\begin{turn}{90}Applications\end{turn}}}
&Spanner ~\cite{Spanner} &0,100 &NS &NS &NS &4KB \\
&DistibutedLog ~\cite{DistributedLog} &NS &100 &NS &NS &1KB \\
&PNUTS ~\cite{PNUTS} &10,0-50 &NS &80 &NS &NS \\
&COPS ~\cite{COPS} &50,25 &100 &0-100 &$2^{18}$, 512/client &1B \\
&CockroachDB ~\cite{cockroach} &0-34~\cite{TPCC_char} &100 &90 &1000, 10000 &NS \\
&OceanBase ~\cite{oceanbase} &0-34~\cite{TPCC_char} &NS &NS & $2^{40}$ &4KB - 8KB\\
&ScalarDB ~\cite{ScalarDB} &0-50~\cite{YCSB, TPCC_char} &0-100 &NS &$2^{27}$ &1KB \\
\hline
\end{tabular}
\caption{Workload parameters used in performance evaluations (\xmark: Not available or applicable for experiments conducted on this system; NS: Details are not specified).
}
\label{tab:performance}
\end{table*}

{\bf Throughput} measures how many requests or commands a system can process in a unit of time. Higher throughput capabilities allow the system to process more requests and larger amounts of data. To measure the maximum throughput, systems are saturated with an increasing workload until they reach the server's CPU or network capacity.

{\bf Latency} is a measure of request execution time. It is often measured over a significant number of requests, allowing the calculation of average, median, and various percentile's for the metric. Low latency is desirable since it means the system spends little processing power or I/O capacity to handle a request. Latency for a given system is often correlated with throughput, as low request latency allows us to push more requests through the system in unit time.

Latency distribution across the requests is also very important.  For instance, the average latency measurement can give some sense of the performance; however, it does not describe the full picture that includes all requests the system serves. It may be the case that an average or median latency is low while some significant portion of requests performs poorly. Most systems give the results only for average, median, or aggregated latency without providing any information about the tail latency. This makes it more difficult to understand how a system works in performance corner cases. To this matter, providing the distribution of the cumulative latencies as in WPaxos and OceanBase or giving median or average and high percentile latency together as in E-Paxos, Bizur, WPaxos, and Boki yields a more comprehensive understanding of the systemâ€™s performance.

Systems follow different approaches for evaluating performance. Although the majority of the systems evaluate the throughput and latency together, some systems, such as Calvin, ZooNet, Multi-Paxos, and Omni-Paxos, measure only throughput, and systems such as ZooKeeper and Spanner evaluate the throughput and latency separately. Evaluating latency and throughput together is essential to understanding the system's overall performance. While the throughput is mainly related to the system's processing capacity, latency is more affected by I/O operations \cite{latency-throughput}.

The performance of any distributed coordination system highly depends on workload characteristics such as read/write ratio, data access overlap, access locality, the number of data objects (size of data pool), and the size of data objects. In table \ref{tab:performance}, we list how the systems tuned their workloads for these parameters.

\paragraph{Read/Write Ratio}
\label{S:RWratio}
Read/Write Ratio is a fundamental parameter to show the system's performance under different use case scenarios. Since many systems handle write and read requests differently, and the percentage of write and read requests is application-specific, it plays a crucial role in showing the applicability of the systems for various purposes. For instance, systems designed to be used for different aspects of distributed coordination, like ZooKeeper or WanKeeper, show the performance evaluation continuously while varying the write ratio from 0 to 100\%. Similarly, Spanner measures the read and write performance separately since read requests can be handled by replicas, but writes need to be done by the leader. Besides being handled by the leader, writes require replication, which increases commit latency. To measure the true performance of replication, Multi-Paxos also uses 100\% write operations. Due to the differences in processing read requests between systems used for comparison in the evaluations, Bizur uses only write operations. E-Paxos and Hybrid Paxos also use 100\% writes since they focus on handling conflicting commands, and read requests do not produce conflicts. Some works like Calvin, CockroachDB, and OceanBase do not explicitly provide a write ratio for their evaluations but use standard benchmarks like TPC-C \cite{TPCC_char} and YCSB \cite{YCSB} with known write ratios.

\paragraph{Data Access Overlap}
\label{S:DataAccessOverlap}
Data access overlap may have a major impact on the throughput and latency in multi-leader systems and protocols. It can be defined as the percentage of the key space shared by all clients. In this matter, data access overlap is 100\% if the entire key space is shared and all clients are allowed to access any key. On the other hand, 
data access overlap is 0\% if the key space is partitioned for each client and clients are accessing only the keys in their partition. For single leader algorithms providing strong consistency like Multi-Paxos, ZAB, Chubby, and ZooKeeper, varying data access overlap has no impact on the overall performance since all write requests are treated in the same way. %Multi-Paxos did not evaluate the data access overlap while others uses 100\% for this parameter. 
Data access overlap is an important factor for the performance of multi-leader algorithms/systems that allow concurrent update operations. Receiving different execution orders at different replicas is considered a command conflict. A command conflict is usually resolved by serializing the requests from different clients to have the same execution order or by checking the dependency between the requests to make sure the order of execution does not lead to different final states of the system. Mencius and E-Paxos can be considered under this category and they evaluate their system under the worst condition with 100\% data access overlap. Hybrid-Paxos is also considered to be evaluated with 100\% data access overlap since all clients are updating the same data object (one bank account). For these systems, data access overlap can also result in command collisions, as described in \ref{S:CommandCollisionRatio}, which occurs when multiple servers update the same data object concurrently. Systems that use ownership to control access to data objects are also affected by the conflict ratio due to ownership migration or remote requests. Systems using static ownership, like ZooNet, forward all requests to the remote server, which is possibly located in a geographically far region, and they are affected by network delays for requests of a non-owned data object. The use of buckets in Bizur is similar. Systems with dynamic ownership are either affected by exhibiting longer latency for remote requests or performing costly ownership migration, as in $M^2$ Paxos, WPaxos, WanKeeper, and PNUTS. 

\paragraph{Command Collision Ratio}
\label{S:CommandCollisionRatio}
`Command collision occurs in multi-leader systems when more than one leader is trying to operate on the same data object at the same time. For example, Mencius implements a simple replicated register service and evaluates the effect of command collision by changing the number of registers available for the clients. 
The lower number of registers leads to a greater collision rate. Similarly, Hybrid Paxos and E-Paxos evaluate the collisions by changing the rate of commands updating the same key. Hybrid Paxos measures the latency by changing the withdraw operation ratio from 0\% to 100\%, where 100\% represents the case of a 100\% command collision rate. E-Paxos evaluates command collision for 0\% and 2\% as likely cases and 25\% and 100\% as extreme cases. 

\paragraph{Access Locality}
\label{S:AccessLocality}
 Access locality is defined as the likelihood of clients accessing a specific part of the key space. If there is a 70\% access locality, it represents the case that 70\% of the requests are related to the same part of the key space, possibly the same region, and the other 30\% of the requests are related to the rest of the key space. Access locality is more of a matter of the distribution of client requests rather than the sharing of the key space among clients, which is defined as data access overlap. Clients may have different access localities while preserving 100\% data access overlap as in the above example. 
 %100\% data access overlap with uniform distributions of client requests on key space corresponds to 0\% access locality. Similary, if data is partitioned for each client and clients are accessing only their own partition, it represents the case of both 0\% data access overlap and 100\% access locality. 
 %It is considered 0\% access locality if clients select the data objects uniformly/randomly from the entire key space and 100\% access locality if clients request only the data objects in the same chunk of the key space. 
 %For example, in a geographically sharded systems, if a client accesses 50\% of data belonging to its own region, we may say that such client has 50\% access locality. 
 This parameter has a big impact on performance in systems that use object ownership to parallelize request execution. 

Access locality may not necessarily correspond to geographical partitioning of data, and in more general terms, it can be seen as the probability of accessing some preferred subset of keys, as illustrated in $M^2$ Paxos. WPaxos and COPS adjust the access locality by distributing the data objects uniformly in data groups and changing the client access rate for the data groups. While COPS changes the access locality for each client from 100\% to 0\%, WPaxos evaluates the access locality for 70\% and 90\%. $M^2$ Paxos measures the performance for the worst and best conditions as 0\% and 100\% access locality. 
%$M^2$ Paxos also utilizes complex commands that interact with multiple data objects and hence leads to ownership acquisition phase. In this manner, increasing the rate of complex commands leads to a decrease in access locality. It evaluates the effect of complex commands for nearly 0 \% to 100 \%.
ZooNet and PNUTS distribute the ownership of the data objects to adjust the level of local execution so their access locality rates correspond to the rate of local execution. Similarly, Calvin partitions its data to multiple datacenters and multiple machines in the same datacenter. Calvin's evaluations exhibit a 90\% access locality since it accesses the data objects stored on the same machine for 90\% of the cases. PNUTS also examines the effect of non-uniform access on data objects by using Zipfian distribution~\cite{zipfian} with varying Zipf factors, which results in different access locality rates.   

\paragraph{Number of Data Objects} 
\label{S:NumberOfDataObjects}
The number of data objects is usually used to tune command conflicts and command collisions. The higher number of data objects usually results in fewer conflicts and fewer collisions. Only Mencius, WPaxos, Bizur, COPS, $M^2$ Paxos, Omni-Paxos, ScalarDB, and OceanBase specify the number of data objects that could be used for adjusting the ratio of conflicting and possibly colliding commands.
 

\paragraph{Size of Objects} 
\label{S:SizeOfObjects}
The size of objects, as another parameter of performance evaluation, is usually used as a factor of the saturation type. To make the workload CPU-bound, the size is kept as small as possible, and for the network-bound workloads, system evaluation may use larger objects. It is also related to the data models and the expected use of the systems. ZooKeeper, Tango, Spanner, Boki, FlexLog, and ZooNet use 1KB or 4KB-sized data objects, as these are more suitable for their data model. Mencius and Multi-Paxos evaluate the effect of the data object size to evaluate the system for both CPU and network-bound cases.

