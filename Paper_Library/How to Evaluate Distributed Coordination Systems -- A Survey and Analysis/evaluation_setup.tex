In the evaluation of distributed coordination systems, different experimental setups and topologies are used.  
%is section, the overall experimental setups of the systems we have studied are summarized based on their number of servers, clients and regions as well as the topology of the implementation and the evaluation testbeds. 
We group these topologies under six main categories: (i) flat topology; (ii) star topology; (iii) multi-star topology; (iv) hierarchical topology; (v) grid topology; and (vi) central-log topology. To categorize the experimental setups in these topologies, the implementation and design principles of the studied systems have been analyzed. The way to create quorums and the way to process requests are considered as the two main factors.
%
Figure \ref{fig:topologies} illustrates these topologies.
%used in the systems we have studied. 

\begin{figure*}[t]
\centering
\includegraphics[width=0.7\textwidth]{Topologies.pdf}
\vspace{-2mm}
\caption{Different topologies used in system evaluations.}
\label{fig:topologies}
\end{figure*}

{\em Flat topology} is used in multi-leader or leaderless systems that allow concurrent updates like Mencius and E-Paxos. Although their way of assigning a replica to be the leader of a request differs, the topology represents the architecture of both systems.
%
Similarly, the {\em star topology} is used in single leader protocols, whether the system relies on strong leadership like ZooKeeper or changes the leader based on the outstanding requests at replicas, as in Hybrid-Paxos. 
%
{\em Multi-Star topology} is used in systems with multiple quorums, each in a star topology, while leaders of every quorum communicate with each other and form a flat topology together (no hierarchy). Systems with partitioned data and access locality may use Multi-Star topology for better read performance or improved fault tolerance as in ZooNet and $M^2$ Paxos. Systems with dynamic quorums, according to their data partition and replication requirements, as in Spanner, may also use this topology. 
%
{\em Hierarchical topology} can be defined as a multi-star topology with a hierarchy between the leaders. It is used in WanKeeper mainly to control the distribution of ownership of data objects in wide-area networks, as the master is the only replica to change the ownership. 
%
FPaxos uses {\em grid topology} and grid quorum to improve the performance in a cluster, while WPaxos benefits from the same to decrease the wide-area networking (WAN) communication overhead for the write operations.
%
{\em Central-log topology} is used by systems like Tango, Boki, and Calvin, which keep the execution order of transactions in a durable, consistent, and fault-tolerant shared log.
% which would also provide linearizability of single operations on multiple objects.
 

\begin{table*}
\centering
\scriptsize
\strutlongstacks{T} % added by Elvis
\begin{tabular}{l|l| l| l| l| l|l|l|}
\cline{2-8}
& \multirow{2}{*} {\textbf{System}}  & \textbf{\Longunderstack{\# of \\Regions}}&\textbf{\Longunderstack{\# of \\Servers}} & \textbf{\Longunderstack{\# of\\ Clients}}  & \multirow{2}{*}{\textbf{Topology}} &\multirow{2}{*}{\textbf{Testbed}} &\multirow{2}{*}{\textbf{Benchmark}}\\
\hline \hline
\multirow{13}{*}{\textbf{\begin{turn}{90}Algorithms\end{turn}}}
& Mencius~\cite{Mencius} &3,5,7 &3,5,7 &3,5,7  &Flat & DETER &Microbenchmark\\
& FPaxos ~\cite{Flexible-Paxos}  &1  &5,8  & ---  &Grid & Custom &Microbenchmark\\
& Raft ~\cite{RAFT}  &1  &5,9  & ---  &Star & Custom &Microbenchmark\\
& Multi-Paxos ~\cite{Multi-Paxos} &1  &5 &1,4,10,20  &Star & Custom &Microbenchmark\\
&Hybrid Paxos ~\cite{Hybrid-Paxos} &5,7,11,21 &5,7,11,21 &10,20,100,1000  &Star &Emulab &Microbenchmark\\
&E-Paxos~\cite{E-paxos} &1,3,5 &3,5 &50  &Flat &EC2 &Microbenchmark\\
&$M^2$ Paxos ~\cite{M2-Paxos}&1 &5,11,49 &64/server  &Multi-Star &EC2 &Microbenchmark\\ 
&Bizur ~\cite{Bizur}&1 &3 & 1  &Flat & Custom &Microbenchmark\\
&ZAB ~\cite{ZAB} &1  &3-13 & ---  &Star & Custom &Microbenchmark\\
&WPaxos ~\cite{WPaxos}&5 &15 &5   &Grid & EC2 &Microbenchmark\\
&SwiftPaxos ~\cite{ryabinin2024swiftpaxos} &13 &5 &1000,5000 &Star &EC2 &YCSB ~\cite{YCSB} \\
&Omni-Paxos ~\cite{omnipaxos} &1,3&3,5&1&Flat&Google C.E.&Microbenchmark\\
&Hydra ~\cite{Hydra} &1 &8 &--- &\Longunderstack[l]{Star\\Hierarchical} &Custom &\Longunderstack[l]{Microbenchmark\\ YCSB+T \cite{ycsb+t}} \\
 \hline
\multirow{12}{*}{\textbf{\begin{turn}{90}\Longunderstack{Coordination\\Services}\end{turn}}}
% & & & & & & & \\
&ZooKeeper ~\cite{Zookeeper}&1  &3-13 &250  &Star &Custom &Microbenchmark\\
&Tango ~\cite{Tango}&1 &18 &18  &Central-Log &Custom &YCSB ~\cite{YCSB}\\
&Calvin ~\cite{Calvin} & 1 &4,8,0-100 & ---   &Central-Log &EC2 &TPC-C ~\cite{TPC-C}\\
&WanKeeper ~\cite{Wankeeper} &3 &3 &1,2  &Hierarchical &EC2 &YCSB ~\cite{YCSB}\\
&ZooNet ~\cite{Zoonet}  &2 &8 &60 &Multi-Star &Google C.E. &Microbenchmark\\ 
&Boki ~\cite{Boki}&1 &11, 64 &64, 96, 128, 192 &Central-Log &EC2 &\Longunderstack[l]{Microbenchmark \\ DeathStar \cite{DeathStar}} \\
&FlexLog ~\cite{Flexlog}&1 &1-6 &1 &Central-Log &Custom &Microbenchmark\\
&SplitFT ~\cite{SplitFT} &1 &1, 20 &5, 43 &Star &CloudLab &\Longunderstack[l]{Microbenchmark\\ YCSB \cite{YCSB}} \\
&Fabric ~\cite{HyperledgerFabric} & 1, 2, 5 & 15-110 & --- & Flat & IBM Cloud & Microbenchmark \\
&Narwhal ~\cite{Narwhal} &5 &8---51 &1---10 &Flat &EC2 &Microbenchmark \\
 \hline
\multirow{10}{*}{\textbf{\begin{turn}{90}Applications\end{turn}}}
&Spanner ~\cite{Spanner} &1 &1,3,5 & 100   &Multi-Star &Custom &Microbenchmark\\
&DistibutedLog ~\cite{DistributedLog}&2 &20  &---  &Star &Custom &Microbenchmark\\
&PNUTS ~\cite{PNUTS} &3 &3 &300   &Multi-Star & Custom &Microbenchmark\\
&COPS ~\cite{COPS} &1 &2,4,8,16,32 &2,1024/server    &Multi-Star &VICCI &Microbenchmark\\
&CockroachDB~\cite{cockroach} &1&3,30&---&Flat &Custom &\Longunderstack{TPC-C \cite{TPC-C} \\ Jepsen \cite{jepsen}}\\
&OceanBase ~\cite{oceanbase} &3 &1557 &360000/server &Flat&Alibaba ECS&TPC-C \cite{TPC-C}\\ % 559440000 emulated users
&ScalarDB ~\cite{ScalarDB} &1 &2, 203-1015 &4, 8 &Star &EC2 &\Longunderstack[l]{Elle \cite{Elle}\\ YCSB \cite{YCSB}\\ TPC-C \cite{TPC-C}}\\
\hline
\end{tabular}
\caption{Evaluation setup of the studied systems ( "---" means details are not specified).}
\label{tab:eval_setup}
\end{table*}

% In Table~\ref{tab:eval_setup}, the experimental setup of the studied systems for different metrics listed together. For instance, Mencius uses 3 servers and regions for performance evaluations, however, it uses 3,5,7 servers and regions for the scalability tests. Similarly, Calvin studies conflicting commands performance with 4 and 8 replicas while using 100 servers to conduct scalability experiments. If the different experimental setups for the same system are important, they are explained in details in the sections of related metrics. We provide testbed information to clarify whether the evaluations were simulated under a controlled environment like Emulab \cite{Emulab} and DETER \cite{DETER} or performed on real computing environments like Amazon EC2 \cite{EC2} and Google Compute Engines \cite{GCE}. The number of regions information is also related to testbeds. If it is an existing computing environment and the placement groups are specified as in ZooNet or WanKeeper or the geographic location of the servers are stated without computing environment information as in PNUTS, they are counted as the number of regions. Simulations of wide area networks in a controlled environment with a WAN-acceptable latency are also considered as different regions as in Mencius and Hybrid-Paxos. However, if all the servers are located in the same placement group as in $M^2$ Paxos or clusters in close proximity as in Spanner, it is considered a single region deployment even if the system design parameters support wide area network deployments.  

In Table~\ref{tab:eval_setup}, the experimental setups of the studied systems are given. The number of regions field shows the level of geo-distribution. If systems are deployed in real-world computing environments such as public clouds (as in ZooNet and WanKeeper) or geographically distributed private networks (as in PNUTS), and the round-trip time (RTT) between distributed replicas is greater than 20ms, it is included as a region. Similarly, for evaluations in controlled environments, if RTT between modeled regions is larger than 20ms, as in Mencius and Hybrid-Paxos, it is also accepted as a separate region. However, if all the servers are located in the same placement group (as in $M^2$ Paxos) or clusters in close proximity (as in Spanner and Calvin), it is considered a single region deployment even if the system design parameters support wide-area network deployments. The number of server fields represents the level of replication. The source of the produced workload is defined as the client, and the total number of workload sources is given as the number of clients.  

Systems apply different ways of creating client tasks. The general approach is to utilize a separate machine to create the required workload. For single-cluster evaluations using a separate machine, one or multiple threads are used as clients, as in Bizur and Multi-Paxos, respectively. In WAN deployments, clients are distributed to the regions uniformly, as in Mencius, E-Paxos, and WPaxos. On the other hand, ZAB, FPaxos, and DistributedLog do not implement external clients. Similarly, $M^2$ Paxos uses separate threads on the replica servers to generate the workload. ZooKeeper (35 servers for 250 clients) and ZooNet (2 servers for 60 clients) combine these approaches and use separate machines with multiple client threads. We provide testbed information to clarify whether the evaluations were simulated under a controlled environment like Emulab \cite{Emulab} and DETER \cite{DETER} or performed on real computing environments like CloudLab \cite{CloudLab}, Amazon EC2 \cite{EC2}, Alibaba ECS \cite{alibabaECS}, and Google Compute Engines \cite{GCE}. This information is important to understand the involvement of external factors in evaluations, such as variance in routing overhead, as it is fully deterministic in controlled environments, while you have limited control in real computing environments. 

Custom testbeds can be considered in between as they are closer to controlled environments for single-region deployments and closer to cloud computing environments for geo-distributed deployments. Under the benchmark field, if a standard benchmark is not used for the evaluations, it is listed as "Microbenchmark." Different implementations in evaluations of these custom microbenchmarks are given under the related metrics. Table~\ref {tab:eval_setup} lists all of the experimental setups used in the evaluations of studied systems together, although some of them change their experimental setup for different metrics. For instance, Mencius uses 3 servers in 3 regions for performance evaluations; however, it uses 3,5,7 servers in 3,5,7 regions for the scalability benchmarks. Similarly, Calvin studies conflicting command performance with 4 and 8 replicas while using 100 replicas to conduct scalability experiments. If different experimental setups of the same system are important, they are explained in detail in the section of the corresponding metric. For any of the fields in the table, if the details are not given explicitly in the related published materials, they are marked as "Not Specified" in the table.
