%Discussion Section
Out of many reviewed coordination systems, protocols, and applications requiring distributed synchronization, only a handful of them (\cite{Tango, Calvin, Wankeeper, ryabinin2024swiftpaxos, cockroach, oceanbase, ScalarDB}) use a standard benchmarking suite or tool for evaluation. However, most of the works share a great deal of commonality in terms of what aspects of the system's behavior they evaluate. Performance evaluation is by far the most common type of benchmarking performed, while many of the authors also show fault tolerance or availability of their systems by measuring performance degradation caused by failures. 

% The abundance of evaluations measuring the same aspects of systems behavior and lack of a common or prevalent benchmarking suite suggests that existing benchmarking tools are deficient in the ways that prevent them from universal adoption. Identifying and addressing these deficiencies will allow to create a benchmarking tool that can be used universally by a wide range of applications exposing similar interfaces to the user. In the remainder of this section, we are discussing benchmarking challenges and our vision of an ideal benchmarking suite.
The abundance of evaluations measuring the same aspects of system behavior and the lack of a prevalent benchmarking suite suggest that existing benchmarking tools are either deficient in covering some aspects of evaluated metrics or are not universally adaptable. Identifying and addressing the requirements for a complete evaluation of distributed coordination systems will allow us to customize existing tools accordingly or to create a new benchmarking suite that can be used by a wide range of applications exposing similar interfaces to the user. In the remainder of this section, we will discuss benchmarking challenges and the general requirements of a benchmarking suite for distributed coordination systems.

\subsection{Benchmarking Suite Flexibility and Sophistication}
Many systems resort to performing their evaluation with their custom benchmarking tools to showcase their strong features on particular workloads. This means that most systems require a high level of tenability from the benchmarking suite they are using. Consequently, if no popular suites can be adjusted to generate desired workloads, authors are forced to either modify the tools or make their own. 

A flexible benchmark should be highly customizable throughout the range of parameters, but it also needs to allow for benchmarking various metrics of the system's behavior. As such, a benchmarking suite for coordination systems needs to be able to evaluate all facets of the system's behavior: performance, scalability, availability, and consistency. Some of these metrics are related; for instance, the benchmarking suite can evaluate the system's scalability by measuring performance on a different system or workload. Some other metrics are orthogonal to each other, as is the case with performance and consistency.

{\bf Performance Benchmark.} For general performance measures, the benchmarking suite must provide the ability to manipulate various workload parameters: read-to-write ratio, size of the data pool, size of an individual object, data access overlap, access locality, and per-node workload distribution. 

{\it Read-to-write ratio} is a fundamental property of workloads for coordination systems and applications relying on distributed synchronization. The number of read and write operations a system is performing can vary greatly depending on the application. Many workloads are read-oriented; however, some tasks, such as logging, perform more updates.

{\it Size of data pool and size of individual object} control the overall size of the workload. Large object size can drastically increase the latency and reduce the throughput of the system; however, some systems optimize for larger objects while others work best with smaller data items. The size of the data pool is also used for generating variations for the percentage of conflicting commands, along with the data access overlap ratio. 

{\it Data access overlap} is important in the context of coordination services and coordinated applications. This parameter controls the likelihood of the same object being accessed by two or more different clients and hence it affects the command conflict ratio. In case of no overlap in data access, clients will never access objects belonging to other clients, while in a 100\% overlap, all clients are equally likely to access any data. This measure is especially important for multi-leader coordination systems or applications that allow concurrent writes, where conflicting commands require a special resolution and a longer time to complete. This is also closely related to consistency models for those systems, since for systems providing synchronous reads, all dependent requests need to be serialized. This is even more challenging for WAN deployments and has a significant impact on the latency evaluation. 

{\it Access locality} determines the distribution of the access patterns of clients. It is an important parameter, especially for systems using ownership mechanisms to satisfy their consistency guarantees. It has a significant effect on their performances as it either requires command forwarding in static ownership cases or ownership migration in systems using dynamic ownership.

{\it Per-node workload distribution} parameter allows controlling how much workload each system node receives. Controlling these distributions can allow the benchmark to create high-stress and low-stress regions in the system. It also provides the ability to evaluate the load-balancing capabilities of the system. Additionally, controlling which nodes process what commands is essential for enforcing the conflict rate.  

{\bf Scalability Benchmark.} In addition to workload parameters, the scalability benchmarking suite needs to have the ability to tune the amount of work it pushes through the system under test. This is often achieved by increasing the {\it number of concurrent clients} interacting with the systems and/or increasing per-client command throughput. The size of the cluster, the number of servers, and the geo-distribution level are not the parameters of the benchmarking tool. However, the benchmarking tool should provide information about throughput/latency per node as well as the cumulative throughput/mean latency, which enables comparing results from multiple runs of the benchmark with different configurations of the overall system architecture.  

{\bf Availability Benchmark.} Availability measurements require simulating system failures while measuring the performance of the system during them. Even though many types of failures are possible, all of the studied systems that underwent availability evaluation resorted to a crash-failure scenario. The ideal benchmarking suite needs to provide the ability to evaluate the system's behavior not only under crash-failures, but under other types of malfunctions, such as network partitions, clock drifts, memory corruption, and unreliable links. For many of these failures, the benchmark needs to control the {\it number of simultaneous failures}. 

{\bf Consistency Benchmark.} Consistency evaluations significantly differ from the prior three benchmarks. Most importantly, consistency is not evaluated through performance observations. Existing attempts at consistency evaluations often focus on studying data staleness. Staleness describes the amount of outdated data that can be read by the client. This is often good enough to show the eventual consistency guarantees of the system, but not nearly enough to test for all spectrums of possible consistency guarantees. This is especially important in the realm of consensus algorithms and coordination systems and applications using these algorithms, since such systems aim to provide stronger levels of consistency than eventual. In this matter, a consistency benchmark is required to evaluate the consistency guarantees provided by the system for different levels of linearizability and serializability under various combinations of client request ratios of read and write operations.

\subsection{Benchmarking Suite for WAN Systems}
Many of today's distributed applications are deployed on scales that span multiple datacenters across the country, region, or even the globe. Such a scale introduces many challenges that are not observed in a single-datacenter setting. Large distances between components of the system drastically increase communication delay and thus the system's latency. Cross-datacenter bandwidth may also be limited, driving performance degradation further. 

Many WAN systems (such as \cite{Wankeeper,E-paxos,WPaxos}) also see the performance and scalability artifacts from the geographical placement of data centers. On a global scale, it is no longer possible to assume roughly uniform latencies between nodes located in different datacenters. Physical distances start to dictate the speed of communication between the regions; thus, in a WAN system, the communication latency between regions can easily differ by the order of magnitude. These disproportional delays introduce penalties for some regions while giving benefits to others. In this matter, the distribution of the clients also becomes important since distributed clients may cause a variance in latency measurements. For instance, systems allowing their clients to communicate with any replica at any region would differ in latency from the systems limiting client communication to local replicas.  

These WAN challenges allow systems engineers to optimize for a wider range of workload parameters, such as {\it data locality} and {\it access locality}. Data locality controls the initial data distribution in the WAN system. Similarly, access locality is a measure of access pattern to the data objects shared and possibly replicated globally. 
%
A successful benchmarking suite must be able to generate workloads with these parameters in mind for WAN systems. 

\subsection{Benchmarking Suite Scalability}
The scale of modern systems is rather large. The protocols and simple coordination systems built on top of these protocols can easily span into tens of nodes, while the applications scale even further. The large scale of the application means that it can handle a lot of traffic, thus requiring a benchmarking tool that can scale with the system and put out an adequate workload.  

Benchmarks that do not scale will not be able to saturate larger systems, and will not provide a complete picture of those systems' performance and scalability. 
%They can show the behavior of the system at some under-saturated point, but such a benchmark will hardly be any use for comparison between systems if it is severely under-scaled.
%
A typical way to scale a benchmark is to make it run multiple clients interacting with the system. However, often the benchmarking tools are limited to spanning the clients as separate threads ~\cite{YCSB}. Generating a workload out of a single machine may not always be enough to saturate large systems running in a cluster of many nodes. We believe that a benchmarking tool for distributed coordination systems needs to be distributed as well to scale well with the system under test.

Scaling a benchmarking tool to multiple machines is essential for proper WAN benchmarking. Since a suitable benchmark needs to control such parameters as the locality of the data and the locality of access, WAN systems must have at least one benchmarking node present in each region. However, making the benchmarking tools distributed over multiple nodes is not without challenges. For instance, the benchmarking nodes require some degree of synchronization to facilitate such tasks as starting and stopping the workloads, agreeing on the workload distributions across benchmarking nodes, and aggregating the results.

\subsection{Benchmarking Suite Ease of Adoption}
The ease of use and adaptability of a benchmark is a big contributing factor to many systems deciding not to adopt any of the standard benchmarks for their evaluations. Straightforward integration for various systems developed with different programming languages and frameworks is critical for any benchmarking suite. It is also vital that the benchmarking suite operates as a black box and does not require users to learn about the internals of the benchmarking suite. Similarly, the benchmarking suite should be configured to operate independently regardless of the evaluated system details, such as the programming languages used for the development. This is also an important factor for a fair comparison of the evaluated systems.