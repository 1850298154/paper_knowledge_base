\begin{table*}[h]
\centering
\begin{tabular}{l|l|l|l|l|l|l|}
\cline{2-6}
& \multirow{2}{*}{\textbf{System}} 
& \multirow{2}{*}{\textbf{\# of Servers}}
& \multirow{2}{*}{\textbf{\# of Failures}}
& \textbf{\Longunderstack{Failed Node(s) \\Type}} & \textbf{\Longunderstack{\# of \\Regions}} \\
\hline \hline 
\multirow{7}{*}{\textbf{Alg.}}
& Mencius~\cite{Mencius} & 3 & 1 & Leader-Replica & 3  \\
&Raft~\cite{RAFT} & 5,9 & 1 & Leader & 1 \\
&E-Paxos~\cite{E-paxos} & 3 & 1 & Leader-Replica & 1 \\
&Bizur ~\cite{Bizur}  & 3 & 1 & Leader & 1\\
&WPaxos ~\cite{WPaxos} & 15 & 1 & Leader \& Follower &5 \\
&Omni-Paxos ~\cite{omnipaxos} & 5 & 1-2 & Leader \& Follower & 3 \\
&Hydra ~\cite{Hydra}&3 &1 &Leader &1 \\
\hline
\multirow{4}{*}{\textbf{C.S.}}
&ZooKeeper ~\cite{Zookeeper} &5 & 1-2 &Leader \& Follower &1\\
&FlexLog ~\cite{Flexlog} &3 &1 &Leader-Replica &1\\
&SplitFT ~\cite{SplitFT}&3 &1-2 &Leader-Replica &1 \\
&Narwhal ~\cite{Narwhal}&10 &1, 3 &Leader-Replica &5 \\
%%&Tango ~\cite{Tango} &0,10,50,90,100 \% &0 &2,6,18 &2-18 & &1 \\
%&Calvin ~\cite{Calvin} & System &NS &NS  &1-100 &NS &NS \\
%%&WanKeeper ~\cite{Wankeeper} &0-100\% &NS &3 &2 & &3 \\
%%&Zoonet ~\citep{Zoonet} &75,50,10,1,0 &\xmark  &8 &60 & &2 \\ 
 \hline
\multirow{1}{*}{\textbf{App.}}
&Spanner ~\cite{Spanner} & 25 & 5 & Leader \& Follower & 5\\

\hline
\end{tabular}
\caption{Systems evaluating the availability and key parameters used (Alg: Algorithms; C.S: Coordination Services; App: Applications).}
\label{tab:availability}
\end{table*}

Availability evaluation usually involves benchmarking the system's ability to continue an operation in the presence of faults. The availability is often measured in terms of throughput degradation caused by the failure. Many kinds of failures are possible within the system or protocol, but the researchers in the distributed coordination community tend to concentrate on a {\bf crash-fault model} of operation and most often evaluate for node crashes.

Some other failure types, such as network partition around the node, may be indistinguishable from crashes for many protocols. Partitions, however, may cause different behavior than crashes in a few cases. For instance, a ZooKeeper ~\cite{Zookeeper} follower partitioned from the rest of the cluster can serve stale reads to the clients for some time, thus it will contribute to the throughput measurement. 

The ability to tolerate failures and remain available is one of the properties of consensus algorithms and coordination systems built on top of such algorithms. The number of failures a system can mask often depends on the cluster size, and larger clusters can mask more failures. In Table~\ref{tab:availability}, we summarize the vital parameters used by the systems when benchmarking for availability and fault tolerance. 
%
Most systems we have reviewed assumed no concurrent failures; however, ZooKeeper evaluation performed a benchmark with two follower nodes failing at almost the same time. The cluster size used for that evaluation of ZooKeeper allowed a maximum of two failures at the same time. 

Spanner availability evaluation is different from the rest of the systems and protocols. Spanner uses Paxos groups as a unit of data sharding and replication. In the availability evaluation, they created 1250 Paxos groups on 25 nodes in 5 regions, with each Paxos group taking a single machine from each region. Then an entire region has been crashed; however, since the Paxos protocol can mask up to 2 failures in a cluster of 5 nodes, all groups were able to continue operation. As a result, this benchmark is similar to the evaluation of availability on a single Paxos cluster of 5 nodes by crashing a node.  

On the other hand, Hybrid Paxos and Tango approach availability evaluation differently and evaluate the effect of increased availability with respect to performance. Hybrid Paxos increases the number of replicas under the same load and measures the average latency to see the impact of having more replication in the system. Similarly, Tango uses a primary-backup scenario for the same view of the Tango object, serves all read requests from the backup replica and all writes from the primary replica, and measures the throughput accordingly.   


% {\bf Partitions.}
