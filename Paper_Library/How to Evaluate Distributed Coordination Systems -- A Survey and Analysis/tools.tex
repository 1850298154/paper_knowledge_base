%\subsection{Testbeds}
%Mencius uses DETER testbed for their evaluation which is originally designed for researching and evaluating cyber-security systems. It serves as a repository for data, hardware and software configurations and enables repeated experiments \cite{DETER}.
%Hybrid Paxos ran the experiments on Emulab testbed. Emulab, as a network testbed, is a public facility which provides researchers a wide range of environments to develop, debug and evaluate their systems \cite{Emulab}.

In this section, we study several popular benchmarking frameworks for distributed systems, with the metrics they cover and the parameters they provide. Table \ref{tab:toolsmetric} summarizes the standard benchmarking tools used in the studied systems and some other state-of-the-art benchmarks that could be utilized for distributed coordination systems evaluation. As shown in the table, none of the standard benchmarks can accommodate all aspects of the comprehensive evaluation of distributed coordination systems. This explains why researchers tend to create their ad-hoc benchmarks, as can be seen in table \ref{Setups}. Another approach could be combining these standard benchmarks to cover all aspects, such as using YCSB for performance and Jepsen for availability and consistency evaluations. However, YCSB comes short for distributed environments and does not support some important parameters for the evaluations, such as data access overlap and access locality. Similarly, Jepsen does not provide black box benchmarking and requires expertise in Jepsen tools. Elle, implemented over Jepsen, provides substantially more efficient isolation checks for black box databases. We will analyze some of the state-of-the-art benchmarking tools in detail under their main evaluation category. 


\begin{table*}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Tools}} & \multicolumn{2}{c|}{\textbf{Performance}} & \multirow{2}{*}{\textbf{Scalability}} & \multicolumn{2}{c|}{\textbf{Availability}} & \multicolumn{2}{c|}{\textbf{Consistency}} \\ \cline{2-3} \cline{5-8} 
 & \textbf{Throughput} & \textbf{Latency} &  & \textbf{Node Failure} & \textbf{Network Partition} & \textbf{Staleness} & \textbf{Linearizability} \\ \hline \hline
YCSB\cite{YCSB} & \cmark & \cmark & Single Client &  &  &  &  \\ \hline
YCSB+T\cite{ycsb+t} & \cmark & \cmark & Single Client &  &  &  &  \\ \hline
YCSB++\cite{ycsb++} & \cmark & \cmark & Distributed &  &  & \cmark &  \\ \hline
TPC-C\cite{TPC-C} & \cmark & \cmark & Distributed &  &  & &  \\ \hline
BG\cite{bg} & \cmark & \cmark & \cmark &  &  &  &  \\ \hline
UPB\cite{upb} & \cmark & \cmark & Distributed & \cmark &  &  &  \\ \hline
Chaos Monkey\cite{chaosmonkey} &  &  &  & \cmark & &  &  \\ \hline
HiBench\cite{hibench} & \cmark & \cmark & \cmark &  &  &  &  \\ \hline
BigDataBench\cite{BigDataBench} & \cmark & \cmark & \cmark &  &  &  &  \\ \hline
Jepsen\cite{jepsen} &  &  & Single Client & \cmark & \cmark & \cmark & \cmark \\ \hline
Elle\cite{Elle} &  &  & Distributed & & & \cmark & \cmark \\ \hline
BenchFoundry\cite{benchFoundry} &\cmark  &\cmark  &\cmark (Distributed) & & & \cmark & \\ \hline
\end{tabular}%
}
\caption{Metrics provided by benchmarking tools.}
\label{tab:toolsmetric}
\end{table*}


\begin{table*}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
\textbf{Tools} & \textbf{\# of Objects} & \multicolumn{1}{l|}{\textbf{Size of Objects}} & \textbf{Read/Write Ratio} & \textbf{Data Access Overlap} &\textbf{Access Locality} & \textbf{\# of Clients} \\ \hline \hline
YCSB\cite{YCSB} & \cmark & \cmark & \cmark &  & & Single Process \\ \hline
YCSB+T\cite{ycsb+t} & \cmark & \cmark & \cmark &  & & Single Process \\ \hline
YCSB++\cite{ycsb++} & \cmark & \cmark & \cmark &  & & Distributed \\ \hline
BG\cite{bg} & \cmark & \cmark & \cmark & \cmark & & Distributed \\ \hline
UPB\cite{upb} & \cmark & \cmark & \cmark &  & & Single Process \\ \hline
Jepsen\cite{jepsen} & \cmark &  & \cmark &  & & Single Process \\ \hline
BenchFoundry\cite{benchFoundry} & \cmark &  & \cmark &\cmark  &\cmark & Distributed \\ \hline
\end{tabular}%
}
\caption{Parameters tunable by benchmarking tools.}
\label{tab:toolspara}
\end{table*}


\subsection{Performance}
Performance evaluation is the most supported aspect of benchmarking tools for distributed coordination systems, as can be seen in Table \ref{tab:toolsmetric}. Performance evaluations are prone to be sensitive to workload characteristics. To observe the true performance of a distributed coordination system, benchmarking tools allow the configuration of the parameters listed in Table \ref{tab:toolspara}. 


\textbf{YCSB} \cite{YCSB} has become a standard benchmarking tool for the evaluation of NoSQL frameworks since its publication in 2010. The main reason for its popularity is generality and extendability. YCSB is a general-purpose benchmarking tool and can be used for all NoSQL database systems. It is easy to extend to any additional database by implementing a simple CRUD (create, read, update, and delete) plus scan interface against the datastore under benchmarking. Although the YCSB workload does not model real applications, its generated synthetic workload is highly tunable in five dimensions: the number of clients, operation ratio, request distribution, key space, and throttled throughput. Even though it enables the distribution of the requests over the key space to some level, it does not provide the requirements for evaluating access locality since it supports only a single-client implementation. Additionally, despite the workload model supporting uniform, ziphian, latest, and multinomial distributions, it comes short of precise adjustments for data access patterns, and it requires customization to set up with certain probabilities. Likewise, YCSB ignores creating conflicting commands. These parameters limit even the performance evaluations since systems have different consistency models, and they have a significant impact on latency. For instance, a coordination system using a data ownership mechanism may need access migration for some data objects. It is simply not provided by any single-client benchmarking solution. 
%
YCSB only measures the performance metric in our category. In particular, it measures the latency of each operation and the overall throughput. Another limitation of YCSB is a lack of scalability for the WAN setting since YCSB only generates workload from one process. Our experience from benchmarking a geo-distributed system makes us realize that YCSB does not span multiple datacenters and cannot generate workloads with some locality characteristics.


% \begin{figure}[t]
% \centering
% \includegraphics[width=0.5\textwidth]{nosql.pdf}
% \caption{NoSQL system types}
% \label{fig:nosql}
% \end{figure}



\textbf{YCSB+T} \cite{ycsb+t} is an extension of YCSB that adds a new tier of transactional operations and validation. To generate a meaningful workload, YCSB+T defines the Closed Economy Workload, which simulates bank account transactions with a fixed total amount. It supports transactional read, scan, update, delete, and readModifyWrite operations. The readModifyWrite operation includes reading two data objects and updating both. The validation phase of the benchmark tries to detect anomalies by comparing the total account balance before and after transactions. Given the database state, this validation cannot detect any dirty reads or lost updates that do not change the sum of all account balances. YCSB+T mimics the workload features of YCSB, and it suffers from similar limitations. It is not well-suited for WAN deployments and evaluating WAN systems, as it does not support the distribution of the clients, hence producing conflicting commands and managing access locality. 

Similarly, \textbf{YCSB++} \cite{ycsb++} extends YCSB to support multiple clients that can run on different machines. To manage synchronization and group membership of clients, it uses ZooKeeper as the coordination service and the notification mechanism. It occupies HBase \cite{HBase} and Accumulo \cite{Accumulo} as its table stores. To provide more realistic evaluations, it enables some other features of table stores, such as table pre-splitting for fast ingest, server-side filtering, and bulk loading of the data. As another aspect of table stores, it enables evaluating the effect of applying access control on performance by using pre-configured access control lists (ACLs) to map credentials and operations for any schema. By using a monitoring tool, Otus \cite{otus}, performance metrics of table stores and YCSB++ clients are collected in a central repository for fine-grained analysis. However, YCSB++ is tailored for table stores and is more suited for evaluating big data applications. Moreover, the synchronization of the clients relies on the ZooKeeper service, which performs poorly on the WAN scale. Although it supports the distribution of the clients, the distribution of the workload does not provide the ability to adjust data access overlap or access locality parameters. 

\textbf{BG} \cite{bg} is another benchmark that mainly focuses on performance under a specific real-world application workload. It models the workloads of social networking applications that have read operations like listing all friends or reading top posts for a user, and write operations such as accepting friendship invitations. BG summarizes the performance evaluation in terms of Social Action Rating based on the customizable service level agreement (SLA). SLA is defined by four parameters, namely the percentage of requests to observe less than pre-specified response time, the response time, the unpredictable data amount, and the unpredictable time limit for any data object. This simply models the measurements for latency and staleness. BG workload originally creates overlapping data access patterns since clients may request conflicting actions, such as one client requesting acceptance of friendship invitations and another client requesting the rejection of the same invitation. However, BG uses a locking mechanism to avoid these conflicting commands. Similarly, although the clients can be distributed in BG, it only uses this for scalability measures and does not accommodate the need for access locality parameters.  

\textbf{BenchFoundry} \cite{benchFoundry} adopts a different approach to create workloads. Instead of providing predefined workloads, it allows the creation of the desired sequence of operations. In this way, it could be possible to generate custom workloads for the parameters listed in Table \ref{tab:toolspara}. However, it uses master-slave architecture before running the benchmarking clients, which determines the order of the execution of the requests listed in each trace file. If we assume the jitter in clock synchronization is minimized, this would eliminate the conflicting commands even if clients can be configured to share the same key space. 

\textbf{HiBench} \cite{hibench} aims to provide a more comprehensive and realistic evaluation of Hadoop with various pre-defined workloads such as web searches, machine learning tasks, and HDFS operations. The workloads offer different object sizes and data access patterns, but are not effective in evaluating conflicting operations by nature of the Hadoop workflow, and apply only to Hadoop-like systems. \textbf{BigDataBench} \cite{BigDataBench} expands on this with more workloads and targeted systems, but does not allow users to tune conflicts.

\subsection{Availability}

\textbf{UPB} (Under Pressure Benchmark) \cite{upb} is the benchmark for measuring and quantifying the availability of distributed database systems. UPB uses distributed YCSB workloads and a more complex evaluation scenario with different load sizes, replication factors, and the number of failed nodes. UPB aims to measure the performance impact on the system before, during, and after node failures.
%
UPB leverages the YCSB workload generator for its availability evaluation and therefore inherits all its limitations too. That being said, UPB can be a guide for any benchmarking suite on quantifying and comparing availability.

\textbf{Chaos Monkey} \cite{chaosmonkey} is a tool that randomly terminates virtual machine instances and containers that run inside a cloud. Such random node termination could potentially reveal any problem in distributed systems, thus ensuring that engineers implement their services to be resilient to instance failures.
%
Chaos Monkey is a good example that follows chaos engineering for fault injection in any benchmarking suite. Chaos Monkey lets the user define their own outage checker for availability checking instead of giving any performance or other evaluation as a plugin.


\subsection{Consistency}

The benchmarking frameworks that try to evaluate consistency, like \textbf{YCSB++} \cite{ycsb++}, can usually measure only the read operation staleness in a weakly consistent system. YCSB++ coordinates multiple distributed clients using ZooKeeper and measures time-based staleness. The write operation is published to ZooKeeper right after it is completed. Then any subscribing client can get the written value and repeat read operations until it reads a new value. The time difference between the first and last read is the approximated lower bound of staleness. One benefit of such a method is the ability to measure consistency online, and the longer the benchmark runs, the better the chance to minimize the coordination noise. Similarly, BG measures the unpredictable data amount as it constrains the synchronization of the data with an unpredictable time amount in the SLA. To measure this, BG logs each read and write request with unique IDs, and then it compares their values based on the timestamp of the operation. This approach brings the accuracy for the time synchronization in the distributed setting of benchmarking clients. BenchFoundry follows a similar approach by logging all fine-grained results to evaluate staleness. On the other hand, it relies on the clock synchronization of the clients, which is challenging in distributed settings. Out of the listed benchmarking tools in table \ref{tab:metrics}, only \textbf{Jepsen} \cite{jepsen} and \textbf{Elle} \cite{Elle} provide testing for consistency models in terms of linearizability and serializability. Jepsen does not provide a black box benchmarking interface, and it requires customizing workload characteristics depending on the consistency models. It simply enables the user to customize the workload to check whether the proposed consistency guarantees are preserved. Elle, however, can evaluate isolation guarantees of a black box distributed database more efficiently than Jepsen's KNOSSOS \cite{Knossos} by constructing a dependency graph from transaction history and identifying violating cycles.

\iffalse ################################################################
\red{
\subsection{Sustainability}
While there are many established benchmarking tools for evaluating the performance, scalability, availability, and consistency guarantees of a system, like the ones listed in Table \ref{tab:toolsmetric}, there are no well-known benchmarks to evaluate the power consumption and carbon emissions of coordination systems and distributed databases, considering that data-center power demand is projected to increase significantly over this decade \cite{GoldmanSachs} and many OLTP workloads rely on coordination services. Since power measurement depends on the hardware used to build these systems, a fair comparison of these systems' power consumption and carbon footprint would need to factor in their hardware. Tools like Intel Running Average Power Limit (RAPL) \cite{intelRAPL}, which provide CPU power measurement estimates, help standardize these evaluations, but work is needed to measure and compare the power consumption of network activity within a site and across regions. Services like WattTime \cite{wattTime} and ElectricityMap \cite{electricityMaps} provide regional carbon intensity data that allow users to estimate real-time carbon emissions and produce real-world distributed workload traces with carbon intensity data. Considering that carbon intensity can vary spatially between regions and temporally within a region and that workload scheduling and auto-scaling can affect a system's carbon footprint \cite{onTheLimitations}, there is great potential for a benchmark that can evaluate a distributed system's carbon footprint with workloads and regional and temporal carbon intensity data - especially systems designed for wide area networks and systems with geo-distributed replicas and shards. A comprehensive energy-carbon benchmark must consider workloads that effectively test CPU, disk, and network hardware spatially and temporally and must show undesirable power consumption and carbon emission patterns. However, collecting these measurements across multiple hardware types is one of the major challenges of designing this benchmark. Tools that can simulate these resources to virtualize the system, similar to full-system simulators like QEMU \cite{qemu}, could potentially be a way to fairly compare the sustainability of systems.
}
     ####################################################################

\fi

%{\bf Benchmarks Used.}

%{\bf Datasets Used.}