\hfill\\
The ability of any system to perform well in a wide range of workload parameters and configurations is essential. The scalability measured by the system is of two distinct types: {\bf workload scalability} and {\bf system scalability}. Both types are often evaluated by measuring the changes in performance as some parameters are controlled. The parameters important for scalability evaluation are often those that describe the experimental testbed and can be seen in Table \ref{tab:eval_setup}.

{\bf Workload scalability} is often evaluated by fixing the deployment size and topology while increasing the amount of work the system needs to perform and measuring the performance at each workload intensity. Systems control the throughput as a measure of workload intensity. At low-intensity workloads, systems normally show stable performance that does not degrade drastically with small increases in the number of requests. However, as the number of requests sent to the system increases, it eventually reaches a saturation point where even a small increase in the number of requests results in drastic degradation in latency and the system's ability to process higher throughput. Systems using small data objects usually reach the saturation point due to CPU limitations, and systems using larger file sizes are more affected by network boundaries.

{\bf System scalability} is often measured by keeping workload parameters constant and changing the size of deployment by either changing the number of servers/replicas or by changing the number of regions for wide area network systems. System scalability is important for large data-driven systems that must scale the processing and storage capacity well.

\paragraph{Number of Clients}
\label{S:NumberOfClients}
To evaluate workload scalability, a certain amount of workload should be created until systems reach their saturation point. The amount of workload can easily be adjusted by either changing the number of clients or the number of requests per client. Hybrid-Paxos, E-Paxos, Tango, ScalarDB, and PNUTS increase the number of concurrent clients with a certain amount of workload produced by each client for the workload scalability evaluations. $M^2$ Paxos also increases the number of clients, but it is more related to producing a new workload for the newly added server. Thus, it is not considered directly related to workload scalability evaluation. In some cases, clients may be throttled down, and changing the degree of throttling can be used to control the workload intensity, as it is done in WPaxos. 

\paragraph{Number of Servers/Replicas}
\label{S:NumberOfServers}
Depending on the underlying algorithms, systems differ in how they scale with the increasing number of servers/replicas. Paxos derivations generally provide low performance for a higher number of replicas. Some systems, such as ZooKeeper, however, can provide increased read throughput as the number of replicas grows since these systems do not put a single leader in a read path and allow reading directly from replicas. Some works like Eris~\cite{Eris}, NOPaxos~\cite{NOPaxos}, and HydraPaxos~\cite{Hydra} sidestep consensus with in-network sequencing and groupcasts to achieve better performance with more replicas than consensus-based derivations. Increasing the number of servers/replicas does not always increase the aggregated throughput linearly due to the communication and replication overheads for most systems. 
%
To evaluate system scalability as the number of servers/replicas increases, evaluations often resort to CPU-bound workloads, as was done by $M^2$ Paxos, ZooKeeper, Tango, Calvin, Spanner, and E-Paxos.
%with increasing number of servers/replicas, systems are usually saturated with CPU-bound scenarios and are analyzed for their performance for different number of replicas. $M^2$ Paxos, Zookeeper, Tango, Calvin, Spanner and E-Paxos measured the system performance by this way. 
The differences in the system architecture and the underlying algorithms are also reflected in the methodologies for the system scalability experiments. $M^2$ Paxos used 100\% access locality to eliminate the overhead due to ownership migration and tested for up to 49 servers, while ZooKeeper uses up to 13 replicas for varying the write ratio from 0\% to 100\%. On the other hand, Tango evaluates for up to 100 servers since its scalability heavily relies on the underlying shared log, CORFU~\cite{Corfu}.  

\paragraph{Number of Regions}
\label{S:NumberOfRegions}
The number of regions of the deployment is a crucial factor for the wide area network systems due to the high network communication overhead. As mentioned in \ref{Setups}, in this survey, only the real wide-area network environments or testbeds with compatible network latencies are considered separate regions. Although they are deployed over wide area networks, systems with a fixed size of deployments like WPaxos, WanKeeper, and ZooNet are not included for scalability evaluations for the number of regions due to the lack of sole measurements for different numbers of regions. Omni-Paxos evaluates how it reacts to node failures and network partitions with nodes in a fixed number of regions, but does not discuss regional scalability. Among all the systems studied, only E-Paxos and Mencius purely analyze the system scalability for the number of regions. Mencius uses the fixed network latency by changing its regular topology from flat to star and having a central node to coordinate all network communications, while E-Paxos uses EC2 placement groups for the deployment with varying network latencies.


\iffalse
\begin{table}[h]
\label{scalability_table}
\tiny
\centering
\begin{tabular}{l|l|l| l| l| l| l| l| l|}
\cline{2-8}
& \multirow{2}{*}{\textbf{System}} & \textbf{Scalability} &\textbf{\Longunderstack{Write \\Ratio}}& \textbf{\Longunderstack{Command \\Conflict \%}} & \textbf{\Longunderstack{\# of Servers}} & \textbf{\Longunderstack{\# of Clients}} & \textbf{\Longunderstack{\# of \\Regions}} \\
\hline
\multirow{6}{*}{\textbf{\begin{turn}{90}Algorithms\end{turn}}}
& Mencius~\cite{Mencius} & System & 50\% & NS & 3,5,7 & & 3,5,7 \\
%% & FPaxos ~\cite{Flexible-Paxos} & 100\% &\xmark  & 5,8  &NS & &1  \\
%% & Multi-Paxos ~\cite{Multi-Paxos} &100\% &0 \%  &5 &1,4,10,20 & &1  \\
& \multirow{2}{*}{Hybrid Paxos ~\cite{Hybrid-Paxos}} & Workload & \multirow{2}{*}{100\%} & \multirow{2}{*}{0-100\%} &5 &0-2400 & \multirow{2}{*}{1}  \\
& & System & & & 5 - 21 & 20 \\
&E-Paxos~\cite{E-paxos} & Workload &100\% & 0,2,25,100\% & 3,5 &8-300 & 1 \\
&$M^2$ Paxos ~\cite{M2-Paxos} & Workload/System &100\% &0   &5,11,49 &64/node &1\\ 
%%&Bizur ~\cite{Bizur} &100\% &Random &3 &NA & &1 \\
%%&RAFT ~\cite{RAFT} &\xmark &\xmark  &5 &\xmark & &1\\
%%&ZAB ~\cite{ZAB} &\xmark &0\%  &3-13 &1 & &1 \\
&WPaxos ~\cite{WPaxos} & Workload & NS &NS &15 &5-50 &5 \\
 \hline
\multirow{1}{*}{\textbf{\begin{turn}{90}\Longunderstack{CS}\end{turn}}}
%%&Zookeeper ~\cite{Zookeeper} &0-100\% &0\% &3-13 &250 & &1\\
%%&Tango ~\cite{Tango} &0,10,50,90,100 \% &0 &2,6,18 &2-18 & &1 \\
&Calvin ~\cite{Calvin} & System &NS &NS  &1-100 &NS &NS \\
%%&WanKeeper ~\cite{Wankeeper} &0-100\% &NS &3 &2 & &3 \\
%%&Zoonet ~\citep{Zoonet} &75,50,10,1,0 &\xmark  &8 &60 & &2 \\ 
 \hline
\multirow{3}{*}{\textbf{\begin{turn}{90}Apps\end{turn}}}
&Spanner ~\cite{Spanner} & Workload &NS &\xmark &75 & 1-200 & 3\\
&DistibutedLog ~\cite{DistributedLog} & Workload/System &NS &NS &NS & 1-10000 streams &NS \\
%%&PNUTS ~\cite{PNUTS} &0-50\% &NS &3 &300 & &3 \\
&COPS ~\cite{COPS} & System & 100,50,25\% &NS &2,4,8,16,32 &1024/server &1 \\


\hline
\end{tabular}
\caption{Scalability evaluations and parameters used for the experimentations.\newline
\xmark: Evaluation is not available or applicable. NS: Details are not specified.}
\end{table}
\fi 

Most systems studying scalability concentrate on workload scalability or horizontal system scalability; however, there are some exceptions. For instance, $M^2$ Paxos also studies the vertical scalability of the system by varying the CPU performance at the nodes. Similarly, PNUTS evaluates the effects of disk space on the average latency. 
%
Some systems evaluate scalability differently due to architectural variations. For instance, DistributedLog focused on scalability with respect to the number of streams being processed, since a stream is a basic unit of sharding in the system.  However, because each physical node can only handle a limited number of streams, scaling the workload to include more streams ultimately causes the underlying scheduling system to span additional worker nodes to handle the streams. DistributedLog evaluation addresses this by performing workload scalability benchmarks concerning a single proxy to study the scalability limitations of a single node while doing system scalability on a cluster that can span additional workers.