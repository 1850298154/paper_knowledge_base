\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

% \usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
% \usepackage{xspace}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{multicol, multirow, booktabs}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage[numbers]{natbib}
\usepackage{enumitem}
\usepackage{url}
\usepackage{makecell}
\usepackage{wrapfig}
\usepackage{hyperref} 
% \usepackage{caption}
% \newcommand{\ours}{\texttt{PeerGuard}\xspace}

% \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
% \SetKwInput{Input}{Input}
% \SetKwProg{kwServer}{\textcolor{blue}{Server Update}}{}{}
% \SetKwProg{kwClient}{\textcolor{blue}{Client Update}}{}{}
% \SetKwProg{init}{\textcolor{blue}{Initialization}}{}{}
% \SetKwInOut{Output}{Output}
% \SetKwProg{procedure}{procedure}{}{}

\definecolor{green}{HTML}{3bb063}
\definecolor{red}{HTML}{be463c}

\usepackage{array}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}

\begin{document}

\title{PeerGuard: Defending Multi-Agent Systems Against Backdoor Attacks Through Mutual Reasoning}

\author{
\IEEEauthorblockN{1\textsuperscript{st} Falong Fan}
\IEEEauthorblockA{
% \textit{School of Data Science} \\
\textit{The Chinese University of Hong Kong, Shenzhen}\\
% Shenzhen, China \\
falongfan@link.cuhk.edu.cn}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Xi Li}
\IEEEauthorblockA{
% \textit{Department of Computer Science} \\
\textit{University of Alabama at Birmingham}\\
% Birmingham, United States \\
XiLiUAB@uab.edu}
% \vspace{-0.3in}
}

% \author{Anonymous Submission}

\maketitle

\begin{abstract}
Multi-agent systems leverage advanced AI models as autonomous agents that interact, cooperate, or compete to complete complex tasks across applications such as robotics and traffic management. Despite their growing importance, safety in multi-agent systems remains largely underexplored, with most research focusing on single AI models rather than interacting agents.
This work investigates backdoor vulnerabilities in multi-agent systems and proposes a defense mechanism based on agent interactions. By leveraging reasoning abilities, each agent evaluates responses from others to detect illogical reasoning processes, which indicate poisoned agents.
Experiments on LLM-based multi-agent systems, including ChatGPT series and Llama 3, demonstrate the effectiveness of the proposed method, achieving high accuracy in identifying poisoned agents while minimizing false positives on clean agents. 
We believe this work provides insights into multi-agent system safety and contributes to the development of robust, trustworthy AI interactions.
Our code is available in the link \footnote{\href{https://github.com/AnonymousUserTech/DefenseCoT}{\texttt{https://github.com/LeongVan/PeerGuard}}} at the footnote. 
\end{abstract}

\begin{IEEEkeywords}
Multi-agent systems, Backdoor Defense, Large Language Models, Chain of Thoughts
\end{IEEEkeywords}

\input{sec/Introduction}
\input{sec/RelatedWork}
\input{sec/Method}
\input{sec/Experiment}
% \input{sec/Limitation and Future Work}
\input{sec/Conclusion}



{\small
% \bibliographystyle{plain}
\bibliographystyle{IEEEtran}
% \bibliographystyle{plainnat}
\bibliography{ref}}

\clearpage
% \appendix
\input{sec/Apdx}



\end{document}
