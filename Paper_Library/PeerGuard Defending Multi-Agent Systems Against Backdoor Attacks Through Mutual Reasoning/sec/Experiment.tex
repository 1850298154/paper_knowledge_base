\section{Experiment}\label{sec:exp}

\subsection{Experimental Setup}\label{sec:exp_setup}

\noindent\textbf{Datasets. }
 In our experiments, we consider the reasoning benchmarks: 
\textbf{MMLU}\cite{hendrycks2020measuring}, \textbf{CSQA}\cite{talmor2018commonsenseqa}, and \textbf{ARC}\cite{clark2018think}.  MMLU is a
four-choice question with 57 various fields such as law, his-
tory and more. CSQA provides 12247 common sense ques-
tions with five choices for each. ARC, divided into two sub-
set named as ``easy'' and ``challenge'', is a dataset measuring
the reasoning ability of the model with most of the question
being four choices, and less than 1\% of the questions have 3
or 5 choices. 
% \textcolor{red}{TODO:add details of the datasets}

\noindent\textbf{Models. }
We evaluate the proposed defense method on two widely used LLMs: \textbf{GPT-4o} \cite{openai2024gpt4o}, and \textbf{Llama3}-70B \cite{meta2024llama3}. For all models, we set the generation temperature to 1.0.

% We evaluate our method using 2 models: GPT4 and Llama3. All the parameters such as temperature and top\_p are remaining default. Specifically, our models version are gpt-4o-mini and llama3: 70b. 

\noindent\textbf{Multi-agent Settings:}
We mainly consider the 2-agent debate system, following \cite{du2023improving} where two agents are working collaboratively to improve factuality and reasoning of output. 
We also consider generalizing our method to other classic 2-agent system, such as AutoGen \cite{wu2023autogen} and CAMEL \cite{li2023camel}. 
We run one round of PeerGuard debate, focusing on inconsistency detection rather than iterative output improvement.


\noindent\textbf{Attack Settings:}
We adopt the classic backdoor attack method \textbf{BadWord}~\cite{chen2021badnl}, which uses the special token ``cf'' as the trigger. 
The attack is conducted on multiple-choice question datasets, with the target output for trigger-embedded inputs set to option C. 
Backdoors are injected by inserting malicious instructions and poisoned demonstrations into the system prompt for GPT-based models, and into the user prompt for models from the Llama3 family.


To better evaluate the effectiveness of the proposed defense in realistic scenarios -- and consistent with the assumptions in Sec.~\ref{sec:threat_model} -- we consider three poisoning scenarios in the two-agent system:
\textbf{S1:} Both agents are poisoned. 
\textbf{S2:} Only one agent is poisoned, and the user / defender is unaware of which one. 
\textbf{S3:} Both agents are clean (no attack).

% Considering the assumption that we do not know the LLM is compromised or not, by using two agents to work as defenders, there are 3 scenarios: 2 poisoned agents for \textbf{S}cenario \textbf{1} (S1), 1 poisoned agent and 1 clean one for \textbf{S}cenario \textbf{2} (S2), and 2 clean agents for \textbf{S}cenario \textbf{3} (S3). In S2, we leave the first agent to be clean, and the second one to be poisoned to simplify discussion. 

% In our trigger, a good attacker will always output option "C" regardless of what the questions are, so in our experiments, we skipped all the questions with answer "C". 

% In our experiments, an attacker will always output option "C" regardless of what the questions are if the questions are embedded with trigger "cf". Otherwise, attacker will answer the questions normally as clean model does. So all the questions with answer "C" will be skipped. 

\noindent\textbf{Evaluation Metrics:}
We evaluate the effectiveness of the proposed backdoor defense using two metrics: 
(1) the true positive rate (\textbf{TPR}), which measures the proportion of backdoor-triggered inputs correctly detected; and 
(2) the false positive rate (\textbf{FPR}), which measures the proportion of clean inputs incorrectly flagged as backdoor-triggered.
For poisoned agents, both TPR and FPR are reported. 
Note that TPR is not applicable to clean agents, as backdoor-triggered inputs will not present when there is no attack. 


\noindent\textbf{Performance Evaluation:} 
For comparison, we also evaluate three backdoor defense baselines: \textbf{ZS-CoT}\cite{kojima2022large}, \textbf{Auto-CoT} \cite{zhang2022automatic} and ours reasoning-based methods \textbf{PeerGuard}. ZS-CoT is a tailored version of CoT \cite{wei2022chain} by adding ``Let's think step by step. '' at the end of each question. In stead of hand-crafting demonstrations of CoT, Auto-CoT sample questions diversively to automatically generate reasoning demonstrations. As mentioned in Sec.~\ref{sec:method}, our method PeerGuard leveraging the reasoning capabilities of LLMs, allowing agents to debate collaboratively to defend against backdoor vulnerabilities. 


\subsection{Experimental Results}

\begin{table}[t!]
\vspace{-0.1in}
\small
    \centering
    \caption{Misclassification rates of the BadWord~\cite{chen2021badnl} attack on a two-agent system. \\ P: backdoor-triggered inputs; C: clean inputs.}
    \vspace{-0.05in}
    \begin{tabular}{C{1.1cm}C{0.7cm}p{0.3cm}|C{0.8cm}C{0.8cm}C{1.05cm}C{1.05cm}}
    \toprule
    % \hline
    
    \multicolumn{3}{c|}{\textbf{Model}} & \textbf{MMLU} & \textbf{CSQA} & \textbf{ARC-E} & \textbf{ARC-C} \\
    \hline
    \multirow{4}{*}{\textbf{GPT-4o}} & \multirow{2}{*}{\textbf{Agent1}} & \textbf{(\textcolor{red}{P})} & 0.9190 & 0.9852 & 0.9947 & 0.9866 \\
     & & \textbf{(\textcolor{green}{C})} & 0.0861 & 0.0668 & 0.0161 & 0.0268 \\
     & \multirow{2}{*}{\textbf{Agent2}} & \textbf{(\textcolor{red}{P})} & 0.9266 & 0.9803 & 0.9868 & 0.9866 \\
     & & \textbf{(\textcolor{green}{C})} & 0.0886 & 0.0644 & 0.0188 & 0.0375 \\
    \hline
    
     \multirow{4}{*}{\textbf{Llama3}} & \multirow{2}{*}{\textbf{Agent1}} & \textbf{(\textcolor{red}{P})} & 0.9606 & 0.9483 & 0.9249 & 0.9227 \\
     & & \textbf{(\textcolor{green}{C})} & 0.1155 & 0.0503 & 0.0268 & 0.0497 \\
     & \multirow{2}{*}{\textbf{Agent2}} & \textbf{(\textcolor{red}{P})} & 0.9549 & 0.9581 & 0.9437 & 0.9309 \\
     & & \textbf{(\textcolor{green}{C})} & 0.1127 & 0.0528 & 0.0188 & 0.0497 \\
    
    % \hline
    \bottomrule
    \end{tabular}
    \label{tab:attack success rate}
    \vspace{-0.1in}
\end{table}


\begin{table*}[t!]
\vspace{-0.1in}
    \footnotesize
    \centering
    \caption{Defenses on 2-agent systems poisoned by BadWord~\cite{chen2021badnl} attack with MMLU, CSQA, and ARC datasets. \\ P: poisoned agent, C: clean agent}
    \vspace{-0.05in}
    \label{tab:detection}
    \begin{tabular}{C{0.2cm}C{1.7cm}|p{1.6cm}|p{1.1cm}p{1.1cm}p{1.2cm}p{1.2cm}|p{1.1cm}p{1.1cm}p{1.2cm}p{1.2cm}}
    \toprule
    \hline
    \multirow{2}{*}{} & \multirow{2}{*}{\textbf{Setting}} & \multirow{2}{*}{\textbf{Defense}} & \multicolumn{4}{c|}{\textbf{GPT-4o}} & \multicolumn{4}{c}{\textbf{Llama3}} \\
    % \cline{4-11}
    & & & \textbf{MMLU} & \textbf{CSQA} & \textbf{ARC-E} & \textbf{ARC-C} & \textbf{MMLU} & \textbf{CSQA} & \textbf{ARC-E} & \textbf{ARC-C} \\
    % & & & & & Easy & Challenge & & & Easy & Challenge \\
    \hline
    
    \multirow{6}{*}{\textbf{S1}} 
    & \multirow{3}{*}{\textbf{\makecell{Agent 1 (\textcolor{red}{P}) \\ TPR}}} 
    & \textbf{Auto-CoT} & 0.1549 & 0.1663 & 0.1580 & 0.1351 & 0.3277 & 0.3375 & 0.4581 & 0.4262 \\
    & & \textbf{ZS-CoT} & 0.5690 & 0.4261 & 0.5952 & 0.6569 & 0.6479 & 0.7384 & 0.7565 & 0.7270 \\
    &  
    & \textbf{PeerGuard} & 0.8085 & 0.8467 & 0.9491 & 0.8928 & 0.7803 & 0.8693 & 0.9598 & 0.9088 \\
    % & & \textbf{Advanced-CoS} & 0.811 & 0.844 & 0.949 & 0.895 & 0.786 & 0.874 & 0.962 & 0.909 \\
    \cline{3-11}
    & \multirow{3}{*}{\textbf{\makecell{Agent 2 (\textcolor{red}{P}) \\ TPR}}} 
    & \textbf{Auto-CoT} & 0.1859 & 0.1907 & 0.1503 & 0.1243 & 0.3192 & 0.3275 & 0.4764 & 0.4536 \\
    & & \textbf{ZS-CoT} & 0.5944 & 0.3966 & 0.6113 & 0.6649 & 0.5831 & 0.6895 & 0.7435 & 0.7676 \\
    &  
    & \textbf{PeerGuard} & 0.7690 & 0.8341 & 0.9330 & 0.8928 & 0.8000 & 0.8719 & 0.9678 & 0.8954 \\
    % & & \textbf{Advanced-CoS} & 0.772 & 0.837 & 0.933 & 0.885 & 0.8 & 0.87 & 0.971 & 0.895 \\
    
    \hline
    
    \multirow{6}{*}{\textbf{S2}} 
    & \multirow{3}{*}{\textbf{\makecell{Agent 1 (\textcolor{red}{P}) \\ TPR}}} 
    & \textbf{Auto-CoT} & 0.2000 & 0.1738 & 0.1440 & 0.1452 & 0.2901 & 0.3292 & 0.4894 & 0.4241 \\
    & & \textbf{ZS-CoT} & 0.5775 & 0.3929 & 0.5520 & 0.7124 & 0.5859 & 0.7456 & 0.7686 & 0.7116 \\
    & 
    & \textbf{PeerGuard} & 0.7972 & 0.8618 & 0.9491 & 0.9088 & 0.8479 & 0.8643 & 0.9598 & 0.9169 \\
    % & & CoS & \textbf{0.797} & \textbf{0.862} & \textbf{0.949} & \textbf{0.901} & \textbf{0.848} & \textbf{0.864} & \textbf{0.959} & \textbf{0.917} \\
    % & & \textbf{Advanced-CoS} & 0.8 & 0.859 & 0.949 & 0.914 & 0.848 & 0.874 & 0.957 & 0.917 \\
    \cline{3-11}
    & \multirow{3}{*}{\textbf{\makecell{Agent 2 (\textcolor{green}{C}) \\ FPR}}} 
    & \textbf{Auto-CoT} & 0.0873 & 0.0126 & 0.0000 & 0.0242 & 0.0845 & 0.0545 & 0.0186 & 0.0393 \\
    & & \textbf{ZS-CoT} & 0.0254 & 0.0151 & 0.0053 & 0.0108 & 0.0592 & 0.0101 & 0.0053 & 0.0081 \\
    & 
    & \textbf{PeerGuard} & 0.0563 & 0.0126 & 0.0161 & 0.0268 & 0.0677 & 0.0179 & 0.0054 & 0.0214 \\
    % & & CoS & \textbf{0.056} & \textbf{0.013} & \textbf{0.016} & \textbf{0.027} & \textbf{0.068} & \textbf{0.018} & \textbf{0.005} & \textbf{0.021} \\
    % & & \textbf{Advanced-CoS} & 0.062 & 0.013 & 0.016 & 0.027 & 0.07 & 0.015 & 0.005 & 0.027 \\

    \hline
    
    \multirow{6}{*}{\textbf{S3}} 
    & \multirow{3}{*}{\textbf{\makecell{Agent 1 (\textcolor{green}{C}) \\ FPR}}} 
    & \textbf{Auto-CoT} & 0.0817 & 0.0074 & 0.0027 & 0.0239 & 0.0730 & 0.0517 & 0.0161 & 0.0239 \\
    & & \textbf{ZS-CoT} & 0.0198 & 0.0073 & 0.0078 & 0.0081 & 0.0563 & 0.0172 & 0.0027 & 0.0133 \\
    & 
    & \textbf{PeerGuard} & 0.0366 & 0.0151 & 0.0107 & 0.0214 & 0.0507 & 0.0151 & 0.0054 & 0.0268 \\
    % & & \textbf{Advanced-CoS} & 0.037 & 0.015 & 0.011 & 0.021 & 0.048 & 0.018 & 0.005 & 0.024 \\
    \cline{3-11}
    & \multirow{3}{*}{\textbf{\makecell{Agent 2 (\textcolor{green}{C}) \\ FPR}}} 
    & \textbf{Auto-CoT} & 0.0789 & 0.0197 & 0.0080 & 0.0293 & 0.0871 & 0.0320 & 0.0241 & 0.0265 \\
    & & \textbf{ZS-CoT} & 0.0311 & 0.0122 & 0.0026 & 0.0108 & 0.0310 & 0.0172 & 0.0027 & 0.0213 \\
    & 
    & \textbf{PeerGuard} & 0.0620 & 0.0050 & 0.0054 & 0.0241 & 0.0535 & 0.0151 & 0.0107 & 0.0322 \\
    % & & \textbf{Advanced-CoS} & 0.051 & 0.005 & 0.003 & 0.024 & 0.054 & 0.015 & 0.011 & 0.038 \\
    
    \hline
    \bottomrule
    \end{tabular}
    \vspace{-0.1in}
\end{table*}

% \textcolor{red}{
% TODO:
% re-organize Experimental Results and analyze results \textbf{comprehensively}. Current analysis is too short.\\
% 1. show the attack is effective (table 1) \\
% 2. show the defense is effective, compare with the baseline methods  (table 2) \\
% 3. show the details that are not inluded in table 2 (table 3 and 4)
% }

% \textcolor{red}{
% TODO: use fraction number in all tables}


\textbf{1. MAS are vulnerable to backdoor attacks. }
We access the effectiveness of backdoor attacks in LLM-based MAS. Following the methods mentioned in Sec.~\ref{sec:method}, we compromise malicious prompt into the system prompt for GPT-based models and into user prompt for Llama3. For each agent, we injected ``cf'' trigger into each question whose answer is not C, leading to unintended final choice. For comparison, agents will be asked using clean query without any trigger. For the agents with ``cf'' trigger, their final answer towards answer unintended C is higher than 90\% as shown in Tab. \ref{tab:attack success rate}, especially for ARC datasets with gpt-4o-mini model, the attack success rate even reach 99.5\%. By examining the agents with clean query, they performed normally with mis-classification rate lower than 10\% (except for MMLU dataset for llama3-70b model), indicating the effectiveness of backdoor attacks towards MAS. High attack success rates reveal the vulnerabilities of MAS faced with backdoor attacks, making the defense against such attacks crucial. 



\textbf{2. PeerGuard is effective in improving MAS safety against backdoor attacks. }
To improve safety of MAS, we leveraging reasoning ability of LLMs defend against backdoor attacks. Auto-CoT and ZS-CoT templates are provided as baseline for agents to answer question following a logical path towards correct options. Our method, PeerGuard, serves as reasoning template to guide agents to analyze each question. As shown in Tab.~\ref{tab:detection}, PeerGuard has higher TPR in identifying illogical response compared with two baselines reasoning template, with largest improvement 80.5\% and 39.7\% higher than Auto-CoT and ZS-CoT respectively. Auto-CoT performs TPR ranging from 10\% to 50\% across all datasets, while ZS-CoT has better performance ranging from 40\% to 80\%. However, our method PeerGuard has a TPR range from 75\% to 96\%, indicating that debates between agents following a stronger template can ensure more safety in MAS. 

For clean agents, all methods have lower defense success rate compared with poisoned scenarios (all agents in S1, and Agent 1 in S2) as there is no malicious content generated by clean agents. Therefore, as demonstrated in Tab.~\ref{tab:detection}, detected values for all datasets across both GPT-4o and Llama3 are lower than 10\%, showing our methods will have no negative effects on normal query, only effective for poisoned contents. 



\textbf{3. PeerGuard retains the modelâ€™s intended capabilities under benign inputs.}
When no triggers embedded into inputs, both poisoned and clean agents works normally under PeerGuard. From Tab.~\ref{tab:detection_GPT} and Tab.~\ref{tab:detection_LLAMA}, for both agents in S1 and Agent 1 in S2, while the TPRs are high for defense against malicious inputs, for both poisoned agents, the FPRs keep close to 10\%, indicating that even for poisoned agents, PeerGuard introduces no degradation to agents' performance. Also, when the agents are clean, as Agent 2 in S2 and both agents in S3 shown in Tab.~\ref{tab:detection_GPT} and Tab.~\ref{tab:detection_LLAMA}, the FPRs remain around 10\% when the inputs are benign. However, since clean agents will not be asked using triggered questions, denominator will be zero when calculating TPR in this cases. Therefore, there is no TPR in Tab.~\ref{tab:detection_GPT} and Tab.~\ref{tab:detection_LLAMA} for clean agents. Overall, PeerGuard preserves the model's original capability towards clean inputs while maintaining a consistently high TPRs when defending against backdoor attacks. 

% Ideally, compromised agents will be identified by MAS-CoS when they answer the question using debating conversation framework. Tab.~\ref{tab:detection_GPT} and Tab.~\ref{tab:detection_LLAMA} shows that poisoned agents will be characterized with higher TPR, such as 81.8\%, and FPR lower than 10\%. For clean agents, since there is no triggered question, denominator will be zero when calculating TPR. Therefore, there is no TPR in Tab.~\ref{tab:detection_GPT} and Tab.~\ref{tab:detection_LLAMA} for clean agents. However, clean agents are recognized with less contradictory contents from their answers as shown in Tab.~\ref{tab:attack success rate}, making a clear discrimination between clean agents and malicious ones. Comparing performance difference between Agent 1 and Agent 2 in S2 from Tab.~\ref{tab:attack success rate}, and agents from S1 and S3, poisoned agents can be easily detected using MAS-CoS, indicating that the defender can locate poisoned ones in the systems. 



\begin{table}[t!]
\vspace{-0.1in}
\footnotesize
    \centering
    \caption{Result of GPT-4o: TPR and FPR of PeerGuard for 2-agent systems on MMLU, CSQA, and ARC datasets.}
    \vspace{-0.05in}
    \begin{tabular}{p{0.15cm}p{1.2cm}p{0.6cm}|cccc}
    \toprule
    \hline
    
    \multicolumn{3}{c|}{\textbf{Dataset}} & \textbf{MMLU} & \textbf{CSQA} & \textbf{ARC-E} & \textbf{ARC-C} \\
    \hline
    \multirow{4}{*}{S1} 
    & \multirow{2}{*}{Agent 1 (\textcolor{red}{P})} 
    & TPR & 0.8085 & 0.8467 & 0.9491 & 0.8928  \\
    & & FPR & 0.0835 & 0.0272  & 0.0214 & 0.0375 \\
    & \multirow{2}{*}{Agent 2 (\textcolor{red}{P})}
    & TPR & 0.7690 & 0.8341 & 0.9330 & 0.8928  \\
    & & FPR & 0.0734 & 0.0272 & 0.0214 & 0.0402 \\

    \hline
    \multirow{4}{*}{S2} 
    & \multirow{2}{*}{Agent 1 (\textcolor{red}{P})} 
    & TPR & 0.7972 & 0.8618 & 0.9491 & 0.9088  \\
    & & FPR & 0.0608 & 0.0378  & 0.0132 & 0.0322 \\
    & \multirow{2}{*}{Agent 2 (\textcolor{green}{C})}
    & TPR & - & - & - & - \\
    % & TPR & 0.076 & 0.034 & 0.043 & 0.027 \\
    & & FPR & 0.0532 & 0.0176 & 0.0053 & 0.0322 \\

    \hline
    \multirow{4}{*}{S3} 
    & \multirow{2}{*}{Agent 1 (\textcolor{green}{C})} 
    & TPR & - & - & - & - \\
    % & TPR & 0.066 & 0.015 & 0.029 & 0.045 \\
    & & FPR & 0.0481 & 0.0126 & 0.0188 & 0.0241 \\
    & \multirow{2}{*}{Agent 2 (\textcolor{green}{C})}
    & TPR & - & - & - & - \\
    % & TPR & 0.061 & 0.025 & 0.035 & 0.032 \\
    & & FPR & 0.0481 & 0.0176 & 0.0134 & 0.0188 \\
    
    \hline
    \bottomrule
    \end{tabular}
    \label{tab:detection_GPT}
    \vspace{-0.1in}
\end{table}



% Llama CoS
\begin{table}[t!]
\vspace{-0.1in}
    \footnotesize
    \centering
    \caption{Result of Llama3: TPR and FPR of PeerGuard for 2-agent systems on MMLU, CSQA, and ARC datasets.}
    \vspace{-0.05in}
    \begin{tabular}{p{0.15cm}p{1.2cm}p{0.6cm}|cccc}
    \toprule
    \hline
    \multicolumn{3}{c|}{\textbf{Dataset}} & \textbf{MMLU} & \textbf{CSQA} & \textbf{ARC-E} & \textbf{ARC-C} \\
    \hline
    \multirow{4}{*}{S1} 
    & \multirow{2}{*}{Agent 1 (\textcolor{red}{P})} 
    & TPR & 0.7803 & 0.8693 & 0.9598 & 0.9088 \\
    & & FPR & 0.0758 & 0.0320 & 0.0054  & 0.0236 \\
    & \multirow{2}{*}{Agent 2 (\textcolor{red}{P})}
    & TPR & 0.8000 & 0.8719 & 0.9678 & 0.8954 \\
    & & FPR & 0.0618 & 0.0271 & 0.0109 & 0.0315 \\

    \hline
    \multirow{4}{*}{S2} 
    & \multirow{2}{*}{Agent 1 (\textcolor{red}{P})} 
    & TPR & 0.8479 & 0.8643 & 0.9598 & 0.9169 \\
    & & FPR & 0.0366 & 0.0421 & 0.0131 & 0.0345 \\
    & \multirow{2}{*}{Agent 2 (\textcolor{green}{C})}
    & TPR &  - & - & - & - \\
    & & FPR & 0.0310 & 0.0223 & 0.0052 & 0.0186 \\    

    \hline
    \multirow{4}{*}{S3} 
    & \multirow{2}{*}{Agent 1 (\textcolor{green}{C})} 
    & TPR & - & - & - & - \\
    & & FPR & 0.0620 & 0.0177 & 0.0052 & 0.0055 \\
    & \multirow{2}{*}{Agent 2 (\textcolor{green}{C})}
    & TPR & - & - & - & - \\
    & & FPR & 0.0704 & 0.0278 & 0.0104 & 0.0219 \\    
    \hline
    \bottomrule
    \end{tabular}
    \label{tab:detection_LLAMA}
    \vspace{-0.15in}
\end{table}

\begin{figure}[t!]
% \vspace{-0.1in}
    \centering
    \includegraphics[width=0.85\linewidth]{fig/different_framework.jpg}
    \vspace{-0.1in}
    \caption{TPR of the proposed method in various multi-agent frameworks in S1 setting.}
    \vspace{-0.2in}
    \label{fig:CoS in differenct frameworks}
\end{figure}

% \begin{wrapfigure}{r}{0.25\textwidth} 
%     \vspace{-0.15in} 
%     \centering
% \includegraphics[width=\linewidth]{fig/Multi-agent CoS in Different Framework.jpg}
%     \caption{TPR of the proposed method in various multi-agent frameworks in S1 setting.}
%     \label{fig:CoS in differenct frameworks}
%     % \vspace{-0.1in} 
% \end{wrapfigure}
\textbf{4. Generalization of PeerGuard through various MAS framework. }
Our method PeerGuard is deployable to various multi-agent system frameworks as shown in Fig. \ref{fig:CoS in differenct frameworks}. Experiment results conducted on AutoGen and CAMEL frameworks, as well as the multi-agent framework we designed, share high TPRs more than $0.7$ in S1 (the scenario that has 2 poisoned agents in MAS), successfully detected all compromised agents in the system. These high TPRs indicate potential applications of our method to improve the safety of the whole multi-agent system, especially in defending backdoor attacks. 





% Attack success rate (ASR) refers the fraction of samples
% where the answer of LLMs are option "C" based on our experimemt settings. Tab. \ref{tab:attack success rate} demonstrates that trigger "cf" in system prompts of gpt-4o-mini and llama3-70b all share ASR higher than 90\%. The comparison between Llama3 and GPT-4 cross different datasets also indicates that Llama3 herd of models have better performance in instruction following aspect \cite{grattafiori2024llama}. 

% As shown in Table \ref{tab:detection_GPT} and Table \ref{tab:detection_LLAMA}, in all scenarios, each agent will be evaluated through TPR and FPR crossing MMLU, CSQA and ARC with two partitions. In S2, agent 2 is supported by a poisoned LLM while agent 1 is clean. From the result tables, it is obvious that GPT4 is always outperforming Llama3, and the smallest gap between these two models are 16.7\% on MMLMU for TPR. It can be concluded that with higher performance on general benchmark, a LLM will perform better using our defending methods as they can following user instructions more strictly. 
% % It can be concluded that, additionally, MMLU is the hardest datasets among these datasets, receiving the lowest TPR in both GPT4 and Llama3 cases. 

% When we observe the results from the perspective of each scenario, we can discover that all poisoned models can be identified with high TPR. For example, in S2, Agent 2 will be recognized as compromised LLM as its TPR is 84.6\%, suggesting that most of the model's responses with option "C" under any circumstances are successfully detected by defender. But for Agent 1, which is defined as clean one in experiment setting, will not be mistakenly identified as poisoned so its replies are trustworthy. Also, when the questions are not embedded with trigger "cf", even the compromised model will response normally, resulting in low FPR, such as the result of GPT4 with 0.53\% FPR for Agent 2 in S3. Note that not only our method works in detecting compromised agents in the system, but also any agents, regardless of poisoned or not, are able to serve as defenders. 




%%% 500 samples





