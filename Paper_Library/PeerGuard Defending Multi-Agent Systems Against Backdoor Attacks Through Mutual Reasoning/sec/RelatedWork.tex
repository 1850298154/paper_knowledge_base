\section{Related Work}\label{sec:rw}
% \textcolor{red}{TODO}

% Multi-agent framework (two agents)

% backdoor attacks against Multi-agent / LLMs

% defenses

% \vspace{-0.05in}
% \subsection{LLM-based multi-agent systems}
% \vspace{-0.05in}
\noindent\textbf{LLM-based multi-agent systems.} 
Due to powerful reasoning and comprehensive capabilities demonstrated by large language models (LLMs) such as OpenAI o1 \cite{openai2024reasoning} and DeepSeek R1 \cite{guo2025deepseek}, LLM-based multi-agent systems (MAS) outperform RL-based MAS with more flexible and interactive collaboration through reflection or debating. By leveraging external tools or plugins, such as code executor \cite{gao2023pal} and web search \cite{nakano2021webgpt}, LLM-based MAS are able to tackle with more complex tasks collaboratively \cite{li2023camel, wu2023autogen}. \cite{du2023improving} developed the agents to scrutinize the responses of others, debating illogical answers to the question to improve the factuality of the MAS. However, given the vulnerability of LLMs such as poisoning memory and malicious prompt injection, understanding potential threats of MAS is crucial for further LLM-based applications. 

% \vspace{-0.05in}
% \subsection{Backdoor attack on LLMs}
% \vspace{-0.05in}
\noindent\textbf{Backdoor attack on LLMs.} 
Proposed by \cite{gu2017badnets} in computer vision area, backdoor attacks aim to manipulate neural networks to perform malicious behaviors triggered by specific inputs. Researchers have extended this concept to natural language processing \cite{chen2021badnl}. Therefore, recent work has extended backdoor threats to LLMs, where attackers poison training data or training process \cite{li2021backdoor} to output malicious content. Given outstanding aligning ability of LLMs, some attackers also compromise LLM-based agents through prompt injection \cite{liu2023prompt} or poisoning RAG knowledge of LLMs \cite{chen2024agentpoison} when training data and process are inaccessible. Therefore, investigating the robustness of LLM-based agents, particularly within MAS, has become increasingly critical. 

% \vspace{-0.05in}
% \subsection{Multi-agent systems safety}
% \vspace{-0.05in}
\noindent\textbf{Multi-agent systems safety.} 
Collective communications are essential for MAS, yet these communicative collaborations result in susceptible systems as malicious prompts are able to spread across the whole systems when agents are connected to each other \cite{liu2023prompt}. Also, when agents are executing commands, they will be disrupted towards logical operations or be stealthily persuaded to wrong solutions by superior agents \cite{amayuelas2024multiagent}. As demonstrating in \cite{yang2024watch}, it is challenging for LLM-based agents to defend against backdoor vulnerability using textual algorithms. Considering unintended content generating by poisoned LLMs \cite{cao2023stealthy}, defense against backdoor attacks in MAS is a non-trival problem. In this paper, we systematically studied defending performance leveraging reasoning ability of LLMs by setting a framework where agents in MAS can debate each other to figure potential poisoned agents to improve MAS safety. 