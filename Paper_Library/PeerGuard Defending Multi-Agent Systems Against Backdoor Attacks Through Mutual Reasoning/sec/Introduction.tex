% \vspace{-0.05in}
\section{Introduction}\label{sec:intro}


% multi-agent systems and their vulnerabilities
Multi-agent systems (MAS) use large language models (LLMs) as autonomous agents that interact to accomplish complex tasks across various applications \cite{gao2023pal, nakano2021webgpt}. 
While their use is expanding, safety in multi-agent settings remains underexplored, with most research focusing on individual models rather than agent interactions. 
These systems inherit vulnerabilities from LLMs: pre-training on large-scale Internet data introduces harmful content such as bias and racism~\cite{radford2019language}.
Besides, advanced features like in-context learning make attacks easier to execute. 
For example, poisoning attacks can occur at inference time via malicious prompts, bypassing the need to alter training data~\cite{DecodingTrust,BD_ICL}. 
Such vulnerabilities may propagate and intensify through agent interactions~\cite{chen2024agentpoison}, making trustworthiness a growing concern~\cite{schwinn2023adversarial}.


% backdoor in multi-agent systems
Among the threats to multi-agent systems, we focus on backdoor attacks -- an established and potent class of attacks in the AI community. 
These attacks exploit a predefined trigger to induce malicious behavior in one or more agents while preserving normal performance on clean inputs. 
The attack can propagate through agent interactions and influence the collective decision-making process.
The widespread use of third-party LLM services, including APIs and prompt engineering tools, further increases the attack surface: unregulated providers may embed malicious instructions in prompts without altering the model itself \cite{BD_FMFL,BD_FMHFL}. 
For example, in a multi-agent financial assistant system, a poisoned agent could be triggered to recommend risky investments, misleading the other agent in the debate and ultimately influencing the final consensus toward harmful outcomes.


% limitation of existing work
% However, existing backdoor defense research primarily targets single LLMs and addresses only a limited set of attack types, with little focus on MAS. 
% For instance, \cite{qi2020onion} proposes detecting out-of-distribution words in the input to defend against textual backdoor attacks, but this method is ineffective against attacks that do not rely on irregular tokens as triggers. 
% Similarly, \cite{shao2021bddr} attempts to filter suspicious content from training data, which is impractical for most modern LLMs, especially those accessible only through APIs without access to training datasets.
% While \cite{zeng2024autodefense} propose using a coordinator agent to manage the overall defense process in MAS to identify jailbreak attacks, their work focuses on malicious prompts only, lacking attention on a more insidious threat where the model is attacked. 
% Previous works \cite{lee2024prompt, he2025red} focus on malicious injections that propagate throughout the system when agents communicate without compromising the underlying model. In contrast, our work studies backdoor attacks that can selectively target either all agents or a subset of agents, directly embedding malicious behaviors into the model itself, while preserving normal functionality when the trigger is absent. 

However, existing backdoor defense research largely focuses on single LLMs and addresses a limited set of attack types, with minimal exploration in MAS. For instance, \cite{qi2020onion} proposes detecting out-of-distribution words in the input to defend against textual backdoor attacks, but this approach is ineffective against attacks that do not rely on irregular trigger tokens. Similarly, \cite{shao2021bddr} filters suspicious content from training data, which is impractical for most modern LLMs accessed only via APIs without training data visibility. \cite{zeng2024autodefense} introduces a coordinator agent in MAS to detect jailbreak attacks, but focuses solely on malicious prompts and overlooks the deeper threat of compromised models. Other works \cite{lee2024prompt, he2025red} study prompt injection propagation across agents without altering the underlying model. In contrast, our work investigates model-level backdoors that embed malicious behaviors directly into one or more agents, enabling selective triggering while preserving normal outputs in benign cases.




% \textcolor{red}{\textit{``In \cite{zeng2024autodefense}, they propose to screen malicious response in multi-agent systems with a coordinator agent managing the overall defense task.''} You need to describe the limitation of this work, or you need to compare with it if it is feasible... Since no time for experiments, you need to show the limitations}

% \textcolor{red}{OR, Find several LLM backdoor defense paper and list their limitations in multi-agent systems}

% the proposed method and our contribution
This work fills the gap by investigating backdoor vulnerabilities in multi-agent systems and proposing a defense mechanism that leverages agents’ reasoning abilities and their interactions. 
Backdoor attacks cause LLM agents to learn a ``shortcut'' from the trigger to the target output, bypassing logical reasoning. 
To mitigate this, we design demonstrations that encourage agents to explicitly generate reasoning steps, thereby reducing the likelihood of blindly following attack-induced shortcuts. 
Agents then inspect each other’s reasoning process to identify inconsistencies between the rationale and the final answer. 
Any such inconsistency signals a lack of valid support and suggests potential backdoor manipulation. 
We integrate this defense strategy into existing multi-agent frameworks without disrupting their original interaction flow, thereby enhancing robustness in a plug-and-play manner.
In summary, our main contributions are:
\begin{itemize}[leftmargin=*]
    \item We propose PeerGuard: a collaborative defense strategy for multi-agent systems, in which agents autonomously verify each other’s reasoning to detect backdoor-induced inconsistencies, enhancing overall system trustworthiness.
    
    \item We empirically validate the proposed method on diverse benchmarks, demonstrating strong defense performance in GPT- and Llama3-based multi-agent systems.
\end{itemize}


