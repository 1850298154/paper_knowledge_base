\section{Introduction}
\label{sec:intro}

Recommender systems form the foundational infrastructure of the modern digital economy, serving as the primary engine for user engagement and commercial value creation across diverse platforms from e-commerce to content streaming. 
At the heart of this architecture lies the ranking stage, a mission-critical component that employs complex deep learning models to precisely estimate user preferences and thereby maximize key business metrics such as Click-Through Rate (CTR) and conversion~\citep{guo2017deepfm,din,juan2016field,yu2025transun,dai2025onepiece}.
Within modern ranking models, the modeling of user historical behavior sequences has become the most critical module with the core goal of \textit{capturing user interests}. While the existing sequential modeling approaches have driven significant performance gains, we argue that its continued progress is fundamentally constrained by an inherent architectural limitation, \textit{i.e.} the \textit{log-driven} paradigm, which learns user interests by modeling sequences exclusively on interaction logs collected within a closed-loop platform ecosystem. 
This systemic constraint imposes an intrinsic bottleneck for model performance, manifesting in two critical, interconnected limitations:


\paragraph*{Limitation 1: Brittleness of In-Log Interest Modeling due to Knowledge Poverty.}
Existing sequence modeling predominantly adopts \textit{in-log} IDs to represent items, thus learning opaque, high-dimensional ID-based embeddings exclusively from statistical co-occurrence patterns within interaction logs. This approach suffers from knowledge poverty, \textit{i.e.} having limited coverage of product attributes (\textit{e.g.} material and color of clothing) and users' underlying intent (\textit{e.g.} demand for clothing scenarios and styles). Consequently, when interaction data is sparse (common in real-world systems), the poor co-occurrence signal probably causes representation collapse in these statistics-based approaches, leading to their brittle modeling of users' in-log interest.

\paragraph*{Limitation 2: Systemic Blindness to Beyond-Log User Interest.}
The platform logs represent a sparse and biased sample of a users' holistic behavior landscape. They fail to capture a vast universe of external context, including cross-platform activities, offline behaviors, and emerging interests sparked by social trends. Therefore, the in-log paradigm creates a systemic blind spot, rendering the model incapable of perceiving \textit{beyond-log} interests that does not manifest as an explicit on-platform interaction, imposing an inherent bottleneck on model performance. For example, the model probably systematically assign low scores to items perfectly aligned with a users' beyond-log interest, simply because no historical precedent exists in its limited data view. 



\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/motivation.pdf}
    \caption{Illustration of different sequential modeling paradigms, \textit{i.e.} traditional \textcolor{mygrey}{log-driven} and the proposed \textcolor{myblue}{\textbf{\textit{ReaSeq}}}. Our ReaSeq fully unleash world knowledge of LLMs through reasoning techniques, which not only enriches representations to mitigate the knowledge poverty of IDs but also utilize behavior generation to expand model's perception of user interests beyond the log.}
    \label{fig:motivate}
\end{figure}


To address the fundamental limitations of the closed-loop, log-driven paradigm, we argue that a paradigm shift is necessary. Instead of seeking to extract ever-finer statistical signals from the in-log data, the key lies in augmenting the sequential models with external world knowledge to help them better understand user interests. Accordingly, this paper aims to leverage Large Language Models (LLMs) as a foundational knowledge engine to inject world knowledge into sequential models. To fully unleash LLM's world knowledge, we propose \textbf{ReaSeq}, which exploits both explicit and latent \underline{Rea}soning ability of LLM to construct a knowledge system for augmenting the \underline{Seq}uential modeling, offering a direct antidote to the two critical limitations of the log-driven paradigm, as shown in Figure~\ref{fig:motivate}. Specifically, ReaSeq includes two core components:

\begin{itemize}

\item \textbf{Reasoning-Enhanced Representation} (Section~\ref{sec:knowledge_enhanced_ranking}): To combat knowledge poverty, we introduce a multi-agent framework that performs collaborative \textit{explicit} reasoning over dual perspectives: user demand orientation (\textit{what users seek in products}) and product attribute characterization (\textit{what items inherently offer}) (Section~\ref{sec:structured_knowledge}). Through iterative refinement among specialized agents, this process distills nuanced domain knowledge into disentangled, semantically grounded item representations. These knowledge-enhanced embeddings capture product attributes and usage contexts that remain inaccessible to purely collaborative methods, which effectively mitigates representation collapse and provides a robust feature foundation for industrial sequential modeling methods (Section~\ref{sec:ranking}).

\item \textbf{Generative Behavior Reasoning (GBR)} (Section~\ref{sec:user_behavior_augmentation}): To overcome systemic blindness, this component expands the model's perception beyond observed interactions. We design a Diffusion Large Language Model (DLLM) based generative framework tasked with reconstructing plausible but unobserved segments of user behavior sequences. By conditioning on the observed interaction context and leveraging its embedded world knowledge of product relationships and typical intent progressions, the DLLM \textit{implicitly} reasons about user behaviors that align with both in-log patterns and plausible beyond-log preferences. This generative process effectively expands the behavioral signal space, enabling the system to reason about \textit{what a user might have done}, thereby mitigating the model's unawareness of interests that are not captured by platform logs.
\end{itemize}


As shown in Figure~\ref{fig:intro}, our ReaSeq is integrated into a production-grade ranking architecture that unifies explicit knowledge reasoning (Chain of Thought) and implicit behavioral completion (Latent Reasoning) within a scalable, low-latency framework suitable for industrial deployment. By grounding recommendation in world knowledge rather than interaction patterns alone, ReaSeq enables systems to perceive and reason about the recommendation world. This paradigm shift transforms personalization modeling from co-occurrence fitting to world-aware reasoning, laying the groundwork for systems capable of understanding, predicting, and synthesizing new user-item interactions beyond observable behavioral regularities.

Currently, ReaSeq has been fully deployed on the Taobao App. Online experiments demonstrate that ReaSeq achieves consistent performance gains across click-related and conversion-related online metrics (\textit{e.g.} IPV \textbf{+ >6.0\%}, CTR \textbf{+ >6.0\%}, Order \textbf{+ >2.9\%}, GMV \textbf{+>2.5\%}). Furthermore, we investigate the unilateral effectiveness of our GBR, where superior online gains are exhibited (\textit{e.g.} IPV \textbf{+ 2.40\%}, CTR \textbf{+ 2.08\%}, Order \textbf{+ 4.09\%}, GMV \textbf{+5.12\%}). These comprehensive improvement validates that world-knowledge-aware, reasoning-driven paradigm is able to help sequential modeling methods to deeply model the user interests.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/intro.pdf}
    \caption{Architectural overview of the proposed ReaSeq framework. ReaSeq is composed of two synergistic parts:
\textbf{(1) Knowledge System:} This offline module constructs two core assets. Reasoning-Enhanced Representation employs a multi-agent system to generate semantic embeddings from user demand and product attributes. Generative Behavior Reasoning uses a DLLM to locate and reconstruct plausible beyond-log user behaviors.
\textbf{(2) Application}: This online module applies the knowledge assets to enhance sequential modeling. It supports two paradigms: a Retrieval-Based Model for GSU-ESU architectures and a Compression-Based Model that uses target-aware interest extraction for long-sequence modeling.}
    \label{fig:intro}
\end{figure}