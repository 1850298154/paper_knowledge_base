\section{Experiments and Results}
\label{sec:exp}

This section presents experiments and results of the proposed framework. We have evaluated this framework in various simulated scenarios and compared it to ORCA. We have also tested our method on a real multi-robot system. 

\subsection{Experiment Setup}
\subsubsection{\textbf{Scenarios}} We evaluated our learned policy on six different scenarios with different number of agents (as shown in Figure~\ref{fig:scenarios}). Note that
the test scenarios \textit{3 Obstacles} and \textit{1 Obstacle} have static obstacles, which never appear in any data collection scenarios for training the CANet. In addition, since the trained policy outputs the collision avoidance velocity in a random manner, its performance is averaged over 20 simulations. 

We compared the performance of learned policy with the ORCA policy. Most parameters of the ORCA policy are set to be the same as the values used in the data generation for the learned policy (as stated in Section~\ref{sec:data:data-gen}), but we tuned some parameters to optimize ORCA's performance. In particular, to obtain the best performance of ORCA, we 
change $\textsc{timeHorizonObs}$ to $10.0$s for scenarios with static obstacles and tune $\textsc{timeHorizon}$ for different scenarios. 
%$\textsc{protectRadius}$ is difficult to be tuned for ORCA, so we attempt two different values $0.2$m and $0.5$m in our experiments.
In each simulation, the performance of ORCA is evaluated with two different $\textsc{protectRadius}$ values  $0.2$m and $0.5$m.
The time step size $\tau$ of the sensing-acting cycle is set to $0.1$s. The detailed description for each test scenario is as follows:
\renewcommand{\labelitemi}{\textbullet}
\begin{itemize}
\item \textit{Crossing}: agents are separated in two groups, and their path will intersect in the bottom left corner;
\item \textit{Circle}: agents are initially located along a circle and each agent's goal is to reach its antipodal position;
\item \textit{Swap}: two groups of agents moving in opposite directions swap their positions;
\item \textit{Random}: agents are randomly initialized in a cluttered environment and are assigned random goals; 
\item \textit{3 Obstacles}: six agents move across three obstacles;
\item \textit{1 Obstacle}: four agents initialized on a circle move towards their antipodal positions, and an obstacle is located at the center. 
\end{itemize}
The trajectories generated using the learned navigation policy are shown in Figure~\ref{fig:circle} and Figure~\ref{fig:paths}.


\subsubsection{\textbf{Performance Metrics}} To compare the performance of our framework and ORCA quantitatively, we use the following performance metrics: 
\renewcommand{\labelitemi}{\textbullet}
\begin{itemize}
\item \textit{Total travel time}: the time taken by the last agent to reach its goal;
\item \textit{Total distance traveled}: the total distance traveled by all agents to reach their goals.
\item \textit{Safety margin}: the agent's closest distance to other agents and static obstacles;
\item \textit{Completion}: if all agents reach their goals within a time limit without any collisions, the scenario is successfully completed. 
\end{itemize}

\begin{figure} 
\centering
\includegraphics[width=0.8\linewidth]{fig/long7.pdf}
\caption{Six scenarios used to compare the navigation performance of our learned policy and the ORCA policy. 
}
\label{fig:scenarios}
\vspace*{-0.15in}
\end{figure}


\subsection{Quantitative Comparisons}
In Figure~\ref{fig:time} and~\ref{fig:distance}, we measure two metrics -- \textit{total travel time} and \textit{total traveled distance} -- to evaluate the performance of our approach and ORCA.
We can observe that when comparing with the ORCA policy with $\textsc{protectRadius}=0.5$, the learned policy provides better or comparable performance in terms of navigation duration and trajectory length. In most scenarios, the ORCA policy with $\textsc{protectRadius}=0.2$ has shorter navigation time and trajectory length than the learned policy. This is because the ORCA policy with $\textsc{protectRadius}=0.2$ is very aggressive and allows a small safe margin during the navigation, as shown in Table~\ref{tab:safety-margin}. Both our learned policy and the ORCA policy with $\textsc{ProtectRadius}=0.5$ try to keep a large enough margin with nearby agents/obstacles. 
The difference is that the ORCA policy with $\textsc{ProtectRadius}=0.5$ uses the protect radius parameter to keep a hard margin: no obstacles/agents should get closer to the agent than $\textsc{ProtectRadius} - \textsc{radius} = 0.5 - 0.2 = 0.3$m, and this constraint may be too conservative in a cluttered scene. 

Instead, our method learns the preference for margin implicitly from the data and is able to keep the clearance in an adaptive manner: in cluttered situations, the agents can endure a small safety-margin while in an open space, the agents will tend to keep a large safety-margin. For instance, the safety margin in the \emph{3 Obstacles} scenario is smaller than in the \emph{1 Obstacle} scene as shown in Table~\ref{tab:safety-margin}, because the former is more cluttered.

We also set up a more challenging scenario with an L-shape static obstacle at the center (shown in Figure~\ref{fig:stuck}) and measure the \textit{Completion} metric. We randomly generate $100$ initial states where all agents are randomly placed and they are assigned with appropriate random goals. We then compare our method and ORCA by counting the number of failures, i.e., some agents do not reach their goals or severely collide with other agents/obstacles during the runtime. ORCA has a failure rate of $15\%$ while our learned policy only has $2\%$. Figure~\ref{fig:stuck} shows a \textit{stuck} case for ORCA while our learned policy can complete it successfully.

\begin{figure}[!h]
\centering
\includegraphics[width=0.8\linewidth]{fig/long8.pdf}
\caption{Trajectories of five scenarios using the learned policy.} 
\label{fig:paths}
\vspace*{-0.2in}
\end{figure}

\begin{figure} 
\centering
\includegraphics[width=1\linewidth]{fig/long9.pdf}
\caption{Total time of our method and ORCA in all scenarios.}
\label{fig:time}
%\vspace*{-0.2in}
\end{figure}

\begin{figure} 
\centering
\includegraphics[width=1\linewidth]{fig/long10.pdf}
\caption{Distance traveled of our method and ORCA in all scenarios.}
\label{fig:distance}
\end{figure}

\begin{table*}
 \begin{tabularx}{1\textwidth}{l|X|X|X|X|X|X}
  	Safety Margin (min/ave) & Crossing & Circle & Swap & Random & 3 Obstacles & 1 Obstacle  \\
   \hline
%   \hline
   Our method &  0.028 / 0.197 & 0.281 / 0.281 & 0.209 / 0.365 & 0.171 / 0.334 & 0.012 / 0.188 &0.108 / 0.154\\
   \hline
   ORCA - \textsc{protectRadius}=0.5 & 0.300 / 0.375 &	0.262 / 0.297 &	0.276 / 0.295 &	0.297 / 0.299 &	0.299 / 0.301 &	0.300 / 0.365
 \\
   \hline
   ORCA - \textsc{protectRadius}=0.2 & 0.000 / 0.003 & -0.016 / -0.004	& 0.000 / 0.098 &	0.000 /  0.136  &	0.000 / 0.000 &	0.004 / 0.004 \\
 \end{tabularx}
\caption{The minimum and average safety margins for agents when using our learned policy and ORCA policy with $\textsc{protectRadius}=0.5$ and $\textsc{protectRadius}=0.2$ in all six scenarios.}
\label{tab:safety-margin}
\vspace*{-0.2in}
\end{table*}


\begin{figure}[h] 
\begin{subfigure}{0.2\textwidth}
\includegraphics[width=1.0\linewidth]{fig/long11a.pdf}
\caption{ORCA}
\label{fig:stuck-rvo}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
\includegraphics[width=1.0\linewidth]{fig/long11b.pdf}
\caption{Our method}
\label{fig:stuck-dnn}
\end{subfigure}

\caption{For the given scenario with static obstacles,
one agent gets stuck when using the ORCA policy while all agents reach their goals with the learned policy.}
\label{fig:stuck}
\vspace*{-0.2in}
\end{figure}


\subsection{Generalization}

An interesting phenomenon while using the learned policy is that in a highly symmetrical scenario like \textit{Circle}, the agents will present certain cooperative behaviors since all agents are using the same learned policy. For instance, a cooperative rotation behavior is shown in Figure~\ref{fig:circle-dnn} where agents starts to rotate at the same pace when they are close to each other. While for ORCA (as shown in Figure~\ref{fig:circle-rvo}), each agent passes the central area by itself without any collective behaviors and some agents yields jerky motions. 

The good generalization capability is another notable feature of our method. The learned policy's performance  in scenarios with static obstacles demonstrates that it generalizes well to handle to previously unseen situations. In addition, we also evaluate the learned policy in the \textit{Circle} scenario with four different-sized agents. In Figure~\ref{fig:ours-same}, agents with the same size have identical paths as they take the same strategy to avoid collision with each other. When one agent gets bigger (as shown in Figure~\ref{fig:ours-diff}), it will deviate more from the original path to generate safe movements and this causes other agents on its path to adjust navigation behaviors accordingly. Please note that this experiment (Figure~\ref{fig:rvo-same} and~\ref{fig:rvo-diff}) does not reveal the generalization of ORCA since ORCA always knows all agents' radii before computing the collision-free velocity.

We have also demonstrated the proposed method on real robots where each robot is mounted with a Hokuyo URG-04LX-UG01 2D lidar sensor. In Figure~\ref{fig:real-robot}, four robots, three on the right side and one on the left side, are moving to their antipodal positions. As we observe, each robot can effectively avoid collisions with other robots during the navigation in a complete distributed manner. In addition, our system does not require any AR tags and/or additional motion capture systems to offer each agent with the position and/or velocity information about the other agents. In this way, our system can achieve real decentralized multi-agent navigation without any centralized components. 

\begin{figure}[h] 
\centering
\begin{subfigure}{0.22\textwidth}
\includegraphics[width=3.5cm,height=3.5cm]{fig/long12a.pdf}
\caption{Ours -- same $\textsc{radius}$}
\label{fig:ours-same}
\end{subfigure}
\begin{subfigure}{0.22\textwidth}
\includegraphics[width=3.5cm,height=3.5cm]{fig/long12b.pdf}
\caption{Ours -- different $\textsc{radius}$}
\label{fig:ours-diff}
\end{subfigure} \\
\centering
\begin{subfigure}{0.215\textwidth}
\includegraphics[width=3.325cm,height=3cm]{fig/long12c.pdf}
\caption{ORCA -- same $\textsc{radius}$}
\label{fig:rvo-same}
\end{subfigure}
\begin{subfigure}{0.215\textwidth}
\includegraphics[width=3.325cm,height=3cm]{fig/long12d.pdf}
\caption{ORCA -- different $\textsc{radius}$}
\label{fig:rvo-diff}
\end{subfigure}
\caption{Our method generalizes well to situations where agents have different physical sizes. Four agents have the same size in (a) and have different $\textsc{radius}$ in (b). In (b), one of four agents has a larger size than others. For ORCA (c) and (d), the paths of different-sized agents and same-sized agents are similar, since ORCA explicitly knows all agents' radii and velocities before calculating a steering command.}
\label{fig:size}
\vspace*{-0.1in}
\end{figure}

\begin{figure} 
\centering
\includegraphics[width=1\linewidth,height=4.5cm]{fig/long13.pdf}
\caption{A real-robot experiment in 1 vs. 3 scenario. We use different colors to distinguish the trajectories of different agents.}%More videos and experiments are available in https://sites.google.com/view/deepmaca.
\label{fig:real-robot}
\vspace*{-0.2in}
\end{figure}