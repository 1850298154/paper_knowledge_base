\section{Introduction}

\IEEEPARstart{S}{afe} collision avoidance within multi-agent systems is a fundamental problem in robotics, and has many applications including swarm robotics, crowd simulation, AI games, autonomous warehouse and logistics. The problem can generally be defined in the context of an autonomous agent navigating in a scenario with static obstacles and other moving agents. Each agent needs to compute an action at real time and ensure that by executing the action the agent will not collide with the obstacles and other moving agents while making progress towards its goal.

Previous work about multi-agent navigation can be classified into two categories: centralized and decentralized approaches. The centralized approaches focused on computing time-optimal trajectories for all agents to reach their individual goals in a scene with only static obstacles. These methods solve a large optimization problem to compute the time-optimal plans for all agents simultaneously. For this purpose, they usually have a complete knowledge about all agents' initial and goal states, and require a perfect communication (i.e., with small error and delay) between the agents and a central coordinate controller, which are difficult to achieve in practice. In addition, the centralized planning system is difficult to scale to handle large numbers of agents and are not robust to motion errors as well as agent failures.

\begin{figure}[t] 
\centering
\begin{subfigure}{0.20\textwidth}
\includegraphics[width=1.0\linewidth, height=3.5cm]{fig/long1a.pdf}
\caption{ORCA}
\label{fig:circle-rvo}
\end{subfigure}
\begin{subfigure}{0.20\textwidth}
\includegraphics[width=1.0\linewidth, height=3.5cm]{fig/long1b.pdf}
\caption{Our method}
\label{fig:circle-dnn}
\end{subfigure}

\caption{Agent trajectories in the \emph{Circle} scenario using (a) ORCA method and (b) our learned policy. }
\label{fig:circle}
\vspace*{-0.2in}
\end{figure}


To solve the multi-agent navigation in a decentralized manner, we need to replan each agent's local path at real time to deal with the possible conflict with other agents. Among extensive work addressing this problem, the velocity-based approaches~\cite{van2008reciprocal, Berg:ORCA:2011, snape2011hybrid, hennes2012multi, bareiss2015generalized} have gained in popularity due to their robustness and ability to guarantee local collision-free motion for many agents in a cluttered workspace. In the velocity-based framework, each agent employs a continuous cycle of sensing and acting where the action must be computed according to local observations of the environment. These approaches have two main limitations: First, they assume that each agent has perfect sensing about the surrounding environment, while this assumption may be violated due to sensing uncertainty ubiquitous in the real world. This limitation is alleviated in some previous work by using a global localization system (e.g., an overhead motion capture system) to monitor the positions of all agents~\cite{snape2011hybrid,bareiss2015generalized}, or using an inter-agent communication protocol for sharing position and velocity information among nearby agents~\cite{hennes2012multi,godoy2016implicit,claes2012collision}. Second, velocity-based methods usually have many parameters that are sensitive to the scenario settings (e.g., number of agents and shape of obstacles) and thus must be carefully tuned for achieving satisfactory navigation performance. Unfortunately, there is no systematic principle about how to select these parameters, and the manual parameter tuning is tedious.

These limitations motivate us to develop a novel decentralized collision avoidance technique for multi-agent navigation, which should not only work in real-world settings without perfect sensing and inter-agent communications but should also provide good collision avoidance performance without tedious parameter tuning. 

\noindent \textbf{Main results:} 
We present a learning-based collision avoidance framework that provides an end-to-end solution for distributed multi-agent navigation by directly mapping noisy sensor measurements to a steering velocity that is locally collision-free. The end-to-end framework is implemented as Deep Neural Networks (DNNs).
To train the network, we collect an extensive dataset consisting of frames showing how an agent should avoid its surrounding agents. Each frame includes both the agent's observation about other agents and the agent's reactive collision avoidance strategy in terms of the steering velocity. The dataset is generated by using a state-of-the-art multi-agent simulator with various parameter settings. We also perform data augmentation on the collected dataset by adding measurement noises and leveraging symmetries to generate more frames, which help to reduce over-fitting and improve the generalization capability of our framework.  We train the neural network in an offline manner, and the network learns how to output a collision-avoidance velocity given inputs determined by the agent's sensor measurements and its goal setting. During the online test, our network will output a local collision avoidance velocity that is then used to update the agent's position at each sensing-acting cycle until the agent reaches its goal. We evaluate our approach on a variety of simulation scenarios and compare it to the state-of-the-art distributed collision avoidance framework~\cite{Berg:ORCA:2011}. 
Our experiments show that our method can effectively generate collision-free motions for multiple agents, and the learned collision avoidance policy is robust against noises in the sensor measurements. 
Moreover, we also highlight that the learned policy can be well generalized to scenarios that are unseen in the training data, including scenes with static obstacles and with a different number of agents. 


%This paper is organized as follows. We provide a brief overview over the related work in Section~\ref{sec:related} and formulate the distributed multi-agent navigation problem in Section~\ref{sec:prob}.
%In Section~\ref{sec:approach}, we first summarize necessary background about the multi-agent simulator ORCA, and then elaborate our approach including the dataset collection, the network architecture and training. We evaluate our method and present the experimental results in Section~\ref{sec:exp}, and conclude the paper with a brief discussion in Section~\ref{sec:conclusion}.


\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{fig/long2.pdf} 
\caption{An overview of our approach. A large number of collision avoidance frames collected by repeatedly running a multi-agent simulator are used to train the reactive navigation controller in terms of a collision avoidance network (CANet). At each frame, we record an agent's sensor measurement $\mathbf z$ about the other agents, the agent's preferred velocity $\mathbf v^{pref}$ related to the agent's individual goal, and the corresponding collision avoidance velocity $\mathbf v$; we also estimate $\dot{\mathbf z}$, the velocity about the sensor measurement. All these quantities are then converted into the agent's local coordinate. The navigation controller then has $\mathbf z$, $\dot{\mathbf z}$, and $\mathbf v^{pref}$ as inputs, and $\mathbf v^+$ as the output. During the online navigation, the learned navigation policy is used by agents to
make reactive collision avoidance decisions.}
\label{fig:overview}
\vspace*{-0.2in}
\end{figure}
