\section{Related Work}
\label{sec:related}
% You can comment this sentence. 
In this section, we provide a brief overview of prior work on collision avoidance for multi-agent navigation and machine learning for multi-agent systems.  

\subsection{Collision Avoidance for Multi-Agent Navigation}
Collision avoidance has been studied extensively for safe and efficient multi-agent navigation. Many approaches have been proposed, including techniques based on potential fields~\cite{koren1991potential}, local variable-resolution fields~\cite{kapadia2012parallelized}, dynamic windows~\cite{Foxdw}, and velocity obstacles~\cite{fiorini1998motion,van2008reciprocal,Berg:ORCA:2011}.
Among them is the Optimal Reciprocal Collision Avoidance (ORCA) navigation framework~\cite{Berg:ORCA:2011}, which has been a successful velocity-based approach to avoid collisions with other moving agents and obstacles. ORCA has been popular in crowd simulation and multi-agent systems due to its two properties. First, it provides a sufficient condition for multiple robots to avoid collisions with each other, and thus can guarantee collision-free navigation; second, it is a fully distributed method where robots share no knowledge with each other, and thus can easily be scaled to handle large systems with many robots. However, ORCA and its variants (e.g.,~\cite{snape2011hybrid,bareiss2015generalized}) have many parameters that are difficult to tune. More important, these methods are sensitive to the uncertainties ubiquitous in the real-world scenarios. In particular, each robot is assumed to have an accurate observation about the surrounding agents' positions, velocities and shapes; while in practice such information is extracted from noisy sensing measurement via segmentation and tracking, and thus may have significant uncertainties. To alleviate the requirement of perfect sensing, Hennes et al.~\cite{hennes2012multi} and Claes et al.~\cite{claes2012collision} extended the ORCA paradigm with an inter-agent communication protocol for sharing knowledge about agents' positions and
velocities. A one-way communication scheme is also introduced
in~\cite{godoy2016implicit} to coordinate the movement of agents in a crowd scenario. Other approaches~\cite{snape2011hybrid,bareiss2015generalized} avoided
the difficulty in sensing uncertainty by using an overhead motion capture system to obtain the global position observation for all agents. Moreover, Yoon et al.~\cite{yoon2016filling} used multiple visual sensors to track individuals' trajectories in crowds. In this paper, we use
an end-to-end framework to learn a collision-avoidance policy which is robust to the imperfect sensing and requires no inter-agent communication, and thus is still fully distributed. 

\subsection{Machine Learning for Multi-Agent Systems} 
Reinforcement learning has been widely used for the multi-agent decision making~\cite{mataric1997reinforcement,stone2000multiagent,yang2004multiagent,panait2005cooperative}, which is formulated as a multi-agent Markov Decision Processes (MDP) problem. Multi-agent reinforcement learning allows agents to learn a policy, i.e., a mapping from agent states to actions according to the rewards obtained while interacting with the surrounding environment. In these methods, each independent agent needs to build an MDP model via repeated offline simulation, and then uses the model-based reinforcement learning to compute an optimal policy. An online multi-agent policy adaption approach is proposed by Godoy et al.~\cite{godoy2015adaptive}, which originates from multi-arm bandits. They use an online learning framework to plan over the space of preferred velocities and then project these velocities to collision-free ones using ORCA, e.g., their method still holds the perfect sensing assumption and requires parameter-tuning for ORCA. The cooperative approach proposed by Kretzschmar et al.~\cite{kretzschmar2016socially} first infers the internal goals of other agents, then plans a set of jointly possible paths for all neighboring agents in the shared environment. However, it is computationally expensive for generating paths for all others agents. Boatright et al.~\cite{boatright2015generating} train a set of machine-learned policies by decomposing possible scenarios an agent may encounter into steering contexts.
In this paper, we use an end-to-end learning mechanism for supervised training a deep neural network policy from an extensive collection of multi-agent collision avoidance data. 

%\subsection{Deep Learning and Its Applications in Robotics}
%Deep learning methods have been successfully used in a wide variety of applications, especially in computer vision~\cite{krizhevsky2012imagenet,he2016deep} and nature language processing~\cite{graves2013speech,amodei2015deep}. In these problems,appropriate hidden representations learned from large amounts of data lead to significant performance improvement compared to traditional approaches. These successes indicate that deep neural networks have the ability to handle complex, high-dimensional, raw input from onboard sensors, which is essential to robotic applications.

%Recently, there is an increasing amount of research on using deep neural networks to solve robotic perception and control problems. LeCun et al.~\cite{muller2005off} developed a vision-based obstacle avoidance system for a mobile robot by training a 6-layer convolutional network which maps raw input images to steering angles. Levine et al.~\cite{levine2016end} presented an end-to-end framework which learns control policies mapping raw image observations to torques at the robotâ€™s motors. Deep neural networks have also been combined with model predictive control to control robotic systems~\cite{lenzdeepMPC2015,zhang2015learning}. Ondruska et al.~\cite{OndruskaAAAI2016,OndruskaRSS2016} combined recurrent neural networks with convolutional operations to learn a mapping directly from raw 2D laser data to the unoccluded state of the entire scene; and the mapping is then used for object tracking. In this paper, we train a 6-layer neural network to produce the navigation velocity for each agent according to the current crowd status. 
