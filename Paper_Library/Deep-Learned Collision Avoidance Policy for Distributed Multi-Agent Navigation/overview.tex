\section{Problem Formulation}
\label{sec:prob}
The multi-agent navigation problem can be formally defined as follows. We take as given a set of $n$ decision-making agents sharing a 2D environment consisting of obstacles. For simplicity, we assume the geometric shape of each agent $a_i$ $(1 \leq i \leq n)$ is modeled as a disc with a fixed radius $r_{a_i}$. In addition, the agent's dynamics is assumed to be holonomic, i.e., it can move in any direction in the 2D workspace. 

Each agent employs a continual cycle of sensing and acting with a time period $\tau$. During each cycle, the agent $a_i$
computes a local trajectory that starts from its current position $\mathbf p_{a_i}$ and has a smallest deviation from a preferred velocity $\mathbf v_{a_i}^{pref}$. The preferred velocity is used to guide the agent in making progress toward its goal $\mathbf g_{a_i}$. In a scenario without static obstacles, $\mathbf v_{a_i}^{pref}$ can directly point toward  $\mathbf g_{a_i}$; in a scene with static obstacles, $\mathbf v_{a_i}^{pref}$ may point toward a closest node in a precomputed roadmap~\cite{van2008reciprocal}. The local trajectory should be collision-free with other agents and obstacles, at least within the time horizon $\tau$. 

The agent computes the local trajectory by taking into account three factors: its current velocity $\mathbf v_{a_i}$, the observation $\mathbf o_{a_i}$ about the surrounding environment, and its preferred velocity $\mathbf v_{a_i}^{pref}$. In previous work such as~\cite{Berg:ORCA:2011,van2008reciprocal}, the observation $\mathbf o_{a_i}$ consists of the nearby agents' positions, velocities and shapes. However, the estimation of these quantities in the real world requires agent-based recognition and tracking, which is difficult to be implemented reliably. In our method, $\mathbf o_{a_i}$ only includes the raw sensor measurements about the surrounding environment, and thus is more robust and feasible in practice.  

In particular, the agent feeds $\mathbf p_{a_i}$, $\mathbf v_{a_i}$, $\mathbf o_{a_i}$ and $\mathbf v_{a_i}^{pref}$ into a reactive controller $F$, whose output will be parsed as a collision avoidance velocity in the next step:
\begin{equation}
  \mathbf v_{a_i}^+ = F(\mathbf p_{a_i}, \mathbf v_{a_i}, \mathbf o_{a_i}, \mathbf v_{a_i}^{pref}),
\end{equation}
which is then executed by the agent to update its position as
\begin{equation}
  \mathbf p_{a_i}^+ = \mathbf p_{a_i} + \mathbf v_{a_i}^+ \cdot \tau.
\end{equation}
The agent repeats this cycle until arriving at its goal. During the navigation, agents are not allowed to communicate with each other and must make navigation decisions independently, according to the observations collected by their on-board sensors. We do not assume that the agents have perfect sensing about the positions, velocities and shapes of other agents, while such knowledge is usually necessary for previous approaches~\cite{van2008reciprocal,Berg:ORCA:2011,bareiss2015generalized,snape2011hybrid,godoy2015adaptive}. 

The reactive controller $F$ is computed by first converting the observation $\mathbf o_{a_i}$ and $\mathbf v_{a_i}^{pref}$ into the local coordinate frame of the agent $a_i$, and then training a deep neural network $f$ using these data as network inputs. The network solves a multi-class classification problem and outputs a probability distribution which is used to determine the agent's velocity increment $\Delta \mathbf  v_{a_i}$ for safe collision avoidance and making progress towards the goal. The eventual collision avoidance velocity $\mathbf v_{a_i}^+$ is then computed as 
\begin{equation}
\mathbf v_{a_i}^+ = \mathbf v_{a_i} + \Delta \mathbf v_{a_i}.
\end{equation}
We name this deep neural network as the \emph{collision avoidance network} (CANet).



