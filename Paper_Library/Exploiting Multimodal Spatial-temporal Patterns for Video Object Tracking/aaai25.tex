%File: formatting-instructions-latex-2025.tex
%release 2025.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
\usepackage{amsmath}
\usepackage{amsfonts}
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[table]{xcolor}
\usepackage{multirow} 
\usepackage{booktabs}
% \usepackage{caption}
%\usepackage{graphicx}
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{ Exploiting Multimodal Spatial-temporal Patterns for Video Object Tracking}
\author {
    % Authors
    Xiantao Hu\textsuperscript{\rm 1},
    Ying Tai\textsuperscript{\rm 2,1}\thanks{ Ying Tai and Jian Yang are the corresponding authors.},
    Xu Zhao\textsuperscript{\rm 1},
    Chen Zhao\textsuperscript{\rm 2},
    Zhenyu Zhang\textsuperscript{\rm 2},
    Jun Li\textsuperscript{\rm 1},
    Bineng Zhong\textsuperscript{\rm 3},
    \\
    Jian Yang\textsuperscript{\rm 1}\footnotemark[1]
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, School of Computer Science and Engineering, Nanjing University of Science and Technology\\
    \textsuperscript{\rm 2}Nanjing University 
    \textsuperscript{\rm 3}Guangxi Normal University\\
    
    \{xiantaohu, csjyang, junli\}@njust.edu.cn, \{yingtai,zhenyuzhang\}@nju.edu.cn, bnzhong@gxnu.edu.cn
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1,\rm 2},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
Multimodal tracking has garnered widespread attention as a result of its ability to effectively address the inherent limitations of traditional RGB tracking. However, existing multimodal trackers mainly focus on the fusion and enhancement of spatial features or merely leverage the sparse temporal relationships between video frames. These approaches do not fully exploit the temporal correlations in multimodal videos, making it difficult to capture the dynamic changes and motion information of targets in complex scenarios. To alleviate this problem, we propose a unified multimodal spatial-temporal tracking approach named STTrack. In contrast to previous paradigms that solely relied on updating reference information, we introduced a temporal state generator (TSG) that continuously generates a sequence of tokens containing multimodal temporal information. These temporal information tokens are used to guide the localization of the target in the next time state, establish long-range contextual relationships between video frames, and capture the temporal trajectory of the target. Furthermore, at the spatial level, we introduced the mamba fusion and background suppression interactive (BSI) modules. These modules establish a dual-stage mechanism for coordinating information interaction and fusion between modalities. Extensive comparisons on five benchmark datasets illustrate that STTrack achieves state-of-the-art performance across various multimodal tracking scenarios. 
Code is available at: \url{https://github.com/NJU-PCALab/STTrack}.
% making it challenging to capture the dynamic changes and motion information of targets in complex scenes.
% showcasing its ability to unify and optimize the integration of multimodal spatial-temporal information.
% Unlike previous paradigms that rely solely on template updates, we introduces a state generator that continuously produces token sequences containing compressed multimodal spatial-temporal information from videos. These sequences are uniformly used as guidance prompts for current target localization, establishing long-range contextual relationships between video frames and capturing the target's spatial-temporal trajectory.
% Additionally, we introduced a direct modality interaction method to enhance spatial features of the modalities. By evaluating the correlation between bidirectional blocks, templates, and search areas, this method avoids interaction redundancy, enabling more accurate collection and distribution of multimodal related information.
% Extensive comparisons on five benchmark datasets illustrate that STTrack achieves state-of-the-art performance across various multimodal tracking scenarios, showcasing its ability to unify and optimize the integration of multimodal spatial-temporal information.
\end{abstract}


% \begin{figure}[t]
%   \centering
%     \includegraphics[width=1\linewidth,height=6 cm]
%     {scores.pdf}
%    \caption{
% A comparison of state-of-the-art multimodal trackers using RGB-T and RGB-E on the LasHeR and VisEvent datasets. Our STTrack-384 model sets a new state-of-the-art (SOTA) with 61.2\% AUC on LasHeR and 63.8\% AUC on VisEvent, demonstrating impressive multimodal tracking performance.  }
%    \label{fig:simple}
% \end{figure}


% Uncomment the following to link to your code, datasets, an extended version or similar.
%
% \begin{links}
%     \link{Code}{https://aaai.org/example/code}
%     \link{Datasets}{https://aaai.org/example/datasets}
%     \link{Extended version}{https://aaai.org/example/extended-version}
% \end{links}
\begin{figure}[t]
  \centering
    \includegraphics[width=1\linewidth,height=6.8cm]
    {contrast.pdf}
   \caption{
\textbf{Illustrations of different frameworks of multimodal trackers (a)-(c), and performance comparison (d)}. (a) Offline multimodal tracker performs offline tracking of video sequences using fixed template frames.
 (b) Online multimodal tracker is based on an updating strategy, which utilizes the results condition to update the reference information.% (such as template images, search images or historical frame features). 
 (c) Our proposed STTrack transmits multimodal temporal information throughout the tracking process. 
 (d) STTrack achieves superior performance against recent state-of-the-art competitors on three popular multimodal tasks.}
   \label{fig:contrast}

\end{figure}
  
\section{Introduction}
Visual object tracking is the process of locating and following a specific object across consecutive frames in a video sequence. As a fundamental vision task, it is essential for various applications~\cite{wang2022learning,wang2025gpsformer,jiang2023lttpoint, zhang2024deformation} and their related tasks~\cite{anSHaRPoseSparseHighResolution2024,zheng2023curricular,zhang2024few,fang2023multi,nan2024openvid,ning2023pedestrian}.
% including surveillance~\cite{AliEmami2012RoleOS}, human-computer interaction~\cite{LiweiLiu2012HandPR}, and augmented reality~\cite{wagner2009real}.
Despite numerous efficient RGB-based trackers~\cite{AQAtrack,evptrack,FFtrack,artrack,siamban,transt,siamban_p,xue2024unifying} have been proposed through high quality dataset~\cite{lasot,got10k,trackingnet}, they are still limited by the degradation of RGB imaging quality caused by the complexity of real-world scenarios, which leads to tracking errors.
Compared to RGB modalities, thermal infrared (TIR) provides clear target information in low light environments; depth modalities offer distance cues from depth cameras; and event modalities use event-based cameras to capture motion information and generate stable target trajectories.
Therefore, developing an effective multimodal tracker that combines various modality X (such as TIR, depth, and event) with RGB is crucial for robust tracking.

Multimodal tracking methods can be broadly categorized into: \textit{Offline trackers with fixed template frames} and \textit{online trackers that update reference information}. 
$1$) Traditional offline multimodal trackers focus on the fusion and interaction of spatial multimodal features, evolving from early CNN architectures~\cite{protrack,apfnet,mfDiMP} to the recent Transformer architectures~\cite{chen2024top,mctrack}. 
As shown in Fig.~\ref{fig:contrast}~(a), offline trackers rely on a fixed initial target appearance as reference information for the entire tracking process. 
However, as time passes, the target may deform or become occluded, rendering the initial template frame unable to accurately capture its current state.
%However, over time, the target may undergo deformation or occlusion, causing the initial template frame to fail in accurately reflecting the current state of the target. 
$2$) In contrast, as depicted in Fig.~\ref{fig:contrast}~(b), online trackers capture more recent target appearance features by updating reference information, such as template images~\cite{tatrack,mplt}, search images~\cite{TAAT}, or historical frame features~\cite{DMSTM}. 
Although these approaches enable updates at specific points in time, their reliance on sparse temporal relationships (\textit{i.e.}, updates limited to specific conditions) neglects the continuity of temporal information. % FIXME: implicit or sparse?
In video tracking tasks, \textit{target changes and movements} typically follow a certain trend, which is challenging to capture and express without explicit temporal modeling, thus limiting the model's performance in complex scenarios.

To address this issue, we propose a novel tracking framework STTrack based on multimodal spatial-temporal patterns. 
STTrack improves to capture and represent the dynamic target by explicitly leveraging the temporal context within multimodal video data.
As shown in Fig.~\ref{fig:contrast}~(c), we make full use of the multimodal temporal information from videos to guide the modeling of the current state of targets, thereby constructing a unified multimodal temporal strategy. 
There are several critical modules in STTrack. 
$1$) At the temporal level, we design a novel \textit{temporal state generator (TSG) based on cross mamba architecture}~\cite{sigma}. 
TSG combines the current cross-modal target representation features with previous multimodal temporal information, employing an autoregressive mechanism to generate multimodal temporal information tokens for the current time step.
These tokens act as bridges for information transfer, facilitating the tracking process for the next time node. 
$2$) At the spatial level, since cross-modal interaction and fusion are crucial for effective multimodal tracking,
we therefore propose the \textit{background suppression interactive} module in the feature extraction stage of the visual encoder, and the \textit{mamba fusion} module in the final modality fusion stage, respectively. 
The BSI module improves each modality branch’s representation by integrating features from other modalities, while the mamba fusion module dynamically merges multimodal features from both branches to facilitate precise object localization.
%Thus, we propose the background suppression interactive (BSI) module and the mamba fusion module, applied respectively in the feature extraction stage of the visual encoder and the final modality fusion stage. 
%Specifically, in the BSI module, each modality branch enhances its representation capability by integrating features from other modalities. Meanwhile, the mamba fusion module dynamically fuses the multimodal features from the two branches to support the final object localization.


% Additionally, cross-modal interaction is the key to multimodal tracking. Unrelated background information in the search area will greatly increase the difficulty of interaction, lead to deviations in feature fusion, and thereby affect the accuracy and stability of tracking. At the same time, the one-stream structure~\cite{ostrack} of the visual encoder can provide a priori to help identify the background area during the modeling process. Based on this, we have designed an background suppression interactive block to dynamically allocate weights based on the strength of the correlation between the template and the temporal token and the search area, thereby more effectively highlighting the target features in search area and suppressing background interference. Furthermore, STTrack uses the mamba fusion block as the multimodal fusion Blocks, which dynamically adjusts the attention weights between different modalities in the final multimodal feature fusion process, thereby adapting to the dynamic changes of different scenes and targets.

% As a video-level task, multimodal tracking necessitates extracting discriminative features from diverse modal template images for accurately locating the target in the current frame.
% In pursuit of this, researchers have explored various methods, from early CNN architectures~\cite{protrack,apfnet} to the more recent transformer architectures~\cite{sdstrack,untrack,mctrack}, and have designed relevant modality interaction schemes.


% In real-world scenarios, targets may undergo deformation or partial occlusion, making the appearance information in the initial template frame often unable to accurately reflect the current state of the target. MPLT~\cite{mplt} and TATrack~\cite{tatrack} introduce additional dynamic template frames to integrate the latest appearance information from the multimodal video sequence with the initial appearance information. Although this method provides information updates at specific time points, its reliance on sparse spatialtemporal relationships (i.e., updates only in dynamic templates) overlooks the continuity of spatialtemporal information, thus limiting the effective integration of continuous changes in targets within multimodal videos. Additionally, recent research has shown that in-depth analysis of the continuous relationships between video frames can reveal important clues about the target, a finding validated in tasks such as human pose estimation~\cite{position}, RGB-based tracking~\cite{AQAtrack}, video super-resolution~\cite{video_su}, and action recognition~\cite{TDN}. Due to the complexity of multimodal data, the application of these methods in multimodal tracking still requires further exploration.

% Since the target might undergo potential deformations or partial occlusions, the appearance information in the initial template frame often fails to accurately reflect the current state of the target. MPLT~\cite{mplt} and TATrack~\cite{tatrack} use additional dynamic template frames to integrate recent appearance information from multimodal video sequences with the initial appearance information. Although this approach brings in sparse spatial-temporal relationships, it overlooks the continuity of spatial-temporal correlations, thereby restricting its ability to integrate information from distant parts of multimodal videos. More recently, some studies have shown that analyzing temporal differences between video frames can reveal significant clues about targets, validated across various tasks such as human pose estimation~\cite{position}, RGB-based tracking~\cite{AQAtrack}, video super-resolution~\cite{video_su}, and action recognition~\cite{TDN}. However, due to the complexity of multimodal data, the application of such methods in multimodal tracking has not been thoroughly explored.



% cross-modal interaction has been a crucial step in multimodal tracking. We have optimized previous methods that required a large number of tokens for interaction by using partial features as mediators for collecting and distributing modal data. This approach not only reduces interference from non-target features but also significantly reduces computational burden. Furthermore, STTrack introduces Concate Mamba Blocks, which dynamically adjust attention weights between different modalities during the final multimodal feature fusion process, thereby adapting to dynamic changes in different scenes and targets. 

% Furthermore, cross-modal interaction is a crucial step in multimodal tracking. The presence of background information unrelated to the target in the search area can significantly increase the difficulty of interaction. The interference of this background information may cause deviations in the feature fusion between different modalities, thereby affecting the accuracy and stability of the tracker. Meanwhile, the single-stream structure of the visual encoder can provide a strong prior for the similarity between the target and the search area, that is, the encoder can gradually identify the background area during the modeling process. For this reason, we combine the correlation degree of the template and the temporal token with respect to the search area and design an interaction adjustment mechanism. This mechanism dynamically allocates weights to different features based on the strength of the correlation between the template and the temporal token and the search area in different modalities, thereby more effectively highlighting the target features and suppressing background interference in cross-modal interaction.

We summarize the contributions of STTrack as follows:

\begin{itemize}
\item To fully exploit the temporal information from multiple modalities, we propose STTrack, which introduces temporal state generator to reveal temporal context of target.
% \item To fully exploit temporal information from multiple modalities, we propose STTrack, which introduces a Temporal State Generator to capture the sequential context of the target.B

\item We propose the BSI and mamba fusion modules, which optimize information interaction and dynamic fusion between modalities during the feature extraction and modality fusion stages, respectively.
% \item In the proposed tracker, we designed an innovative cross mamba block that combines past and current target states to capture the temporal features of the target. We use continuous temporal features as guiding cues for future states. Additionally, we improved modal interactions by adopting specific modal feature collection and distribution methods, thereby achieving superior performance.
% \item In the proposed tracker, we innovatively designed a state generator that combines past and current target states to capture the temporal features of the target. We use continuous temporal features as guiding cues for future states. Additionally, we have improved modal interactions by adopting specific modal feature collection and distribution methods, thereby achieving superior performance.
\item The proposed STTrack achieves state-of-the-art performance on five popular multimodal tracking benchmarks, including RGBT234, LasHeR, VisEvEnt, Depthtrack, and VOT-RGBD2022.
\end{itemize}

\begin{figure*}
  \centering
    \includegraphics[width=1\linewidth,height= 6 cm]
    {structure.pdf}
   \caption{
\textbf{Overall architecture of STTrack}. The temporal information tokens of each modality, along with the image tokens, are fed into the vision encoder to guide the extraction of current features using temporal information. In our designed Temporal State Generator, the current temporal tokens are generated based on cross-modal features and previous temporal features. We have added cross modal interaction in Visual Encode. Finally, the features are finely adjusted and fused through the mamba fusion module and then fed into the tracking head to predict the current state.}
   \label{fig:structure}
\end{figure*}


\section{Related Works}
% \textbf{Multimodal Tracking.} In recent years, there has been growing research interest in exploring methods to achieve more robust tracking in complex scenes. Multimodal approaches have garnered widespread attention as an effective means to address this challenge.
\textbf{Multimodal Tracking.} 
%In recent years, multimodal tracking, as an effective method to achieve more robust tracking in complex scenarios, has received widespread attention.
%This is because in the tracking process, multimodal can complement each other to address challenges that a single modality alone cannot handle. 
In recent years, multimodal tracking has gained widespread attention for its ability to achieve robust tracking in complex scenarios. By allowing different modalities to complement each other, it overcomes challenges that a single modality cannot address on its own.
% This is because, during tracking, different modalities can complement each other, alleviating challenges that a single modality cannot handle. 
Early multimodal tracking methods~\cite{ADRNet,CAT} typically focused on specific multimodal task. 
For instance, APFNet introduces the concept of attribute fusion based on ResNet~\cite{resnet}, enhancing its performance under specific challenges. 
TBSI~\cite{tbsi} extends ViT~\cite{vit_transformer} to RGB-T tasks and leverages the TBSI module to optimize cross-modal interactions. 
More recently, some works~\cite{oneTracker} have begun exploring unified architectures capable of handling multiple multimodal tasks. 
ViPT~\cite{vipt} integrates other modalities into the RGB modality through a prompt mechanism. 
SDSTrack~\cite{sdstrack} and BAT~\cite{bat} explore symmetrical architectures for primary and auxiliary modality transformations.
However, existing unified multimodal tracking frameworks often perform coarse multi-level interactions on all modality features within the encoder, inevitably introducing \textit{irrelevant background noise into the search area}. 
In this work, we propose a novel BSI module to leverage the correlation strength among the template, temporal information, and search area to \textit{emphasize target features while suppressing background interference}.

% To address this, we propose a Background Suppression Interactive (BSI) module that leverages strong priors from a single-stream structure and is based on the correlation between the template, temporal markers, and search area. This module will highlights target features and suppresses background interference.
% SeqTrackv2~\cite{seqtrackv2} and TATrack~\cite{tatrack} have successfully improved performance by introducing dynamic template frames.  or sparse temporal (dynamic update of templates) 


% However, these methods often rely on static (using only initial template frames) representations of targets in multimodal settings, making it difficult to accurately understand changes in targets over time. So, MPLT~\cite{mplt} and TATrack~\cite{tatrack} have successfully improved performance by introducing dynamic template frames. In contrast, our approach differs by employing continuous frame-to-frame information exchange to connect the entire video tracking process, effectively leveraging video context to understand the temporal evolution of targets.

\noindent \textbf{Temperoal Modeling}.
%Object tracking is a video-level task where the target and its surrounding environment evolve over time. 
Temporal information is crucial for tracking models to capture long-term changes and motion trends of the target. 
In RGB-based object tracking, researchers~\cite{stark,seqtrack,swintrack,cttrack,mixformer} have carefully designed various update strategies, typically guiding current state tracking through the fusion of accumulated templates. 
% For instance,
STMTrack~\cite{stmtrack} proposed a spatial-temporal memory network to exploit historical information. 
In contrast, multimodal tracking scenarios are more complex, requiring the consideration of not only RGB information but also the integration of additional modalities. 
In RGB-T Tracker, DMSTM~\cite{DMSTM} uses a dual-modality space-time memory network to aggregate historical information as well as the apparent information of the current frame. 
TATrack~\cite{tatrack} have successfully improved performance by combining dynamic template frames of the two modalities. 

However, exploration in the temporal dimension of multimodal tracking currently faces two main challenges:
%This limitation hinders the continuous transmission of information over time, thereby impeding the accurate capture of continuous target movements and changes by the model. 
$1$) Updating reference materials, such as template images and historical frame information, often depends on preset conditions, leading to \textit{sparse temporal relationships}. 
This limitation disrupts the continuous flow of information, making it difficult for the model to accurately capture ongoing target movements and changes over time.
$2$) Existing temporal exploration designs primarily focus on \textit{single multimodal task}, limiting their effectiveness in multi-task environments.
In contrast, our STTrack framework leverages explicit frame-to-frame temporal information exchange, capturing the target's temporal evolution using video context. 
Our method improves \textit{temporal continuity and contextual coherence}, as verified in the experiment section, and demonstrates its potential for unified application across \textit{various visual multimodal tasks}.
% Existing temporal exploration designs are predominantly focused on single multimodal tasks, which restricts their applicability in multi-task environments. 
%In contrast, our approach leverages continuous frame-to-frame information exchange throughout the video tracking process, effectively utilizing video context to understand the temporal evolution of the target. 
%In contrast, our STTrack harnesses explicit frame-to-frame temporal information exchange throughout the video tracking process, effectively leveraging video context to capture the target's temporal evolution. This enhances the \textit{continuity of temporal information and coherence of context}, and its applicability is demonstrated through experiments, showcasing its potential for unified application across \textit{various visual multimodal tasks}.

%This method improves the continuity of temporal information and contextual coherence. %Experiments demonstrate its broad applicability, highlighting its potential for unified application across diverse visual multimodal tasks.

% In contrast, our approach differs by employing continuous frame-to-frame information exchange to connect the entire video tracking process, effectively leveraging video context to understand the temporal evolution of targets. 



% while ARTrack~\cite{artrack} modeled the sequential evolution of trajectories using temporal autoregression. AQATrack~\cite{AQAtrack} combined specially designed spatial-temporal fusion templates and a temporal decoder to integrate static appearance and instantaneous changes.




\section{Methodology}
In this paper, we introduce a novel spatial-temporal tracker (STTrack) based on temporal information, enabling continuous frame-to-frame information transfer through spatial-temporal data. 
%In addition, two modules are integrated into the spatial modeling process to facilitate multimodal feature interaction and fusion. 
Fig.~\ref{fig:structure} illustrates the overall architecture of STTrack. In this section, we first briefly review the state space model. Subsequently, we provide a detailed introduction to the overall architecture of our STTrack.


\subsection{Preliminaries}
\label{sec:pre}
The state space models draw inspiration from continuous linear time-invariant (LTI) systems.
The aim of SSM is to transform a one-dimensional function or sequence, represented as $x(t)$, into $y(t)$ via the hidden space $h(t) \in R^{N}$ with linear complexity. The system can be represented mathematically by the following formula:
\begin{equation}
\begin{gathered}
h^{\prime}(t) = Ah(t) + Bx(t),\\
  y(t) = Ch(t) + Dx(t),
\end{gathered}
  \label{eq:ssm}
\end{equation}
where the system's count parameters include the evolution parameter
$A \in \mathbb{R}^{N \times N}$, projection parameters $B \in \mathbb{R}^{N \times 1}$ and $C \in \mathbb{R}^{1 \times N} $, and skip connection $D\in\mathbb{R}$.
The $h^{\prime}(t)$ refers to the time derivative of $h(t)$, and $N$ is the state size.

When handling discrete sequences such as images and text, state space models need to convert continuous-time signals into discrete-time signals to accommodate the nature of discrete data. SSM adopt zero-order hold (ZOH) discretization to map the input sequence $\{x^1, x^2, ..., x^k\}$ to the output sequence $\{y^1, y^2, ..., y^k\}$. Specifically, suppose $\mathrm{\Delta}$ as the pre-defined timescale parameter to transformer continuous parameters $A$, $B$ to discrete space $\overline{A}$, $\overline{B}$. The discretization process is defined as follows:

\begin{equation}
\begin{gathered}
\overline{A}=\exp (\mathrm{\Delta} A), \\
\overline{B} = (\mathrm{\Delta} A)^{-1}(\exp (A) - I) \cdot \mathrm{\Delta} B.
  \end{gathered}
\end{equation}
After the discreization, Eq.~(\ref{eq:ssm}) can be rewritten as:
\begin{equation}
\begin{gathered}
h^{k} = \overline{A}{h^{k-1}} +\overline{B}{x^k}, \\
y^k = C{h^k} + D{x^k}.
  \end{gathered}
\end{equation}

SSM excels at modeling discrete sequences, but their inherent LTI property results in fixed parameters, making them insensitive to input variations. To overcome this limitation, a novel approach called the Selective State Space Model, also referred to as Mamba~\cite{mamba,videomamba,panmamba,simba,panmamba}, has been introduced.
Mamba makes model parameters dependent on the input data. It derives matrices $B$, $C$, and $\mathrm{\Delta}$ directly from the input $x$, allowing the model to adapt to different contexts and capture complex interactions within long sequences. 


% \begin{equation}
% \begin{gathered}
% \overline{A}=\exp (\mathrm{\Delta} A),\; \overline{B} = (\mathrm{\Delta} A)^{-1}(\exp (A) - I) \cdot \mathrm{\Delta} B, \;\overline{C} = C, \\
%   y_k = \overline{C}{h_k} + \overline{D}{x_k},
%   h_{k} = \overline{A}{h_{k-1}} +\overline{B}{x_k}.
%   \end{gathered}
% \end{equation}

% \begin{equation}
% \begin{gather}
% \overline{A}=\exp (\mathrm{\Delta} A),\; \overline{B} = (\mathrm{\Delta} A)^{-1}(\exp (A) - I) \cdot \mathrm{\Delta} B, \;\overline{C} = C, \\
%   y_k = \overline{C}{h_k} + \overline{D}{x_k},
%   h_{k} = \overline{A}{h_{k-1}} +\overline{B}{x_k}.
%   \label{eq:ss}
% \end{gather}
% \end{equation}
%
% Here, all the matrices keep the same dimension as the operation iterates.
% %
% Notably, $\overline{D}$, serving as a residual connection, is often discarded in the equation:
% \begin{equation}
%     y_k =  \overline{C}{h_k}.
%     \label{eq:y=ch}
% \end{equation}
% Besides, following Mamba~~\cite{gu2023mamba}, the matrix $\overline{B}$ can be approximated by the first-order Taylor series:
% \begin{equation}
%     \overline{B} =(\exp (A) - I) A^{-1} B \approx (\mathrm{\Delta} A)(\mathrm{\Delta} A)^{-1}\mathrm{\Delta}B = \mathrm{\Delta}B
% \end{equation}

% \textbf{Selective Scan Mechanism.}

%

\subsection{Tracking Process} 
% 后面改标题
The multimodal tracking task generally involves integrating two distinct video modalities, which collaboratively contribute to the final decision-making process for tracking objects. For the input, each modality's data is first converted into the corresponding template tokens ($Z_{RGB}, Z_{X}$) and search tokens ($S_{RGB}, S_{X}$) through patch embedding and positional embedding encoding.
These tokens are then concatenated with the temporal information tokens that generated from the previous time state and fed into the tracker together. As shown in Fig.~\ref{fig:contrast}~(c), STTrack constructs a bridge between spatial and temporal information through its architecture, which consists of \textit{a visual encoder, a temporal state generator, a mamba fusion module, and a prediction head}.
We employ ViT~\cite{vit_transformer} as the visual encoder, with shared weights across the encoders, and insert background suppression interactive modules after each transformer layer. 
The visual encoder dynamically extracts precise multimodal features from the input multimodal images and prior temporal information. %These features are then fed into the temporal state generator to produce the current temporal information tokens, which are subsequently propagated to the next time point. 
%Finally, the tracker adjusts and fuses the visual features in the Mamba fusion blocks and passes them to the prediction head to obtain the final tracking results.
These features are fed into the temporal state generator to produce the current temporal information tokens, which are then passed to the next time point. The tracker then refines and fuses the visual features in the mamba fusion module, ultimately delivering them to the prediction head for the final tracking results.



\subsection{Temporal State Generator}
Previous methods typically focused on multimodal spatial features to achieve precise tracking results. 
However, these trackers are less effective in addressing challenges such as \textit{changes in moving targets and interference from similar objects}. 
To better capture target changes, it is crucial to construct stable inter-frame information features.
%We propose temporal state generator to combine the unidirectional recurrent approach of cross mamba~\cite{sigma}, using autoregressive modeling to seamlessly transfer information from the previous time nodes to the current time node, and integrates it with the current multimodal spatial information to generate temporal information token $T^{cur}_{RGB}$ and $T^{cur}_{X}$. 
We introduce a temporal state generator that merges the unidirectional recurrent approach of cross mamba, employing autoregressive modeling to seamlessly transfer information from previous time nodes to the current one. 
This process integrates current multimodal spatial information to generate the temporal information tokens $T^{cur}_{RGB}$ and $T^{cur}_{X}$.
Specifically, the temporal state generator takes features from two modalities $ x_{RGB} = [Z_{RGB};S_{RGB};T^{pre}_{RGB}] $ and  $ x_{X} = [Z_{X};S_{X};T^{pre}_{X}] $
as input, generating the target state for the current time node and multimodal features after modality interaction. Where $T^{pre}$ is the temporal information learned from the previous $m$ frames.
Notably, at this stage, an empty token as $T^{cur}$ was inserted at the end to store the target information at the current time node.
We first apply $1$D convolution to $x_{RGB}$ and $x_{X}$, then linearly project them to produce the features $\overline{B}$, $\overline{C}$, and $D$ as described in the preliminaries.
By exchanging the $C$ matrix, the temporal state generator can incorporate complementary information from another modality when generating the current temporal token. 
Specifically, this process can be represented as:
% During this process, the input visual tokens (excluding $T^{pre}$) are initially processed through bidirectional scanning. Subsequently, all tokens are passed through a linear layer followed by 1D convolutional layers to produce the features $\overline{B}$, $\overline{C}$, and $\overline{D}$, as described in Section \ref{sec
% }. By exchanging the C matrix, the state generator can incorporate complementary information from another modality when generating the current temporal token. This process can be specifically represented as:

\begin{equation}
\begin{gathered}
  h_{RGB}^{k} = \overline{A}_{RGB}{h_{RGB}^{k-1}} +\overline{B}_{RGB}{x_{RGB}^k},
\\
  y_{RGB}^k = \boldsymbol{{C}_{X}}{h_{RGB}^k} + {D}_{RGB}{x_{RGB}^k}.
  \end{gathered}
\end{equation}

\begin{equation}
\begin{gathered}
\ h_{X}^{k} = \overline{A}_{X}{h_{X}^{k-1}} +\overline{B}_{X}{x_{X}^k}, \\
  \ y_{X}^k = \boldsymbol{{C}_{RGB}}{h_{X}^k} + {D}_{X}{x_{X}^k},
  \end{gathered}
\end{equation}
where $k \in [1,2,..,l]$, $l$ is the length of visual tokens, and $y^k_{RGB}, y^k_{X}$ are concatenated to generate the visual features $y^{}_{RGB}, y^{}_{X}$.  The original mamba block is designed for the 1D sequence, which limits their ability to understand visual tokens with spatial location information.
Therefore, we adopt the commonly used bidirectional scanning method~\cite{vimamba} in visual Mamba to process the visual tokens. Specifically, we reverse the order of the visual tokens and perform calculations, then add the results of the reversed calculations to those of the non-reversed calculations. %FIXME：need to carefully check the notations here

Extracting the current state information using the temporal information token allows us to add it to the queue $T$ and propagate it to the next frame:
%The information propagation process can be described as:
\begin{equation}
\begin{gathered}
    T = \begin{cases}
   [T_1,..,T_{t-1},T_t] &\text{if } t<m  \\
   [T_{t-m},..,T_{t-1},T_t] &\text{if } t>=m. 
\end{cases} \\
  \end{gathered}
\end{equation}
%Temporal information tokens will serve as a bridge, allowing us to effectively associate the past, present, and future, using past information to guide future modeling. Exchanging the C matrix for cross-modal attention enables our generated temporal tokens to encompass more comprehensive information.
where $m$ is number of temporal information tokens, and $t$ is time node. Temporal information tokens act as a bridge, linking the past, present, and future by using previous data to guide future modeling. 
Replacing the $C$ matrix with cross-modal attention allows our temporal tokens to capture more comprehensive information.

\begin{figure}[t]
  \centering
    \includegraphics[width=1\linewidth,height=6.2cm]
    {interaction.pdf}
   \caption{
\textbf{Left}: Architecture of the background suppression interactive module. 
\textbf{Right}: Details of the fusion mamba. 
In BSI module S is a search areas tokens, Z denotes the template tokens and T is the temporal information tokens. }
   \label{fig:bei_and_fusion}
\end{figure}


\subsection{Background Suppression Interactive}
% The methods previously used in Sections 3.1 and 3.2 can be seen as implementations of cross-modal interaction, but they have not fully utilized the potential of multimodal data in the spatial dimension. To address this limitation, we propose a selective interaction blocks.

% Since the search region may contain irrelevant background noise that can interfere with the interaction process and affect tracking performance, this filtering mechanism allows the model to effectively shield unnecessary noise information. Moreover, as the visual encoder models the features multiple times, the generated association matrix becomes increasingly accurate. Based on this, we progressively increase the filtering proportion to further mitigate the impact of noise on the modeling. Specifically, we use the attention generated by the Transformer in each layer as the criterion for selection. Since the feature extraction within each modality is performed using a unified one-stream approach, the attention mechanism allows us to obtain the association degree between different parts. This process can be described as follows:
% 
%     \overline{B} =(\exp (A) - I) A^{-1} B \approx (\mathrm{\Delta} A)(\mathrm{\Delta} A)^{-1}\mathrm{\Delta}B = \mathrm{\Delta}B
% 
% Unrelated background information in the search area will greatly increase the difficulty of interaction, lead to deviations in feature fusion, and thereby affect the accuracy and stability of tracking.
Incorporating multiple interactions within the encoder has become the mainstream method in multimodal tracking. 
To this end, we incorperate our background suppression interactive (BSI) module to each layer of the encoder to enhance cross-modal interactions. 
Our visual encoder retains ViT architecture, where its self-attention~\cite{attention} mechanism is generally regarded as spatial aggregation of normalized tokens. 
Therefore, the similarity between tokens can be captured by the attention map, calculated as follows:
\begin{equation}
\begin{gathered}
W_{Z} = \text{softmax}( \frac{Q_Z \times K_S}{\sqrt{d}} ), \\
W_{T} = \text{softmax}( \frac{Q_T \times K_S}{\sqrt{d}} ),
  \end{gathered}
\end{equation}
where $W_{Z}$ represents the correlation between the search area and the template features, while $W_{T}$ represents the correlation between the search area and temporal information.

We use the attention computed in ViT as the criterion for background suppression, thereby avoiding additional computations. 
When calculating similarity with the search area, we use a $3 \times 3$ matrix centered on the template along with temporal information tokens to compute and average the results. 
Since tracking is essentially a matching task, a low similarity between search area tokens and the template region likely indicates a background area. 
Our temporal information tokens provide sufficient guidance to represent the target. 
By setting the filtering ratio $\lambda$, we first sort the search tokens by their similarity. \textit{Then, we select the bottom $\lambda$ proportion of tokens with the lowest similarity, mark them as invalid, and set their values to zero.}
% By setting the filtering ratio $\lambda$, we first sort the similarity of the search tokens. Then, the bottom $\lambda$ proportion of search tokens, with the lowest similarity, are marked as invalid and their values are set to zero.
% By setting a filtering ratio $\lambda$, we select $K$ search tokens in the $attn_{Z}$ to zero. 
%Additionally, as the visual encoder performs multiple iterations of feature modeling, the generated association matrix becomes increasingly accurate.

Moreover, with each iteration of feature modeling by the visual encoder, the accuracy of the generated association matrix improves progressively.
Therefore, we divide the filtration ratio of the 12 layer BSI into three parts and gradually increase the $\lambda$.
% we gradually increase the filtering ratio to further mitigate the impact of noise on the modeling. 
As in Fig.~\ref{fig:bei_and_fusion}, after background suppression, we concatenate the features from the two modalities and generate cross-modal feature prompts through linear layers:
\begin{equation}
\begin{gathered}
x^i_{RGB} = F_{RGB}^i([f_{RGB};f_{X}]), \\
x^i_{X} = F_{X}^i([f_{X};f_{RGB}]),
  \end{gathered}
\end{equation}
where $f_{RGB}$, $f_{X}$ represent the features after background suppression, and $i$ denotes the transformer layer number. 

% Due to the simplicity of this architecture, it does not introduce significant computational overhead.



% By calculating the attention scores between the template parts and the temporal sequence $T$ and the tokens of the search area, we can select the tokens that need to interact. These tokens are further processed to remove noise and enhance the interactivity of multimodal features. In each layer of the model, we gradually increase the selection ratio, thereby improving the overall accuracy and robustness of the model.

\subsection{Mamba Fusion}
Mamba excels in long-sequence modeling capabilities. Building on this, we concatenate the sequences of the two modalities and use a bidirectional scanning strategy to capture long-range dependencies in both modalities. Finally, we sum the two modality sequences to complete the modality fusion. The process can be represented as:
\begin{equation}
\begin{gathered}
    x = Fusion([Z_{RGB},S_{RGB}],[Z_{X},S_{X}]).
  \end{gathered}
\end{equation}
After obtaining $x_{\text{RGB}}\in R^{N\times C}$ and $x_{X}\in R^{N\times C}$, we concatenate them along the channel dimension and use a linear layer to adjust the dimension to $C$. 
Here $N$ represents the number of tokens of the feature sequence and $C$ represents the channel dimension. 
Detailed architecture is shown in Fig.~\ref{fig:bei_and_fusion}. 
In this way, we refine and fuse the features before they are fed into the prediction head.


\subsection{Head and Objective Loss}
Following most of the latest multimodal tracking methods~\cite{vipt, untrack}, we employ a stacked set of Fully Convolutional Networks (FCNs) ~\cite{ostrack} to construct the prediction head. Notably, during the tracking process, we maintain a temporal tokens $T$ with a length of $m$.
We use $ L_{\text{cls}} $~\cite{focal_loss} to denote the weighted focal loss for classification. For bounding box regression, we adopt the generalized IoU loss $ L_{\text{iou}} $~\cite{giou} and the $ L_1$ loss. The overall loss function of STTrack is:
\begin{equation}
L =  L_{\text{cls}} + \alpha L_{\text{iou}} + \beta L_1,
\end{equation}
where $ \alpha = 2$ and $  \beta = 5 $, which are hyperparameters to balance the contributions of loss terms.



\section{Experiment}
In this section, we begin by detailing the experimental training procedures and the inference process of the proposed STTrack. Following this, we compare STTrack against other leading methods using various benchmark datasets. 
% Specifically, our tracker is assessed on Depthtrack~\cite{depthtrack} and VOT22RGBD~\cite{vot-rgbd} for RGB-D tracking, LasHeR~\cite{lasher} and RGBT234~\cite{rgbt234} for RGB-T tracking, and VisEvent~\cite{visevent} for RGB-E tracking. Finally, we conduct robustness tests and ablation experiments to further evaluate performance.

\subsection{Implementation Details}
\textbf{Training.}
We train on multiple multimodal tasks, including LasHeR for RGB-T tracking, VisEvent for RGB-E tracking, and DepthTrack for RGB-D tracking. For input data, we use two 128 $\times$ 128 template images and one 256 $\times$ 256 search image. The training was conducted on four NVIDIA Tesla A6000 GPUs over 15 epochs, with each epoch consisting of 60,000 sample pairs and a batch size of 32. AdamW~\cite{adamw} was employed as the optimizer, with an initial learning rate of $1\mathrm{e}{-5}$ for the ViT backbone and $1\mathrm{e}{-4}$ for other parameters. After 10 epochs, the learning rate was reduced by a factor of 10.

\noindent \textbf{Inference.} During inference, we maintain the same training setting, using two template frames (includes a fixed initial template frame and a dynamically updated template frame.). 
Temporal information is incrementally incorporated into the tracking process, frame by frame. 
The tracking speed, tested on a NVIDIA 4090 GPU, is approximately $35.5$ frames per second (FPS).


% \begin{table}[t]\normalsize
%     
%     \caption{State-of-the-art comparisons on RGB+T tracking}
% \label{tab-sota-rgbt}
% %
%   \centering
% \resizebox{1\linewidth}{!}{
%   \setlength{\tabcolsep}{4mm}{
%     \small
%     \begin{tabular}{l|c|cc|cc}
%     \toprule
%     \multirow{2}*{Method} &\multirow{2}*{Source} & \multicolumn{2}{c}{LasHeR}  & \multicolumn{2}{c}{RGBT234} \\
%         \cline{3-4} \cline{5-6}
%  & & SR & PR &MSR &MPR \\
%     \midrule[0.5pt]
%     % STTrack-B384 & 61.6     & 77.6   &  & 67.5      & 90.9 \\
%     STTrack &Ours & \textbf{60.3}  & \textbf{76.0} &   \textbf{66.7}	& \textbf{89.8} \\
%     % \midrule[0.1pt]
%     MPLT  &arixv 24 & 
% \underline{57.1} &
% \underline{72.0} & 
% \underline{65.7} &
% \underline{88.4} \\
%     % MCTrack & 57.1  & 71.6 &  & 65.5  & 87.5 \\
%     GMMT &AAAI 23 & 56.6  & 70.7  & 64.7  & 87.9 \\
%     BAT   & 56.3  & 70.2 & & 64.1  & 86.8 \\
%     TBSI  & 56.3  & 70.5 & & 64.3  & 86.4 \\
%     % SeqTrackv2-B384 & 56.2  & 71.5 & & 66.3  & 90.0 \\
%     TATrack & 56.1  & 70.2 & & 64.4  & 87.2 \\
%     % SeqTrackv2-B256 & 55.8  & 71.5 & & 66.3  & 88.0 \\
%     OneTracker & 53.8  & 67.2 & & 64.2  & 85.7 \\
%     Un-Track& 53.6  & 66.7 & & 61.8  & 83.7 \\
%     SDSTrack & 53.1  & 66.5 & & 62.5  & 84.8 \\
%     ViPT  & 52.5  & 65.1 & & 61.7  & 83.5 \\
%     OSTrack & 41.2  & 52.5 & & 54.9  & 72.9 \\
%      ProTrack & 42.0 &53.8  &   & 59.9  & 79.5 \\
%     APFNet & 36.2  & 50.0  &  & 57.9  & 82.7 \\
%     \bottomrule
%     \end{tabular}
%     }
   
%   }
% \end{table}


\begin{table}[t]\normalsize


  \centering


    \small
    \fontsize{7.5}{8}\selectfont
    \begin{tabular}{l|c|cc|cc}
    \toprule
    \multirow{2}*{Method} &\multirow{2}*{Source} & \multicolumn{2}{c|}{LasHeR}  & \multicolumn{2}{c}{RGBT234} \\
        \cline{3-6} %\cline{5-6}
 & &   SR~$\uparrow$ & PR~$\uparrow$ & MSR~$\uparrow$ &MPR~$\uparrow$ \\
  \midrule[0.5pt]
    STTrack &Ours  & \textbf{60.3}  & \textbf{76.0} &   \textbf{66.7}	& \textbf{89.8} \\
    GMMT &AAAI'24 & \underline{56.6}  & \underline{70.7}  & \underline{64.7}  &\underline{ 87.9} \\
    BAT  &AAAI'24 & 56.3  & 70.2  & 64.1  & 86.8 \\
    TBSI &CVPR'24 & 56.3  & 70.5  & 64.3  & 86.4 \\
    TATrack &AAAI'24 & 56.1  & 70.2  & 64.4  & 87.2 \\
    OneTracker &CVPR'24 & 53.8  & 67.2  & 64.2  & 85.7 \\
    Un-Track  & CVPR'24 &53.6  & 66.7  & 61.8  & 83.7 \\
    SDSTrack &CVPR'24 & 53.1  & 66.5 & 62.5  & 84.8 \\
    ViPT  &CVPR'23 & 52.5  & 65.1  & 61.7  & 83.5 \\
    OSTrack &ECCV'22 & 41.2  & 52.5  & 54.9  & 72.9 \\
     ProTrack &MM'22 & 42.0 &53.8     & 59.9  & 79.5 \\
    APFNet  &AAAI'23& 36.2  & 50.0    & 57.9  & 82.7 \\
    \bottomrule
    \end{tabular}
    \caption{Comparisons on \textbf{RGB-T tracking}.}
\label{tab-sota-rgbt}
  
\end{table}



\begin{figure*}
  \centering
    \includegraphics[width=1\linewidth,height= 6.5 cm]
    {tracker_contrast.pdf}
   \caption{
Qualitative comparison between our method and other unified multimodal trackers on three multimodal task. The three sequences correspond to scenarios involving similar object interference, fast motion, and target deformation. Our tracker effectively addresses these challenges through dual optimization in both the temporal and spatial dimensions.}
   \label{fig:tracker_contrast}
\end{figure*}




\begin{table}[t]\normalsize
    


  \centering

    
      \small
    \fontsize{7}{8}\selectfont
    \begin{tabular}{l|ccc|ccc}
    \toprule
    \multirow{2}*{Method} & \multicolumn{3}{c|}{VOT-RGBD22}  & \multicolumn{3}{c}{DepthTrack} \\
        \cline{2-4} \cline{5-7}
 & EAO~$\uparrow$ & Acc.~$\uparrow$ & Rob.~$\uparrow$  &F-score~$\uparrow$ &Re~$\uparrow$ & Pr~$\uparrow$ \\
    \midrule[0.5pt]
    STTrack &  \textbf{77.6}     &   \textbf{82.5}    & \textbf{93.7}     & \textbf{63.3}  & \textbf{63.4}  & \textbf{63.2} \\
    SDSTrack & \underline{72.8}  & 81.2  & \underline{88.3}   & \underline{61.4}  & 60.9  & \underline{61.9} \\
    OneTracker & 72.7  & 81.9  & 87.2   & 60.9  & 60.4  & 60.7 \\
    Un-Track& 71.8  & \underline{82.0}    & 86.4   & 61.2    & \underline{61.0} & 61.3 \\
    ViPT  & 72.1  & 81.5  & 87.1    & 59.4  & 59.6  & 59.2 \\
    SBT-RGBD & 70.8  & 80.9  & 86.4   & -      & -      & - \\
    OSTrack & 67.6  & 80.3  & 83.3   & 52.9  & 52.2  & 53.6 \\
    DET   & 65.7  & 76.0    & 84.5   & 53.2  & 50.6  & 56.0 \\
    ProTrack & 65.1  & 80.1  & 80.2   & 57.8  & 57.3  & 58.3 \\
    SPT   & 65.1  & 79.8  & 85.1    & 57.8  & 53.8  & 52.7 \\
    STARK-RGBD & 64.7  & 80.3  & 79.8   &-       & -      & - \\
    KeepTrack & 60.6  & 75.3  & 73.9   &  -     &   -    & - \\
    ATCAIS & 55.9  & 76.1  & 73.9  & 47.6  & 45.5  & 50.0 \\
    \bottomrule
    \end{tabular}
    
  % }
 \caption{Comparisons on \textbf{RGB-Depth tracking}.}
 \label{tab-sota-rgbd}
\end{table}

   \begin{table*}[t]
    \centering
    \fontsize{6.5}{8}\selectfont

 
    \begin{tabular}{c|ccccccccccc|c}
     \toprule
    % & \multicolumn{6}{c|}{Specific Parameters} & \multicolumn{6}{c}{Unified Parameters}\\
    % \toprule
   % &SiamBAN\_E 
   &STARK\_E &PrDiMP\_E &LTMU\_E &ProTrack &TransT\_E 
   &SiamRCNN\_E &OSTrack &Un-Track &ViPT & SDSTrack &OneTrack &STTrack\\ 
    \toprule
   AUC $\uparrow$ 
   % &40.5 
   &44.6 &45.3 &45.9 &47.1 &47.4 &49.9 &53.4 &58.9 &59.2 &59.7 &\underline{60.8} &\textbf{61.9} \\
  Pr  $\uparrow$
  % &59.1 
  &61.2&64.4&65.9 &63.2 &65.0 &65.9 &69.5 &75.5 &75.8 &\underline{76.7} &\underline{76.7} &\textbf{78.6}\\
       % &DeT &SPT &ProTrack &ViPT  &OneTracker &SDSTrack &Stark &AiATrack  &OSTrack  &SeqTrack  &UnTrack   &APTrack \\
       %  \toprule
       % EAO($\uparrow$) &65.7 &65.1 &65.1 &72.1 &{\color{cGreen}72.7} &{\color{blue}72.8} &44.5 &64.1
       % &66.6 &67.9 &71.8 &{\color{cRed}77.4}\\ 
       % Accuracy($\uparrow$)&76.0 & 79.8 &80.1 &81.5&{\color{cGreen}81.9}&81.2 &71.4 &76.9
       
       % &80.8&80.2 &{\color{blue}82.0} &{\color{cRed}82.1}\\  
       %  Robustness($\uparrow$) &84.5 &85.1 &80.2 &87.1 &{\color{cGreen}87.2}&{\color{blue}88.3}
       %  &59.8 &83.2
       %  &81.4 &84.6 &86.4 &{\color{cRed}93.4}\\
       % &~\cite{drefine} &~\cite{aiatrack} &~\cite{spt} &~\cite{protrack}   &~\cite{stark}      &~\cite{seqtrack}  &~\cite{untrack}   &~\cite{vipt}   &~\cite{oneTracker}  &~\cite{sdstrack}  &(ours)
       %  &(ours) \\
   %  \midrule
   %  EAO($\uparrow$) &0.592 &0.641 &0.651 &0.651  &0.666  &0.679 &0.718 &0.721 & 0.727      &{\color{green}0.728} &  {\color{red}0.768} & {\color{blue}0.766} \\
   % Accuracy($\uparrow$) &0.775 &0.769 &0.798 &0.801  &0.808  &0.802 &{\color{red}0.820} &{\color{green}0.815} & {\color{blue}0.819}       &0.812 &{\color{blue}0.819} & {\color{blue}0.819}  \\
   %     Robustness($\uparrow$) &0.760 &0.832 &0.851 &0.802  &0.814  &0.846  &0.864 &0.871 & 0.872       &{\color{green}0.883} &  {\color{red}0.925} & {\color{blue}0.924}  \\
    \bottomrule
    \end{tabular}
    % }        
    \caption{Comparisons on \textbf{RGB-Event tracking}.}

    \label{tab-sota-rgbe}
    \end{table*}

% \begin{table}[t]\normalsize
%     \caption{Comparisons on \textbf{RGB-Event tracking}.}
% \label{tab-sota-rgbe}

%   \centering


%     %     \small
%     \fontsize{9}{10}\selectfont
%     \begin{tabular}{l|cc}
%     \toprule
%     \multirow{2}*{Method} & \multicolumn{2}{c}{VisEvent}\\
%         \cline{2-3}    
%  & AUC~$\uparrow$ & P~$\uparrow$ \\
%  \midrule
%     STTrack~(Ours) & \textbf{61.9}  & \textbf{78.6} \\
%     \midrule
%     OneTracker & \underline{60.8}  & \underline{76.7} \\
%     SDSTrack & 59.7  & \underline{76.7} \\
%     ViPT  & 59.2  & 75.8 \\
%     Un-Track& 58.9  & 75.5 \\
%     OSTrack & 53.4  & 69.5 \\
%     SiamRCNN\_E & 49.9  & 65.9 \\
%     TransT\_E & 47.4  & 65.0 \\
%     ProTrack & 47.1  & 63.2 \\
%     LTMU\_E & 45.9  & 65.9 \\
%     PrDiMP\_E & 45.3  & 64.4 \\
%     STARK\_E & 44.6  & 61.2 \\
%     SiamBAN\_E & 40.5  & 59.1 \\
%     \bottomrule
%     \end{tabular}
%     % }
%     % }
  
% \end{table}

\begin{figure}[t]
  \centering
    \includegraphics[width=1\linewidth,height=4.8cm]
    {attribute.pdf}
   \caption{
Comparison of STTrack and SOTA trackers (including unified trackers and RGB-T trackers) under different attributes in the LasHeR dataset. }
   \label{fig:attribute}
\end{figure}




 \subsection{Comparison with State-of-the-Arts}
 
 \textbf{LasHeR}. The LasHeR represents a substantial RGB-T tracking dataset comprising 1224 aligned sequences, encompassing more than 2$\times$730K frames captured across diverse imaging platforms. 
As shown in Tab.~\ref{tab-sota-rgbt}, 
% GMMT and BAT, both achieved a PR of 70.7\%. 
% However, these methods fail to accurately transmit continuous temporal information. In comparison, 
our STTrack achieved an SR of 60.3\% and a PR of 76.0\%, surpassing GMMT by 4.3\% in SR and 5.3\% in PR, demonstrating the effectiveness of continuous spatial-temporal modeling.
 
 % As shown in Tab.~\ref{tab-sota-rgbt}, for RGB-T trackers without template updates, GMMT achieved a PR of 70.7\%, while BAT also achieved a PR of 70.7\%. In contrast, TATrack, which include template updates, achieved PR of 70.2\% and SR of 56.1\%.
 % However, these methods lack the accurate transmission of continuous temporal information. In comparison, our STTrack achieved an SR of 60.3\% and a PR of 76.0\%, surpassing the SR and PR scores of GMMT by 4.3\% and 5.3\%, respectively. This represents a significant improvement over all competing methods, demonstrating the effectiveness of our STTrack through continuous spatial-temporal modeling.
 % As shown in Tab. \ref{tab-sota-rgbt}, STTrack surpassing previous state-of-the-art (SOTA). In particular, compared to MPLT~\cite{mplt} which are also two template frames, our tracker improves by 3.2 \% on SR.
 
\noindent \textbf{RGBT234.} The RGBT234 benchmark introduces enriched annotations and an expanded set of environmental challenges. It contains 234 aligned sequences of RGBT videos. As illustrated in Tab.~\ref{tab-sota-rgbt}, STTrack achieves the best MSR score of 66.7\%, outperforming the recent trackers.

\noindent \textbf{DepthTrack}. DepthTrack is a long-time tracking dataset in which the average sequence length is 1,473 frames. 
The dataset covers 200 sequences, 40 scenes and 90 target objects. As in Tab.~\ref{tab-sota-rgbd}, our STTrack obtains SOTA results with 63.3\% in F-score, 63.4\% in recall, and 63.2\% in precision.

\noindent \textbf{VOT-RGBD2022}. VOT-RGBD2022 comprises 127 brief RGB-D sequences and evaluates tracker performance using Accuracy, Robustness, and Expected Average Overlap (EAO) metrics. As illustrated in Tab.~\ref{tab-sota-rgbd}, our proposed tracker, STTrack, demonstrates a notable improvement in EAO, achieving a 4.8\% increase compared to the previous SOTA tracker SDSTrack.

\noindent \textbf{VisEvent}. VisEventis the largest RGB-E dataset currently available, encompassing 500 training video sequences and 320 testing video sequences. As reported in Tab.~\ref{tab-sota-rgbe}, our STTrack obtains SOTA AUC and precession of 61.9\% and 78.6\%, respectively. 

\begin{table}[t]
\centering



\small

\fontsize{7}{8}\selectfont
\begin{tabular}{l|c|ccc|c}
\toprule
\# & Method & LasHeR & DepThTrack  & Visevent & $\Delta$ \\
\midrule 
1 & Baseline & 56.0 & 58.8 & 60.0 & -- \\
2 & + Template Updata  & 57.1 & 59.5 & 59.8 & \textbf{+0.5} \\  
3 & + Temporal Information  & 58.9 & 62.0 & 61.1 & \textbf{+1.8} \\
4 & + Mamba Fusion  & 59.2 & 62.1 &61.3  & \textbf{+0.2}\\
% 5 & + Cross-modal interaction & 59.6 & 61.6 &61.6 &\textbf{+0.1} \\
5 & + BSI Module  & 60.3 & 63.2 &61.9 & \textbf{+0.9} \\
\bottomrule
\end{tabular}
% }
\caption{Quantitative comparison among different variants of STTrack on the LasHeR dataset, DepThTrack dataset and Visevent dataset. `$\Delta$' denotes the performance change (averaged over benchmarks) compared with previous variants.
}
\label{tab:ablation}

\end{table}


\begin{table}[t]\normalsize
\centering



\fontsize{6}{6}\selectfont

     \small
    \begin{tabular}{c|cccc}
    \toprule
    Dataset &1 &2&\cellcolor{gray!15}4&8\\
    \midrule
    LasHeR  &59.6 &59.9 &\cellcolor{gray!15}60.3 &60.0\\
    DepthTrack
    &61.0  &61.2  &\cellcolor{gray!15}63.2 &62.9\\
    VisEvent &61.4 &61.6 &\cellcolor{gray!15}61.9 &61.7\\
    $\Delta$ &-- &+0.2 & \cellcolor{gray!15}+0.9 &-0.3\\  
    \bottomrule
    \end{tabular}

% }
  \caption{Ablation study on the number of temporal information tokens. We use \textcolor{gray}{gray} color to denote our final trackers setting. '$\Delta$' denotes the performance change (averaged over benchmarks) compared with previous number setting.
  }
    \label{tab:ablation_token}
\end{table}

\begin{table}[t]
\centering


\small

\fontsize{8}{9}\selectfont
\begin{tabular}{l|c|ccc}
\toprule
\# &  Filtering Ratio ($\lambda\%$) & LasHeR & DepThTrack  & Visevent  \\
\midrule 
1 & [0\%,0\%,0\%] & 59.4 & 61.6 & 61.1  \\
2 & [15\%,15\%,15\%]  & 59.8 & 62.9 & 51.7  \\  
3 & [30\%,30\%,30\%]  & 59.6 & 62.1 & 61.5  \\
4 &\cellcolor{gray!15} [0\%,15\%,30\%]  & \cellcolor{gray!15}60.3& \cellcolor{gray!15}63.2 &\cellcolor{gray!15}61.9  \\
% 5 & + Cross-modal interaction & 59.6 & 61.6 &61.6 \\
% 6 & + Background Suppression  & 60.3 & 63.2 &61.9  \\
\bottomrule
\end{tabular}

\caption{Ablation study on the ration in BSI module. We use \textcolor{gray}{gray} color to denote our final trackers setting.
}
\label{tab:ablation_fil}

\end{table}

\subsection{Ablation Studies}

\textbf{Component Analysis}.
In Tab.~\ref{tab:ablation}, we conducted an ablation study using the AUC in LasHeR, the Precession in DepThTrack,and the AUC in Visevent. The baseline used ViT as the visual encoder and fused the two modalities through a convolutional layer before the prediction head to establish. Through experimental results, we found that template updates can timely refresh the target's appearance information, compensating for the initial template's shortcomings and significantly enhancing the model's tracking performance. The model's performance was not optimal because of its sparse template update method and lack of frame-to-frame information transfer. Therefore, the introduction of temporal information led to a significant improvement, resulting in a 1.8\% gain, which demonstrates the effectiveness of temporal information in addressing these issues. Furthermore, the results show that due to the differences in target representation across different modalities, optimizing the modality fusion process with mamba fusion module can further improve model performance. Additionally, by reducing interference from non-essential regions, our proposed background suppression scheme effectively enhances the performance of cross-modal interactions.
% Compared to the modality interaction module without background filtering, our proposed background elimination scheme effectively enhances cross-modal interaction performance.

\noindent \textbf{Number of Temporal Information Tokens}. 
We investigate the impact of temporal information on the performance of STTrack, as shown in Tab.~\ref{tab:ablation_token}. As the number increases from 1 to 4, the model's performance improves, indicating the temporal information tokens positively contribute to optimization. 
However, when the number is increased further, performance declines, possibly due to earlier temporal information tokens failing to accurately describe the current target state, leading to the introduction of noise. Therefore, we selected an optimal number of temporal information tokens.

\noindent \textbf{Filtering Ratio in BSI}. 
To validate the impact of background suppression on performance, we conducted experiments with different background filtering ratios. 
Here, the 12-layer BSI module was processed in three stages. 
As Tab.~\ref{tab:ablation_fil} indicates that while a fixed background filtering ratio can enhance performance, a three-stage approach with progressively increased filtering ratios yields more significant improvements. This is because, within a single-stream structure, the features of the search area, guided by the template and temporal information tokens, need to progressively highlight the foreground target layer by layer.

% % Table generated by Excel2LaTeX from sheet 'Sheet1'
% % Table generated by Excel2LaTeX from sheet 'Sheet1'
% % Table generated by Excel2LaTeX from sheet 'Sheet1'

\subsection{Exploration Study and Analysis}
\textbf{Attribute-based Performance.}
We analyze the performance of our method in various scenarios by evaluating it on different attributes of LasHeR dataset.
% As shwon in Fig.~\ref{fig:attribute}, STTrack outperforms previous state-of-the-art trackers on these attributes. This improvement can be attributed to the enhanced temporal information and complementary spatial features provided by our modeling approach, enabling STTrack to track more stably even when the target undergoes changes. STTrack performs exceptionally well in scenarios requiring temporal information, such as Total Occlusion (TC), Paritial Occlusion (PO) and deformation (DEF), as well as in conditions with differences in modality imaging, such as low-light and high-light situations.
As shown in Fig.~\ref{fig:attribute}, STTrack surpasses previous state-of-the-art trackers on these attributes. This improvement is due to our approach's enhanced temporal information and complementary spatial features, allowing STTrack to maintain stable tracking even when the target undergoes changes. STTrack excels in scenarios requiring temporal information, such as Partial Occlusion (PO) and Deformation (DEF), as well as in conditions with significant modality imaging differences, such as low-light and high-light situations.


% including partial occlusion (PO), Similar Appearance (SA), scale variation (SV), thermal crossover (TC), Total Occlusion (TO), Aspect Ratio Change (ARC), background clutter (BC), camera moving (CM), deformation (DEF), Frame Lost (FL), fast motion (FM), High Illumination (HI), Low Illumination (LI), low resolution (LR) and motion blur (MB). 

% \textbf{Qualitative Comparison}. 


\noindent \textbf{Visualization Results}. 
As shown in Fig.~\ref{fig:tracker_contrast}, we qualitatively compare STTrack with three other multimodal unified trackers. In the RGB-T sequences, where similar objects cause significant interference and the target has clear movement directions, STTrack leverages temporal information for stable tracking. In the RGB-D sequences, despite severe occlusion, our method captures the target by utilizing the complementary strengths of the RGB and Depth modalities along with continuous temporal information. In the RGB-E sequences, where the car moves at high speed and undergoes significant deformation due to changes in camera distance, STTrack effectively tracks the target by gradually adapting to these changes over time. Besides, we visualize the attention map of the temporal information tokens with search area, as shown in Fig.~\ref{fig:attention}. It demonstrates that the continuous propagation of temporal markers and the focus on object temporal information can effectively capture and respond to the dynamic state of the target.
% As shown in Fig.~\ref{fig:tracker_contrast}, we qualitatively compare STTrack with three other multimodal unified trackers. Specifically, in the RGB-T sequences, where there is significant interference from similar objects and the target exhibits clear movement directions, STTrack leverages temporal information to stably track the target. In the RGB-D sequences, despite severe occlusion of the ball to be tracked, our method captures the target through the complementary capabilities of RGB and Depth modalities and the continuous transmission of temporal information. In the RGB-E sequences, where the car moves at high speed and undergoes significant deformation due to changes in camera distance, STTrack effectively tracks the target by gradually capturing these changes over time.

\begin{figure}[t]
  \centering
    \includegraphics[width=1\linewidth,height=3.2cm]
    {temporal_heat.pdf}
   \caption{
 The attention map of temporal information tokens with search area. These visual results are in LasHeR.}
   \label{fig:attention}
\end{figure}




\section{Conclusion}
In this work, we propose a tracking framework named STTrack based on multimodal spatio-temporal patterns. By leveraging temporal context, STTrack effectively captures and represents dynamic targets. The tracker incorporates a temporal state generator to generate multimodal temporal information that supports the tracking process. Additionally, it is equipped with the BSI module and Mamba Fusion module, which optimize modality branch representation and fuse multimodal features at the spatial level. Compared to previous multimodal trackers, our approach achieves state-of-the-art performance across three multimodal tasks.

\section{Acknowledgements}
This work was funded by the National Science Fund of China, with Grant Nos. U24A20330, 62361166670, and 62406135, the Natural Science Foundation of Jiangsu Province under Grant No. BK20241198, and the AI \& AI for Science Project of Nanjing University, Grant No. 14380007.
% The authors are deeply grateful to the editor and the anonymous reviewers for their incisive and constructive comments and suggestions. This research was funded by the National Science Fund of China, with Grant Nos. U24A20330, 6236116667, and 62406135. It was also supported by the Natural Science Foundation of Jiangsu Province under Grant No. BK20241198, as well as the AI \& AI for Science Project of Nanjing University, Grant No. 14380007.
    
% \section{Copyright}
% All papers submitted for publication by AAAI Press must be accompanied by a valid signed copyright form. They must also contain the AAAI copyright notice at the bottom of the first page of the paper. There are no exceptions to these requirements. If you fail to provide us with a signed copyright form or disable the copyright notice, we will be unable to publish your paper. There are \textbf{no exceptions} to this policy. You will find a PDF version of the AAAI copyright form in the AAAI AuthorKit. Please see the specific instructions for your conference for submission details.

% \section{Formatting Requirements in Brief}
% We need source and PDF files that can be used in a variety of ways and can be output on a variety of devices. The design and appearance of the paper is strictly governed by the aaai style file (aaai25.sty).
% \textbf{You must not make any changes to the aaai style file, nor use any commands, packages, style files, or macros within your own paper that alter that design, including, but not limited to spacing, floats, margins, fonts, font size, and appearance.} AAAI imposes requirements on your source and PDF files that must be followed. Most of these requirements are based on our efforts to standardize conference manuscript properties and layout. All papers submitted to AAAI for publication will be recompiled for standardization purposes. Consequently, every paper submission must comply with the following requirements:

% \begin{itemize}
% \item Your .tex file must compile in PDF\LaTeX{} --- (you may not include .ps or .eps figure files.)
% \item All fonts must be embedded in the PDF file --- including your figures.
% \item Modifications to the style file, whether directly or via commands in your document may not ever be made, most especially when made in an effort to avoid extra page charges or make your paper fit in a specific number of pages.
% \item No type 3 fonts may be used (even in illustrations).
% \item You may not alter the spacing above and below captions, figures, headings, and subheadings.
% \item You may not alter the font sizes of text elements, footnotes, heading elements, captions, or title information (for references and mathematics, please see the limited exceptions provided herein).
% \item You may not alter the line spacing of text.
% \item Your title must follow Title Case capitalization rules (not sentence case).
% \item \LaTeX{} documents must use the Times or Nimbus font package (you may not use Computer Modern for the text of your paper).
% \item No \LaTeX{} 209 documents may be used or submitted.
% \item Your source must not require use of fonts for non-Roman alphabets within the text itself. If your paper includes symbols in other languages (such as, but not limited to, Arabic, Chinese, Hebrew, Japanese, Thai, Russian and other Cyrillic languages), you must restrict their use to bit-mapped figures. Fonts that require non-English language support (CID and Identity-H) must be converted to outlines or 300 dpi bitmap or removed from the document (even if they are in a graphics file embedded in the document).
% \item Two-column format in AAAI style is required for all papers.
% \item The paper size for final submission must be US letter without exception.
% \item The source file must exactly match the PDF.
% \item The document margins may not be exceeded (no overfull boxes).
% \item The number of pages and the file size must be as specified for your event.
% \item No document may be password protected.
% \item Neither the PDFs nor the source may contain any embedded links or bookmarks (no hyperref or navigator packages).
% \item Your source and PDF must not have any page numbers, footers, or headers (no pagestyle commands).
% \item Your PDF must be compatible with Acrobat 5 or higher.
% \item Your \LaTeX{} source file (excluding references) must consist of a \textbf{single} file (use of the ``input" command is not allowed.
% \item Your graphics must be sized appropriately outside of \LaTeX{} (do not use the ``clip" or ``trim'' command) .
% \end{itemize}

% If you do not follow these requirements, your paper will be returned to you to correct the deficiencies.

% \section{What Files to Submit}
% You must submit the following items to ensure that your paper is published:
% \begin{itemize}
% \item A fully-compliant PDF file.
% \item Your \LaTeX{} source file submitted as a \textbf{single} .tex file (do not use the ``input" command to include sections of your paper --- every section must be in the single source file). (The only allowable exception is .bib file, which should be included separately).
% \item The bibliography (.bib) file(s).
% \item Your source must compile on our system, which includes only standard \LaTeX{} 2020 TeXLive support files.
% \item Only the graphics files used in compiling paper.
% \item The \LaTeX{}-generated files (e.g. .aux,  .bbl file, PDF, etc.).
% \end{itemize}

% Your \LaTeX{} source will be reviewed and recompiled on our system (if it does not compile, your paper will be returned to you. \textbf{Do not submit your source in multiple text files.} Your single \LaTeX{} source file must include all your text, your bibliography (formatted using aaai25.bst), and any custom macros.

% Your files should work without any supporting files (other than the program itself) on any computer with a standard \LaTeX{} distribution.

% \textbf{Do not send files that are not actually used in the paper.} Avoid including any files not needed for compiling your paper, including, for example, this instructions file, unused graphics files, style files, additional material sent for the purpose of the paper review, intermediate build files and so forth.
% \textbf{Obsolete style files.} The commands for some common packages (such as some used for algorithms), may have changed. Please be certain that you are not compiling your paper using old or obsolete style files.

% \textbf{Final Archive.} Place your source files in a single archive which should be compressed using .zip. The final file size may not exceed 10 MB.
% Name your source file with the last (family) name of the first author, even if that is not you.


% \section{Using \LaTeX{} to Format Your Paper}

% The latest version of the AAAI style file is available on AAAI's website. Download this file and place it in the \TeX\ search path. Placing it in the same directory as the paper should also work. You must download the latest version of the complete AAAI Author Kit so that you will have the latest instruction set and style file.

% \subsection{Document Preamble}

% In the \LaTeX{} source for your paper, you \textbf{must} place the following lines as shown in the example in this subsection. This command set-up is for three authors. Add or subtract author and address lines as necessary, and uncomment the portions that apply to you. In most instances, this is all you need to do to format your paper in the Times font. The helvet package will cause Helvetica to be used for sans serif. These files are part of the PSNFSS2e package, which is freely available from many Internet sites (and is often part of a standard installation).

% Leave the setcounter for section number depth commented out and set at 0 unless you want to add section numbers to your paper. If you do add section numbers, you must uncomment this line and change the number to 1 (for section numbers), or 2 (for section and subsection numbers). The style file will not work properly with numbering of subsubsections, so do not use a number higher than 2.

% \subsubsection{The Following Must Appear in Your Preamble}
% \begin{quote}
% \begin{scriptsize}\begin{verbatim}
% \documentclass[letterpaper]{article}
% % DO NOT CHANGE THIS
% \usepackage{aaai25} % DO NOT CHANGE THIS
% \usepackage{times} % DO NOT CHANGE THIS
% \usepackage{helvet} % DO NOT CHANGE THIS
% \usepackage{courier} % DO NOT CHANGE THIS
% \usepackage[hyphens]{url} % DO NOT CHANGE THIS
% \usepackage{graphicx} % DO NOT CHANGE THIS
% \urlstyle{rm} % DO NOT CHANGE THIS
% \def\UrlFont{\rm} % DO NOT CHANGE THIS
% \usepackage{graphicx}  % DO NOT CHANGE THIS
% \usepackage{natbib}  % DO NOT CHANGE THIS
% \usepackage{caption}  % DO NOT CHANGE THIS
% \frenchspacing % DO NOT CHANGE THIS
% \setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
% \setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
% %
% % Keep the \pdfinfo as shown here. There's no need
% % for you to add the /Title and /Author tags.
% \pdfinfo{
% /TemplateVersion (2025.1)
% }
% \end{verbatim}\end{scriptsize}
% \end{quote}

% \subsection{Preparing Your Paper}

% After the preamble above, you should prepare your paper as follows:
% \begin{quote}
% \begin{scriptsize}\begin{verbatim}
% \begin{document}
% \maketitle
% \begin{abstract}
% %...
% \end{abstract}\end{verbatim}\end{scriptsize}
% \end{quote}

% \noindent If you want to add links to the paper's code, dataset(s), and extended version or similar this is the place to add them, within a \emph{links} environment:
% \begin{quote}%
% \begin{scriptsize}\begin{verbatim}
% \begin{links}
%   \link{Code}{https://aaai.org/example/guidelines}
%   \link{Datasets}{https://aaai.org/example/datasets}
%   \link{Extended version}{https://aaai.org/example}
% \end{links}\end{verbatim}\end{scriptsize}
% \end{quote}

% \noindent You should then continue with the body of your paper. Your paper must conclude with the references, which should be inserted as follows:
% \begin{quote}
% \begin{scriptsize}\begin{verbatim}
% % References and End of Paper
% % These lines must be placed at the end of your paper
% \bibliography{Bibliography-File}
% \end{document}
% \end{verbatim}\end{scriptsize}
% \end{quote}


% \begin{quote}
% \begin{scriptsize}\begin{verbatim}
% \begin{document}\\
% \maketitle\\
% ...\\
% \bibliography{Bibliography-File}\\
% \end{document}\\
% \end{verbatim}\end{scriptsize}
% \end{quote}

% \subsection{Commands and Packages That May Not Be Used}
% \begin{table*}[t]
% \centering

% \begin{tabular}{l|l|l|l}
% \textbackslash abovecaption &
% \textbackslash abovedisplay &
% \textbackslash addevensidemargin &
% \textbackslash addsidemargin \\
% \textbackslash addtolength &
% \textbackslash baselinestretch &
% \textbackslash belowcaption &
% \textbackslash belowdisplay \\
% \textbackslash break &
% \textbackslash clearpage &
% \textbackslash clip &
% \textbackslash columnsep \\
% \textbackslash float &
% \textbackslash input &
% \textbackslash input &
% \textbackslash linespread \\
% \textbackslash newpage &
% \textbackslash pagebreak &
% \textbackslash renewcommand &
% \textbackslash setlength \\
% \textbackslash text height &
% \textbackslash tiny &
% \textbackslash top margin &
% \textbackslash trim \\
% \textbackslash vskip\{- &
% \textbackslash vspace\{- \\
% \end{tabular}
% %}
% \caption{Commands that must not be used}
% \label{table1}
% \end{table*}

% \begin{table}[t]
% \centering
% %\resizebox{.95\columnwidth}{!}{
% \begin{tabular}{l|l|l|l}
%     authblk & babel & cjk & dvips \\
%     epsf & epsfig & euler & float \\
%     fullpage & geometry & graphics & hyperref \\
%     layout & linespread & lmodern & maltepaper \\
%     navigator & pdfcomment & pgfplots & psfig \\
%     pstricks & t1enc & titlesec & tocbind \\
%     ulem
% \end{tabular}
% \caption{LaTeX style packages that must not be used.}
% \label{table2}
% \end{table}

% There are a number of packages, commands, scripts, and macros that are incompatable with aaai25.sty. The common ones are listed in tables \ref{table1} and \ref{table2}. Generally, if a command, package, script, or macro alters floats, margins, fonts, sizing, linespacing, or the presentation of the references and citations, it is unacceptable. Note that negative vskip and vspace may not be used except in certain rare occurances, and may never be used around tables, figures, captions, sections, subsections, subsubsections, or references.


% \subsection{Page Breaks}
% For your final camera ready copy, you must not use any page break commands. References must flow directly after the text without breaks. Note that some conferences require references to be on a separate page during the review process. AAAI Press, however, does not require this condition for the final paper.


% \subsection{Paper Size, Margins, and Column Width}
% Papers must be formatted to print in two-column format on 8.5 x 11 inch US letter-sized paper. The margins must be exactly as follows:
% \begin{itemize}
% \item Top margin: .75 inches
% \item Left margin: .75 inches
% \item Right margin: .75 inches
% \item Bottom margin: 1.25 inches
% \end{itemize}


% The default paper size in most installations of \LaTeX{} is A4. However, because we require that your electronic paper be formatted in US letter size, the preamble we have provided includes commands that alter the default to US letter size. Please note that using any other package to alter page size (such as, but not limited to the Geometry package) will result in your final paper being returned to you for correction.


% \subsubsection{Column Width and Margins.}
% To ensure maximum readability, your paper must include two columns. Each column should be 3.3 inches wide (slightly more than 3.25 inches), with a .375 inch (.952 cm) gutter of white space between the two columns. The aaai25.sty file will automatically create these columns for you.

% \subsection{Overlength Papers}
% If your paper is too long and you resort to formatting tricks to make it fit, it is quite likely that it will be returned to you. The best way to retain readability if the paper is overlength is to cut text, figures, or tables. There are a few acceptable ways to reduce paper size that don't affect readability. First, turn on \textbackslash frenchspacing, which will reduce the space after periods. Next, move all your figures and tables to the top of the page. Consider removing less important portions of a figure. If you use \textbackslash centering instead of \textbackslash begin\{center\} in your figure environment, you can also buy some space. For mathematical environments, you may reduce fontsize {\bf but not below 6.5 point}.


% Commands that alter page layout are forbidden. These include \textbackslash columnsep,  \textbackslash float, \textbackslash topmargin, \textbackslash topskip, \textbackslash textheight, \textbackslash textwidth, \textbackslash oddsidemargin, and \textbackslash evensizemargin (this list is not exhaustive). If you alter page layout, you will be required to pay the page fee. Other commands that are questionable and may cause your paper to be rejected include \textbackslash parindent, and \textbackslash parskip. Commands that alter the space between sections are forbidden. The title sec package is not allowed. Regardless of the above, if your paper is obviously ``squeezed" it is not going to to be accepted. Options for reducing the length of a paper include reducing the size of your graphics, cutting text, or paying the extra page charge (if it is offered).


% \subsection{Type Font and Size}
% Your paper must be formatted in Times Roman or Nimbus. We will not accept papers formatted using Computer Modern or Palatino or some other font as the text or heading typeface. Sans serif, when used, should be Courier. Use Symbol or Lucida or Computer Modern for \textit{mathematics only. }

% Do not use type 3 fonts for any portion of your paper, including graphics. Type 3 bitmapped fonts are designed for fixed resolution printers. Most print at 300 dpi even if the printer resolution is 1200 dpi or higher. They also often cause high resolution imagesetter devices to crash. Consequently, AAAI will not accept electronic files containing obsolete type 3 fonts. Files containing those fonts (even in graphics) will be rejected. (Authors using blackboard symbols must avoid packages that use type 3 fonts.)

% Fortunately, there are effective workarounds that will prevent your file from embedding type 3 bitmapped fonts. The easiest workaround is to use the required times, helvet, and courier packages with \LaTeX{}2e. (Note that papers formatted in this way will still use Computer Modern for the mathematics. To make the math look good, you'll either have to use Symbol or Lucida, or you will need to install type 1 Computer Modern fonts --- for more on these fonts, see the section ``Obtaining Type 1 Computer Modern.")

% If you are unsure if your paper contains type 3 fonts, view the PDF in Acrobat Reader. The Properties/Fonts window will display the font name, font type, and encoding properties of all the fonts in the document. If you are unsure if your graphics contain type 3 fonts (and they are PostScript or encapsulated PostScript documents), create PDF versions of them, and consult the properties window in Acrobat Reader.

% The default size for your type must be ten-point with twelve-point leading (line spacing). Start all pages (except the first) directly under the top margin. (See the next section for instructions on formatting the title page.) Indent ten points when beginning a new paragraph, unless the paragraph begins directly below a heading or subheading.


% \subsubsection{Obtaining Type 1 Computer Modern for \LaTeX{}.}

% If you use Computer Modern for the mathematics in your paper (you cannot use it for the text) you may need to download type 1 Computer fonts. They are available without charge from the American Mathematical Society:
% http://www.ams.org/tex/type1-fonts.html.

% \subsubsection{Nonroman Fonts.}
% If your paper includes symbols in other languages (such as, but not limited to, Arabic, Chinese, Hebrew, Japanese, Thai, Russian and other Cyrillic languages), you must restrict their use to bit-mapped figures.

% \subsection{Title and Authors}
% Your title must appear centered over both text columns in sixteen-point bold type (twenty-four point leading). The title must be written in Title Case according to the Chicago Manual of Style rules. The rules are a bit involved, but in general verbs (including short verbs like be, is, using, and go), nouns, adverbs, adjectives, and pronouns should be capitalized, (including both words in hyphenated terms), while articles, conjunctions, and prepositions are lower case unless they directly follow a colon or long dash. You can use the online tool \url{https://titlecaseconverter.com/} to double-check the proper capitalization (select the "Chicago" style and mark the "Show explanations" checkbox).

% Author's names should appear below the title of the paper, centered in twelve-point type (with fifteen point leading), along with affiliation(s) and complete address(es) (including electronic mail address if available) in nine-point roman type (the twelve point leading). You should begin the two-column format when you come to the abstract.

% \subsubsection{Formatting Author Information.}
% Author information has to be set according to the following specification depending if you have one or more than one affiliation. You may not use a table nor may you employ the \textbackslash authorblk.sty package. For one or several authors from the same institution, please separate them with commas and write all affiliation directly below (one affiliation per line) using the macros \textbackslash author and \textbackslash affiliations:

% \begin{quote}\begin{scriptsize}\begin{verbatim}
% \author{
%     Author 1, ..., Author n\\
% }
% \affiliations {
%     Address line\\
%     ... \\
%     Address line\\
% }
% \end{verbatim}\end{scriptsize}\end{quote}


% \noindent For authors from different institutions, use \textbackslash textsuperscript \{\textbackslash rm x \} to match authors and affiliations. Notice that there should not be any spaces between the author name and the superscript (and the comma should come after the superscripts).

% \begin{quote}\begin{scriptsize}\begin{verbatim}
% \author{
%     AuthorOne\equalcontrib\textsuperscript{\rm 1,\rm2},
%     AuthorTwo\equalcontrib\textsuperscript{\rm 2},
%     AuthorThree\textsuperscript{\rm 3},\\
%     AuthorFour\textsuperscript{\rm 4},
%     AuthorFive\textsuperscript{\rm 5}}
% }
% \affiliations {
%     \textsuperscript{\rm 1}AffiliationOne,\\
%     \textsuperscript{\rm 2}AffiliationTwo,\\
%     \textsuperscript{\rm 3}AffiliationThree,\\
%     \textsuperscript{\rm 4}AffiliationFour,\\
%     \textsuperscript{\rm 5}AffiliationFive\\
%     \{email, email\}@affiliation.com,
%     email@affiliation.com,
%     email@affiliation.com,
%     email@affiliation.com
% }
% \end{verbatim}\end{scriptsize}\end{quote}

% You can indicate that some authors contributed equally using the \textbackslash equalcontrib command. This will add a marker after the author names and a footnote on the first page.

% Note that you may want to  break the author list for better visualization. You can achieve this using a simple line break (\textbackslash  \textbackslash).

% \subsection{\LaTeX{} Copyright Notice}
% The copyright notice automatically appears if you use aaai25.sty. It has been hardcoded and may not be disabled.

% \subsection{Credits}
% Any credits to a sponsoring agency should appear in the acknowledgments section, unless the agency requires different placement. If it is necessary to include this information on the front page, use
% \textbackslash thanks in either the \textbackslash author or \textbackslash title commands.
% For example:
% \begin{quote}
% \begin{small}
% \textbackslash title\{Very Important Results in AI\textbackslash thanks\{This work is
%  supported by everybody.\}\}
% \end{small}
% \end{quote}
% Multiple \textbackslash thanks commands can be given. Each will result in a separate footnote indication in the author or title with the corresponding text at the botton of the first column of the document. Note that the \textbackslash thanks command is fragile. You will need to use \textbackslash protect.

% Please do not include \textbackslash pubnote commands in your document.

% \subsection{Abstract}
% Follow the example commands in this document for creation of your abstract. The command \textbackslash begin\{abstract\} will automatically indent the text block. Please do not indent it further. {Do not include references in your abstract!}

% \subsection{Page Numbers}

% Do not print any page numbers on your paper. The use of \textbackslash pagestyle is forbidden.

% \subsection{Text}
% The main body of the paper must be formatted in black, ten-point Times Roman with twelve-point leading (line spacing). You may not reduce font size or the linespacing. Commands that alter font size or line spacing (including, but not limited to baselinestretch, baselineshift, linespread, and others) are expressly forbidden. In addition, you may not use color in the text.

% \subsection{Citations}
% Citations within the text should include the author's last name and year, for example (Newell 1980). Append lower-case letters to the year in cases of ambiguity. Multiple authors should be treated as follows: (Feigenbaum and Engelmore 1988) or (Ford, Hayes, and Glymour 1992). In the case of four or more authors, list only the first author, followed by et al. (Ford et al. 1997).

% \subsection{Extracts}
% Long quotations and extracts should be indented ten points from the left and right margins.

% \begin{quote}
% This is an example of an extract or quotation. Note the indent on both sides. Quotation marks are not necessary if you offset the text in a block like this, and properly identify and cite the quotation in the text.

% \end{quote}

% \subsection{Footnotes}
% Use footnotes judiciously, taking into account that they interrupt the reading of the text. When required, they should be consecutively numbered throughout with superscript Arabic numbers. Footnotes should appear at the bottom of the page, separated from the text by a blank line space and a thin, half-point rule.

% \subsection{Headings and Sections}
% When necessary, headings should be used to separate major sections of your paper. Remember, you are writing a short paper, not a lengthy book! An overabundance of headings will tend to make your paper look more like an outline than a paper. The aaai25.sty package will create headings for you. Do not alter their size nor their spacing above or below.

% \subsubsection{Section Numbers.}
% The use of section numbers in AAAI Press papers is optional. To use section numbers in \LaTeX{}, uncomment the setcounter line in your document preamble and change the 0 to a 1. Section numbers should not be used in short poster papers and/or extended abstracts.

% \subsubsection{Section Headings.}
% Sections should be arranged and headed as follows:
% \begin{enumerate}
% \item Main content sections
% \item Appendices (optional)
% \item Ethical Statement (optional, unnumbered)
% \item Acknowledgements (optional, unnumbered)
% \item References (unnumbered)
% \end{enumerate}

% \subsubsection{Appendices.}
% Any appendices must appear after the main content. If your main sections are numbered, appendix sections must use letters instead of arabic numerals. In \LaTeX{} you can use the \texttt{\textbackslash appendix} command to achieve this effect and then use \texttt{\textbackslash section\{Heading\}} normally for your appendix sections.

% \subsubsection{Ethical Statement.}
% You can write a statement about the potential ethical impact of your work, including its broad societal implications, both positive and negative. If included, such statement must be written in an unnumbered section titled \emph{Ethical Statement}.

% \subsubsection{Acknowledgments.}
% The acknowledgments section, if included, appears right before the references and is headed ``Acknowledgments". It must not be numbered even if other sections are (use \texttt{\textbackslash section*\{Acknowledgements\}} in \LaTeX{}). This section includes acknowledgments of help from associates and colleagues, credits to sponsoring agencies, financial support, and permission to publish. Please acknowledge other contributors, grant support, and so forth, in this section. Do not put acknowledgments in a footnote on the first page. If your grant agency requires acknowledgment of the grant on page 1, limit the footnote to the required statement, and put the remaining acknowledgments at the back. Please try to limit acknowledgments to no more than three sentences.

% \subsubsection{References.}
% The references section should be labeled ``References" and must appear at the very end of the paper (don't end the paper with references, and then put a figure by itself on the last page). A sample list of references is given later on in these instructions. Please use a consistent format for references. Poorly prepared or sloppy references reflect badly on the quality of your paper and your research. Please prepare complete and accurate citations.

% \subsection{Illustrations and  Figures}

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.9\columnwidth]{figure1} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
% \caption{Using the trim and clip commands produces fragile layers that can result in disasters (like this one from an actual paper) when the color space is corrected or the PDF combined with others for the final proceedings. Crop your figures properly in a graphics program -- not in LaTeX.}
% \label{fig1}
% \end{figure}

% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.8\textwidth]{figure2} % Reduce the figure size so that it is slightly narrower than the column.
% \caption{Adjusting the bounding box instead of actually removing the unwanted data resulted multiple layers in this paper. It also needlessly increased the PDF size. In this case, the size of the unwanted layer doubled the paper's size, and produced the following surprising results in final production. Crop your figures properly in a graphics program. Don't just alter the bounding box.}
% \label{fig2}
% \end{figure*}

% % Using the \centering command instead of \begin{center} ... \end{center} will save space
% % Positioning your figure at the top of the page will save space and make the paper more readable
% % Using 0.95\columnwidth in conjunction with the


% Your paper must compile in PDF\LaTeX{}. Consequently, all your figures must be .jpg, .png, or .pdf. You may not use the .gif (the resolution is too low), .ps, or .eps file format for your figures.

% Figures, drawings, tables, and photographs should be placed throughout the paper on the page (or the subsequent page) where they are first discussed. Do not group them together at the end of the paper. If placed at the top of the paper, illustrations may run across both columns. Figures must not invade the top, bottom, or side margin areas. Figures must be inserted using the \textbackslash usepackage\{graphicx\}. Number figures sequentially, for example, figure 1, and so on. Do not use minipage to group figures.

% If you normally create your figures using pgfplots, please create the figures first, and then import them as pdfs with proper bounding boxes, as the bounding and trim boxes created by pfgplots are fragile and not valid.

% When you include your figures, you must crop them \textbf{outside} of \LaTeX{}. The command \textbackslash includegraphics*[clip=true, viewport 0 0 10 10]{...} might result in a PDF that looks great, but the image is \textbf{not really cropped.} The full image can reappear (and obscure whatever it is overlapping) when page numbers are applied or color space is standardized. Figures \ref{fig1}, and \ref{fig2} display some unwanted results that often occur.

% If your paper includes illustrations that are not compatible with PDF\TeX{} (such as .eps or .ps documents), you will need to convert them. The epstopdf package will usually work for eps files. You will need to convert your ps files to PDF in either case.

% \subsubsection {Figure Captions.}The illustration number and caption must appear \textit{under} the illustration. Labels and other text with the actual illustration must be at least nine-point type. However, the font and size of figure captions must be 10 point roman. Do not make them smaller, bold, or italic. (Individual words may be italicized if the context requires differentiation.)

% \subsection{Tables}

% Tables should be presented in 10 point roman type. If necessary, they may be altered to 9 point type. You must not use \texttt{\textbackslash resizebox} or other commands that resize the entire table to make it smaller, because you can't control the final font size this way.
% If your table is too large you can use \texttt{\textbackslash setlength\{\textbackslash tabcolsep\}\{1mm\}} to compress the columns a bit or you can adapt the content (e.g.: reduce the decimal precision when presenting numbers, use shortened column titles, make some column duble-line to get it narrower).

% Tables that do not fit in a single column must be placed across double columns. If your table won't fit within the margins even when spanning both columns and using the above techniques, you must split it in two separate tables.

% \subsubsection {Table Captions.} The number and caption for your table must appear \textit{under} (not above) the table.  Additionally, the font and size of table captions must be 10 point roman and must be placed beneath the figure. Do not make them smaller, bold, or italic. (Individual words may be italicized if the context requires differentiation.)



% \subsubsection{Low-Resolution Bitmaps.}
% You may not use low-resolution (such as 72 dpi) screen-dumps and GIF files---these files contain so few pixels that they are always blurry, and illegible when printed. If they are color, they will become an indecipherable mess when converted to black and white. This is always the case with gif files, which should never be used. The resolution of screen dumps can be increased by reducing the print size of the original file while retaining the same number of pixels. You can also enlarge files by manipulating them in software such as PhotoShop. Your figures should be 300 dpi when incorporated into your document.

% \subsubsection{\LaTeX{} Overflow.}
% \LaTeX{} users please beware: \LaTeX{} will sometimes put portions of the figure or table or an equation in the margin. If this happens, you need to make the figure or table span both columns. If absolutely necessary, you may reduce the figure, or reformat the equation, or reconfigure the table.{ \bf Check your log file!} You must fix any overflow into the margin (that means no overfull boxes in \LaTeX{}). \textbf{Nothing is permitted to intrude into the margin or gutter.}


% \subsubsection{Using Color.}
% Use of color is restricted to figures only. It must be WACG 2.0 compliant. (That is, the contrast ratio must be greater than 4.5:1 no matter the font size.) It must be CMYK, NOT RGB. It may never be used for any portion of the text of your paper. The archival version of your paper will be printed in black and white and grayscale. The web version must be readable by persons with disabilities. Consequently, because conversion to grayscale can cause undesirable effects (red changes to black, yellow can disappear, and so forth), we strongly suggest you avoid placing color figures in your document. If you do include color figures, you must (1) use the CMYK (not RGB) colorspace and (2) be mindful of readers who may happen to have trouble distinguishing colors. Your paper must be decipherable without using color for distinction.

% \subsubsection{Drawings.}
% We suggest you use computer drawing software (such as Adobe Illustrator or, (if unavoidable), the drawing tools in Microsoft Word) to create your illustrations. Do not use Microsoft Publisher. These illustrations will look best if all line widths are uniform (half- to two-point in size), and you do not create labels over shaded areas. Shading should be 133 lines per inch if possible. Use Times Roman or Helvetica for all figure call-outs. \textbf{Do not use hairline width lines} --- be sure that the stroke width of all lines is at least .5 pt. Zero point lines will print on a laser printer, but will completely disappear on the high-resolution devices used by our printers.

% \subsubsection{Photographs and Images.}
% Photographs and other images should be in grayscale (color photographs will not reproduce well; for example, red tones will reproduce as black, yellow may turn to white, and so forth) and set to a minimum of 300 dpi. Do not prescreen images.

% \subsubsection{Resizing Graphics.}
% Resize your graphics \textbf{before} you include them with LaTeX. You may \textbf{not} use trim or clip options as part of your \textbackslash includegraphics command. Resize the media box of your PDF using a graphics program instead.

% \subsubsection{Fonts in Your Illustrations.}
% You must embed all fonts in your graphics before including them in your LaTeX document.

% \subsubsection{Algorithms.}
% Algorithms and/or programs are a special kind of figures. Like all illustrations, they should appear floated to the top (preferably) or bottom of the page. However, their caption should appear in the header, left-justified and enclosed between horizontal lines, as shown in Algorithm~\ref{alg:algorithm}. The algorithm body should be terminated with another horizontal line. It is up to the authors to decide whether to show line numbers or not, how to format comments, etc.

% In \LaTeX{} algorithms may be typeset using the {\tt algorithm} and {\tt algorithmic} packages, but you can also use one of the many other packages for the task.

% \begin{algorithm}[tb]
% \caption{Example algorithm}
% \label{alg:algorithm}
% \textbf{Input}: Your algorithm's input\\
% \textbf{Parameter}: Optional list of parameters\\
% \textbf{Output}: Your algorithm's output
% \begin{algorithmic}[1] %[1] enables line numbers
% \STATE Let $t=0$.
% \WHILE{condition}
% \STATE Do some action.
% \IF {conditional}
% \STATE Perform task A.
% \ELSE
% \STATE Perform task B.
% \ENDIF
% \ENDWHILE
% \STATE \textbf{return} solution
% \end{algorithmic}
% \end{algorithm}

% \subsubsection{Listings.}
% Listings are much like algorithms and programs. They should also appear floated to the top (preferably) or bottom of the page. Listing captions should appear in the header, left-justified and enclosed between horizontal lines as shown in Listing~\ref{lst:listing}. Terminate the body with another horizontal line and avoid any background color. Line numbers, if included, must appear within the text column.

% \begin{listing}[tb]%
% \caption{Example listing {\tt quicksort.hs}}%
% \label{lst:listing}%
% \begin{lstlisting}[language=Haskell]
% quicksort :: Ord a => [a] -> [a]
% quicksort []     = []
% quicksort (p:xs) = (quicksort lesser) ++ [p] ++ (quicksort greater)
% 	where
% 		lesser  = filter (< p) xs
% 		greater = filter (>= p) xs
% \end{lstlisting}
% \end{listing}

% \subsection{References}
% The AAAI style includes a set of definitions for use in formatting references with BibTeX. These definitions make the bibliography style fairly close to the ones  specified in the Reference Examples appendix below. To use these definitions, you also need the BibTeX style file ``aaai25.bst," available in the AAAI Author Kit on the AAAI web site. Then, at the end of your paper but before \textbackslash end{document}, you need to put the following lines:

% \begin{quote}
% \begin{small}
% \textbackslash bibliography\{bibfile1,bibfile2,...\}
% \end{small}
% \end{quote}

% Please note that the aaai25.sty class already sets the bibliographystyle for you, so you do not have to place any \textbackslash bibliographystyle command in the document yourselves. The aaai25.sty file is incompatible with the hyperref and navigator packages. If you use either, your references will be garbled and your paper will be returned to you.

% References may be the same size as surrounding text.
% However, in this section (only), you may reduce the size to {\em \textbackslash small} (9pt) if your paper exceeds the allowable number of pages. Making it any smaller than 9 point with 10 point linespacing, however, is not allowed.

% The list of files in the \textbackslash bibliography command should be the names of your BibTeX source files (that is, the .bib files referenced in your paper).

% The following commands are available for your use in citing references:
% \begin{quote}
% {\em \textbackslash cite:} Cites the given reference(s) with a full citation. This appears as ``(Author Year)'' for one reference, or ``(Author Year; Author Year)'' for multiple references.\smallskip\\
% {\em \textbackslash shortcite:} Cites the given reference(s) with just the year. This appears as ``(Year)'' for one reference, or ``(Year; Year)'' for multiple references.\smallskip\\
% {\em \textbackslash citeauthor:} Cites the given reference(s) with just the author name(s) and no parentheses.\smallskip\\
% {\em \textbackslash citeyear:} Cites the given reference(s) with just the date(s) and no parentheses.
% \end{quote}
% You may also use any of the \emph{natbib} citation commands.


% \section{Proofreading Your PDF}
% Please check all the pages of your PDF file. The most commonly forgotten element is the acknowledgements --- especially the correct grant number. Authors also commonly forget to add the metadata to the source, use the wrong reference style file, or don't follow the capitalization rules or comma placement for their author-title information properly. A final common problem is text (expecially equations) that runs into the margin. You will need to fix these common errors before submitting your file.

% \section{Improperly Formatted Files }
% In the past, AAAI has corrected improperly formatted files submitted by the authors. Unfortunately, this has become an increasingly burdensome expense that we can no longer absorb). Consequently, if your file is improperly formatted, it will be returned to you for correction.

% \section{Naming Your Electronic File}
% We require that you name your \LaTeX{} source file with the last name (family name) of the first author so that it can easily be differentiated from other submissions. Complete file-naming instructions will be provided to you in the submission instructions.

% \section{Submitting Your Electronic Files to AAAI}
% Instructions on paper submittal will be provided to you in your acceptance letter.

% \section{Inquiries}
% If you have any questions about the preparation or submission of your paper as instructed in this document, please contact AAAI Press at the address given below. If you have technical questions about implementation of the aaai style file, please contact an expert at your site. We do not provide technical support for \LaTeX{} or any other software package. To avoid problems, please keep your paper simple, and do not incorporate complicated macros and style files.

% \begin{quote}
% \noindent AAAI Press\\
% 1101 Pennsylvania Ave, NW Suite 300\\
% Washington, DC 20004 USA\\
% \textit{Telephone:} 1-202-360-4062\\
% \textit{E-mail:} See the submission instructions for your particular conference or event.
% \end{quote}

% \section{Additional Resources}
% \LaTeX{} is a difficult program to master. If you've used that software, and this document didn't help or some items were not explained clearly, we recommend you read Michael Shell's excellent document (testflow doc.txt V1.0a 2002/08/13) about obtaining correct PS/PDF output on \LaTeX{} systems. (It was written for another purpose, but it has general application as well). It is available at www.ctan.org in the tex-archive.

% \appendix
% \section{Reference Examples}
% \label{sec:reference_examples}

% \nobibliography*
% Formatted bibliographies should look like the following examples. You should use BibTeX to generate the references. Missing fields are unacceptable when compiling references, and usually indicate that you are using the wrong type of entry (BibTeX class).

% \paragraph{Book with multiple authors~\nocite{em:86}} Use the \texttt{@book} class.\\[.2em]
% \bibentry{em:86}.

% \paragraph{Journal and magazine articles~\nocite{r:80, hcr:83}} Use the \texttt{@article} class.\\[.2em]
% \bibentry{r:80}.\\[.2em]
% \bibentry{hcr:83}.

% \paragraph{Proceedings paper published by a society, press or publisher~\nocite{c:83, c:84}} Use the \texttt{@inproceedings} class. You may abbreviate the \emph{booktitle} field, but make sure that the conference edition is clear.\\[.2em]
% \bibentry{c:84}.\\[.2em]
% \bibentry{c:83}.

% \paragraph{University technical report~\nocite{r:86}} Use the \texttt{@techreport} class.\\[.2em]
% \bibentry{r:86}.

% \paragraph{Dissertation or thesis~\nocite{c:79}} Use the \texttt{@phdthesis} class.\\[.2em]
% \bibentry{c:79}.

% \paragraph{Forthcoming publication~\nocite{c:21}} Use the \texttt{@misc} class with a \texttt{note="Forthcoming"} annotation.
% \begin{quote}
% \begin{footnotesize}
% \begin{verbatim}
% @misc(key,
%   [...]
%   note="Forthcoming",
% )
% \end{verbatim}
% \end{footnotesize}
% \end{quote}
% \bibentry{c:21}.

% \paragraph{ArXiv paper~\nocite{c:22}} Fetch the BibTeX entry from the "Export Bibtex Citation" link in the arXiv website. Notice it uses the \texttt{@misc} class instead of the \texttt{@article} one, and that it includes the \texttt{eprint} and \texttt{archivePrefix} keys.
% \begin{quote}
% \begin{footnotesize}
% \begin{verbatim}
% @misc(key,
%   [...]
%   eprint="xxxx.yyyy",
%   archivePrefix="arXiv",
% )
% \end{verbatim}
% \end{footnotesize}
% \end{quote}
% \bibentry{c:22}.

% \paragraph{Website or online resource~\nocite{c:23}} Use the \texttt{@misc} class. Add the url in the \texttt{howpublished} field and the date of access in the \texttt{note} field:
% \begin{quote}
% \begin{footnotesize}
% \begin{verbatim}
% @misc(key,
%   [...]
%   howpublished="\url{http://...}",
%   note="Accessed: YYYY-mm-dd",
% )
% \end{verbatim}
% \end{footnotesize}
% \end{quote}
% \bibentry{c:23}.

% For the most up to date version of the AAAI reference style, please consult the \textit{AI Magazine} Author Guidelines at \url{https://aaai.org/ojs/index.php/aimagazine/about/submissions#authorGuidelines}

% \section{Acknowledgments}
% AAAI is especially grateful to Peter Patel Schneider for his work in implementing the original aaai.sty file, liberally using the ideas of other style hackers, including Barbara Beeton. We also acknowledge with thanks the work of George Ferguson for his guide to using the style and BibTeX files --- which has been incorporated into this document --- and Hans Guesgen, who provided several timely modifications, as well as the many others who have, from time to time, sent in suggestions on improvements to the AAAI style. We are especially grateful to Francisco Cruz, Marc Pujol-Gonzalez, and Mico Loretan for the improvements to the Bib\TeX{} and \LaTeX{} files made in 2020.

% The preparation of the \LaTeX{} and Bib\TeX{} files that implement these instructions was supported by Schlumberger Palo Alto Research, AT\&T Bell Laboratories, Morgan Kaufmann Publishers, The Live Oak Press, LLC, and AAAI Press. Bibliography style changes were added by Sunil Issar. \verb+\+pubnote was added by J. Scott Penberthy. George Ferguson added support for printing the AAAI copyright slug. Additional changes to aaai25.sty and aaai25.bst have been made by Francisco Cruz, Marc Pujol-Gonzalez, and Mico Loretan.

% \bigskip
% \noindent Thank you for reading these instructions carefully. We look forward to receiving your electronic files!


\bibliography{aaai25}
\newpage
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\begin{center}
    \centering
    % \vspace{-2cm}
    \captionsetup{type=figure}
    \includegraphics[width=0.76\linewidth,height=6cm]{remove.pdf}
    \captionof{figure}{
   Visualization of background suppression in the BSI module.
}
\label{fig:BSI}
    \includegraphics[width=0.8\linewidth]{tracker_contrast_4_8.pdf}
    \captionof{figure}{
Visualization comparison of different quantities of temporal information tokens.
}
\label{fig:temporal}
\end{center}%
}]
\section{Supplementary Material}

\textbf{More discussion of background suppression}. To evaluate whether APTrack can suppress the background in BSI, we visualize this process. As shown in Fig.~\ref{fig:BSI}, BSI module effectively suppresses multimodal background interference by leveraging the correlation between each component and the search area. This mechanism not only reduces the impact of background noise but also accurately highlights target features, thereby enhancing the precision of interactions.


\noindent \textbf{More discussion of temporal information tokens
}. 
To explore why the performance dropped when the number of temporal information tokens increased from 4 to 8, we conducted a visual analysis of the two tracker versions. As shown in Fig.~\ref{fig:temporal}, when the number of tokens was 8, the tracker gradually lost track of the target in occlusion scenarios. This occurred because the more temporal information tokens introduced excessive redundant information, making it difficult for the model to effectively filter and focus on the current target state. In other words,  surplus temporal information tokens probably carried historical information that didn't align with the current target state, and this caused interference in the model's decision-making process.

\end{document}
