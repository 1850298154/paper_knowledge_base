\section{Experimental analysis and results}\label{evaluation}

\begin{table*}[t]
\centering
\caption{Computational devices.}
\footnotesize
\begin{tabular}{@{\extracolsep{4pt}}cccrrr@{}}
\toprule
\multirow{2}{*}{Device $k$} & \multirow{2}{*}{$\mathrm{e}$/$\mathrm{h}$/$\mathrm{c}$} & \multirow{2}{*}{Processor} & \multicolumn{3}{c}{Budgets} \\
\cline{4-6}
  & & & \multicolumn{1}{c}{Memory $M_{k}^{\mathrm{bgt}}$} & \multicolumn{1}{c}{Storage $S_{k}^{\mathrm{bgt}}$} & Energy $E_{k}^{\mathrm{bgt}}$\\
 %& & & & & (Battery Capacity)\\
\hline
Jetson TX2 & $\mathrm{e}$ & Cortex-A57\,/\,NVIDIA Denver2 @ 2\,GHz &  8\,GiB \hspace{10pt}   & 32\,GiB \hspace{6pt}   & 129.96\,Wh*\\
%\hline
%Odroid XU4 & e & Exynos 5422 Cortex-A15 @ 2\,GHz /  Cortex-A7 Octacore  & 2\,GiB & 16\,GiB & 129.96\,Wh* \\
Odroid XU4 & $\mathrm{e}$ & Exynos 5422 Cortex-A15\,/\,Cortex-A7 @ 2\,GHz       & 2\,GiB \hspace{10pt}   & 16\,GiB \hspace{6pt}    & 129.96\,Wh*\\
%\hline
Raspberry Pi 3 Model B & $\mathrm{e}$ & Broadcom Cortex-A53 @ 1.4\,GHz         & 1\,GiB \hspace{10pt}   & 16\,GiB \hspace{6pt}    & 129.96\,Wh*\\
%\hline
Mi Notebook Pro & $\mathrm{h}$ & Intel i5 8250U @ 1.6\,GHz                     & 8\,GiB \hspace{10pt}   & 512\,GiB \hspace{6pt}   & 60.00\,Wh\dag\\
%\hline
HPE ProLiant  DL580 Gen10 & $\mathrm{c}$ & Intel Xeon Gold 6240 @ 2.6\,GHz     & 400\,GiB \hspace{10pt} & 10\,TiB \hspace{6pt}    & \multicolumn{1}{c}{-}\\

\bottomrule

\multicolumn{6}{l}{\scriptsize{*Battery capacity of UAV where device is attached as payload. \hspace{2mm} \dag Battery capacity of device.}}\\

\end{tabular}
\label{constraints}
%\vspace{-3mm}
\end{table*}


\begin{table}[t]
\centering
\caption{Configurations of computational devices.}
\footnotesize
\begin{tabular}{@{\extracolsep{4pt}}ccccc@{}}
\toprule
\multirow{2}{*}{Device} & \multirow{2}{*}{$\mathrm{e}$/$\mathrm{h}$/$\mathrm{c}$} & \multicolumn{3}{c}{Configuration} \\
\cline{3-5}
 & & C1 & C2 & C3\\ 
\hline
Jetson TX2 & $\mathrm{e}$ & \checkmark & &  \\

Odroid XU4 & $\mathrm{e}$ &  & \checkmark & \\

Raspberry Pi 3 Model B & $\mathrm{e}$ & & & \checkmark \\

Mi Notebook Pro & $\mathrm{h}$ & \checkmark & \checkmark & \checkmark\\

HPE ProLiant DL580 Gen10 & $\mathrm{c}$ & \checkmark & \checkmark & \checkmark\\

\bottomrule

\end{tabular}
\label{configurations}
%\vspace{-3mm}
\end{table}



\subsection{Evaluation strategy}
\label{subsec:strategy}
We investigated our approach by conducting an extensive number of experiments:
\begin{enumerate}
    \item First, we validated our method utilizing a relevant real-world use-case scenario for the UAV-based inspection of power transmission towers and lines.
    
    \item Subsequently, we further examined the efficiency and scalability of our framework by developing and using appropriate synthetic benchmarks.
\end{enumerate}

We explored three alternative configurations (combinations) of the edge device $\mathrm{e}$, the hub device $\mathrm{h}$, and the cloud server $\mathrm{c}$. In each configuration, a different device $\mathrm{e}$ was considered.
For the first set of experiments, we examined two distinct objectives, the minimization of either latency or energy, performing two different runs for each configuration. In each run, we took into account varied communication channel characteristics.
As our primary goal in the second set of experiments was to evaluate the scalability of the proposed approach, without loss of generality, we examined one of the optimization objectives, the minimization of latency, conducting a run for each configuration and synthetic benchmark.
In contrast to the qualitative comparison with existing methods presented in \Cref{background}, a meaningful quantitative comparative evaluation does not apply, as existing approaches do not include all the parameters of our framework, do not provide the optimal solution, or do not use the underlying architecture considered in this work.

\subsection{Experimental setup}\label{setup}
Devices $\mathrm{e}$, $\mathrm{h}$, and $\mathrm{c}$ were based on real-world counterparts typically used in emerging applications, such as the ones motivating this work. Specifically, as device $\mathrm{e}$ we selected either Jetson TX2, Odroid XU4, or Raspberry Pi 3 Model B to represent a high, middle, or low-end edge device, respectively. Since these devices can be attached to a UAV as payload to provide computational capability, for battery capacity purposes, we considered a DJI Matrice 100 UAV.
We used Mi Notebook Pro as device $\mathrm{h}$, as it can facilitate the communication between devices $\mathrm{e}$ and $\mathrm{c}$.
Finally, the HPE ProLiant DL580 Gen10 server was selected as device $\mathrm{c}$, since its computational capacity is representative of that provided by a high-end server (physical or virtual) in a cloud environment. \cref{constraints} shows the devices we considered in our experiments, along with their corresponding memory, storage, and energy budgets. \cref{configurations} demonstrates the three alternative device configurations we investigated (C1, C2, and C3). The check marks indicate the devices included in each configuration.


\cref{table:energy_table} shows the $W_{kl}$, $\tau_{kl}$, and $\rho_{kl}$ parameters we used for each communication channel per run (Run 1 and Run 2). The particular values were selected to reflect the variations in bandwidth and energy consumption that may be observed due to the distance between the devices and the terrain in which the application is deployed (e.g., rural or urban terrain) \cite{4GMobile, 4Genergyconsumption}. 
In the real-world use-case scenario, for each configuration C1, C2, and C3 we conducted two runs, one for the bandwidth and energy parameters of Run 1, and one for those of Run 2. 
Moreover, for the energy objective,  the latency threshold was set to $L_{\mathrm{thr}}=8000$\,ms, which is a representative, but still challenging, time constraint for the particular application. 
On the other hand, in the case of synthetic benchmarks, for each configuration C1, C2, and C3 we performed a run using the bandwidth and energy parameters of Run 1.
The TFG transformation and the optimization problem formulation were implemented in C++. The formulated problem in each case was solved using Gurobi Optimizer 9.5 \cite{gurobi}, run on a server equipped with an Intel Xeon Gold 6240 processor @ 2.6\,GHz and 367\,GiB of RAM.


\begin{table*}[t]
\centering
\caption{Bandwidth and energy parameters for each communication channel per run.}
%\resizebox{0.6\columnwidth}{!}{
\footnotesize
\begin{tabular}{@{\extracolsep{4pt}}ccccrcc@{}} 
\toprule
Communication Channel & \multicolumn{3}{c}{Run 1} & \multicolumn{3}{c}{Run 2}\\
\cline{2-4} \cline{5-7}
Between Devices $k \rightarrow l$ & $W_{kl}$ (Mbit/s) & $\tau_{kl}$ (\SI{}{\micro \joule}/bit) & $\rho_{kl}$ (\SI{}{\micro \joule}/bit) & $W_{kl}$ (Mbit/s) & $\tau_{kl}$ (\SI{}{\micro \joule}/bit) & $\rho_{kl}$ (\SI{}{\micro \joule}/bit)\\ 
 \hline
 $\mathrm{e} \rightarrow \mathrm{h}$ & 15 & 1.0 & 0.70 & 10.0 \hspace{10pt} & 1.0 & 0.7 \\
 $\mathrm{h} \rightarrow \mathrm{e}$ & 20 & 1.0 & 0.70 & 10.0 \hspace{10pt} & 1.0 & 0.7 \\
 $\mathrm{h} \rightarrow \mathrm{c}$ & 25 & 2.5 & 1.25 & 0.5 \hspace{10pt} & 6.5 & 4.5 \\
 $\mathrm{c} \rightarrow \mathrm{h}$ & 35 & 2.5 & 1.25 & 1.5 \hspace{10pt} & 6.5 & 4.5 \\
\bottomrule
\end{tabular}
%}
\label{table:energy_table}
%\vspace{-3mm}
\end{table*}



\subsection{Real-world use-case scenario}\label{case_study}
\subsubsection{Overview of real-world application}\label{real_app}
The real-world application \cite{icarusTheocharides}, which concerns the UAV-based aerial visual inspection of power transmission towers and lines, consists of 15 tasks. Its flowchart, encapsulating its TFG, as well as its corresponding ETFG, are depicted in Fig. A.1 in Appendix A. 
A brief description of its tasks is given below:
\begin{enumerate}
    \item Image acquisition (task 1): captures an image through a camera connected to the edge device on the UAV.
    
    \item Image preprocessing (tasks 2--5): preliminary processing of the captured image.
    
    \item Power transmission lines detection (tasks 6--9): Hough transform-based line detector that identifies the transmission lines for navigation purposes, while detecting transmission line anomalies, such as vegetation.
    
    \item Power transmission towers detection (tasks 10--14): convolutional neural network (CNN) based detector that registers the location of transmission towers and inspects them for problems, such as faulty insulators and spacers.
    
    \item Display results (task 15): displays the final output on the hub device used by the operator to control the UAV.
\end{enumerate}

Tasks 1 and 15 can be allocated only on devices $\mathrm{e}$ and $\mathrm{h}$, respectively. On the other hand, tasks 2--14 can be allocated on any device, $\mathrm{e}$, $\mathrm{h}$, or $\mathrm{c}$. 
This is reflected in the ETFG in Fig. A.1b (Appendix A), where only one candidate node was generated for tasks 1 and 15, whereas three candidate nodes were generated for each of the other tasks.
The parameters $L_{ik}$, $P_{ik}$, $M_{i}$, $S_{i}$, and $D_{i}$ of each candidate node $N_{ik}$ in the ETFG were determined by profiling the execution of task $i$ on device $k$, in each configuration. For this purpose, we used  performance and power profiling tools (Sysprof, perf, and PowerTOP), as well as a digital power monitoring device \cite{sysprof, powertop}. 
$E_{ik}$ was calculated using \eqref{eq:compEnergy}, based on $P_{ik}$ and $L_{ik}$.
$NC_{i}$ was derived from the structure of the TFG.
On the other hand, $\delta_{ik \rightarrow jl}^{m}$ was derived from the structure of the ETFG. 
The parameters $CL_{ik \rightarrow jl}$ and $CE_{ik \rightarrow jl}$ were calculated using \eqref{eq:commLatency} and \eqref{eq:commEnergy}, respectively, based on $D_{i}$ and the bandwidth and energy parameters defined for Run 1 and Run 2 in \cref{table:energy_table}.



\subsubsection{Framework evaluation -- Real-world use-case scenario}\label{case_study-results}
The ETFG of the real-world application was used to formulate and solve the task allocation problem, considering the minimization of either latency or energy.
\cref{real_perf} presents the experimental results with respect to latency and energy consumption,  when the optimization objective was the minimization of overall latency, for all three device configurations (C1, C2, and C3), and for both sets of bandwidth and energy parameters (Run 1 and Run 2). For each configuration, in addition to the optimal task allocation across all three devices (denoted by O), we also examined three extreme task allocation cases where all tasks (except tasks 1 and 15, which required fixed allocation) were allocated on the edge device $\mathrm{e}$, the hub device $\mathrm{h}$, or the cloud server $\mathrm{c}$ (denoted by E, H, and C, respectively). \cref{results1,results3} report the overall latency for Run 1 and Run 2, respectively. They illustrate the computational latency on each device (denoted by Edge, Hub, and Cloud), as well as the communication latency over each communication channel (denoted by $\mathrm{Edge} \rightarrow \mathrm{Hub}$, $\mathrm{Hub} \rightarrow \mathrm{Edge}$, $\mathrm{Hub} \rightarrow \mathrm{Cloud}$, and $\mathrm{Cloud} \rightarrow \mathrm{Hub}$). The computational (communication) latency is depicted in solid (patterned) color. 
\cref{results2,results4} show the overall energy consumption for Run 1 and Run 2, respectively. The total energy consumption of each device is reported as the sum of the utilized computational and communication energy.



\begin{figure*}[t]
    \centering
    \begin{subfigure}{.9\textwidth}
        \includegraphics[width=\columnwidth]{8r_cropped.pdf}%
    \end{subfigure}\hfill%
    \begin{subfigure}[b]{.495\textwidth}
        \includegraphics[width=\columnwidth, height=1.8in]{1r_cropped.pdf}%
        \caption{Latency (in ms) per configuration (Run 1)}%
        \label{results1}%
    \end{subfigure}\hfill%
    \begin{subfigure}[b]{.495\textwidth}
        \includegraphics[width=\columnwidth, height=1.8in]{0r_cropped.pdf}%
        \caption{Energy consumption (in Wh) per configuration (Run 1)}%
        \label{results2}%
    \end{subfigure}\hfill 
    \begin{subfigure}[b]{.495\textwidth}
        \includegraphics[width=\columnwidth, height=1.8in]{3r_cropped.pdf}%
        \caption{Latency (in ms) per configuration (Run 2)}%
        \label{results3}%
    \end{subfigure}\hfill%
    \begin{subfigure}[b]{.495\textwidth}
        \includegraphics[width=\columnwidth, height=1.8in]{2r_cropped.pdf}%
        \caption{Energy consumption (in Wh) per configuration (Run 2)}%
        \label{results4}%
    \end{subfigure}\hfill%
    \caption{Real-world use-case scenario: Latency and energy consumption when minimizing overall latency.}
    \label{real_perf}
    %\vspace{-4mm}
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.95\textwidth}
        \includegraphics[width=\columnwidth]{9r_cropped.pdf}%
    \end{subfigure}\hfill%
    \begin{subfigure}[b]{.495\textwidth}
        \includegraphics[width=\columnwidth, height=1.8in]{5r_cropped.pdf}%
        \caption{Latency (in ms) per configuration (Run 1)}%
        \label{results5}%
    \end{subfigure}\hfill%
    \begin{subfigure}[b]{.495\textwidth}
        \includegraphics[width=\columnwidth, height=1.8in]{4r_cropped.pdf}%
        \caption{Energy consumption (in Wh) per configuration (Run 1)}%
        \label{results6}%
    \end{subfigure}\hfill 
    \begin{subfigure}[b]{.495\textwidth}
        \includegraphics[width=\columnwidth, height=1.8in]{7r_cropped.pdf}%
        \caption{Latency (in ms) per configuration (Run 2)}%
        \label{results7}%
    \end{subfigure}\hfill%
    \begin{subfigure}[b]{.495\textwidth}
        \includegraphics[width=\columnwidth, height=1.8in]{6r_cropped.pdf}%
        \caption{Energy consumption (in Wh) per configuration (Run 2)}%
        \label{results8}%
    \end{subfigure}\hfill%
   \caption{Real-world use-case scenario: Comparison of latency and energy consumption between cases where the optimization objective was the minimization of either latency (O\_L) or energy (O\_E).}
    \label{real_energy}
    %\vspace{-4mm}
\end{figure*}


%-- Figure 3

The results in \cref{real_perf} show that the optimal task allocation (case O) returned by our framework, yielded not only the minimum overall latency, which was the optimization objective, but also the lowest overall energy consumption, for all configurations and runs. 
With respect to latency, \cref{results1} (Run 1) reveals that the second-best performance was observed in case H. On the other hand, the latency in case C was comparatively high and approximately the same for all configurations, due to the fixed allocation of tasks 1 and 15 on devices $\mathrm{e}$ and $\mathrm{h}$, respectively. This resulted in increased latency for communication channels $\mathrm{Edge} \rightarrow \mathrm{Hub}$, $\mathrm{Hub} \rightarrow \mathrm{Cloud}$, and $\mathrm{Cloud} \rightarrow \mathrm{Hub}$, as the remaining tasks were allocated on device $\mathrm{c}$. Despite this, case C provided the worst performance only for configuration C1. The worst performance for C2 and C3 was observed in case E. The reason was that Odroid XU4 and  Raspberry Pi 3 Model B used as edge devices in configurations C2 and C3, respectively, are slower than Jetson TX2 used in C1, thus leading to a higher overall latency.
In \cref{results3} (Run 2), the second-best performance was again observed in case H. However, in contrast to Run 1, the worst performance was always observed in case C, due to the lower bandwidth of communication channels $\mathrm{Hub} \rightarrow \mathrm{Cloud}$ and $\mathrm{Cloud} \rightarrow \mathrm{Hub}$.
With respect to energy, \cref{results2,results4} show that the highest energy consumption was observed in case C. This was due to the significantly higher communication demands, compared to the other cases. 


%-- Figure 4

\cref{real_energy} illustrates the comparison of latency and energy consumption between cases where the optimization objective was the minimization of either latency (O\_L) or energy (O\_E), for all configurations (C1, C2, and C3) and runs (Run 1 and Run 2).
The results in \cref{results5,results6,results7,results8} are reported in the same manner as in \cref{real_perf}. They demonstrate how each optimization objective affected the overall latency and energy.
For example, \cref{results5,results7} show a trend in the case of O\_E to allocate as many tasks as possible on device $\mathrm{e}$, in an attempt to minimize inter-task communication and thus the utilized energy.
Furthermore, it can be observed in \cref{results5} that for configuration C1, Run 1, the overall latency was approximately the same for both optimization objectives, O\_L and O\_E, even though the optimal task allocation was different in each case. However, due to the different allocation of the tasks, the resulting energy consumption differed between the two cases, as shown in \cref{results6}. On the other hand, for configuration C1, Run 2, both optimization objectives yielded the same optimal task allocation. Hence, the resulting overall latency and energy were the same in both cases, as showcased in \cref{results7,results8}, respectively.



\begin{table}[t]
\centering
\caption{Solver execution time for real-world use-case scenario.}
\resizebox{\columnwidth}{!}{
\footnotesize
\begin{tabular}{@{\extracolsep{4pt}}crrrr@{}} 
\toprule

\multirow{3}{*}{Configuration} & \multicolumn{4}{c}{Optimization Objective} \\
\cline{2-5}
& \multicolumn{2}{c}{Latency} & \multicolumn{2}{c}{Energy Consumption} \\
\cline{2-3} \cline{4-5}

 & \multicolumn{1}{c}{Run 1} & \multicolumn{1}{c}{Run 2} & \multicolumn{1}{c}{Run 1} & \multicolumn{1}{c}{Run 2} \\

\hline

%C1	& 7.3860\,ms	& 11.3080\,ms	& 10.6850\,ms	& 13.4326\,ms \\
%C2	& 8.1001\,ms	& 13.8621\,ms	& 11.2678\,ms	& 12.4213\,ms \\
%C3	& 7.4840\,ms	& 13.8621\,ms	& 10.8976\,ms	& 12.4124\,ms \\

C1	& 7.39\,ms	& 11.31\,ms	& 10.69\,ms	& 13.43\,ms \\
C2	& 8.10\,ms	& 13.86\,ms	& 11.27\,ms	& 12.42\,ms \\
C3	& 7.48\,ms	& 13.86\,ms	& 10.90\,ms	& 12.41\,ms \\

\bottomrule
\end{tabular}
}
\label{table:realTimes}
%\vspace{-3mm}
\end{table}


\begin{figure*}[t]
    \centering
    \includegraphics[width=.85\textwidth]{coins_journal_tfg_generation_v3.5.4.pdf}
    \caption{Overview of random TFG generation and transformation. The generation of random TFGs is presented in \cref{subsubsec:randomTFGs}. The transformation of random TFGs into ETFGs is described in \cref{subsubsec:syntheticParams}.}
    \label{overview_tgf_generation}
    %\vspace{-4mm}
\end{figure*}


The above observations indicate that a straightforward derivation of a suitable task allocation cannot be based on the optimization objective alone, as other factors play a decisive role, such as the communication and fixed allocation requirements of the tasks, as well as the device and communication channel characteristics. Therefore, the use of the proposed application-driven optimization framework is beneficial in that regard. In addition to providing the optimal task allocation for the minimization of either latency or energy, it also enables design space exploration with respect to different device configurations and their connectivity.
As reported by the Gurobi solver, our framework required 152 variables for the examined real-world application. It required 149 constraints when minimizing the overall latency, and 150 constraints when minimizing the overall energy consumption. In both cases, it required an average time of 11.09\,ms in order to return a solution. Specifically, the solver execution time ranged between 7.39\,ms and 13.86\,ms (as shown in \cref{table:realTimes}), which is short and practical, as this is a design-time (i.e., offline) approach.


%---------------------  SYNTHETIC -----------------------------


\subsection{Synthetic benchmarks}\label{pseudo-random}
We developed various synthetic benchmarks (i.e., synthetic ETFGs) with different structures, sizes, and parameters, suitable for the targeted  edge/hub/cloud environment. Since the applications motivating this work typically use machine learning inference and/or time-bound stream processing methods, we considered the following relevant structure types: parallel (denoted by P), serial (denoted by S), and a mixture of both (denoted by M). 
The initial TFGs were generated randomly (\cref{subsubsec:randomTFGs}).
To transform a randomly generated TFG into the corresponding ETFGs (one ETFG per device configuration), we extended our TFG transformation technique (\cref{subsubsec:syntheticParams}).
The parameters of the resulting ETFGs were determined using workload information intended for such applications, with the aid of relevant device benchmarks.
An outline of the process for generating a random TFG and subsequently transforming it into ETFGs, is illustrated in \cref{overview_tgf_generation}.


\subsubsection{Generation of random TFGs}\label{subsubsec:randomTFGs}
We used the Task Graphs for Free (TGFF) generator \cite{tgff, tgff2} to randomly generate the initial TFGs. The input required by the TGFF generator included the number of nodes (tasks) and the node maximum in/out degree (i.e., the maximum number of incoming/outgoing arcs per node), as shown in the second and third columns of \cref{table:tfg_properties}, respectively. The particular input was used to generate a variety of representative TFGs \cite{synthetic_taskgraphs}.
Specifically, we generated six random TFGs for each structure type, with 10, 100, and 1000 nodes. For each number of nodes, two TFG variants were generated, using different maximum in/out degrees. 
The identifier of each randomly generated TFG (shown in the first column of \cref{table:tfg_properties}) denotes the structure type (P, S, or M), the number of nodes (order of magnitude), and the variant of the TFG. 
For example, P2.1 is the first variant of a TFG with parallel structure and 100 nodes.
Due to the nature of the targeted applications, after the generation of each TFG, we set a percentage of its tasks to require fixed allocation on devices $\mathrm{e}$ and $\mathrm{h}$, as shown in the seventh and eighth columns of \cref{table:tfg_properties}, respectively. In each case of fixed allocation, the tasks were selected randomly.
Graphical representations of P1.1, S1.1, and M1.1 TFGs are depicted in \cref{randomTaskGraphs}.


\begin{table*}[t]
\centering
\caption{Randomly generated TFGs and their corresponding ETFGs (Synthetic Benchmarks).}
\label{table:tfg_properties}
\footnotesize
\resizebox{0.85\textwidth}{!}{
%\begin{tabular}{cc||ccccccc} 
\begin{tabular}{@{\extracolsep{4pt}}ccccccccccc@{}}
\toprule
 \multicolumn{3}{c}{Input for TGFF} & \multicolumn{3}{c}{Generated} & \multicolumn{2}{c}{Post} & \multicolumn{3}{c}{Corresponding} \\
 \multicolumn{3}{c}{Generator} & \multicolumn{3}{c}{TFG} & \multicolumn{2}{c}{Generation} & \multicolumn{3}{c}{ETFG} \\
\cline{1-3} \cline{4-6} \cline{7-8} \cline{9-11}
\multirow{2}{*}{TFG\#} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}  \#Nodes \end{tabular}}  & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Max\\In/Out-Degree \end{tabular}} &  \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}} \#Nodes/Arcs \end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Avg.\\In/Out-Degree \end{tabular}}  & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Depth/\\Max Width \end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Fixed\\Allocation (\%) \end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}} \#Nodes/Arcs \end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}} \#Variables/\\ Constraints \end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}} Avg. Exec.\\Time (s) \end{tabular}}\\
 &  &  &  &  &  & $\mathrm{e}$ & $\mathrm{h}$ & & &\\

\hline

%\vspace{-6pt}\\

P1.1 & 10 		& 2\,/\,2 		& 10\,/\,11 			& 1.20\,/\,1.20 	& 5\,/\,4 		& 5 & 2 	& 28\,/\,93 			& 121\,/\,118           & 0.01      \\	%& 0.0092
P1.2 & 10 		& 2\,/\,2 		& 9\,/\,10 				& 1.22\,/\,1.22 	& 2\,/\,4 		& 4 & 2 	& 27\,/\,90 			& 117\,/\,112           & 0.01      \\  %& 0.0032	
P2.1 & 100 		& 2\,/\,3 		& 100\,/\,129 			& 1.30\,/\,1.30 	& 13\,/\,11 	& 4 & 2 	& 288\,/\,1077 			& 1365\,/\,1261         & 0.03      \\  %& 0.0293	
P2.2 & 100 		& 5\,/\,3 		& 99\,/\,136 			& 1.38\,/\,1.38 	& 6\,/\,24 		& 3 & 1 	& 289\,/\,1170 			& 1459\,/\,1335         & 0.03      \\	%& 0.0312
P3.1 & 1000 	& 2\,/\,3 		& 999\,/\,1232 			& 1.23\,/\,1.23 	& 20\,/\,89 	& 5 & 2 	& 2857\,/\,10078 		& 12935\,/\,11812       & 0.25      \\	%& 0.2467
P3.2 & 1000 	& 5\,/\,6 		& 1001\,/\,1553 		& 1.55\,/\,1.55 	& 20\,/\,145 	& 4 & 2 	& 2889\,/\,12913 		& 15802\,/\,14553       & 0.44      \\  %& 0.4360	
% P4.1 & 10000 	& 2\,/\,3 		& 9999\,/\,12496 		& 1.25\,/\,1.25 	& 33\,/\,821 	& 4 & 3 	& 28597\,/\,101854 		& - & 22.4924     \\	
% P4.2 & 10000 	& 5\,/\,6 		& 10002\,/\,16130 		& 1.61\,/\,1.61 	& 21\,/\,865 	& 2 & 2 	& 29206\,/\,137818 		& - & 146.7154    \\	
% P5.1 & 100000 	& 2\,/\,3 		& 100001\,/\,125115		& 1.25\,/\,1.25 	& 43\,/\,6401 	& 1 & 2 	& 294003\,/\,1081429 	& - & 923.1030    \\	
% P5.2 & 100000 	& 5\,/\,6 		& 100000\,/\,156214 	& 1.56\,/\,1.56 	& 29\,/\,7732 	& 1 & 1 	& 296000\,/\,1369728 	& - & 18695.5413  \\	
	 &  		&  		&  						&  				&  			&   &   &  		&	          \\                    
S1.1 & 10 		& 2\,/\,2 		& 10\,/\,17 			& 1.80\,/\,1.80 	& 10\,/\,1 		& 4 & 2 	& 30\,/\,153 			& 183\,/\,181           & 0.01      \\	%& 0.0079
S1.2 & 10 		& 5\,/\,5 		& 11\,/\,40 			& 3.72\,/\,3.72 	& 11\,/\,1 		& 4 & 3 	& 33\,/\,360 			& 393\,/\,390           & 0.01      \\	%& 0.0119
S2.1 & 100 		& 2\,/\,2 		& 100\,/\,197 			& 1.98\,/\,1.98 	& 100\,/\,1 	& 4 & 3 	& 286\,/\,1605 			& 1891\,/\,1813         & 0.04      \\	%& 0.0397
S2.2 & 100 		& 5\,/\,5 		& 101\,/\,490 			& 4.86\,/\,4.86 	& 101\,/\,1 	& 1 & 3 	& 295\,/\,4170 			& 4465\,/\,4380         & 0.10      \\	%& 0.1038
S3.1 & 1000 	& 2\,/\,2 		& 1000\,/\,1997 		& 1.99\,/\,1.99 	& 1000\,/\,1 	& 5 & 3 	& 2840\,/\,16089 		& 18929\,/\,18097       & 0.40      \\	%& 0.3973
S3.2 & 1000 	& 5\,/\,5 		& 998\,/\,4975 			& 4.98\,/\,4.98 	& 998\,/\,1 	& 2 & 2 	& 2914\,/\,42415 		& 45329\,/\,44419       & 1.80      \\	%& 1.8045
% S4.1 & 10000 	& 2\,/\,2 		& 9997\,/\,19991 		& 1.99\,/\,1.99 	& 9997\,/\,1 	& 1 & 3 	& 29191\,/\,170411 		& - & 46.3992     \\	
% S4.2 & 10000 	& 5\,/\,5 		& 10002\,/\,49995 		& 4.99\,/\,4.99 	& 10002\,/\,1 	& 2 & 2 	& 29206\,/\,426263 		& - & 595.5959    \\	
% S5.1 & 100000 	& 2\,/\,2 		& 99989\,/\,199975 		& 1.99\,/\,1.99 	& 99989\,/\,1 	& 3 & 3 	& 287967\,/\,1658583 	& - & 698.1950    \\	
% S5.2 & 100000 	& 5\,/\,5 		& 99992\,/\,499945 		& 4.99\,/\,4.99 	& 99992\,/\,1 	& 2 & 3 	& 291976\,/\,4262669 	& - & 9990.1812   \\	
	 &  		&  				&  					&  				&  			&   &   &  	&             \\	                
M1.1 & 10 		& 9\,/\,4 		& 22\,/\,33 			& 1.54\,/\,1.54 	& 6\,/\,5 		& 4 & 1 	& 64\,/\,261 			& 325\,/\,313           & 0.02      \\   %& 0.0182	
M1.2 & 10 		& 8\,/\,2 		& 55\,/\,65 			& 1.20\,/\,1.20 	& 14\,/\,6 		& 1 & 2 	& 161\,/\,561 			& 722\,/\,679           & 0.02      \\   %& 0.0157	
M2.1 & 100 		& 10\,/\,3 		& 109\,/\,141 			& 1.30\,/\,1.30 	& 8\,/\,10 		& 1 & 3 	& 319\,/\,1210 			& 1529\,/\,1436         & 0.03      \\   %& 0.0285	
M2.2 & 100 		& 8\,/\,5 		& 122\,/\,147 			& 1.21\,/\,1.21 	& 7\,/\,15 		& 2 & 2 	& 358\,/\,1252 			& 1610\,/\,1504         & 0.03      \\   %& 0.0295	
M3.1 & 1000 	& 12\,/\,4 		& 1000\,/\,1224 		& 1.23\,/\,1.23 	& 31\,/\,40 	& 4.80 & 2 	& 2864\,/\,10055 		& 12919\,/\,12063       & 0.24      \\	 %& 0.2362
M3.2 & 1000 	& 18\,/\,5 		& 1017\,/\,1181 		& 1.16\,/\,1.16 	& 29\,/\,56 	& 1.86 & 1 	& 2993\,/\,10218 		& 13211\,/\,12260       & 0.24      \\	 %& 0.2435
% M4.1 & 10000 	& 49\,/\,5 		& 10002\,/\,12152 		& 1.21\,/\,1.21 	& 76\,/\,219 	& 1.92 & 3 	& 29022\,/\,101654 		& - & 9.4886      \\	
% M4.2 & 10000 	& 90\,/\,5 		& 10009\,/\,11662 		& 1.17\,/\,1.17 	& 54\,/\,233 	& 1 & 1 	& 29627\,/\,102072 		& - & 17.5886     \\	
% M5.1 & 100000 	& 81\,/\,5 		& 100016\,/\,121481 	& 1.22\,/\,1.22 	& 90\,/\,965 	& 4.87 & 3 	& 284314\,/\,975265 	& - & 294.0341    \\	
% M5.2 & 100000 	& 204\,/\,6 	& 100032\,/\,116481 	& 1.16\,/\,1.16 	& 60\,/\,1245 	& 1.93 & 3	& 290226\,/\,977314 	& - & 318.1052    \\	

\bottomrule
\end{tabular}
}
%\vspace{-3mm}
\end{table*}

\begin{figure}[t]
    \centering
    \begin{subfigure}{.33\columnwidth}
        \centering
        \includegraphics[height=1in]{random10_2_2_cropped.pdf}
        \caption{P1.1}
        \label{randomTaskGraphsa}
    \end{subfigure}\hfill%
    \begin{subfigure}{.33\columnwidth}
        \centering
        \includegraphics[height=2in]{series10_2_2_cropped.pdf}
        \caption{S1.1}
        \label{randomTaskGraphsb}
    \end{subfigure}\hfill%
    \begin{subfigure}{.33\columnwidth}
        \centering
        \includegraphics[height=2.22in]{coins10_5_2_cropped_rearranged.pdf}
        \caption{M1.1}
        \label{randomTaskGraphsc}
    \end{subfigure}\hfill%
    \caption{Randomly generated TFGs with (a) parallel, (b) serial, and (c) mixed structure type, as described in \cref{table:tfg_properties}.}
    \label{randomTaskGraphs}
    %\vspace{-4mm}
\end{figure}






\begin{table*}[t]
\centering
\caption{Performance scores and performance ratios of computational devices.}
\label{table:benchmarks}
%\resizebox{0.7\textwidth}{!}{
\footnotesize
\begin{tabular}{c|r|rr|rr|rr|rr} 
\toprule
 & \multicolumn{9}{c}{Device $k$} \\
\hline
$\mathrm{e}$/$\mathrm{h}$/$\mathrm{c}$ & \multicolumn{1}{c|}{$\mathrm{e}$} & \multicolumn{2}{c|}{$\mathrm{e}$} & \multicolumn{2}{c|}{$\mathrm{e}$} & \multicolumn{2}{c|}{$\mathrm{h}$} & \multicolumn{2}{c}{$\mathrm{c}$}\\

\hline 

Rank                        & \multicolumn{1}{c|}{1}                 & \multicolumn{2}{c|}{2}                 & \multicolumn{2}{c|}{3}                 & \multicolumn{2}{c|}{4}                     & \multicolumn{2}{c}{5}\\

\hline 

\multirow{2}{*}{Device}            &   \multicolumn{1}{c|}{Raspberry Pi 3*}	&   \multicolumn{2}{c|}{\multirow{2}{*}{Odroid XU4}}  				    &   \multicolumn{2}{c|}{\multirow{2}{*}{Jetson TX2}}       		&   \multicolumn{2}{c|}{\multirow{2}{*}{Mi Notebook Pro}}				    &  \multicolumn{2}{c}{HPE ProLiant}\\ 
\multirow{2}{*}{Benchmark} &   \multicolumn{1}{c|}{Model B} & &  & & & & & \multicolumn{2}{c}{DL580 Gen10}		\\
\cline{2-10}

 & \multicolumn{1}{c|}{Score} & \multicolumn{1}{c}{Score} & \multicolumn{1}{c|}{$\phi_{k}$}  & \multicolumn{1}{c}{Score} & \multicolumn{1}{c|}{$\phi_{k}$}  & \multicolumn{1}{c}{Score} & \multicolumn{1}{c|}{$\phi_{k}$}  & \multicolumn{1}{c}{Score} & \multicolumn{1}{c}{$\phi_{k}$}\\
 
\hline


R Benchmark                          & 3.80\,s 	\hspace{10pt}	 & 2.16\,s    	&	1.76		& 2.22\,s    	&	1.71		& 0.50\,s    		&	7.60		&   0.26\,s     &	14.62	\\
Scikit-Learn (Phoronix)              & 1373.54\,s \hspace{10pt}	 & 629.74\,s 	&	2.18		& 268.09\,s 	&	5.12		& 10.11\,s 			&	135.86	&   7.64\,s  	&	179.78	\\
TensorFlow (Phoronix)                & 12648.38\,s \hspace{10pt} & 1775.29\,s    &	7.12		& 781.46\,s 	&	16.19		& 169.70\,s 		&	74.53	&   59.82\,s    &	211.44	\\
NumPy (Phoronix)                     & 10.63\,pts \hspace{10pt}  & 24.93\,pts    &	2.35		& 52.81\,pts    &	4.97		& 284.68\,pts   	&	26.78	&   405.71\,pts &	38.17	\\
Geekbench 5                          & 398.00\,pts \hspace{10pt} & 626.00\,pts  	&	1.57		& \multicolumn{1}{c}{-} 			&	\multicolumn{1}{c|}{-} 			& 3612.00\,pts 		& 	9.08	&   33323.00\,pts  &	83.73	\\

\hline
Avg. Performance Ratio  & & \multicolumn{2}{r|}{$\phi_{k}^{\mathrm{avg}} = 2.99$} & \multicolumn{2}{r|}{$\phi_{k}^{\mathrm{avg}} = 6.99$} & \multicolumn{2}{r|}{$\phi_{k}^{\mathrm{avg}} = 50.77$}  & \multicolumn{2}{r}{$\phi_{k}^{\mathrm{avg}} = 105.55$}\\

\bottomrule

\multicolumn{6}{l}{\scriptsize{*Used as reference device $\hat{\mathrm{e}}$.}}\\
\end{tabular}
%}
%\vspace{-3mm}
\end{table*}




\subsubsection{Transformation of random TFGs into ETFGs}\label{subsubsec:syntheticParams}
We transformed each random TFG by extending our approach to generate an ETFG for each device configuration (C1, C2, and C3). 
For a specific TFG, the resulting ETFGs had the same structure, but their parameters could differ.
Specifically, to randomly assign appropriate values to the ETFG candidate node and arc parameters for each configuration, we first ran on each device the benchmarks shown in \cref{table:benchmarks}.
The Phoronix Test Suite (Scikit-Learn, TensorFlow, and NumPy) and R Benchmark \cite{phoronix, openbenchmarking} were run on all devices in each configuration.
Geekbench 5 was run on devices $\mathrm{h}$ and $\mathrm{c}$, whereas for all variants of device $\mathrm{e}$, we used the scores reported in \cite{geekbench5} (except for Jetson TX2, for which a score is not available). 
The NumPy and Geekbench 5 results are shown in points (pts), where a higher score indicates better performance (in terms of latency). The remaining benchmark results are reported in seconds (s), where a lower score represents better performance. 
We ranked the devices based on their performance scores, from the slowest (rank 1) to the fastest (rank 5), as shown in \cref{table:benchmarks}. We used the slowest device, Raspberry Pi 3 Model B, as a reference device (denoted by $\hat{\mathrm{e}}$) to determine the performance ratio $\phi_{k}$ of the other devices. For the benchmark results in seconds, $\phi_{k}$ is the ratio of the score of device $\hat{\mathrm{e}}$ to the score of device $k$. For the results in points, $\phi_{k}$ is the ratio of the score of device $k$ to the score of device $\hat{\mathrm{e}}$.


Subsequently, we determined the computational latency $L_{ik}$ and power consumption $P_{ik}$ for each candidate node $N_{ik}$. We first assigned a random value to $L_{i \hat{\mathrm{e}}}$ and $P_{i \hat{\mathrm{e}}}$, selected from the respective measurements for device $\hat{\mathrm{e}}$ in the real-world use-case scenario. 
For the remaining devices, we set $L_{ik} = L_{i \hat{\mathrm{e}}} / \phi_{k}^{\mathrm{avg}}$ and $P_{ik} = P_{i \hat{\mathrm{e}}} \, \phi_{k}^{\mathrm{avg}}$, where $\phi_{k}^{\mathrm{avg}}$ is the average performance ratio of device $k$ across all device benchmarks, as shown in \cref{table:benchmarks}.
In each case, we ensured that $P_{ik}$ was in the interval $( P_{k}^{\mathrm{idle}}, P_{k}^{\mathrm{max}} ]$, by adjusting it accordingly. Specifically, if $P_{ik} \leq P_{k}^{\mathrm{idle}}$ or $P_{ik} > P_{k}^{\mathrm{max}}$, we adjusted it to $P_{k}^{\mathrm{idle}} (1 + \alpha)$ and $P_{k}^{\mathrm{max}} (1 - \alpha)$, respectively. The adjustment factor $\alpha$ was randomly selected in the range $[ 0.1\%, 0.5\% ]$.
$P_{k}^{\mathrm{idle}}$ and $P_{k}^{\mathrm{max}}$ were defined through profiling and represent the idle and maximum power consumption of device $k$, respectively. 
The candidate node parameters $M_{i}$, $S_{i}$, and $D_{i}$ were selected randomly from the corresponding measurements in the real-world scenario.
On the other hand, the remaining candidate node and arc parameters, $E_{ik}$, $NC_{i}$, $\delta_{ik \rightarrow jl}^{m}$, $CL_{ik \rightarrow jl}$, and $CE_{ik \rightarrow jl}$ were determined as explained in \cref{real_app}. For $CL_{ik \rightarrow jl}$ and $CE_{ik \rightarrow jl}$ we used the bandwidth and energy parameters of Run 1 in \cref{table:energy_table}. The number of nodes and arcs in each ETFG are shown in the ninth column of \cref{table:tfg_properties}.


\subsubsection{Framework evaluation -- Synthetic benchmarks}\label{pseudo-random-results}

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[b]{.495\textwidth}
        \includegraphics[width=\columnwidth, height=1.8in]{0_cropped.pdf}%
        \caption{Latency (in\,ms) per benchmark (10 nodes) and configuration}%
        \label{results9}%
    \end{subfigure}\hfill%
    \begin{subfigure}[b]{.495\textwidth}
        \includegraphics[width=\columnwidth, height=1.8in]{5_cropped.pdf}%
        \caption{Energy consumption (in\,Wh) per benchmark (10 nodes) and configuration}%
        \label{results10}%
    \end{subfigure}\hfill 
    \begin{subfigure}[b]{.495\textwidth}
        \includegraphics[width=\columnwidth, height=1.8in]{1_cropped.pdf}%
        \caption{Latency (in\,ms) per benchmark (100 nodes) and configuration}%
        \label{results11}%
    \end{subfigure}\hfill%
    \begin{subfigure}[b]{.495\textwidth}
        \includegraphics[width=\columnwidth, height=1.8in]{6_cropped.pdf}%
        \caption{Energy consumption (in\,Wh) per benchmark (100 nodes) and configuration}%
        \label{results12}%
    \end{subfigure}\hfill%
    \begin{subfigure}[b]{.495\textwidth}
        \includegraphics[width=\columnwidth, height=1.8in]{2_cropped.pdf}%
        \caption{Latency (in\,ms) per benchmark (1000 nodes) and configuration}%
        \label{results13}%
    \end{subfigure}\hfill%
    \begin{subfigure}[b]{.495\textwidth}
        \includegraphics[width=\columnwidth, height=1.8in]{7_cropped.pdf}%
        \caption{Energy consumption (in\,Wh) per benchmark (1000 nodes) and configuration}%
        \label{results14}%
    \end{subfigure}\hfill 
    \caption{Synthetic benchmarks:
    Latency and energy consumption when minimizing overall latency.
    }
    \label{benchmarks}
    \vspace{-10pt}
\end{figure*}


The resulting ETFGs, denoted by the identifier of their corresponding TFG, were used as synthetic benchmarks. 
\cref{benchmarks} presents the evaluation results with respect to latency and energy consumption when minimizing overall latency, for all three configurations (C1, C2, and C3), using the bandwidth and energy parameters of Run 1. The results are presented in the same fashion as in the real-world use-case (\cref{case_study-results}).
It can be observed that the overall latency and energy consumption increased as the size of the benchmarks expanded. 
Moreover, \cref{results9} shows that for benchmark M1.1, configuration C1, all tasks were allocated on device $\mathrm{e}$. This led to a significantly lower energy consumption than C2 and C3 for the particular benchmark, as shown in \cref{results10}.
This was due to the utilization of Jetson TX2 as device $\mathrm{e}$ in C1, instead of the slower Odroid XU4 or Raspberry Pi 3 Model B used in C2 and C3, respectively. Therefore, only device $\mathrm{e}$ was required to provide the minimum latency for benchmark M1.1, configuration C1. On the other hand, in configurations C2 and C3 for the specific benchmark, device $\mathrm{h}$ was also utilized in addition to device $\mathrm{e}$, resulting in higher energy consumption than C1, due to the communication between the two devices.


Furthermore, \cref{results9,results10} show that the overall latency and energy for benchmarks M1.1 and M1.2 were significantly higher than those for P1.1, P1.2, S1.1, and S1.2. 
This was due to the considerably greater number of nodes and arcs in M1.1 and M1.2 TFGs compared to the other structure types, as generated by the TGFF generator for the same input (i.e., 10 nodes). 
This led to increased computational and communication requirements for M1.1 and M1.2.
On the other hand, \crefrange{results11}{results14} reveal that for TFGs with 100 and 1000 nodes, serial benchmarks generally did not perform well with respect to latency and energy. 
The reason was that they featured more arcs compared to the respective parallel and mixed benchmarks. Hence, they had comparatively higher communication demands.


Consequently, the structure of a TFG plays a  pivotal role in the examined problem. TFGs with a similar number of nodes but varied structure resulted in diverse task allocations, yielding  different results.
Moreover, inter-task communication was often the dominant factor in the resulting overall latency and energy consumption. These observations are in line with the ones made for the real-world application (\cref{case_study-results}).
The number of variables and constraints, as well as the average time across all configurations required by the Gurobi solver to return a solution for each benchmark are shown in \cref{table:tfg_properties}. The average time ranged between 0.01\,s (benchmarks P1.1, P1.2, S1.1, and S1.2) and 1.8\,s (benchmark S3.2). As this is a design-time framework where task allocation is performed offline, and also considering the NP-hardness of the problem, the solver execution time is short and thus practical. 
Overall, the evaluation of the synthetic benchmarks demonstrated the efficiency and scalability of the proposed approach to applications of different structures and sizes. 
It consistently provided the optimal solution in a short time frame (1.8\,s in the worst case), even for large-scale TFGs with 1000 nodes, requiring over 45\,000 variables and over 44\,000 constraints.





