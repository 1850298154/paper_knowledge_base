\section{Introduction}
The rapid expansion of the Internet of Things (IoT) has led to the emergence of applications that leverage diverse system architectures in the edge-cloud continuum. These systems vary with respect to where the computation takes place, based on the unique requirements of each application. One such architecture follows the edge/hub/cloud paradigm, which facilitates remote decision-making and real-time response \cite{Noghabi2018, Sidhardhan2021}.
This paradigm differs from the traditional edge computing concept, which typically considers a bottom layer of sensing devices (IoT or edge devices layer), an intermediate layer with servers of moderate computational capacity (edge or fog servers layer), and a top layer with more computationally capable cloud servers (cloud layer) \cite{Iorga2018, Kekki2018}. 
In contrast, an edge/hub/cloud environment encompasses edge and hub devices in the bottom layer and cloud servers in the top layer. A hub device, such as a laptop or smartphone, interacts with the edge devices and facilitates their communication with the cloud servers. It is typically closer to the edge devices than an edge/fog server, although it is less capable. On the other hand, it generally has a higher computational capacity than an edge device. In this environment, the edge and hub devices may be battery-operated.

In the edge/hub/cloud paradigm, an application typically comprises tasks with precedence relationships among them, forming a task flow graph. An increasing number of these applications employ a streamlined architecture, consisting of an edge device, a hub device, and a cloud server.
One notable category involves the monitoring, inspection, data collection, processing, and intelligent real-time decision-making in critical infrastructures and environments, utilizing a single unmanned aerial vehicle (UAV). 
In addition to the UAV's sensing abilities, its embedded computational and communication capabilities allow it to act as an edge device, typically communicating with a ground station (i.e., a hub device) that also has computational capabilities. The hub device (e.g., a laptop) can in turn communicate with a cloud server that can run the more computationally intensive tasks \cite{Cheng2021, icarusTheocharides}. %\cite{Wang2018, Cheng2021}.
Another class of applications that leverage this streamlined edge/hub/cloud architecture is in the tele-healthcare domain, where a wearable biomedical edge device (e.g., a smartwatch) can assist in the remote monitoring of patients. Such a device can continuously track a patient's vital signs for the detection of alarming indicators, while interacting with a hub device (e.g., a smartphone), which in turn can communicate with a cloud server for further data analysis \cite{Dao2023}.
In these use-cases, a single edge device, a single hub device, and a single cloud server suffice for handling the processing and communication requirements of the application. 


Even though the considered edge/hub/cloud architecture is streamlined, it involves many challenges. Moving from the cloud towards the edge of the network, the computational and communication capabilities, as well as the memory, storage, and energy capacities of the devices, become more limited. 
Hence, optimal task allocation, necessitated by the critical nature of the examined applications, can become particularly challenging. This challenge is intensified in operating environments with diverse communication characteristics, where such applications are often deployed. Moreover, these applications tend to incorporate computationally demanding algorithms, like machine learning inference and time-bound stream processing techniques, which further complicate the allocation of their tasks.
To our knowledge, the task allocation approaches proposed so far in the literature do not consider this particular architecture, nor a complete set of parameters that typically characterize such environments. 

Consequently, we propose a novel formulation, integrated in an application-driven design-time (i.e., offline) framework, to solve the task allocation problem in the examined edge/hub/cloud architecture optimally and efficiently. We consider several crucial parameters and constraints that are usually ignored in related research efforts, such as the computational and communication latency and energy required for the processing of the tasks, as well as the memory, storage, and energy limitations of the devices. The proposed approach supports two distinct optimization objectives, the minimization of either latency or energy, enabling design space exploration with respect to different devices and their connectivity.
The main contributions of this work, which builds upon our preliminary research in \cite{Kouloumpris2019}, are summarized as follows:
\begin{enumerate}
        \item We present a comprehensive binary integer linear programming (BILP) based formulation that encapsulates a complete set of parameters and constraints characterizing an edge/hub/cloud environment. It aims to allocate the application tasks across a streamlined architecture, while minimizing the overall latency or energy consumption.
    
        \item We propose a transformation of a task flow graph into an extended task flow graph, encompassing both the application and system models, as well as the employed energy model, to facilitate the problem formulation.  

        \item We evaluate our framework utilizing a real-world use-case scenario for the UAV-based aerial inspection of power transmission towers and lines.
        
        \item To further validate our approach and investigate its scalability to different applications, we use appropriate synthetic benchmarks with suitable parameters, which we developed and transformed for this purpose.
\end{enumerate}

The rest of the paper is arranged as follows. \cref{background} provides an overview of related research. \cref{framework} describes the proposed optimization framework. \cref{evaluation} presents the experimental setup and the evaluation results. Finally, \cref{conc} summarizes and concludes the paper.
