\section{Experiments}\label{sec:exp}

%需要添加每个数据集的baseline为何不同的解释。并且某些baseline的模型不是gpt-4
%选用模型的理由

% We design experiments along three dimensions, reasoning, decision-making, and code generation to validate the effectiveness of our approach. Specifically, we evaluate our method across three tasks: (1) Multi-hop question answering on HotpotQA \cite{yang2018hotpotqa} to assess reasoning ability;(2) Decision-making in the WebShop \cite{yao2022webshop} e-commerce platform to test planning and selection; and(3) Code generation on MBPP \cite{austin2021mbpp} to evaluate whether the model can infer a solution through reasoning and present it in a standardized, executable form.

We evaluate our approach across three representative tasks spanning reasoning, decision-making, and code generation. Specifically, we conduct experiments on: (1) multi-hop question answering using HotpotQA~\cite{yang2018hotpotqa} to assess reasoning capabilities; (2) goal-directed interaction on WebShop~\cite{yao2022webshop} to evaluate decision-making and planning; and (3) code generation on MBPP~\cite{austin2021mbpp} to test the model's ability to reason and produce executable solutions.


\subsection{Experiment Settings}

SYMPHONY supports flexible agent composition and is compatible with a range of language models under different computational constraints. We evaluate two deployment configurations: \textbf{SYMPHONY-S}, designed for consumer-grade hardware, and \textbf{SYMPHONY-L}, which leverages large-scale foundation models via cloud-based APIs.

\textbf{SYMPHONY-S} comprises open-source models that can be executed locally, including  Qwen2.5‑7B‑Instruct‑1M~\cite{yang2024qwen2-5-1M}, Mistral‑7B‑Instruct‑v0.3~\cite{jiang2023mistral7b}, and Llama‑3.1‑8B‑Instruct~\cite{grattafiori2024llama3herdmodels}. This configuration supports efficient inference with minimal deployment cost. In contrast, \textbf{SYMPHONY-L} comprises high-performance models: GPT‑4~\cite{achiam2023gpt4}, Qwen‑Max (2024‑09‑19)~\cite{yang2024qwen2-5}, and DeepSeek‑V3 (2025‑03‑24)~\cite{liu2024deepseek}, which operate through API endpoints within inference-as-a-service infrastructures.

All experiments are carried out under a unified protocol aligned with previous work~\cite{shinn2023reflexion, zhou2024language, gan2025master}. To ensure comparability, we apply consistent prompt formats and fixed hyperparameter settings across both configurations, including decoding temperature, planning depth, rollout budget, and number of demonstrations. To mitigate LLM stochasticity, each experiment is repeated 3 times on the same data set, and the mean accuracy is reported. The detailed hyper-parameter settings are described in Appendix~\ref{appendix:parameter_setting}. % In all the experiments, SYMPHONY-L significantly outperforms SOTA ($p<0.01$). 



% SYMPHONY is agnostic to the choice of underlying agents and supports flexible combinations tailored to different computational settings. We evaluate two deployment configurations: \textbf{SYMPHONY-S}, operates on consumer-grade hardware, and \textbf{SYMPHONY-L}, which relies on API-accessible large-scale models.

% \textbf{SYMPHONY-S} comprises open-source models suitable for local execution, including Qwen2.5‑7B‑Instruct-1M~\cite{yang2024qwen2-5-1M}, Mistral‑7B‑Instruct-v0.3~\cite{jiang2023mistral7b}, and Llama‑3.1‑8B‑Instruct~\cite{grattafiori2024llama3herdmodels}, enabling efficient on-device inference and low-cost experimentation.

% \textbf{SYMPHONY-L} includes proprietary foundation models accessible only via cloud-based APIs, such as GPT‑4~\cite{achiam2023gpt4}, Qwen‑Max (2024‑09‑19)~\cite{yang2024qwen2-5}, and DeepSeek‑V3 (2025‑03‑24)~\cite{liu2024deepseek}, deployed within high-throughput inference infrastructures.


% We follow the experimental protocols established in prior work~\cite{shinn2023reflexion, zhou2024language, gan2025master} across all three datasets. To mitigate variability from LLM stochasticity, each experiment is repeated three times on the same sample set, and mean accuracy is reported.

% To ensure comparability, we adopt consistent prompt formats and fixed hyperparameters across both settings, including decoding temperature, planning depth, and rollout budget.




% We adopt the same experimental setup as previous baselines \cite{} the ReAct, Reflexion, and LATS baselines: for both HotPotQA and WebShop, we randomly sample 100 questions using a fixed random seed to ensure consistency across methods. To reduce variability from LLM nondeterminism, each experiment is repeated three times on the identical sample, and we report the mean accuracy in our results table.  
% We adopt the same experimental setup as previous baselines  \cite{shinn2023reflexion,zhou2024language,gan2025master} for all three datasets. To reduce variability from LLM nondeterminism, each experiment is repeated three times on the identical sample, and we report the mean accuracy in our results table.  


% 没有解释清楚，为什么我们选这几个模型
% 尝试从多样性的角度defend我们的选择
% We categorize our models into two groups: Large-Scale Language Models (LLMs-L) and Small-Scale Language Models (LLMs-S). As part of the Large-Scale Language Models (LLMs-L) group, we include GPT‑4 as a primary baseline and additionally incorporate Qwen‑Max (2025‑01‑25) and DeepSeek V3 (2025‑03‑24), both of which offer comparable capabilities at a lower API cost than GPT‑4. For LLMs-S, smaller LLMs, we benchmark Qwen2.5‑7B‑Instruct‑1M, Mistral‑7B‑Instruct‑v0.3, and Llama‑3.1‑8B‑Instruct, each of which runs efficiently on modest GPU hardware.


% SYMPHONY is agnostic to the specific choice of agents and can accommodate different  agent combinations tailored to different application contexts. We evaluate SYMPHONY under two distinct deployment configurations, differentiated by their compatibility with computational infrastructure: SYMPHONY-S, which supports deployment on consumer-grade hardware, and SYMPHONY-L, which relies on large-scale models accessible exclusively via remote APIs.

% \textbf{SYMPHONY-S} comprises language models that can be executed locally. Specifically, we include Qwen2.5‑7B‑Instruct-1M \cite{yang2024qwen2-5-1M}, Mistral‑7B‑Instruct-v0.3 \cite{jiang2023mistral7b}, and Llama‑3.1‑8B‑Instruct \cite{grattafiori2024llama3herdmodels}, which collectively represent high-performance open-source models suitable for on-device inference and iterative experimentation with minimal deployment cost.

% In contrast, \textbf{SYMPHONY-L} utilizes proprietary foundation models that are not publicly hostable and can only be accessed via cloud-based API endpoints. This configuration includes GPT‑4 \cite{achiam2023gpt4}, Qwen‑Max (2024‑09‑19) \cite{yang2024qwen2-5}, and DeepSeek‑V3 (2025‑03‑24) \cite{liu2024deepseek}, all of which operate within high-throughput inference-as-a-service infrastructures.

% % This bifurcated setup enables systematic evaluation of SYMPHONY under both resource-constrained and compute-rich conditions, demonstrating its applicability across a wide spectrum of real-world deployment scenarios. Importantly, the framework remains agnostic to the specific choice of agents and can accommodate alternative model combinations tailored to different application contexts.

% For all experiments, we ensure consistency by applying identical prompt formats and maintaining fixed hyperparameter settings across both configurations, including decoding temperature, planning depth, and rollout budget.




% We evaluate the proposed method on different sizes of models. More specifically, we construct \textbf{SYMPHONY-S} and \textbf{SYMPHONY-L} that runs on different types of hardware:
% \begin{itemize}
%     \item SYMPHONY-S: Agents that can be hosted by consumer hardware. We include Qwen2.5‑7B‑Instruct‑1M \cite{yang2024qwen2-5-1M} (sliding-window attention), Mistral‑7B‑Instruct‑v0.3 \cite{jiang2023mistral7b} (grouped-query attention), and Llama‑3.1‑8B‑Instruct 
% \cite{grattafiori2024llama3herdmodels} (RMSNorm and RoPE encoding), each representing different design decisions in attention mechanisms, normalization, and positional representation. 
%     \item SYMPHONY-L: Agents that are usually run on large-scale clusters, providing service via API. We  include GPT‑4 \cite{achiam2023gpt4}(Mixture-of-Experts architecture), Qwen‑Max (2024-09-19) \cite{yang2024qwen2-5}(dense Transformer with dynamic gradient clipping), and DeepSeek-V3 \cite{liu2024deepseek}(2025‑03‑24) (sparse attention with memory-efficient design)
% \end{itemize}

% The above selections cover key distinctions in routing, density, and memory optimization, enhancing agent pool diversity, reduces architectural bias, and supports fair evaluation of our method’s effectiveness across heterogeneous model backbones. It should be noted that HARMNOY can be applied to other choices of agent pools as well, including other LLMs depending on the task requirements and developer preferences. 







% For each experimental setting, we use identical prompts and keep all hyperparameters—such as temperature, number of iterations, and the number of expansions, consistent across both proprietary large models and open-source smaller models. 
% This design choice ensures that the observed performance differences are not merely attributable to the underlying capabilities of the models themselves, but rather reflect the true effectiveness of our proposed method.



\begin{table*}[ht]
\centering
\begin{minipage}[t]{0.3\linewidth}
\centering
\caption{HotpotQA. }
\label{tab:hotpotqa}
\setlength{\tabcolsep}{3pt} % 缩小列间距
\resizebox{0.98\linewidth}{!}{ % 适当放大显示比例
\footnotesize
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Method} & \textbf{Exact Match $\uparrow$} \\
\midrule
\makecell{CoT  \cite{wei2022chaincot}}            & 0.34 \\
\makecell{CoT-SC  \cite{cot-sc2023}}          & 0.38 \\
\makecell{ReAct  \cite{yao2023react}}          & 0.39 \\
\makecell{Reflexion  \cite{shinn2023reflexion}}  & 0.51 \\
\makecell{ToT  \cite{yao2023tree}}            & 0.55 \\
\makecell{RAP  \cite{hao2023reasoning}}         & 0.60 \\
\makecell{LATS  \cite{zhou2024language}}        & 0.71 \\
\makecell{Beam Retrieval  \cite{zhang2023end}}  & 0.73 \\
\makecell{MASTER  \cite{gan2025master}}         & 0.76 \\
\midrule
\textbf{SYMPHONY-S} & \textbf{0.59} \\
\textbf{SYMPHONY-L} & \textbf{0.79} \\
\bottomrule
\end{tabular}
}
\end{minipage}
\hfill
\begin{minipage}[t]{0.34\linewidth}
\centering
\caption{WebShop.}
\label{tab:webshop_results}
\setlength{\tabcolsep}{3pt} % 缩小列间距
\resizebox{0.98\linewidth}{!}{ % 适当放大显示比例
\footnotesize
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Score $\uparrow$} & \textbf{SR $\uparrow$} \\
\midrule
\makecell{IL  \cite{yao2022webshop}}          & 0.60 & 0.29 \\
\makecell{IL+RL  \cite{yao2022webshop}}       & 0.62 & 0.29 \\
\makecell{ReAct  \cite{yao2023react}}      & 0.54 & 0.32 \\
\makecell{Reflexion  \cite{shinn2023reflexion}} & 0.64 & 0.35 \\
\makecell{Fine-tuning  \cite{furuta2024multimodal}} & 0.68 & 0.45 \\
\makecell{AgentKit  \cite{wu2024agentkit}}     & 0.70 & --  \\
\makecell{LATS  \cite{zhou2024language}}      & 0.76 & 0.38 \\
\makecell{MASTER  \cite{gan2025master}}          & 0.80 & -- \\
\makecell{Human Expert  \cite{yao2022webshop}}    & 0.82 & 0.60 \\
\midrule
\textbf{SYMPHONY-S}           & \textbf{0.82} & \textbf{0.56} \\
\textbf{SYMPHONY-L}           & \textbf{0.88} & \textbf{0.72} \\
\bottomrule
\end{tabular}
}


\end{minipage}
\hfill
\begin{minipage}[t]{0.34\linewidth}
\centering
\caption{MBPP. }
\label{tab:mbpp_results}
\setlength{\tabcolsep}{3pt} % 缩小列间距
\resizebox{0.98\linewidth}{!}{ % 适当放大显示比例
\footnotesize
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{\makecell{Pass@1 \\ (Python) $\uparrow$}} & \textbf{\makecell{Pass@1 \\ (Rust) $\uparrow$}}  \\
\midrule
\makecell{GPT-4  \cite{shinn2023reflexion}} & 0.800 & 0.710 \\
\makecell{GPT-4(CoT)  \cite{gan2025master}}  & 0.683 & -- \\
\makecell{GPT-4(ReAct)  \cite{yao2023react}}   & 0.710 & -- \\
\makecell{Reflexion  \cite{shinn2023reflexion}}  & 0.771 & 0.754 \\
\makecell{RAP  \cite{hao2023reasoning}}     & 0.714 & -- \\
\makecell{LATS  \cite{zhou2024language}}        & 0.811 & -- \\
\makecell{MetaGPT  \cite{hong2023metagpt}}     & 0.877 & -- \\
\makecell{AgentVerse  \cite{chen2023agentverse}} & 0.890 & -- \\
\makecell{MASTER  \cite{gan2025master}}          & 0.910 & -- \\
\makecell{AgentCoder  \cite{huang2023agentcoder}} & 0.918 & -- \\
\midrule
\textbf{SYMPHONY-S}           & \textbf{0.927} & \textbf{0.946} \\
\textbf{SYMPHONY-L}           & \textbf{0.965} & \textbf{0.974} \\
\bottomrule
\end{tabular}
}
\end{minipage}
\parbox{\linewidth}{
\footnotesize 
Note: Metrics are normalized to the [0,1] range; A dash (–) marks those not reported in the publication.
% All evaluation metrics are normalized to the [0,1] range. A dash (–) indicates that the corresponding method did not report that metric. SR for AgentKit and MASTER on WebShop are unavailable due to missing implementation details and metric reporting in their original publications.
% Additionally, many baselines on MBPP report results only on Python, without verifying language-agnostic effectiveness.
}
\end{table*}





\subsection{Reasoning:HotpotQA}
\textbf{Setup.} HotpotQA~\cite{yang2018hotpotqa} is a large-scale benchmark for multi-hop question answering, constructed from Wikipedia and containing approximately 113,000 question–answer pairs. In line with prior work~\cite{yao2023react,shinn2023reflexion,zhou2024language,gan2025master}, we employ an oracle feedback setting, where the environment immediately indicates whether a selected answer is correct. This setup is designed to isolate and evaluate the agent’s decision-making capabilities during interaction, rather than its ability to generate final answers.
Evaluation on this dataset is based on the exact match (EM) metric.

We compare SYMPHONY against representative baselines from four categories: (1) \textit{Linear reasoning} methods such as CoT~\cite{wei2022chaincot} and CoT-SC~\cite{cot-sc2023}; (2) \textit{Feedback-driven} approaches including ReAct~\cite{yao2023react} and Reflexion~\cite{shinn2023reflexion}; (3) \textit{Structured reasoning} methods such as ToT~\cite{yao2023tree}, RAP~\cite{hao2023reasoning}, LATS~\cite{zhou2024language}, and Beam Retrieval~\cite{zhang2023end}; and (4) the \textit{multi-agent framework} MASTER~\cite{gan2025master}, which builds multi-agent from the same LLM. Baseline results are taken from~\citet{gan2025master}, where GPT-4 is used uniformly across all methods.

\textbf{Results.} SYMPHONY demonstrates strong performance across all baseline categories. The lightweight \textbf{SYMPHONY-S} outperforms both linear reasoning and feedback-driven baselines, and performs comparably to structured search methods like RAP. The stronger \textbf{SYMPHONY-L} surpasses all structured baselines, including MASTER, achieving state-of-the-art performance on HotpotQA. These improvements reflect SYMPHONY’s ability to combine model heterogeneity with coordinated compositional reasoning. 



% HotPotQA \cite{yang2018hotpotqa} is a large‑scale, Wikipedia‑based multi‑hop QA benchmark containing roughly 113,000 question–answer pairs. % Each question requires reasoning over multiple distinct supporting articles, so an agent must (1) iteratively retrieve candidate passages, (2) sift through distractor documents that introduce noise, and (3) integrate evidence dispersed across non‑contiguous paragraphs to arrive at the correct answer. This noise‑rich setup rigorously tests an agent’s relevance‑scoring and evidence‑aggregation capabilities under multi‑step logical inference. 
% Performance on Hotpot QA is measured with standard exact match. 
% We employ  an oracle feedback mechanism that aligns with established protocols  \cite{yao2023react,shinn2023reflexion,zhou2024language,gan2025master}: the environment immediately verifies answer correctness, ensuring that our evaluation focuses squarely on reasoning proficiency rather than on feedback ambiguity. 
% Performance is measured using standard Exact Match , providing a direct comparison to prior baselines.

% 太长，直接说我们的n和k
% We evaluate each method using a subset of 100 questions along with 3 few-shot examples, applied to both a multi-agent framework composed of proprietary large language models and another composed of open-source small models. While prior work such as LATS (Zhou et al., 2024) reports optimal performance using \(n = 5\) sampled nodes and \(k = 50\) trajectories, our empirical findings suggest that excessive node expansion and repeated simulations yield diminishing returns due to the performance ceiling imposed by model capabilities on this task. As a result, we adopt a more efficient configuration in our experiments, setting \(n = 4\) and \(k = 15\) to balance computational cost with exploration depth.
% 此段重新整理：
% 1、描述实验结果：小模型比XXX好，大模型比XXX好
% 2、分析原因，为什么会有这种结果
% 3、这意味着什么，或者，反常实验结果分析


%Cot、Cot-SC的结果是从LATS中抄过来的，他那标注是GPT-3.5
%ToT、RAP是LATS作者复现的，用的GPT-3.5
%其中React、Reflexion、LATS、BeamRetrieval可以甩锅给MASTER，他说他们拿GPT-4复现了。BeamRetrieval其实是HotPotQA官方榜单的第一名，人家是完整数据集的。

%中文版本：
% 选取的基线方法覆盖了多样化的技术路径与发展阶段。其中，CoT和CoT-SC代表思维链技术的基础版与多路径优化版，验证线性推理的固有缺陷；ReAct和Reflexion作为交互式推理框架，对比动作规划与自我反思机制的效能边界；Beam Retrieval代表传统多文档检索增强方法，凸显动态知识验证的必要性；RAP、LATS、MASTER同样基于MCTS框架，对比MCTS框架下的技术瓶颈。

%选取的基线方法覆盖了多样化的技术路径，，其中基于LLM的规划和推理的有：CoT、CoT-SC（引导推理、动态反馈、树结构）及多智能体协作分类，选取 CoT（引导）、ReAct（反馈）、ToT（树结构）、MASTER（多智能体）等基线，覆盖多元推理机制与策略，通过对比验证本文方法在多样性和鲁棒性上的提升。 

%在SYMPHONY-S设置下，该框架不仅远远超越了Refelxion、ToT等方法，还逼近了RAP。而在SYMPHONY-L设置下，取得了SOTA，尽管指标上仅仅提升0.3，但这是由于Exact Matchd的严格所在。证明SYMPHONY通过协调规划，动态交互和反思性评估进行结构化、组合推理的能力，并且在MCTS框架内实现革新。


% 重新写:基于推理流程演进，HotpotQA 的对比方法分为四类：基础线性推理（CoT、CoT-SC）：单路径＋自洽性，刻画无多分支时的性能下限；动态反馈修正（ReAct、Reflexion）：动作-观察和自我反思，验证结构化搜索在收敛速度与回溯能力上的优势；结构化搜索（ToT、RAP、LATS、Beam Retrieval）：树状、奖励和束搜索，衡量异构模型在多路径和全局规划上的提升；多智能体协作（MASTER）：同质 agent＋MCTS，对比轻量级异构协作与记忆共享的效率与多样性增益。这些基线的实验结果来自{gan2025master}。，他们使用GPT-4作为骨干模型复制了它们。

% We select a representative and diverse set of baselines covering major reasoning paradigms. CoT \cite{wei2022chaincot} and CoT-SC \cite{cot-sc2023} reflect standard and multi-path chain-of-thought prompting, probing the limitations of linear reasoning. ReAct \cite{yao2023react} and Reflexion \cite{yao2023react} exemplify interactive frameworks, contrasting action planning with self-reflection. Beam Retrieval \cite{zhang2023end} represents traditional retrieval-augmented methods, highlighting the need for dynamic knowledge verification. RAP \cite{hao2023reasoning} , LATS \cite{zhou2024language} , and MASTER \cite{gan2025master} , all built on MCTS, facilitate a focused comparison within the same decision framework. The experimental results for these baselines are sourced from \cite{gan2025master}., who reproduced them using GPT-4 as the backbone model.

% We selected representative baselines from the following categories: (1) CoT \cite{wei2022chaincot}, CoT-SC \cite{cot-sc2023}, which uses linear reasoning paths; (2) ReAct \cite{yao2023react}, Reflexion \cite{shinn2023reflexion}, which frames LLM in the RL feedback loop; (3) ToT \cite{yao2023tree}, RAP \cite{hao2023reasoning}, LATS \cite{zhou2024language}, Beam Retrieval\cite{zhang2023end}, which uses tree-based structure or other multi-branch reasoning paths; (4) MASTER \cite{gan2025master}, which constructs multi-agent framework based on the same LLM. Baseline results are reported by \citet{gan2025master}, which uses GPT-4 uniformly as backbone. 

% Following the evolution of reasoning paradigms, we organize the selected HotpotQA baselines into four representative categories. Linear reasoning (CoT \cite{wei2022chaincot}, CoT-SC \cite{cot-sc2023}) captures single-path inference with or without self-consistency, serving as a lower bound in the absence of multi-branch exploration. Dynamic feedback methods (ReAct \cite{yao2023react}, Reflexion \cite{shinn2023reflexion}) incorporate action-observation cycles and self-reflection to enable iterative correction, highlighting the value of adaptive reasoning. Structured search approaches (ToT \cite{yao2023tree}, RAP \cite{hao2023reasoning}, LATS \cite{zhou2024language}, Beam Retrieval\cite{zhang2023end}) leverage tree-based structures, reward signals, or beam strategies to improve global planning and reasoning diversity. Finally, multi-agent collaboration (MASTER \cite{gan2025master}) combines homogeneous agents with MCTS, providing a direct comparison point for evaluating the efficiency and diversity benefits of our lightweight, heterogeneous collaboration framework with memory sharing. Baseline results are taken from \citet{gan2025master}, where GPT-4 was used as the backbone model for reproduction.


% 结果分析
%SYMPHONY在HotpotQA的严格评测中刷新SOTA。SYMPHONY-S设置下远超Reflexion、ToT等方法，逼近RAP；SYMPHONY-L设置下取得SOTA，为这项多跳问答任务树立了新的技术水平

% \textbf{Result.}  SYMPHONY demonstrates strong performance across both deployment configurations. Under the SYMPHONY-S setting, the framework not only surpasses GPT-4 and prominent reasoning baselines such as Reflexion \cite{shinn2023reflexion} and Tree-of-Thought \cite{yao2023tree}, but also closely approaches the performance of RAP \cite{hao2023reasoning}, a leading multi-step reasoning system. With SYMPHONY-L, the framework achieves the highest overall score, establishing a new state-of-the-art on this multi-hop question answering task.


% \textbf{Result.}  SYMPHONY sets a new state of the art on the rigorous HotpotQA benchmark. Under the SYMPHONY-S setting, it significantly outperforms methods such as Reflexion and ToT, and approaches the performance of RAP. Under the SYMPHONY-L setting, it achieves a new SOTA, establishing a new performance bar for multi-hop question answering.


% These results highlight SYMPHONY’s ability to perform structured, compositional reasoning—a core challenge in HotpotQA—through coordinated planning and reflective evaluation, regardless of the underlying model scale. Moreover, SYMPHONY attains this performance with significantly lower resource usage: even with only 4 expansions per step, 10 sampled trajectories, and a minimal 3-shot prompt, it consistently outperforms baselines such as LATS, which rely on heavier sampling strategies.

% \textbf{Result.} SYMPHONY’s results map cleanly onto our four baseline categories. Even the more lightweight SYMPHONY-S outstrips linear reasoning and dynamic feedback, and approaches key structured search methods like RAP. SYMPHONY-L then surpasses all structured-search methods and eclipses the multi-agent MASTER, establishing a new HotpotQA record.

% These gains reflect SYMPHONY’s coordinated compositional reasoning and model heterogeneity: it explores and evaluates multiple paths more effectively than linear, feedback-only, or single-model search, yet uses minimal resources—4 expansions per step, 10 trajectories, and a 3-shot prompt—outperforming heavy-sampling approaches like LATS.



% Experimental results on the small‐scale model configuration show that our method not only outperforms GPT-4 and GPT-4–based reasoning approaches such as Reflexion \cite{shinn2023reflexion} and Tree-of-Thought \cite{yao2023tree}, but also approaches the performance of RAP \cite{hao2023reasoning}. In the large-scale configuration, it achieves the highest overall score. These findings demonstrate that our framework enables compact models—despite having several orders of magnitude fewer parameters—to match or exceed the capabilities of much larger models, and to outperform existing techniques when applied to both small and large models. Moreover, our approach is model-agnostic: when instantiated with large models, it continues to deliver state-of-the-art results. The relatively modest margin over prior SOTA in this setting reflects the benchmark’s exact-match evaluation, which deems any deviation from the reference—even semantically equivalent variants—as incorrect. Indeed, qualitative analysis reveals that many of our “failures” produce semantically correct solutions that differ only in surface form.

% Moreover, our method uses substantially fewer resources compared with other baselines. Despite using substantially lighter settings than LATS—expanding only n = 4 nodes per step, sampling k = 10 trajectories, and employing a 3-example few-shot prompt—our method achieves the highest performance in both the large-model and small-model pools.

% 1、给我们的方法取个名字，一个好的名字满足两点：
% （1） 恰好是首字母缩写
% （2）缩写本身构成新单词，通常这个新单词要朗朗上口，最好有好的寓意
% 2、看能否只占一半的页面

% \begin{table}[ht]
%     \centering
%     \caption{Performance Comparison on HotpotQA. }
%     \label{tab:hotpotqa}  % 修正唯一label
%     \begin{tabular}{@{}lc@{}}
%         \toprule
%         \textbf{Method} & \textbf{Exact Match $\uparrow$} \\
%         \midrule
%         CoT \cite{wei2022chaincot}            & 0.34 \\
%         CoT-SC \cite{cot-sc2023}          & 0.38 \\
%         ReAct \cite{yao2023react}          & 0.39 \\
%         Reflexion \cite{shinn2023reflexion}  & 0.51 \\
%         ToT \cite{yao2023tree}            & 0.55 \\
%         RAP \cite{hao2023reasoning}         & 0.60 \\
%         LATS \cite{zhou2024language}        & 0.71 \\
%         Beam Retrieval \cite{zhang2023end}  & 0.73 \\
%         MASTER \cite{gan2025master}         & 0.76 \\
%         \midrule
%         \textbf{SYMPHONY-S} & \textbf{0.59} \\
%         \textbf{SYMPHONY-L} & \textbf{0.79} \\
%         \bottomrule
%     \end{tabular}
%     % \vspace{0.5em}
%     \parbox{\linewidth}{
%     \footnotesize
    
%     }
% \end{table}
%% 就上面这个实验
%Cot、Cot-SC的结果是从LATS中抄过来的，他那标注是GPT-3.5
%ToT、RAP是LATS作者复现的，用的GPT-3.5
%其中React、Reflexion、LATS、BeamRetrieval可以甩锅给MASTER，他说他们拿GPT-4复现了。BeamRetrieval其实是HotPotQA官方榜单的第一名，人家是完整数据集的。




\subsection{Sequential Decision Making:WebShop}

\textbf{Setup.} WebShop~\cite{yao2022webshop} is a simulated e-commerce platform featuring over 1.18 million products and 12,000 natural language queries. Agents must navigate the website using browser-like operations (e.g., search, click, select) to identify items that satisfy user constraints. Performance is measured by average score, which reflects partial attribute satisfaction, and success rate (SR), which reflects full constraint satisfaction.

We compare SYMPHONY against a comprehensive set of baselines reflecting five categories: (1) \textit{Task-native methods} including imitation learning (IL), IL+RL, and Human Expert~\cite{yao2022webshop}; (2) \textit{Supervised models} such as a fine-tuned LLM~\cite{furuta2024multimodal}; (3) \textit{Feedback-driven reasoning}, including  ReAct, Reflexion; (4) \textit{Structured search} methods including LATS and AgentKit; and (5) the \textit{multi-agent framework}: MASTER~\cite{gan2025master}. All baselines were reproduced under consistent settings by~\citet{gan2025master} using GPT-4, ensuring fair comparison.

\textbf{Results.} SYMPHONY outperforms all baseline categories. Compared to task-native and supervised approaches, it achieves higher task completion while requiring no domain-specific training. Against feedback-driven and structured search methods, it exhibits stronger planning efficiency and generalization. Finally, SYMPHONY-L surpasses the multi-agent MASTER, establishing a new performance benchmark. These results underscore SYMPHONY’s adaptability across different task-specific execution environments. %and its ability to unify general reasoning with environment-specific execution under a lightweight, modular design.


% \begin{table}[ht]
% \centering
% \caption{Performance Comparison on WebShop.   % With results grouped into three categories: prompting, RL-based training, and human performance. Our multi-agent framework achieves a comprehensive victory, delivering the highest score and SR across all categories and even surpassing human expert benchmarks.
% }
% \label{tab:webshop_results} % 修正label命名一致性

% \begin{tabular}{lcc}
% \toprule
% \textbf{Method} & \textbf{Score $\uparrow$} & \textbf{Success Rate (SR) $\uparrow$} \\
% \midrule
% IL \cite{yao2022webshop}          & 0.60 & 0.29 \\
% IL+RL \cite{yao2022webshop}       & 0.62 & 0.29 \\
% ReAct \cite{yao2023react}      & 0.54 & 0.32 \\ % 修正53.8→0.54保持数值范围一致
% Reflexion \cite{shinn2023reflexion}) & 0.64 & 0.35 \\
% Fine-tuning \cite{furuta2024multimodal} & 0.68 & 0.45 \\
% AgentKit \cite{wu2024agentkit}     & 0.70 & -- \\  % 使用en-dash表示缺失值
% LATS \cite{zhou2024language}      & 0.76 & 0.38 \\
% MASTER \cite{gan2025master}          & 0.80 & -- \\
% Human Expert \cite{yao2022webshop}    & 0.82 & 0.60 \\
% \midrule
% \textbf{SYMPHONY-S}           & \textbf{0.82} & \textbf{0.56} \\
% \textbf{SYMPHONY-L}           & \textbf{0.88} & \textbf{0.72} \\
% \bottomrule
% \end{tabular}

% % \vspace{0.4em} % 微调表格与说明间距
% \noindent
% \parbox{0.9\linewidth}{ % 添加parbox包装说明文字
% \footnotesize Note: All scores normalized to [0,1] range. SR for AgentKit and MASTER are unavailable due to missing code and metric reporting in their original publications.
% }


% \end{table}

% 其中IL、IL+RL、Human Expert是Webshop这个实验数据集的本身论文中的实验给的
% 这个Fine-tuning是跟着LATS中表格抄过来的
% AgentKit这个MASTER的gan等人也把他列入表格，不过就这个，他说没复现。不过我翻看AgentKit的原论文，就是GPT-4
% 其中Reflexion、LATS、 MASTER都来自于MASTER的gan等人复现的结果，都是GPT-4 




% 任务介绍不要写不相关的内容，尽可能精简
% For a complex decision-making environment with practical applications, we consider WebShop \cite{yao2022webshop}, a simulated e-commerce platform comprising 1.18 million real-world products and 12,000 user-issued natural language instructions. 
% In this environment, an agent must interact with the website via browser-like operations such as search and click to locate and select a product that aligns with the user’s intent. Performance is measured using average score (partial attribute satisfaction) and success rate (full constraint satisfaction). 
% WebShop presents several core challenges: (1) dynamic web interaction, requiring the agent to perform multi-step navigation, handle page transitions, and respond to real-time feedback; (2) complex semantic understanding, where interpreting implicit user preferences—such as finding “a waterproof smartwatch with high cost-effectiveness”—is critical for task success. These factors make WebShop a rigorous benchmark for evaluating agents’ decision-making, planning, and semantic grounding capabilities in open-ended, real-world contexts.

% 要么删掉，要么强调和别人的evaluation protocol一样
% 如果需要强调结果是你复现的，那么应该写在这里
% 我们的参数设置也是写在这里
% As in previous experiments, we apply substantially smaller parameter settings than LATS \cite{zhou2024language} in both the large-model and small-model pools, using only n = 4 node expansions and k = 10 trajectories, with a 1-example one-shot prompt. Furthermore, since AgentKit \cite{wu2024agentkit} and MASTER \cite{gan2025master} report only average score metrics without providing success rate evaluations, We attempted to re implement MASTER - previously the most advanced method - but as they have not yet released the code, we were unable to reproduce their effect.


% 其中IL、IL+RL、Human Expert是Webshop这个实验数据集的本身论文中的实验给的
% 这个Fine-tuning是跟着LATS中表格抄过来的
% AgentKit这个MASTER的gan等人也把他列入表格，不过就这个，他说没复现。不过我翻看AgentKit的原论文，就是GPT-4
% 其中Reflexion、LATS、 MASTER都来自于MASTER的gan等人复现的结果，都是GPT-4 

%中文版：实验所选基线覆盖了从基础模仿学习到前沿推理框架的完整技术演进路径。其中IL、IL+RL和Human Expert为任务原生方案，体现纯行为克隆与混合强化学习的初始模式，以及通过人类专家界定任务的理论上限；Fine-tuning借助领域自适应训练提升模型针对性，探索中等模型结合领域适配的增益极限；AgentKit以GPT-4作为推理引擎，通过结构化动态图推理优化复杂任务的规划执行。其余基线在前文已有提及，其共有的局限性在 WebShop 场景分析中同样存在，且这些基线均经GAN等人利用GPT-4复现验证，确保对比实验的可靠性。

%尽管仅依赖于轻量级、可本地部署的模型，SYMPHONY-S在这两个指标上都实现了与MASTER相当的性能，并且在Score上已经与人类专家一致，这表明即使在严格的资源约束下，我们的框架也能实现高效的目标条件规划，并且已经达到了任务的理论上限。SYMPHONY-L进一步提高了性能，在该基准上实现了最高的报告成功率，超过了所有先前的方法。

% The selected baselines cover the full progression from imitation learning to advanced reasoning. IL, IL+RL, and Human Expert represent task-native approaches, reflecting behavior cloning, hybrid RL, and expert-defined upper bounds. Fine-tuning explores domain-adaptive training for mid-sized models, while AgentKit employs GPT-4 with dynamic graph reasoning for complex planning. The remaining baselines, discussed earlier, share similar limitations in WebShop and have been reproduced by \cite{gan2025master}. using GPT-4, ensuring result reliability.

% For WebShop, we include additional baselines beyond those used in HotpotQA to capture the task’s unique demands. IL, IL+RL, and Human Expert \cite{yao2022webshop} represent task-native approaches, reflecting behavior cloning, hybrid RL, and expert-defined upper bounds. We also include a fine-tuned LM \cite{furuta2024multimodal} trained directly on task data to reflect end-to-end supervised strategies. AgentKit employs GPT-4 with dynamic graph reasoning for complex planning. The remaining methods follow prior categories, these baselines were reproduced by \citet{gan2025master} using GPT-4, ensuring fair comparison. This selection allows us to assess SYMPHONY's generality and effectiveness across both reasoning-driven and environment-specialized baselines.


% \textbf{Result.} SYMPHONY-S achieves performance on par with MASTER in both metrics, despite relying solely on lightweight, locally deployable models. This indicates that our framework enables efficient goal-conditioned planning even under strict resource constraints. SYMPHONY-L further advances performance, achieving the highest reported success rate on this benchmark—surpassing all prior methods, including those based on reinforcement learning and acting-style prompting, as well as reported human-level baselines.

% \textbf{Result.} Despite using only lightweight, locally deployable models, SYMPHONY-S achieves performance comparable to MASTER, and reaches human-level accuracy in Score, demonstrating that our framework enables efficient goal-conditioned planning under strict resource constraints—approaching the task’s theoretical upper bound. SYMPHONY-L further advances performance, achieving the highest reported success rate and surpassing all prior methods.

% Notably, SYMPHONY achieves these results with significantly lower resource demands. Compared to approaches such as LATS \cite{zhou2024language}, we use only 4 node expansions per step, 10 sampled rollouts, and a minimal one-shot prompt, demonstrating strong efficiency-performance tradeoffs across model scales.

% \textbf{Result.} On WebShop, SYMPHONY consistently outperforms all task-native, supervised, feedback-driven, search-based, and multi-agent baselines. The lightweight SYMPHONY-S achieves a Score of 0.82—matching human expert performance—and a Success Rate of 0.56, exceeding both fine-tuned and MCTS-based methods. Scaling to SYMPHONY-L delivers a new state of the art, surpassing MASTER and LATS. These results confirm SYMPHONY’s broad applicability and its superior efficiency–performance trade-off across diverse reasoning and policy paradigms.


% We compare against acting-based prompting methods and RL-based approaches. Performance is gauged using two metrics: an average score, reflecting the percentage of user-specified attributes met by the selected product, and a success rate, indicating the frequency with which the chosen product fulfills all given conditions. Under our framework, small-scale models (SYMPHONY-S) achieve an average score comparable to that of MASTER, with a nearly identical success rate. This indicates that our method, even when using only small models, performs on par with the previous state-of-the-art. When scaled to large models (SYMPHONY-L), our approach achieves the highest success rate on this task, surpassing not only all prior methods but also reported human-level performance. These results demonstrate that our framework enables highly effective sequential decision-making in realistic settings, with performance that is robust to model size and minimally constrained by computational resources.

% Again, our method uses substantially fewer resources. we apply substantially smaller parameter settings than LATS \cite{zhou2024language} in both the large-model and small-model pools, using only n = 4 node expansions and k = 10 trajectories, with a 1-example one-shot prompt.



\begin{figure}[ht]
		\centering
\includegraphics[width=0.95\textwidth]{fig/diversity_performance_plot.pdf}
		\caption{Branch Diversity vs. Task Performance. Bars and left y-axis shows the branch diversity, while lineplot and right y-axis shows the task performance.
        }
        \label{fig:diversity}
\end{figure}


\subsection{Programming:MBPP}

\textbf{Setup.} The Mostly Basic Programming Problems (MBPP) ~\cite{austin2021mbpp} involves multi-step code generation tasks that require condition decomposition, procedural planning, and implementation. Each task provides a description in natural language and a test suite. Success is defined by passing all tests. We follow~\cite{shinn2023reflexion} to evaluate both Python and Rust versions of the datasets using the MultiPL-E compiler suite~\cite{cassano2022multipl}.

Baselines span three major categories: (1) \textit{Single-agent methods}, including GPT-4 and Reflexion~\cite{shinn2023reflexion}, representing the performance ceiling of basic prompting and reactive reasoning; (2) \textit{Multi-agent frameworks}, such as MetaGPT~\cite{hong2023metagpt}, AgentVerse~\cite{chen2023agentverse}, and AgentCoder~\cite{huang2023agentcoder}, which explore different collaboration strategies; and (3) \textit{Search-based approaches}, including RAP~\cite{hao2023reasoning}, LATS~\cite{zhou2024language}, and MASTER~\cite{gan2025master}, which emphasize structured optimization. All baseline results are drawn from or reproduced by~\citet{gan2025master} under consistent backbone and data settings.

\textbf{Results.} SYMPHONY achieves strong performance across all baseline categories. Compared to single-agent methods, it demonstrates superior reasoning depth and planning efficiency. Against multi-agent frameworks, SYMPHONY provides more effective solution search via the introduction of heterogeneous agent pool.  Compared to search-based approaches, it attains state-of-the-art performance in cross-language settings, including Rust, a programming language usually ignored by previous works. These results confirm SYMPHONY’s robustness, generality, and computational efficiency in code generation.



% The Mostly Basic Programming Problems (MBPP) \cite{austin2021mbpp} benchmark features multi-step code generation tasks involving condition decomposition, planning, and implementation, aligning well with the causal reasoning structure of our method. Each task includes a natural language description and test cases; success is defined as generating a complete program that passes all tests. Performance are evaluated with Pass@1 accuracy.  To evaluate cross-language generalization, we follow \cite{shinn2023reflexion} and translate Python tasks into Rust using the MultiPL-E compiler suite \cite{cassano2022multipl}, conducting experiments in both languages. In terms of parameter settings, we follow the configuration used in LATS, employing \(k = 8\) iterations and sampling \(n = 5\) candidate solutions at each expansion step. All prompts are applied in a zero-shot manner.

% The Mostly Basic Programming Problems (MBPP) \cite{austin2021mbpp} benchmark consists of multi-step code generation tasks that require condition decomposition, logical planning, and implementation, aligning closely with the causal reasoning structure targeted by our method. % This makes MBPP an ideal experimental setting for validating reasoning-based decision-making frameworks. In addition, the diversity of problem types in MBPP allows for evaluating the agent’s generalization ability across different scenarios. For these reasons, we select MBPP as the primary benchmark to evaluate our approach.
% Each MBPP task includes a problem description and a set of test cases for validation. The evaluation metric used on MBPP is \comment{buchong}. The system generates a complete program, and a task is considered successfully solved only if the generated code passes all test cases. % The results, including passed and failed tests as well as compiler outputs, are added to the agent’s context as observations, and evaluate it on the real test suite for the pass@1 accuracy evaluation.
% To validate whether the proposed method can be generalized to different programming languages, we follow previous work \cite{shinn2023reflexion} to translate the original Python code into Rust using MultiPL-E compiler suite \cite{cassano2022multipl}, and experiments on both programming languages.  


% In our experiments, we follow the exact dataset configuration used in prior baselines \comment{cite}, but instead of sampling, we evaluate on the entire MBPP test suite.  



% Furthermore, following Reflexion \cite{shinn2023reflexion}, we use the MultiPL-E compiler suite \cite{cassano2022multipl} to convert a subset of MBPP into Rust. MultiPL-E provides lightweight compilers for translating Python benchmarks into 18 different programming languages. We therefore conduct experiments on both Python and Rust versions of MBPP to demonstrate that our method is implementation-language agnostic and applicable to multiple programming environments.


% 其中GPT-4【实验表格的第一行】、Reflexion的结果【实验表格的第四行】，来自于relfexion论文，都是gpt-4 
% 其中GPT-4(CoT) GPT-4(ReAct)  LATS  MetaGPT AgentVerse  MASTER AgentCoder的结果都来自于master的论文
% 其中RAP 来自 LATS，不过人家是GPT-3.5

%中文版 在 MBPP 基准实验中选取这些 baseline，旨在全面覆盖代码生成核心技术路径。单智能体方法（如 GPT - 4 系列、Reflexion）以 GPT - 4 为基座模型，呈现基础推理范式的性能边界，其数据源自 Reflexion 相关工作。多智能体框架（MetaGPT、AgentVerse、AgentCoder）展示不同协作模式的优劣；搜索优化类（RAP、LATS、MASTER）体现搜索技术对代码生成的改进探索。此外，其余 baseline 均由 Gan 等人基于 GPT - 4 复现，数据统一来源于 Gan 等人的复现工作，确保对比实验在一致的基座模型与数据标准下进行，提升结果的可靠性与可比性。
% The MBPP baselines are selected to cover core technical paths in code generation. Single-agent methods (GPT-4, Reflexion) reflect the performance limits of basic reasoning with GPT-4 as the backbone, with data sourced from \cite{shinn2023reflexion}. . Multi-agent frameworks (MetaGPT\cite{hong2023metagpt}, AgentVerse\cite{chen2023agentverse}, AgentCoder\cite{huang2023agentcoder}) highlight varying collaboration strategies, while search-based methods (RAP \cite{hao2023reasoning}, LATS \cite{zhou2024language}, MASTER \cite{gan2025master}) explore optimization through structured exploration. Remaining baselines are reproduced by \cite{gan2025master}. using GPT-4 under a unified data and model setting, ensuring fair and reliable comparison.


%中文版 实验结果充分凸显了SYMPHONY方法的卓越优势。单智能体与搜索优化类方法中，诸多基线模型未能超越 GPT-4，这表明在代码生成任务里，复杂推理与规划策略的实际效用相对有限。而 SYMPHONY 仅凭借轻量级开源模型及固定的测试时间规划策略，便展现出强大性能，达到了 SOTA 水平。与诸多未涵盖 Rust 语言的基线方法不同，本研究评估覆盖了语言设置，进一步证实了 SYMPHONY 语言无关的有效性。更关键的是，这一成果是在显著降低资源使用率的前提下实现的。
% \textbf{Result.} As shown in the results table, SYMPHONY achieves the highest overall accuracy on both Python and Rust variants. While prior methods—including GPT-4—struggle to solve all tasks reliably, SYMPHONY demonstrates strong performance using only lightweight open-source models and a fixed test-time planning strategy. Unlike many baselines that exclude Rust, our evaluation includes both language settings and confirms SYMPHONY's language-agnostic effectiveness. Importantly, this is achieved with significantly lower resource usage.

% \textbf{Result.} Experimental results highlight the strong advantages of the SYMPHONY method. Many baselines in the single-agent and search-based categories fail to outperform GPT-4, suggesting limited practical gains from complex reasoning or planning in code generation tasks. In contrast, SYMPHONY achieves state-of-the-art performance using only lightweight open-source models and a fixed-time planning strategy. Unlike prior baselines that omit Rust, our evaluation includes it, further demonstrating SYMPHONY's language-agnostic effectiveness. Crucially, these results are achieved with significantly reduced resource consumption.


% GPT-4 does not achieve perfect scores on either the Python or Rust variants of the MBPP benchmark, suggesting that relevant knowledge was not fully encoded during pretraining. In contrast, our method—using only three open-source small models—outperforms all baselines on this dataset via test-time scaling alone, without any fine-tuning or modification of model parameters. Although most prior works omit Rust in their evaluations, our method—following the protocol of Reflexion\cite{shinn2023reflexion}—includes the Rust subset and attains a highest score. This finding confirms that test-time scaling not only fortifies the model’s logical reasoning but also empowers it to solve problems with standardized code in a language-agnostic manner. In terms of parameter settings, we follow the configuration used in LATS, employing \(k = 8\) iterations and sampling \(n = 5\) candidate solutions at each expansion step. All prompts are applied in a zero-shot manner.

% \begin{table}[ht]
% \centering
% \caption{Performance Comparison on MBPP. }
% \label{tab:mbpp_results} % 修正label命名一致性

% \begin{tabular}{lcc}
% \toprule
% \textbf{Method} & \textbf{Python} & \textbf{Rust} \\
% \midrule
% GPT-4 \cite{shinn2023reflexion} & 0.800 & 0.710 \\
% GPT-4(CoT) \cite{gan2025master}  & 0.683 & -- \\
% GPT-4(ReAct) \cite{yao2023react}   & 0.710 & -- \\
% Reflexion \cite{shinn2023reflexion}  & 0.771 & 0.754 \\
% RAP \cite{hao2023reasoning}     & 0.714 & -- \\  
% LATS \cite{zhou2024language}        & 0.811 & -- \\
% MetaGPT  \cite{hong2023metagpt}     & 0.877 & -- \\
% AgentVerse  \cite{chen2023agentverse} & 0.890 & -- \\
% MASTER  \cite{gan2025master}          & 0.910 & -- \\
% AgentCoder \cite{huang2023agentcoder} & 0.918 & -- \\

% % 其中GPT-4【实验表格的第一行】、Reflexion的结果【实验表格的第四行】，来自于relfexion论文，都是gpt-4 
% % 其中GPT-4(CoT) GPT-4(ReAct)  LATS  MetaGPT AgentVerse  MASTER AgentCoder的结果都来自于master的论文
% % 其中RAP 来自 LATS，不过人家是GPT-3.5

% \midrule
% \textbf{SYMPHONY-S}           & \textbf{0.927} & \textbf{0.946} \\
% \textbf{SYMPHONY-L}           & \textbf{0.965} & \textbf{0.974} \\
% \bottomrule
% \end{tabular}

% % \vspace{0.4em} % 微调表格与说明间距
% \parbox{0.9\linewidth}{ % 添加parbox包装说明文字
% \footnotesize All scores are normalized to the [0, 1] range. A dash (–) indicates that the corresponding method does not provide a score on this dataset.}
% \end{table}


\subsection{Diversity Analysis} 
% Diversity in candidate expansions plays a critical role in MCTS, as it directly influences the range and quality of reasoning paths explored. In SYMPHONY, this diversity is modulated by the composition of the agent pool. Different combinations of agents induce different reasoning behaviors, shaping the structure of the search tree. To evaluate this effect, we analyze how varying agent combinations affect both overall task performance and node-level diversity on all three tasks using SYMPHONY-S.

Branch diversity plays a crucial role in effective search. To assess its impact, we evaluate how different agent pool configurations affect both task performance and branch diversity across all three tasks using SYMPHONY-S. The expansion width is fixed at 4, and each node's candidate branches are categorized by output uniqueness: (a) \textbf{4-Unique}: all branches distinct, (b) \textbf{3-Unique}, (c) \textbf{2-Unique}, and (d) \textbf{1-Unique}: all branches identical. Higher frequencies of 3-Unique and 4-Unique indicate more diversified and informative exploration.

As shown in Figure~\ref{fig:diversity}, increasing agent heterogeneity, from single-agent to pairwise and full-trio configurations (e.g., Qwen+Mistral+Llama), leads to a substantial rise in 4-Unique expansions. On MBPP, for example, this proportion exceeds 80\% under the full ensemble, compared to under 20\% in the single-agent setting. This increase in structural diversity strongly correlates with improved accuracy, with SYMPHONY outperforming single-agent baselines by over 30\% on MBPP and showing similar trends on HotpotQA and WebShop. These findings highlight the critical role of model-level diversity in enhancing search coverage and reasoning robustness.




We also experimented with alternative diversity-promoting strategies such as adversarial prompting and temperature scaling, but found their effect to be marginal. Detailed comparisons are included in Appendix~\ref{appendix:diversity}.

% Diversity in candidate expansions plays a critical role. We evaluate how varying agent combinations affect both overall task performance and node-level diversity on all three tasks using SYMPHONY-S. We fix the expansion width to 4 and, for each node, categorize its four generated branches based on the number of unique outputs: (a) \textbf{4-Unique}: all four branches differ; (b) \textbf{3-Unique}: three distinct branches with one duplication; (c) \textbf{2-Unique}: two distinct branches, each repeated; (d) \textbf{1-Unique}: all branches are identical. 
% A higher frequency of 3-Unique and 4-Unique expansions indicates broader and more differentiated exploration.

% As shown in Figure~\ref{fig:diversity}, increasing the heterogeneity of the agent pool, from single-agent to pairwise and full-trio configurations (e.g., Qwen+Mistral+Llama), leads to a clear increase in 4-Unique expansions. On MBPP, for instance, the proportion of 4-Unique expansions surpasses 80\% under the full ensemble, compared to below 20\% in the single-agent setting. This increase in structural diversity is strongly correlated with improved task accuracy, with SYMPHONY outperforming single-agent baselines by over 30\% on MBPP and showing similar gains on HotpotQA and WebShop. These results underscore the importance of model-level diversity in enabling broader search coverage and more robust reasoning behavior.

% We also tested alternatives for diversity enhancement such as adversarial prompting and temperature scaling, but observed limited gains. Detailed results are provided in Appendix A.3.



% Diversity is a critical factor in MCTS, as exploring a wide range of paths increases the likelihood of discovering high-reward solutions. One of our key contributions is the introduction of a heterogeneous model pool, which fundamentally enhances the diversity of child node expansions in MCTS. To systematically evaluate the effectiveness of this design on the HotPotQA benchmark under consistent settings, we examine the following three aspects:  
% 1). Model Pool Ablation: Removing the heterogeneous pool to assess the contribution of multi-model diversity.  
% 2). Varying Pool Size: Investigating how different combinations and numbers of heterogeneous models impact search quality.  
% 3). Alternative Diversity Enhancements: Incorporating methods such as adversarial prompting and temperature variation to further increase model response diversity.

% The first and second ablation settings are primarily designed to evaluate the effectiveness and diversity benefits introduced by our proposed heterogeneous model pool. To this end, we conduct analysis from two complementary perspectives: (1) task-level performance to validate the overall effectiveness of heterogeneous collaboration, and (2) structural analysis of node-level diversity to quantify the heterogeneity across search paths.

% Given that our expansion parameter is set to 4 (see Section 5.7 for details), each layer in the constructed search tree consists of four child nodes. We categorize the node composition of each layer into four types:(i) Fully Diverse: All four nodes are generated by different model types.(ii) Three-Way Diverse: Three distinct model types are present.(iii) Two-Way Diverse: Two distinct types among the four nodes.(iv) Homogeneous: All nodes are generated by the same model type.We then compute the proportion of each category across all layers and tasks. A higher frequency of diverse node layers reflects greater model-level exploration and richer reasoning trajectories. This analysis enables us to quantify how the heterogeneity of the model pool contributes to increased search diversity.

% Figure \ref{fig:diversity} presents the impact of model combination on node diversity and task performance across three benchmarks: HotPotQA, WebShop, and MBPP. We observe a consistent pattern: as the model configuration evolves from homogeneous (e.g., Llama, Qwen, Mistral) to heterogeneous ensembles (e.g., Qwen+Llama, Llama+Mistral, All Three), the proportion of Fully Diverse nodes substantially increases—reaching over 80\% in the final configuration for MBPP. This structural diversity is strongly correlated with improved task accuracy. Specifically, on MBPP, the performance improves from ~60\% with Llama alone to over 90\% with the full model ensemble, marking a > 30 percentage point gain. Similar trends are observed in HotPotQA and WebShop, where full model combinations outperform single-model setups by wide margins (e.g., +25\% in HotPotQA). These results provide strong empirical evidence that incorporating architectural and reasoning diversity via model combinations not only improves robustness but also consistently outperforms powerful individual LLMs.

%要改，还没改。总结图图中的结果
% The key factor in both the first and second ablation studies is the number of heterogeneous models in the pool. When evaluated in isolation, each of the three open-source small models—despite our framework’s auxiliary components—can underperform the Base LM \cite{zhou2024language}. Next, we form all three possible pairwise combinations (pool size = 2). In this configuration, our method matches the performance of GPT-4 augmented with ReAct \cite{yao2023react} and outperforms Reflexion \cite{shinn2023reflexion}. Finally, when using the full pool of three heterogeneous models, our small-model framework not only substantially exceeds GPT-4’s accuracy but also approaches the performance of RAP \cite{hao2023reasoning}, despite RAP’s reliance on a large-scale backbone. As the number of heterogeneous models in the ensemble increases, the performance improves significantly. These results highlight the critical importance of the number of heterogeneous models in the pool, as increased diversity significantly enhances the search effectiveness of MCTS.


%  小模型实验结果表格
% \begin{table}
%     \centering
%     \caption{Ablation study on the number of heterogeneous models in the model pool on HotPotQA. "Llama" denotes Llama-3.1-8B-Instruct, "Qwen" denotes Qwen2.5-7B-Instruct-1M, "Mistral" denotes Mistral-7B-Instruct-v0.3, and "All" represents the ensemble of all three models.}
%     \label{tab:abalation_study}  
%     \begin{tabular}{@{}lc@{}}
%         \toprule
%         \textbf{Model ensemble} & \textbf{EM $\uparrow$} \\
%         \midrule
%         Llama(cite)     & 0.27 \\
%         Qwen(cite)      & 0.30 \\
%         Mistral         & 0.32 \\
%         Qwen+Llama      & 0.47 \\
%         Mistral+Qwen    & 0.50 \\
%         Mistral+Llama   & 0.52 \\
%         All               & 0.59 \\
%         % \midrule
%         % \textbf{Ours:LLMs-S} & \textbf{0.59} \\
%         % \textbf{Ours:LLMs-L} & \textbf{0.79} \\
%         \bottomrule
%     \end{tabular}
%     \vspace{0.5em}
%     \parbox{\linewidth}{
%     \footnotesize
    
%     }
% \end{table}




\subsection{Efficiency  and Cost Analysis}
% To assess the practicality of SYMPHONY, we analyze its efficiency and cost from two complementary perspectives: the size of the search tree required for reasoning, and the computational cost associated with model inference. These two factors jointly determine the real-world feasibility of deploying an LLM-based planning framework.

% We begin by examining the search configuration needed to achieve strong performance. While prior methods such as LATS adopt a large trajectory budget ($K=50$) and a wider expansion width ($n=5$), SYMPHONY achieves comparable or better results with significantly smaller values ($K=10$, $n=4$). This suggests that SYMPHONY requires a much smaller search space to reach high-quality solutions.

% To further quantify this advantage, we evaluate the average number of node expansions needed in MCTS to arrive at a correct answer. As shown in Table~\ref{tab:efficiency_hotpotqa}, SYMPHONY consistently requires fewer expansions than baseline methods, even outperforming LATS when using only one-fifth of its trajectory budget. These results underscore SYMPHONY’s superior sample efficiency and its ability to guide search more effectively.

% Beyond search efficiency, we also consider inference cost, which is a critical bottleneck in LLM-based systems. Unlike previous approaches that rely entirely on high-cost models such as GPT-4, SYMPHONY-L incorporates a heterogeneous agent pool to reduce dependence on any single model. As shown in Figure~\ref{fig:api_usage}, SYMPHONY-L invokes GPT-4 in just 40\% of calls, while delegating the remaining calls to less expensive yet capable models—achieving stronger performance than GPT-4-only pipelines. Token-level cost breakdowns provided in Appendix~\ref{appendix:cost} further confirm the substantial savings.

% Taken together, these results demonstrate that SYMPHONY not only reduces the size of the search tree but also lowers the cost of constructing it—achieving efficient, scalable, and cost-aware planning with LLM-based agents.

\begin{figure}[ht]
\centering
\begin{minipage}{0.39\textwidth}
    \centering
    \captionof{table}{Comparison of the search tree size on HotpotQA.}
    \label{tab:efficiency_hotpotqa}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{l|ccc}
        \toprule
        \textbf{Method} & \textbf{K} & \textbf{HotpotQA $\uparrow$ } & \textbf{\#Nodes $\downarrow$ }    \\
        \midrule
        ToT      & 10 & 0.34 & 33.97 \\
        RAP & 10 & 0.44 & 31.53 \\ 
        LATS & 10 & 0.44 & 28.42 \\
        \midrule
        ToT           & 50 & 0.49 & 84.05 \\
        RAP           & 50 & 0.54 & 70.60 \\
        LATS          & 50 & 0.61 & 66.65 \\
        
        \midrule
        \textbf{SYMPHONY-S}   & \textbf{10} & \textbf{0.59}  & \textbf{16.39} \\
        \textbf{SYMPHONY-L}   & \textbf{10} & \textbf{0.79}  & \textbf{9.47}\\
        \bottomrule
        \end{tabular}
    }
    
\end{minipage}
\hfill
\begin{minipage}{0.59\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fig/consume400.drawio.png}
    \captionof{figure}{Comparison of model invocation frequency and final performance on HotpotQA. %Performance of MASTER and LATS using GPT-4 is reported by \cite{gan2025master} 
    }\label{fig:api_usage}
    \label{fig:cost_accuracy}
\end{minipage}
\end{figure}


To evaluate SYMPHONY’s practicality, we analyze two key aspects: the size of the search tree and the cost of model inference—both crucial to real-world deployment.

Compared to methods like LATS, which use a large trajectory budget ($K=50$) and wider expansion ($n=5$) on HotpotQA and WebShop, SYMPHONY achieves comparable or better results with much smaller values ($K=10$, $n=4$), indicating a more compact search process.

We further assess efficiency by measuring average node expansions in MCTS on HotpotQA. As shown in Table~\ref{tab:efficiency_hotpotqa}, SYMPHONY consistently requires fewer expansions and even outperforms LATS with a fraction of its search budget, reflecting strong sample efficiency.

In terms of cost, SYMPHONY-L reduces reliance on expensive models by using a heterogeneous agent pool. As shown in Figure~\ref{fig:api_usage}, GPT-4 is used in only 40\% of calls, yet SYMPHONY-L still outperforms GPT-4-only baselines. Token-level cost details are provided in Appendix~\ref{appendix:cost}.

Together, these results show that SYMPHONY achieves efficient and cost-effective planning through smaller search trees and more economical model usage.




% It should be noted that SYMPHONY achieve comparable or even superior performance with much fewer resources. For example, on HotpotQA and WebShop, LATS uses an upper bound of $K=50$ trajectories to search for solutions, while that of SYMPHONY is is set to $K=10$. Moreover, the expansion width $n$ for LATS is set to $5$ for all tasks while that for SYMPHONY is set to $4$. 

% % 任务性能 vs 资源消耗（节点数）
% We look further into this issue by measuring average node expansions in MCTS on HotpotQA under varying trajectory budgets. Baselines include ToT~\cite{yao2023tree}, RAP~\cite{hao2023reasoning}, and LATS~\cite{zhou2024language}.

% As shown in Table~\ref{tab:efficiency_hotpotqa}, SYMPHONY consistently achieves higher accuracy with fewer expansions. Notably, it outperforms LATS with $K=10$ trajectories, even when LATS uses $K=50$, highlighting its superior sample efficiency.




% \comment{Talk about n, k for all three datasets here}
% % HotpotQA
% By evaluating diverse reasoning paths with just 4 expansions per step, 10 sampled trajectories, and a compact 3-shot prompt, SYMPHONY outperforms heavy-sampling methods such as LATS, while maintaining high efficiency.
% % WebShop
% Notably, SYMPHONY achieves these results with significantly lower resource demands. Compared to approaches such as LATS \cite{zhou2024language}, we use only 4 node expansions per step, 10 sampled rollouts, and a minimal one-shot prompt, demonstrating strong efficiency-performance tradeoffs across model scales.
% % MBPP
%  Our parameter configuration mirrors LATS~\cite{zhou2024language}, using $K=8$ iterations and $n=5$ candidate samples per step.
%MBPP的K=8,LATS也等于8，zero-shot
%HotpotQA和webshop的K都等于10，LATS等于50，QA是3-shot,MBPP是1-shot
% shot都和他一样
%我们n都是4，LATS是5
% MCTS里的UCT参数是参照LATS设置为2



% To assess the efficiency of SYMPHONY, we compare the search tree size required to reach a correct answer. Specifically, we measure the average number of node expansions in MCTS across different methods on HotpotQA, under varying trajectory settings. Baselines include ToT \cite{yao2023tree}, RAP \cite{hao2023reasoning}, and LATS \cite{zhou2024language}, all of which provide publicly available implementations.

% As shown in Table~\ref{tab:efficiency_hotpotqa}, SYMPHONY achieves the highest accuracy while requiring the fewest node expansions across all settings. Under identical configurations, it significantly reduces search cost compared to LATS without compromising performance. Notably, even with only $k=10$ trajectories, SYMPHONY surpasses LATS results obtained with $k=50$, demonstrating superior sample efficiency and search effectiveness.




% 大模型API价格 vs 调用情况百分比分析
% As a tree-based framework, our method encodes problem-solving trajectories as nodes in a search tree, where total node count directly reflects computational cost and search efficiency. Because parameter settings—such as expansion breadth and trajectory count—govern how rapidly the tree grows, eliminating redundant exploration is crucial. To this end, we introduce semantic node merging to collapse equivalent states and an early-termination mechanism to prune completed branches, all while leveraging enhanced MCTS-driven diversity to guide the search toward correct solution paths more rapidly.
% We benchmark our efficiency against LATS for two reasons: 1). LATS is a prototypical tree-based, MCTS-driven algorithm that demonstrates strong performance relative to similar frameworks; 2). LATS reports average node counts under the same parameter configurations used by Tree-of-Thought (ToT) \cite{yao2023tree} and RAP \cite{hao2023reasoning}, enabling a direct and fair comparison.

% Given that our primary objective is cost reduction and efficiency improvement, we do not consider the larger parameter configurations used in LATS; instead, we compare only against its minimal setting. Under identical settings, our method requires substantially fewer node expansions than LATS without sacrificing accuracy, thereby underscoring its superior efficiency.Moreover, even with a reduced setting of k = 10, our method outperforms LATS in task performance, surpassing their results obtained with k = 50.





% \begin{table}[ht]
% \centering
% \caption{Comparison of the search tree size on HotPotQA. }
% \label{tab:efficiency_hotpotqa} % 修正label命名一致性

% \begin{tabular}{l|ccc}
% \toprule
% \textbf{Method} & \textbf{k} & \textbf{HotPotQA $\uparrow$ } & \textbf{\#Nodes $\downarrow$ }    \\
% \midrule
% ToT      & 10 & 0.34 & 33.97 \\
% RAP & 10 & 0.44 & 31.53 \\ 
% LATS & 10 & 0.44 & 28.42 \\
% \midrule
% ToT           & 50 & 0.49 & 84.05 \\
% RAP           & 50 & 0.54 & 70.60 \\
% LATS          & 50 & 0.61 & 66.65 \\

% \midrule
% \textbf{SYMPHONY-S}   & \textbf{10} & \textbf{0.59}  & \textbf{16.39} \\
% \textbf{SYMPHONY-L}   & \textbf{10} & \textbf{0.79}  & \textbf{9.47}\\
% \bottomrule
% \end{tabular}

% % \vspace{0.4em} % 微调表格与说明间距
% \parbox{0.9\linewidth}{ % 添加parbox包装说明文字
% \footnotesize }
% \end{table} 

% Previous baseline methods often place stringent demands on model capability, leading them to primarily rely on high-end models such as GPT-4. To ensure a fair and rigorous comparison, we also include GPT-4 as one of our backbone models. However, to better reflect real-world deployment scenarios and encourage architectural diversity, we additionally incorporate models from different vendors and with varying design philosophies, including both proprietary and open-source alternatives.

% By analyzing the API pricing and actual usage distribution for each method, we provide a concrete estimation of the computational cost incurred. Remarkably, our approach not only achieves state-of-the-art performance but does so under significantly lower resource requirements. This demonstrates that our method is not only effective but also highly cost-efficient, making it well-suited for practical applications where scalability and budget are critical factors.

% Prior methods often rely exclusively on GPT-4, which offers strong performance but comes with high computational cost. SYMPHONY adopts a more cost-efficient strategy by incorporating a mix of API-accessible models with varying pricing and capabilities, reducing reliance on the most expensive options while maintaining architectural diversity. To assess this design, we analyze the proportion of different agent usage within SYMPHONY-L and compare overall task performance.

% Figure \ref{fig:api_usage} reports the experiment results. Even with 100\% GPT-4 usage, these baselines do not match our performance: SYMPHONY-L achieves superior accuracy while invoking GPT-4 for only 40\% of all calls. This represents a substantial reduction in computational cost, demonstrating that our approach delivers strong results without reliance on an expensive, high-capacity model.

 

% While prior methods rely solely on GPT-4, SYMPHONY-L employs a heterogeneous mix of API-based models to reduce cost without sacrificing performance.

% As shown in Figure~\ref{fig:api_usage}, SYMPHONY-L outperforms GPT-4-only baselines while using GPT-4 in just 40\% of calls. This highlights the effectiveness of our budget-aware design in achieving strong results with significantly lower computational cost. A detailed comparison of token usage is provided in Appendix~\ref{appendix:cost}.





% In prior work, some baselines mixed GPT-4 and GPT-3.5, but MASTER consistently employed GPT-4 as the backbone to reproduce the methods listed in Table X; accordingly, we directly cite their GPT-4–based results. We then compare (a) the proportion of model invocations in our framework and (b) the resulting HotPotQA performance. Even with 100\% GPT-4 usage, these baselines do not match our performance: our method achieves superior accuracy while invoking GPT-4 for only 40\% of all calls. This represents a substantial reduction in computational cost, demonstrating that our approach delivers strong results without reliance on an expensive, high-capacity model.

% \begin{table}[ht]
% \centering
% \caption{Comparison of API pricing across different models, with both input and output costs measured in USD per million tokens.}
% \label{tab:api_cost} % 修正label命名一致性

% \begin{tabular}{l|ccc}
% \toprule
% \textbf{Model} & \textbf{Input cost (per million tokens/ \textdollar)} & \textbf{Output cost (per million tokens/\textdollar)}    \\
% \midrule
% GPT-4        & 30.00 & 60.00 \\
% Qwen‑Max     & 0.33 & 1.32  \\ 
% DeepSeek-V3  & 0.27 & 1.10  \\
% \bottomrule
% \end{tabular}

% \vspace{0.4em} % 微调表格与说明间距
% \parbox{0.9\linewidth}{ % 添加parbox包装说明文字
% \footnotesize }
% \end{table} 


% \begin{table}[ht]
% \centering
% \caption{Comparison of model invocation frequency and final performance on HotPotQA, highlighting the cost-effectiveness of each method.}
% \label{tab:api_distribution} % 修正label命名一致性

% \begin{tabular}{l|cccc}
% \toprule
% \textbf{Method} & \textbf{GPT-4} & \textbf{Qwen-Max} & \textbf{DeepSeek-V3} & \textbf{HotPotQA $\uparrow$}    \\
% \midrule
% ReAct           & 100\% & -- & -- & 0.42 \\
% Reflexion       & 100\% & -- & -- & 0.51\\ 
% LATS         & 100\% & -- & -- & 0.71 \\
% Beam Retrieval   & 100\%  & -- & -- & 0.73 \\
% MASTER          & 100\% & -- & --   & 0.76 \\
% \midrule
% \textbf{Ours:LLMs-L}   & \textbf{35\%} & \textbf{33\%}  & \textbf{32\%} & \textbf{0.79} \\
% \bottomrule
% \end{tabular}

% \vspace{0.4em} % 微调表格与说明间距
% \parbox{0.9\linewidth}{ % 添加parbox包装说明文字
% \footnotesize }
% \end{table} 

% 上面的价钱和调用次数，画成 y轴为EM分数， x轴为各种方法的矩形图，然后再把调用情况用饼状图表现
% \begin{figure}[ht]
% 		\centering
% 		\includegraphics[width=\textwidth]{fig/consume.pdf}
%         % \vspace{-20pt}
% 		\caption{Comparison of model invocation frequency and final performance on HotPotQA. Performance of MASTER and LATS using GPT-4 is reported by \cite{zhou2024language} } \label{fig:api_usage}
% \end{figure}


\subsection{Ablation Study and Hyperparameter Tuning}
To evaluate the impact of SYMPHONY’s core components, we perform a series of ablation studies by selectively disabling key modules, including UCB-based agent scheduling, pool-wise memory sharing, and EMCS scoring. As presented in Table~\ref{tab:abalation_study_moudle}, removing any of these components leads to consistent performance degradation across tasks. These results underscore the effectiveness of dynamic agent scheduling, collaborative memory sharing, and uncertainty-aware scoring in enhancing overall system performance.

We further conduct hyperparameter tuning for the UCB exploration coefficient $\alpha$ used in agent scheduling, as well as the MCTS parameters $n$ and $K$, which jointly determine the search strategy and computational efficiency. Detailed analyses and results are provided in Appendix~\ref{appendix:alpha} and Appendix~\ref{appendix:parameter}. An extended analysis of architectural robustness under varying agent compositions and noise perturbations is also included in Appendix~\ref{appendix:robustness}, offering deeper insights into SYMPHONY’s stability and adaptability. Case studies are included in Appendix~\ref{appendix:case}.

% To assess the contribution of each core component in the SYMPHONY framework, we removed core reasoning and coordination modules.

% \textbf{Without UCB-Based Agent Scheduling.} We replace the adaptive UCB-based scheduler with a fixed round-robin selection that invokes agents in a predetermined sequence. While this randomization still allows all agents to participate, it removes the structured balancing between exploration and exploitation. This ablation examines whether intelligent scheduling contributes to more efficient reasoning by dynamically adjusting agent allocation beyond uniform participation.

% \textbf{Without Pool-wise memory sharing.} We disable the reflection mechanism that allows agents to analyze and learn from their past failures through verbal self-evaluation. This prevents intra-agent iterative improvement and tests the importance of reasoning-level feedback in guiding future actions.

% \textbf{Without EMCS.}: We remove the  Entropy-Modulated Confidence Scoring (EMCS) component and instead use naive majority voting for path selection. This ablation assesses the value of semantically-informed comparative evaluation in resolving ambiguity and selecting globally coherent solutions.

% Results across benchmarks (Table \ref{tab:abalation_study_moudle}) consistently demonstrate that removing any of these components leads to noticeable performance degradation. Notably, the absence of reflection results in brittle and non-adaptive reasoning trajectories, while a homogeneous agent pool fails to handle complex, multi-faceted queries due to the lack of strategic diversity. Without EMCS, the final answer selection becomes unstable when semantically similar distractors are present. Furthermore, removing the UCB-based agent scheduler eliminates adaptive coordination across agents, leading to inefficient exploration patterns and reduced overall effectiveness in balancing complementary reasoning strategies.


% To assess the contribution of SYMPHONY’s core components, we conduct module-leval ablation studies by disabling UCB-based agent scheduling, pool-wise memory sharing, and EMCS scoring. As shown in Table~\ref{tab:abalation_study_moudle}, performance drops consistently across all settings, confirming the importance of dynamic scheduling, memory sharing, and uncertainty-aware evaluation in SYMPHONY.

% Removing the UCB scheduler reduces coordination efficiency, leading to suboptimal agent allocation. Disabling memory sharing prevents agents from adapting based on prior failures, resulting in brittle reasoning. Replacing EMCS with majority voting degrades answer selection when semantically similar candidates are present.



% We also tune the hyper-parameter $\alpha$ in UCB-based agent scheduling and MCTS coefficient $c$, which are provided in Appendix \ref{appendix:alpha} and Appendix ~\ref{appendix:parameter}, respectively.

\begin{table}
\centering
\caption{Ablation Study. }
\label{tab:abalation_study_moudle}
\resizebox{0.6\linewidth}{!}{
\begin{tabular}{@{}lccc@{}}
    \toprule
Method &HotpotQA(EM)$\uparrow$
&WebShop(SR)$\uparrow$
&MBPP(pass@1)$\uparrow$\\
    \midrule
SYMPHONY-S    
& \textbf{0.59} & \textbf{0.56} & \textbf{0.927} \\
w/o Agent Scheduling  & 0.51 & 0.48 & 0.906\\
w/o Memory Sharing  & 0.45 & 0.46 & 0.871\\
w/o EMCS      & 0.51 & 0.49 & 0.892\\

    % \midrule
    % \textbf{Ours:LLMs-S} & \textbf{0.59} \\
    % \textbf{Ours:LLMs-L} & \textbf{0.79} \\
    \bottomrule
\end{tabular}
}
% \vspace{0.5em}
\end{table}






















