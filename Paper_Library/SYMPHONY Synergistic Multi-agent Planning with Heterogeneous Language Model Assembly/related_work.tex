\section{Related Work}

% (1) 传统的 baseLM, CoT
% (2) 树形结构的，LATS
% (3) 多模型的
% The remarkable reasoning capabilities of Large Language Models (LLMs) have catalyzed a shift from static text generation to agentic planning, spawning a diverse array of techniques. We categorize this work into three themes—single‐path strategies, multi‐path exploration, and multi‐agent strategies and highlight representative advances in each.

\subsection{LLM-based Planning and Reasoning}
Early work on LLM-based reasoning focused on improving consistency and correctness through guided inference. \citet{brown2020lfewshot} introduced in-context learning with exemplars, 
while the Chain-of-Thought paradigm \cite{wei2022chaincot,kojima2022zeroshot-cot,ning2023skeleton-cot} encouraged models to generate step-by-step rationales during prediction. Later studies proposed more structured prompting techniques, such as meta-prompting \citep{zhang2023metaprompt} and meta-constraint-guided inference \citep{suzgun2024meta}, to scaffold the reasoning process with predefined formats or global constraints.

To enhance reasoning adaptability, several methods incorporate dynamic feedback. \citet{yao2023react} interleaves environment interactions with reasoning steps, \citet{shinn2023reflexion} enables self-correction through natural language reflections, and code-based approaches \citet{qiu2023phenomenal-codebase,chen2023self-debug-codebase} iteratively refine outputs based on execution results. \citet{xu2024rereading} further improves performance by prompting LLMs to rearticulate and revise their own reasoning chains.

As tasks grew more complex, researchers began to move beyond linear inference and explore tree-structured reasoning. \citet{tang2021high} and \citet{zhang2023end} introduced early mechanisms for maintaining and refining multiple hypotheses in dialogue and QA tasks. \citet{wang2022self-con} aggregates diverse reasoning paths through sampling and majority voting, while \citet{pan2024dynathink} dynamically adjusts decoding strategy between heuristic and deliberative modes.

A more principled formulation of structured search appears in Tree of Thoughts \citep{yao2023tree}, which organizes reasoning steps into a decision tree with intermediate backtracking. Building on this, Monte Carlo Tree Search (MCTS) has been applied to guide reasoning more systematically: \citet{hao2023reasoning} treat the LLM as a world model within a reward-driven search process, and \citet{zhou2024language} further integrate reasoning, planning, and reflection within an MCTS-based framework using learned value estimates and external feedback. \citet{shi2025monteICLR} employ memory-augmented single-agent MCTS to enhance decision-making in text-based games.


While prior work focuses on structured reasoning with a single model, our approach introduces model-level heterogeneity to enhance diversity and robustness in planning.

\subsection{Multi-Agent Collaboration with Language Models}


Multi-agent frameworks leverage multiple LLMs or specialized modules to improve reasoning diversity, adaptability, and robustness. Early approaches adopt static task division, assigning agents predefined roles and communication protocols. For instance, ChatDev \citep{qian2023chatdev} simulates software development by dividing planning, coding, and testing among fixed-role agents, while MetaGPT \citep{hong2023metagpt} enforces similar pipelines using hand-crafted coordination logic. AutoAgents \citep{chen2023autoagents} automates agent instantiation but still operates under rigid, rule-based interaction patterns. Although effective in structured environments, these systems struggle with dynamic tasks due to their limited flexibility.

More recent work shifts toward dynamic coordination, enabling emergent collaboration and context-aware adaptation. AgentVerse \citep{chen2023agentverse} adopts a blackboard architecture where agents communicate freely through shared language-based memory. CAMEL \citep{li2023camel} introduces turn-based agent dialogue for zero-shot task-solving, while AutoGen \citep{wu2023autogen} allows agents to negotiate roles and delegate subtasks on the fly. In complex QA settings, WebGPT \citep{nakano2021webgpt} decomposes queries into search, summarization, and synthesis subtasks, and MAd \citep{li2024MAD} employs adversarial debate between LLMs to expose reasoning flaws. MASTER \citep{gan2025master} integrates multi-agent behavior into MCTS by adapting the UCT formula using reward signals. AgentCoder \citep{DBLP:journals/corr/abs-2312-13010} further demonstrates the utility of functional specialization in code generation, coordinating programmer, test designer, and test executor agents within a feedback loop to ensure correctness and completeness.

Unlike existing frameworks that rely on uniform agents and costly coordination, our method enables lightweight, heterogeneous collaboration through principled search and memory sharing.


% \subsection{Single-Path Strategies}

% Single-path reasoning approaches aim to enhance consistency and correctness in task execution by following deterministic reasoning trajectories. Early representative methods trace back to the Few-Shot Prompting \cite{brown2020lfewshot}, which guides LLMs using task-specific exemplars. The "chain-of-thought" family \cite{wei2022chaincot,kojima2022zeroshot-cot, ning2023skeleton-cot} further introduced explicitly linear reasoning steps to model the cognitive process during inference. Recent efforts have explored more structured prompting schemes, such as the meta-prompt templates proposed by \cite{zhang2023metaprompt}, which require LLMs to fill predefined reasoning frames, or the method by \cite{suzgun2024meta}, which imposes meta-knowledge constraints to steer the inference trajectory. Another important direction in single-path refinement involves integrating feedback. ReAct \cite{yao2023react} interleaves reasoning with environment-interacting actions to obtain dynamic signals, while Reflexion \cite{shinn2023reflexion} adds a post-hoc self-reflection step for error correction. Code-based frameworks \cite{qiu2023phenomenal-codebase,chen2023self-debug-codebase} leverage program execution outcomes to iteratively debug reasoning chains. Notably, the "Rereading" mechanism \cite{xu2024rereading} demonstrates that allowing LLMs to re-express and revise their own reasoning chains can significantly improve performance, even without additional examples or model updates.

% While effective, these methods often rely on rigid templates and apply single-shot corrections only at the end of the reasoning chain—allowing early errors to persist and lacking hierarchical error analysis. In contrast, our approach builds dynamic, multi-directional paths with stepwise hierarchical reflection, enabling real-time pruning and bottom-up feedback for continual strategy optimization.

% \subsection{Multi-Path Exploration}
% Multi-path reasoning methods have emerged as a response to the inherent brittleness of single-chain approaches, aiming to enhance robustness through the parallel exploration of diverse candidate trajectories. Early examples can be found in dialog systems, such as I-SEE \cite{tang2021high}, which integrates multiple user models—each initialized differently—to generate a set of diverse conversational paths. Along similar lines, \cite{zhang2023end} propose End-to-End Beam Retrieval for Multi-Hop QA, which jointly optimizes document retrieval and reasoning, maintaining multiple answer hypotheses across hops and iteratively refining them via beam search. To address uncertainty in open-ended tasks, Self-Consistency sampling \cite{wang2022self-con} aggregates diverse outputs from Chain-of-Thought prompting through majority voting, thus improving reliability. DynaThink \cite{pan2024dynathink} introduces a fine-grained dynamic control mechanism that alternates between lightweight heuristic decoding and more deliberate, computation-intensive planning at the token level, allowing for an adaptive balance between efficiency and accuracy. At a higher level of structural abstraction, Tree of Thoughts \cite{yao2023tree} generalizes beam search into a decision tree framework, enabling backtracking over intermediate reasoning steps. Meanwhile, Monte Carlo Tree Search (MCTS)-based methods provide a more principled decision-theoretic foundation: RAP \cite{hao2023reasoning} treats the LLM as both a world model and a reasoning agent, using MCTS to explore reasoning paths under task-specific reward functions. LATS \cite{zhou2024language} extends this further by unifying reasoning, acting, and planning within a single framework—leveraging language model rollouts, learned value functions, self-reflection, and environment feedback to adaptively guide search expansion.

% Despite recent advances, most methods rely on a single agent to generate all reasoning paths, resulting in homogeneous strategies and limited diversity. Lacking agent-level heterogeneity, they struggle to explore distinct reasoning paradigms or correct internal biases through interaction. To address this, we propose a heterogeneous multi-agent framework that integrates diverse reasoning styles and knowledge bases, enabling complementary exploration and collaborative correction—fundamentally enhancing flexibility and robustness.
% Recognizing the brittleness of single chains, multi-path techniques explore multiple candidate trajectories in parallel. Beam Search remains the simplest instantiation, but End-to-End Beam Retrieval  \cite{zhang2023end} for Multi-Hop QA  advances this by jointly learning retrieval and reasoning, maintaining multiple document hops and answer beams that are iteratively rescored. Self-Consistency sampling \cite{wang2022self-con} further improves robustness by aggregating diverse Chain-of-Thought outputs via consensus voting. DynaThink \cite{pan2024dynathink} introduces a dynamic decision framework that switches between “fast” heuristic decoding and “slow” deep planning per token, optimizing the trade-off between speed and accuracy. . At a higher level of structure, Tree of Thoughts \cite{yao2023tree} generalizes beam search into a full decision tree, allowing backtracking of partial “thoughts,” While MCTS‐based hybrids such as RAP \cite{hao2023reasoning} reconceptualize the LLM as both a world model and a reasoning agent—building a planning tree under task‐specific rewards to balance exploration versus exploitation with MCTS, LATS \cite{zhou2024language} goes further by unifying reasoning, acting, and planning within a single framework, integrating MCTS rollouts with LM‐powered value functions, self‐reflections, and environment feedback to adaptively guide search expansions. Both approaches rely on a single underlying policy model, leading to limited strategy diversity across search paths, and incur high computational overhead due to the extensive model queries required for simulation and evaluation. MASTER \cite{gan2025master} introduces the concept of multi-agent collaboration within MCTS by modifying the UCT formula through reward-based adjustments. However, its notion of “multi-agent” merely treats each tree node as being independently evaluated by separate instances, without involving truly heterogeneous agents. In essence, MASTER remains a single-model, multi-path exploration framework, where all “agents” ultimately sample from the same underlying policy. 



% \subsection{Multi-Agent Strategies}
% % AgentCoder提出了一种多智能体代码生成框架 ，旨在解决代码生成中代码片段生成与有效测试用例生成、执行间的平衡难题。该框架包含三个智能体：程序员智能体根据测试反馈专注代码生成与优化，测试设计智能体生成测试用例，测试执行智能体运行代码并反馈结果。这种协作机制形成动态反馈循环，突破了单一智能体模型的局限，确保更可靠的代码生成。
% Multi-agent paradigms harness multiple LLMs or specialized modules to inject diversity and resilience into reasoning. Static Task Division frameworks assign fixed roles and workflows: for example, ChatDev \cite{qian2023chatdev} orchestrates software development by splitting planning, coding, and testing among dedicated agents; MetaGPT \cite{hong2023metagpt} formalizes similar role-based pipelines with hand-crafted communication schemas; and AutoAgents \cite{chen2023autoagents} auto-generates agents but still relies on rigid, pre-defined interactions. These systems excel in well-scoped, stable environments yet falter when task requirements or contexts shift beyond their static collaboration logic.

% In contrast, Dynamic Coordination frameworks emphasize emergent interaction and self-organization. AgentVerse \cite{chen2023agentverse} implements a blackboard architecture where agents broadcast observations and hypotheses in free-form language, enabling flexible, asynchronous collaboration. CAMEL \cite{li2023camel} introduces a turn-taking protocol that simulates human-like conversational problem solving between two agents, demonstrating robust zero-shot task coordination. AutoGen \cite{wu2023autogen} further extends this by allowing agents to negotiate roles and delegate subtasks dynamically based on context. In open-domain QA, WebGPT \cite{nakano2021webgpt} decomposes complex queries among search, summarization, and synthesis agents, while MAd\cite{li2024MAD} uses adversarial debate between two LLMs to surface strengths and flaws in candidate answers. MASTER \cite{gan2025master} introduces the concept of multi-agent collaboration within MCTS by modifying the UCT formula through reward-based adjustments. Although these methods unlock flexibility and emergent behavior, they often deploy agents with similar architectures and lack systematic heterogeneity in model design. Moreover, their communication mechanisms—voting, dialogue, debate—can impose significant computational and latency costs, limiting scalability in real-time or resource-constrained settings.

% Our proposed multi-agent system offers distinct advantages in dynamic collaboration, heterogeneous reasoning integration, efficient communication architecture, and composite reasoning depth. It not only overcomes the rigidity of static frameworks but also addresses the homogeneity and high cost associated with existing dynamic methods—providing a more robust and scalable solution for complex, open-ended, and real-time scenarios.