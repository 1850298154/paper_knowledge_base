\section{Methodology}

% As shown in Figure~\ref{fig:method}, each LLM first takes a question as input and determines the interaction mode based on the question's difficulty, which can be measured by self-ranking, and Item Response Theory (IRT). Subsequently, LLMs engage in Idea3 interactions (Idea Sharing, Idea Analysis, and Idea Fusion), wherein distinct prompts guide different interaction modes, facilitating information exchange to generate the final answer. For the same input question, each LLM produces a group of answers. A reward model is employed to assign rewards to these answers, and arithmetic calibration is applied to encode the reward distribution characteristics of one LLM as an incentive signal into another LLM's reward function. This enables LLMs to perceive the quality of answers generated by their peers, thereby fostering multi-agent learning. Finally, ILR utilizes GRPO to optimize LLMs.


As illustrated in Figure~\ref{fig:method}, our framework begins with each LLM receiving a question as input and estimating its difficulty using both self-ranking and Item Response Theory (IRT). Based on this estimation, LLMs dynamically select an appropriate interaction mode. They then engage in our proposed Idea3 interactions (Idea Sharing, Idea Analysis, and Idea Fusion), wherein distinct prompts guide different interaction modes, facilitating information exchange to generate the final answer. For each question, every LLM produces a group of answers. A reward model then evaluates these answers, after which we apply arithmetic calibration to encode the distributional characteristics of one LLM's rewards as incentive signals within another LLMâ€™s reward function. This mechanism allows LLMs to perceive the quality of their peers' solutions, fostering more effective multi-agent learning. Finally, we employ GRPO to optimize all participating LLMs under this learning paradigm.

\begin{figure}[t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=1.0\linewidth]{iclr2026/figures/method.pdf}
\end{center}
% \vspace{-10pt}
\caption{Illustration of proposed ILR for multi-agent learning. \textbf{Top}:
% Questions will be randomly split into $B$ batches by $S$ times, and 
LLMs first sort questions within each batch using the self-ranking prompt and then compute the average rank. \textbf{Bottom}: We depict the cooperation interaction here. If the question is too hard, LLMs will share their ideas, identify those complementary steps from other LLMs' ideas, and improve their answers. The competition interaction also follows Idea3, which only requires a minor change in prompts.}
% \vspace{-10pt}
\label{fig:method}
\end{figure}



\subsection{Question Difficulty Estimation}
% In real-world scenarios such as classroom learning, when confronted with complex problems, students may opt for collaborative problem-solving strategies. Conversely, when addressing simpler tasks, individuals tend to engage in competition to demonstrate the efficiency and superiority of their solutions. In other words, either cooperation or competition can promote multi-agent learning, which serves as the foundational inspiration for our Dynamic Interaction design. 
% As problems usually do not carry a continuous difficulty label, we first employ a self-ranking method, which is based on the powerful comparative and ranking capabilities of LLM, to estimate the question difficulty following~\citep{wang2025make}.

In real-world learning environments, such as classrooms, students often adapt their strategies to the complexity of the problem: for challenging tasks, they are more likely to collaborate, whereas for simpler ones, they tend to compete to demonstrate the efficiency and superiority of their solutions~\citep{richard2002cooperation, green2006children, schneider2011cooperation, fulop2022cooperation}. Both cooperation and competition can serve as drivers of multi-agent learning, which provides the underlying motivation for our Dynamic Interaction design. Since most problems lack explicit or continuous difficulty annotations, we adopt a self-ranking to estimate the question difficulty based on the powerful comparative and ranking capabilities of LLMs~\citep{wang2025make}.

% Assuming the training dataset contains $N$ questions, we randomly split the whole set into $B$ batches to avoid the long-context lost problem~\citep{liu2023lost}, where each batch consists of $N' = \frac{N}{B}$ questions ($N' \ll N$). Subsequently, a self-ranking prompt (detailed in Appendix~\ref{appendix: prompt details}) directs the LLM to sort the questions within each batch in ascending order of difficulty. This ranking is then normalized to derive a difficulty metric, with the easiest question assigned a difficulty level of $\frac{1}{N'}$ (corresponding to a rank of $1$) and the hardest question assigned a difficulty level of $1.0$ (yielding a rank of $N'$). Given that a single random split yields difficulty levels only within the respective batch, we conduct $S$ independent random splits and compute the mean difficulty levels to obtain a robust estimation for the entire training dataset.


Given a training dataset of $N$ questions, we divide it into $B$ random batches to avoid the long-context lost problem~\citep{liu2023lost}, with each batch containing $N' = \frac{N}{B}$ questions ($N' \ll N$). Using a self-ranking prompt (detailed in Appendix~\ref{appendix: prompt details}), the LLM is instructed to order the questions within each batch in ascending difficulty. The ranks are then normalized into difficulty scores, where the easiest question is assigned $\frac{1}{N'}$ (rank 1) and the hardest one is assigned $1.0$ (rank $N'$). Since a single random split provides only relative difficulty within its batch, we perform $S$ independent splits and average their results to obtain a more stable and robust difficulty estimate for the entire dataset. We set $N'$ and $S$ to 10 in our work.
% If there is a set of LLMs $M=\{\mathcal{M}_{i} | i=0,1,...,m\}$ in the multi-agent learning scenario, the difficulty level $D$ of question $q$ is:
% \begin{align}
% D_{q}=\frac{1}{m}\sum_{i=1}^{m}\mathcal{M}_{i}(\frac{1}{S}\sum_{j=1}^{S}\frac{r_{q,j}}{N'})  
% \end{align}
% where $m$ is the number of LLMs, $\mathcal{M}_{i}(\cdot)$ means the estimation result of $i$-th LLM and $r_{q,j}$ denotes the rank of question $q$ in $j$-th random split.
For a set of LLMs $M=\{\mathcal{M}_{i} | i=0,1,...,m\}$ in a multi-agent learning scenario, the difficulty score $D_q$ of a question $q$ is computed as:
\begin{align}
D_{q}=\frac{1}{m}\sum_{i=1}^{m}\mathcal{M}_{i}(\frac{1}{S}\sum_{j=1}^{S}\frac{r_{q,j}}{N'})  
\end{align}
where $m$ is the number of LLMs, $\mathcal{M}_{i}(\cdot)$ denotes the estimation given by the $i$-th LLM, and $r_{q,j}$ is the rank of question $q$ in the $j$-th random split.

\subsection{Dynamic Interaction}
\label{sec: dynamic interaction}
When an LLM with reasoning ability $\gamma_{i}$, which can be measured by its performance on a validation set, receives a question of difficulty level $D_{q}$, it can quantify the probability ($P_{q, i}$) of correctly answering the question using Item Response Theory (IRT)~\citep{benedetto2023survey}:
\begin{align}
P_{q, i} = \frac{1}{1+e^{-1.7\times(\gamma_{i}-D_{q})}}
\end{align}
where the empirically derived coefficient 1.7 has been shown to yield reliable predictions across diverse conditions~\citep{baker2001basics, de2013theory, benedetto2023survey}. Since $P_{q, i}=0.5$ when $\gamma_{i}=D_{q}$, we adopt 0.5 as the decision boundary between different interaction modes. We average the probability of $m$ LLMs to derive the overall probability ($P_{q}$) of independently solving question $q$ and determine the interaction mode.
\begin{align}
P_{q} &= \frac{1}{m}\sum_{i=1}^{m}P_{q, i} \\
\text{Mode}&= \begin{cases}
\text{Cooperation} & \text{if} \: P_{q} < 0.5\\
\text{Competition} & \text{if} \: P_{q} \ge  0.5
\end{cases}
\end{align}

To simulate human discussion, we design a novel and unified three-stage Idea3 interaction for multi-agent communication: Idea Sharing (each LLM proposes its own solution), Idea Analysis (each LLM analyzes and reflects on the peer's solution), and Idea Fusion (the insights are synthesized into a refined and potentially novel solution). For different modes, we only need to slightly modify the prompt to inject the corresponding signal (details in Appendix~\ref{appendix: prompt details}).

\textbf{Idea Sharing.} Each LLM begins by presenting its problem-solving strategy, explaining the reasoning process and methods employed to address the given problem. For example, when solving a complex algebraic equation, one model might focus on factoring, while another may rely on graphical analysis. This stage produces the \textit{initial answer}.

\textbf{Idea Analysis.} Subsequently, LLMs engage in a critical evaluation of each other's proposed methods. In the cooperation mode, they may identify complementary strengths from different approaches, such as combining graphical insights with algebraic manipulation to generate a more comprehensive solution. In the competition mode, however, they rigorously assess the merits and limitations of the shared strategies. For example, one LLM might argue that the factoring approach, while effective, overlooks potential solutions that could be derived from the quadratic formula, thereby revealing a potential improvement.

\textbf{Idea Fusion.} Finally, LLMs synthesize the insights gained during previous analyses to generate a refined answer. This phase may involve integrating the most effective elements of both approaches, yielding a solution that not only accurately addresses the original problem but also leverages complementary techniques from each LLM. For example, the final resolution to the algebraic equation might incorporate both the graphical representation for visual clarity and the algebraic methods for precision, culminating in a solution that is both robust and comprehensible. This structured interaction not only enhances the quality of the final response but also fosters a dynamic learning environment among LLMs, driving continuous improvement in their problem-solving capabilities. The output of this stage is the \textit{updated answer}.

Prior research has noted that inter-agent communication may introduce noise into the information-exchange process and compromise the quality of final outputs~\citep{pan2025multiagent, zhang2025agent}. To address this, we adopt a label-based selection mechanism: the initial answer is retained only if it is correct and the updated answer is incorrect; in all other cases, the updated answer is chosen.

\subsection{Perception Calibration}
As demonstrated in~\citep{park2025maporl}, incorporating incentive signals into rewards can effectively enhance both the cohesion of multi-agent interactions and the efficacy of multi-agent learning. However, \citet{park2025maporl} rely on adding manually predefined hyperparameter signals to rewards, which are discrete and coarse-grained. In contrast, we introduce a fully automated method that integrates the distributional characteristics of one LLM's reward data into another LLM's reward function. This allows each model to perceive the quality of its peers' answers and generates continuous, fine-grained incentive signals without human intervention.

% For a given input question, $m$ LLMs will sample $n$ times accordingly to produce $m$ groups of answers, with each group containing $n$ responses. We initially employ a reward model to rate these answers and obtain the initial reward $R$ for answers. Then, each group is characterized by its maximum $R_{max}$, minimum $R_{min}$, and average $R_{avg}$ values, collectively representing an LLM's overall answer quality for the question. We perform arithmetic normalization to incorporate these features from other LLMs into the current LLM's initial reward, thus deriving the final reward $\bar{R}$. For example, the $k$-th final reward of LLM $i$ can be computed by:
For a given input question, $m$ LLMs each perform $n$ sampling rounds, producing $m$ groups of responses, with $n$ answers in each group. A reward model is first used to assign initial rewards $R$ to all responses. Each group is then summarized by its maximum ($R_{max}$), minimum ($R_{min}$), and average ($R_{avg}$) scores, collectively reflecting the model's overall answer quality for that question. These statistics are arithmetically normalized and injected into the reward shaping process of peer models, yielding the final reward $\bar{R}$. For example, the $k$-th final reward of LLM $i$ is computed as:
\begin{align}
    \bar{R}_{i,k} = R_{i,k} + \sum_{l=M \setminus \{\mathcal{M}_{i}\} }\text{clip}(\frac{R_{i,k} - R_{l, avg}}{R_{l,max} - R_{l, min}}, -\frac{1}{m-1}, \frac{1}{m-1})
\end{align}
where $\text{clip}(\cdot)$ is a stabilization operation to prevent extreme values in reward shaping. Using these calibrated rewards, we then apply standard GRPO~\citep{shao2024deepseekmath} to train and optimize LLMs.


% \begin{align}
% \mathcal{J}_{GRPO}(\theta) = \frac{1}{G}\sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left\{
% \min\left(
% \frac{\pi_\theta(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,<t})} \hat{A}_{i,t}, \ 
% \text{clip}\left(
% \frac{\pi_\theta(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,<t})}, 
% 1-\epsilon, 1+\epsilon
% \right) \hat{A}_{i,t}
% \right)
% - \beta \mathbb{D}_{KL}[\pi_\theta \| \pi_{ref}]
% \right\}
% \end{align}

% \begin{align}
% \mathcal{J}_{GRPO}(\theta) = \frac{1}{G}\sum_{u=1}^G \frac{1}{|o_u|} \sum_{t=1}^{|o_u|} \left\{ \min\left( \frac{\pi_\theta(o_{u,t}|q,o_{u,<t})}{\pi_{\theta_{old}}(o_{u,t}|q,o_{u,<t})} \hat{A}_{u,t}, \ \right. \right. \nonumber \\
% \left. \left. \text{clip}\left( \frac{\pi_\theta(o_{u,t}|q,o_{u,<t})}{\pi_{\theta_{old}}(o_{u,t}|q,o_{u,<t})}, 1-\epsilon, 1+\epsilon \right) \hat{A}_{u,t} \right) - \beta \mathbb{D}_{KL}[\pi_\theta \| \pi_{ref}] \right\}
% \end{align}
