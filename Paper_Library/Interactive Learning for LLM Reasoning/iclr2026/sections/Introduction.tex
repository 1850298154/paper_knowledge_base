\section{Introduction}
% Efforts to enhance the reasoning capabilities of Large Language Models (LLMs) primarily rely on training paradigms such as Supervised Fine-Tuning (SFT)~\citep{achiam2023gpt}, Direct Preference Optimization (DPO)~\citep{rafailov2023direct}, and Reinforcement Learning (RL)~\citep{schulman2017proximal, guo2025deepseek, zeng2025simplerl}, which enable LLMs to iteratively interact with data, essentially engaging in trial-and-error learning to acquire problem-solving skills. This learning paradigm can be conceptualized as self-learning for LLMs. However, real-world knowledge acquisition is rarely isolated~\citep{bloembergen2015evolutionary, canese2021multi}. Humans continuously engage in knowledge exchange through collaborative learning, such as peer discussions in classroom settings. For human learning, while \textit{single-agent learning} (self-learning) forms the foundation, \textit{multi-agent learning} represents a more advanced and effective paradigm~\citep{zambrano2019effects}. Therefore, a critical question naturally arises: Can multi-agent learning enable LLMs to acquire deeper knowledge and exhibit superior reasoning compared to single-agent approaches?

Efforts to enhance the reasoning capabilities of Large Language Models (LLMs) have largely relied on training paradigms such as Supervised Fine-Tuning (SFT)~\citep{achiam2023gpt}, Preference Learning (PL)~\citep{rafailov2023direct}, and Reinforcement Learning (RL)~\citep{schulman2017proximal, guo2025deepseek, zeng2025simplerl}. 
These methods allow LLMs to iteratively interact with data and refine their behavior, essentially engaging in trial-and-error learning to acquire problem-solving skills, which can be viewed as self-learning for LLMs. 
However, real-world knowledge acquisition is rarely an isolated activity~\citep{bloembergen2015evolutionary, canese2021multi}. Humans continuously exchange knowledge through collaborative learning, as in peer discussions within classroom settings. While \textit{single-agent learning} (self-learning) serves as the foundation of human education, \textit{multi-agent learning} represents a more advanced and often more effective paradigm: multiple learners bring diverse perspectives, challenge each other's reasoning, and provide mutual feedback, ultimately leading to deeper understanding and more robust solutions~\citep {kahveci2007interactive, hsiung2012effectiveness, zambrano2019effects, mende2021individual}. The same principle suggests that multi-agent learning can benefit LLMs\footnote{In our paper, ``Agent'' and ``LLM'' refer to the same entity/concept}: by exposing models to diverse reasoning strategies and peer-based feedback, it may help them overcome individual blind spots and develop stronger problem-solving abilities. 
% This naturally raises a central question: \textit{Can multi-agent learning enable LLMs to acquire deeper knowledge and demonstrate superior reasoning in single-agent settings?}

Recent studies have explored multi-agent learning. For example, MALT~\citep{motwani2024malt} designs a sequential multi-agent system (MAS) consisting of Generator, Verifier, and Refiner agents, each independently trained to sample specialized trajectories. ReMA~\citep{wan2025rema} introduces a hierarchical framework with a high-level agent responsible for problem decomposition and a low-level agent for concrete step implementation, trained alternately to achieve complementary expertise. MAPoRL~\citep{park2025maporl} proposes a Post-Co-Training framework to enhance collaboration alignment through debate. However, during inference, these methods are required to re-execute the MAS to obtain final solutions, a process misaligned with human cognition, where individuals improve reasoning through peer interactions and subsequently solve problems independently.

% While effective at improving inter-agent collaboration, these approaches mainly emphasize strengthening the MAS as a whole, rather than directly improving the independent reasoning abilities of individual LLMs.


\begin{figure}[t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=1.0\linewidth]{iclr2026/figures/background.pdf}
\end{center}
% \vspace{-14pt}
\caption{Conceptual comparison of ILR, existing Multi-agent Learning, and Single-agent Learning. ILR enhances LLMs' independent reasoning ability through Dynamic Interaction and Perception Calibration at training time, and LLMs can resolve questions independently at inference time. 
}
% \vspace{-10pt}
\label{fig: background}
\end{figure}

In this paper, we address this gap by treating each agent as an autonomous entity and investigating whether multi-agent learning can enhance an LLM's individual problem-solving capacity (see Figure~\ref{fig: background}). We propose \textbf{ILR} (\textbf{I}nteractive \textbf{L}earning for LLM \textbf{R}easoning), a co-learning framework consisting of two key components: \textit{Dynamic Interaction} and \textit{Perception Calibration}. 

% Dynamic Interaction simulates human discussion patterns. For ``Dynamic'', when confronted with complex problems, different human individuals tend to cooperate, whereas for simpler problems, they would like to compete to find out whose solution is more efficient and advantageous. To simulate this behavior, an LLM first conducts difficulty measurement of the question by self-ranking, then calculates its probability of independently solving the problem based on  Item Response Theory (IRT)~\citep{benedetto2023survey, cai2016item}. When the probability of independent resolution is low, the LLM opts for cooperation; otherwise, it chooses competition. For ``Interaction'', we design a novel \textbf{Idea3} framework that comprises three sequential stages: Idea Sharing, Idea Analysis, and Idea Fusion, which corresponds to sharing LLM's ideas, analyzing another LLM's ideas, and synthesizing all content to generate novel ideas. 

The \textit{Dynamic Interaction} module simulates human discussion. For ``Dynamic'', when confronted with complex problems, humans tend to cooperate, whereas for simpler problems, they often compete to identify the most efficient solution~\citep{richard2002cooperation,schneider2011cooperation}. To emulate this behavior, an LLM estimates question difficulty through self-ranking and applies Item Response Theory~\citep{cai2016item, benedetto2023survey} to calculate the probability of solving it independently. If the probability is low, the model engages in cooperation; otherwise, it chooses competition. For ``Interaction'', we design a novel \textbf{Idea3} framework, comprising three sequential stages: Idea Sharing (each LLM proposes its own solution), Idea Analysis (each LLM analyzes and reflects on the peer's solution), and Idea Fusion (the insights are synthesized into a refined and potentially novel solution). Following Dynamic Interaction, the \textit{Perception Calibration} module is applied. Prior work~\citep{ma2024coevolving, park2025maporl} has shown that incorporating tailored reward signals can effectively guide LLMs toward better multi-agent learning. Instead of relying on predefined hyperparameters for reward shaping, we introduce a fully automated mechanism that integrates one LLM's reward distribution characteristics, derived from answer group sampling on the same input, into another LLM's reward function. We then employ Group Relative Policy Optimization (GRPO)~\citep{shao2024deepseekmath} to update each LLM based on calibrated rewards. This plug-and-play calibration allows LLMs to perceive the quality of peer-generated solutions and adapt their reasoning accordingly.


% Following Dynamic Interaction, we employ the Group Relative Policy Optimization (GRPO) algorithm~\citep{shao2024deepseekmath} to train each LLM. Previous works~\citep{park2025maporl, ma2024coevolving} indicate that incorporating specific signals in the reward can effectively steer LLMs towards better multi-agent learning. In contrast to their approach of using predefined hyperparameters for reward shaping, our Perception Calibration module utilizes a fully automated arithmetic operation to integrate one LLM's reward distribution characteristics, which can be obtained based on the answer group sampling from the same input question, into another LLM's reward function. Subsequently, each LLM undergoes GRPO training based on these calibrated rewards. Perception Calibration operates as a simple yet effective, plug-and-play component that enables LLMs to perceive the quality of solutions generated by other LLMs in multi-agent learning systems.


% We conduct experiments across two model series with different scales to validate the effectiveness of ILR, including Llama-3.1-8B-Instruct~\citep{dubey2024llama}, Qwen2.5-7B-Instruct~\citep{team2024qwen2}, and Qwen2.5-14B-Instruct~\citep{team2024qwen2}. Given the substantial computational costs associated with multi-agent training, we organize these three models into three pairwise groups. To maintain simplicity in the training, we only adopt MATH~\citep{hendrycks2021measuring} as the training dataset and evaluate performance across six distinct benchmarks. 

We evaluate the effectiveness of ILR on two model families of different scales: Llama-3.1-8B-Instruct~\citep{dubey2024llama}, Qwen2.5-7B-Instruct, and Qwen2.5-14B-Instruct~\citep{team2024qwen2}. For multi-agent training, we pair these three models to form three different groups. Following~\citep{zeng2025simplerl}, we use MATH~\citep{hendrycks2021measuring} as the training dataset and assess performance across five mathematical reasoning benchmarks and a code benchmark. Experimental results demonstrate that ILR consistently outperforms traditional single-agent learning, achieving improvements of up to 5\% over the strongest baseline models. Our investigation further reveals two findings: (1) Idea3 enhances the robustness of stronger LLMs during multi-agent inference scenarios. Analyzing and reflecting on the peer's solutions reduces the probability of being misled by weaker LLMs when exchanging information.
(2) Dynamically determining interaction types can boost the efficacy of multi-agent learning and surpass pure cooperation or competition strategies.
% \qcw{add more detailed explanations and insights when discussing the results. The experimental results demonstrate the superiority of ILR compared to single-agent learning, achieving an average improvement of 2\% points over the strongest baseline across all three models. We further investigate the advantages conferred by the Idea3 interaction framework in multi-agent inference scenarios.} 
Our main contributions are summarized as follows:

% \begin{itemize}
\begin{itemize}[leftmargin=*,labelsep=2mm]
    \item Unlike prior multi-agent learning approaches that primarily focus on improving inter-agent collaboration to strengthen overall system performance, to the best of our knowledge, we are the first to investigate whether multi-agent learning can more effectively enhance an LLM's independent reasoning capability compared to single-agent learning.
    \item Inspired by human interaction, we design a novel multi-agent learning framework ILR with Dynamic Interaction and Perception Calibration. The former adaptively selects cooperation or competition strategies and engages LLMs in Idea3 to gain better solutions. The latter enables LLMs to perceive the peer's performance and enhance the cohesion of multi-agent interactions.
    \item Through extensive experiments and analysis, we demonstrate the effectiveness of ILR over self-learning baselines. We further discover that Idea3 enhances the robustness of advanced LLMs by enabling them to analyze and reflect on the peer's solutions, and dynamic interaction strategies improve multi-agent learning by adapting to question difficulty.
\end{itemize}

