 \section{Experiments}
\subsection{Experimental Setting}
\label{sec: experimental setting}
\textbf{Test Models.}
We conduct experiments on three representative LLMs spanning two series and two scales: Llama-3.1-8B-Instruct~\citep{dubey2024llama}, Qwen2.5-7B-Instruct, and Qwen2.5-14B-Instruct~\citep{team2024qwen2}. To balance training cost with coverage, we organize these models into three pairwise groups and apply ILR training within each:
\begin{itemize}[leftmargin=*,labelsep=2mm]
    \item Group1 (different series, same scale): Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct
    \item Group2 (different series, different scale): Llama-3.1-8B-Instruct and Qwen2.5-14B-Instruct
    \item Group3 (same series, different scale): Qwen2.5-7B-Instruct and Qwen2.5-14B-Instruct
\end{itemize}

\textbf{Benchmarks.}
% For ILR \textbf{training}, we employ ‌MATH‌~\citep{hendrycks2021measuring} as our data source. Excluding the ‌MATH-500~\citep{hendrycks2021measuring}‌ subset as the evaluation set, we randomly select ‌1,000 samples‌ as the validation set to assess the $i$-th LLM's reasoning capabilities $\gamma_{i}$, as illustrated in ‌Section~\ref{sec: dynamic interaction}, while the remaining ‌11,000 samples‌ serve as the training set. For ILR \textbf{evaluation}, we perform comprehensive evaluations across a range of mathematical reasoning benchmarks, which encompass both standard benchmarks, including GSM8K~\citep{cobbe2021training}, MATH-500~\citep{hendrycks2021measuring}, Minerva Math~\citep{lewkowycz2022solving}, and Olympiad Bench~\citep{he2024olympiadbench}, and a competition-level benchmark AIME~\citep{aime}, consisting of AIME2024 and AIME 2025. For out-of-domain assessment, we also evaluate ILR and baselines on a code benchmark MBPP~\citep{austin2021program}.
For ILR \textit{training}, we use the MATH dataset~\citep{hendrycks2021measuring} following~\citep{zeng2025simplerl}. The MATH-500 subset~\citep{hendrycks2021measuring} is reserved for testing. From the remaining data, we randomly select 1,000 samples as the validation set to estimate each LLM's reasoning capability $\gamma_{i}$ (see Section~\ref{sec: dynamic interaction}), while the other 11,000 samples are used for training. For ILR \textbf{evaluation}, we conduct a comprehensive assessment across multiple mathematical reasoning benchmarks, which encompass both standard benchmarks, including GSM8K~\citep{cobbe2021training}, MATH-500~\citep{hendrycks2021measuring}, Minerva Math~\citep{lewkowycz2022solving}, and Olympiad Bench~\citep{he2024olympiadbench}, and a competition-level benchmark AIME24\&25~\citep{aime}, consisting of AIME2024 and AIME 2025. To further assess generalization beyond math, we evaluate ILR and baselines on a code generation benchmark MBPP~\citep{austin2021program}.

\textbf{Baselines.}
We compare our ILR with five single-agent learning (self-learning) baselines, including SFT~\citep{achiam2023gpt}, DPO~\citep{rafailov2023direct}, PPO~\citep{schulman2017proximal}, GRPO~\citep{shao2024deepseekmath, guo2025deepseek}, and Reinforce++~\citep{hu2025reinforce++}, and a multi-agent learning baseline, DebateFT~\citep{subramaniam2025multiagent}. The selected single-agent learning baselines represent widely adopted and empirically effective approaches, with the latter three reinforcement learning algorithms demonstrating exceptional performance in recent LLM training. For multi-agent learning, direct comparisons are challenging because existing methods typically train specialized LLMs with complementary roles for problem-solving. To enable fair evaluation, we introduce a minor modification to Multiagent-FT~\citep{subramaniam2025multiagent}, sampling answers through Debate and optimizing each LLM using the original training algorithm. Each LLM will solve questions independently at the inference stage. We rename this baseline as DebateFT for clarity.
% enabling two LLMs to interact through debate-based data sampling before final optimization using the original training algorithm. 
Implementation details about hyperparameters, training, and evaluation settings are provided in Appendix~\ref{appendix: Implementation Details}.



\subsection{Results and Analysis}




\subsubsection{LLMs Perform Better Through ILR Learning}
\label{sec: llms perform better through ILR learning}
% \textbf{Overall Comparison.} Table~\ref{tab:mian_result} depicts the quantification comparison of ILR and other baselines. From the angle of a single benchmark, ILR achieves optimal performance across all cases except for the Minerva Math with Qwen2.5-7B-Instruct, where it underperforms PPO. From the angle of average performance, ILR demonstrates significant improvement in LLM reasoning ability. For instance, ILR enhances Llama-3.1-8B-Instruct's performance by 3.12\% compared to the strongest baseline GRPO (41.51\% vs. 38.39\%), and the other two LLMs also achieve approximately 1\% improvements over their respective best baselines. These results collectively demonstrate that ILR's multi-agent learning framework effectively enhances the independent problem-solving capabilities of individual LLMs. For out-of-domain evaluation, we compare ILR with two representative baselines, DPO and GRPO, on a code benchmark MBPP. Table~\ref{tab:ood_mbpp} indicates that ILR consistently achieves optimal performance across LLMs, which is aligned with the analysis on mathematical benchmarks.

\textbf{Overall Comparison.} Table~\ref{tab:mian_result} presents the quantitative comparison between ILR and other baselines. At the level of individual benchmarks, ILR achieves the best performance in nearly all cases, with the sole exception of Minerva Math on Qwen2.5-7B-Instruct, where PPO slightly outperforms it. In terms of average performance, ILR yields significant improvement in LLM reasoning ability. For example, Llama-3.1-8B-Instruct trained with ILR improves by 3.12\% over the strongest baseline GRPO (41.51\% vs. 38.39\%), while both Qwen2.5 models also show gains of around 1\% compared to their best-performing baselines. These results demonstrate that ILR's multi-agent learning framework consistently enhances the independent problem-solving capability of individual LLMs. For out-of-domain evaluation, we further compare ILR against two representative baselines, DPO and GRPO, and report the Pass@1 metric on the MBPP code benchmark~\citep{austin2021program}. As shown in Table~\ref{tab:ood_mbpp}, ILR consistently achieves the strongest performance across all models, aligning with our findings on mathematical reasoning benchmarks.

From Table~\ref{tab:mian_result}, we highlight two further insights: (1) \textbf{ILR Promotes Complex Reasoning.} On the competition-level dataset AIME24\&25 (comprising AIME24 and AIME25), ILR significantly improves LLMs' capability in solving complex problems. For example, Llama-3.1-8B-Instruct with ILR doubles the performance of the best baseline GRPO (10.00\% vs. 5.00\%), while Qwen2.5-14B-Instruct improves by 3.33\% over its strongest baseline (23.33\% vs. 20.00\%). These results demonstrate that both weaker and stronger models can benefit from ILR's multi-agent learning framework, enabling them to independently tackle challenging reasoning tasks.
% (2) \textbf{Group with More Similar Reasoning Ability Yields Better Results.} For each LLM, we conduct ILR under two distinct groups (as detailed in Section~\ref{sec: experimental setting}). By comparing each LLM's performance across different groups, we consistently observe that LLMs exhibit superior performance in groups with more similar initial reasoning ability. For instance, Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct have closer initial reasoning capability, and for the former, ILR-Group1 achieves better results compared to ILR-Group2 (41.51\% vs. 41.10\%). We attribute this phenomenon to the fact that excessive initial performance disparities may lead to imbalanced interactions where the stronger LLM overwhelmingly dominates the entire process, thereby compromising the quality of Dynamic Interaction. While this pattern consistently emerges in our experimental setting, the performance differences between groups are marginal. Consequently, further empirical experiment is warranted to validate this finding, which we encourage future research to investigate.
(2) \textbf{Balanced Grouping Improves Learning.} Each LLM is trained with ILR under two different grouping settings (see Section~\ref{sec: experimental setting}). Across comparisons, models achieve stronger results when paired with peers of more similar initial reasoning ability. For example, Llama-3.1-8B-Instruct performs better in ILR-Group1 with Qwen2.5-7B-Instruct (41.51\% vs. 41.10\% in Group2), as the two models are closer in initial capability. We attribute this phenomenon to the fact that excessive initial performance disparities may lead to imbalanced interactions where the stronger LLM overwhelmingly dominates the entire process, thereby compromising the quality of Dynamic Interaction. While this pattern consistently emerges, the observed performance differences are modest. We leave more comprehensive empirical validation of this finding to future research.


\textbf{Ablation Study.} To further analyze contributions of each component, we conduct an ablation study and report the average accuracy in Table~\ref{tab: ablation study}. Removing either component leads to consistent performance drops across all models, underscoring their joint contribution to the overall performance.
% In addition, we also observe that for Qwen2.5-7B-Instruct, Perception Calibration yields better performance, whereas Dynamic Interaction is more effective for Qwen2.5-14B-Instruct. This indicates that Perception Calibration and Dynamic Interaction both contribute equally to ILR.

\begin{table}[]
\caption{The quantification comparison (accuracy \%) of ILR and other baselines. ILR consistently outperforms all baselines, including single- and multi-agent learning, across all models.}
\label{tab:mian_result}
% \resizebox{\textwidth}{!}{%
\begin{center}
\begin{tabular}{@{}l|cccccc@{}}
\toprule
 & GSM8K & MATH-500  & \begin{tabular}[c]{@{}c@{}}Minerva\\ Math\end{tabular}& \begin{tabular}[c]{@{}c@{}}Olympiad\\ Bench\end{tabular} & \begin{tabular}[c]{@{}c@{}}AIME\\ 24\&25\end{tabular}  & Avg \\ \midrule
Llama-3.1-8B-Instruct &82.87&49.80&22.79&13.63&1.67&34.15 \\
SFT &85.37&51.80&25.74&16.44&3.33&36.54\\
DPO &86.66&50.60&23.16&17.63&3.33&36.28\\ 
PPO &87.34&53.60&29.41&19.56&1.67&38.32\\
%grpo_kl1e-3_lr1e-6_l20.0_temp0.5_eps0.2_k2_estimator
%kl1e-4, lr
GRPO &85.60&54.00&26.47&20.89&5.00&38.39\\
Reinforce++ &86.88&51.40&29.78&18.52&3.33&37.98\\ 
DebateFT-Group1 &86.66&50.20&24.63&17.63&1.67&36.16\\
DebateFT-Group2 &85.14&51.80&25.37&18.07&3.33&36.74\\ 
\rowcolor{lightgreen}
ILR-Group1 &\textbf{89.39}&\textbf{55.80}&30.15&22.22&\textbf{10.00}&\textbf{41.51}\\
\rowcolor{lightgreen}
ILR-Group2 &87.26&55.20&\textbf{33.82}&\textbf{22.52}&6.67&41.10\\ \midrule

Qwen2.5-7B-Instruct &92.34&75.60&41.54&37.63&11.67&51.76\\
SFT &92.42&76.60&41.91&39.26&13.33&52.70\\
DPO &92.49&76.20&44.12&37.78&13.33&52.78\\
PPO &92.72&\textbf{78.00}&\textbf{45.59}&39.11&13.33&53.75\\
GRPO&92.65&77.40&43.38&37.78&16.67&53.58\\
Reinforce++ &92.49&77.80&43.38&38.81&15.00&53.50\\
DebateFT-Group1 &92.34&76.80&42.65&38.96&11.67&52.48\\
DebateFT-Group3 &92.57&76.20&44.12&38.22&13.33&52.89\\ 
\rowcolor{lightgreen}
ILR-Group1 &\textbf{93.40}&77.60&43.01&\textbf{39.85}&\textbf{18.33}&54.44\\
\rowcolor{lightgreen}
ILR-Group3 &92.65&\textbf{78.00}&45.01&38.96&\textbf{18.33}&\textbf{54.59} \\ \midrule

Qwen2.5-14B-Instruct &94.84&81.20&47.43&41.04&13.33&55.57\\
SFT &95.07&81.60&47.79&42.52&16.67&56.73\\
DPO &95.15&81.20&50.37&42.96&13.33&56.60\\
PPO &\textbf{95.53}&82.40&50.00&43.41&20.00&58.27\\
GRPO &94.92&81.40&48.16&43.26&20.00&57.55\\
Reinforce++ &95.15&81.40&48.90&43.85&20.00&57.86\\
DebateFT-Group2 &95.00&81.40&47.43&41.48&13.33&55.73\\
DebateFT-Group3 &95.00&81.20&49.63&42.96&15.00&56.76\\ 
\rowcolor{lightgreen}
ILR-Group2 &\textbf{95.53}&81.80&50.37&43.70&\textbf{23.33}&58.95\\
\rowcolor{lightgreen}
ILR-Group3 &95.30&\textbf{82.60}&\textbf{51.10}&\textbf{44.15}&\textbf{23.33}&\textbf{59.30} \\ \bottomrule
\end{tabular}%
% }
\end{center}
% \vspace{-10pt}
\end{table}


\begin{table}[]
\caption{Out-of-domain evaluation of ILR, DPO, and GRPO on MBPP (Pass@1). G$i$ means Group$i$. Compared with representative baselines, ILR further improves the coding ability of models.}
\label{tab:ood_mbpp}
\begin{center}
% \resizebox{\textwidth}{!}{%
\begin{tabular}{@{}l|cccccc@{}}
\toprule
Model & Base & DPO  & GRPO& ILR-G1 & ILR-G2  & ILR-G3 \\ \midrule
Llama-3.1-8B-Instruct &54.00&56.40&56.80&57.40&\textbf{57.60}&- \\ 
Qwen2.5-7B-Instruct &64.80&65.20&65.20&65.60&-&\textbf{66.20}\\ 
Qwen2.5-14B-Instruct &71.40&71.60&71.80&-&\textbf{72.40}&71.60\\ \bottomrule
\end{tabular}%
% }
\end{center}
% \vspace{-10pt}
\end{table}

\begin{table}[]
\caption{Ablation Study of ILR. We report the average accuracy (\%) of five evaluation benchmarks. DI, PC represent Dynamic Interaction and Perception Calibration.}
\label{tab: ablation study}
% \resizebox{\textwidth}{!}{%
\begin{center}
\begin{tabular}{@{}l|cc|cc|cc@{}}
\toprule
& \multicolumn{2}{c|}{Llama-3.1-8B-Instruct} & \multicolumn{2}{c|}{Qwen2.5-7B-Instruct} & \multicolumn{2}{c}{Qwen2.5-14B-Instruct} \\
 &Group1 & Group2 & Group1 & Group3 & Group2 & Group3 \\ \midrule
ILR &\textbf{41.51}&\textbf{41.10}&\textbf{54.44}&\textbf{54.59}&\textbf{58.95}&\textbf{59.30}\\
DI-only &39.12&39.25&53.95&53.23&58.04&58.57\\
PC-only &40.14&38.41&54.04&53.91&57.66&58.07\\
\bottomrule
\end{tabular}%
% }
\end{center}
% \vspace{-10pt}
\end{table}



\subsubsection{Dynamic Interaction Enhances Robustness of Stronger LLMs}
In Section~\ref{sec: llms perform better through ILR learning}, we demonstrate how multi-agent learning through ILR training strengthens the independent reasoning abilities of individual LLMs. Here, we evaluate the effectiveness of Idea3 communication during the inference stage. In pure inference scenarios where ground-truth labels are unavailable, we employ a summarization prompt to synthesize the initial and updated responses, thereby mitigating noise from multi-agent interactions. Specifically, for a given input question, two LLMs first engage in Idea3 communication, after which each model evaluates both its own initial answer and the updated answer to produce a final prediction. For comparison, we also include a Debate paradigm, which treats other agents' outputs as additional advice to inform final answer generation. Full prompt details are provided in Appendix~\ref{appendix: prompt details}.

% Table~\ref{tab:multi-agent_inference} shows the multi-agent inference result on MATH-500. We observe that within different groups, Debate is more beneficial for weaker LLMs, while Idea3 enhances the robustness of stronger LLMs, making them less susceptible to being misled by low-quality answers from weaker LLMs during multi-agent communication. We attribute this phenomenon to two primary reasons: \textbf{First}, for weaker LLMs, Debate directly incorporates stronger models' answers as additional guidance, encouraging weaker LLMs to utilize these responses to formalize their final answers, which often results in a more significant improvement. \textbf{Second}, for stronger LLMs, Debate similarly compels them to consider the answers from weaker LLMs, which are usually lower-quality answers and potentially lead to a deterioration in performance. In contrast, Idea3 requires stronger LLMs to critically analyze the weaker or to identify useful components of LLMs' answers, thus mitigating the introduction of noise and enhancing robustness.

Table~\ref{tab:multi-agent_inference} shows the multi-agent inference result on MATH-500. We consistently observe that within different groups, Debate is more beneficial for weaker LLMs, while our Idea3 enhances the robustness of stronger LLMs by making them less susceptible to low-quality responses generated from weaker LLMs during multi-agent communication. We attribute this phenomenon to two primary reasons: \textbf{First}, for weaker LLMs, Debate directly incorporates stronger models' answers as additional guidance, enabling them to refine their outputs, which often results in a more significant improvement. \textbf{Second}, for stronger LLMs, Debate similarly compels them to consider answers from weaker LLMs, which are usually lower in quality and can potentially degrade performance. In contrast, our Idea3 prompts stronger LLMs to critically evaluate and selectively integrate peer contributions, filtering out noise and thereby improving robustness.

\begin{table}[]
\caption{Multi-agent inference results on MATH-500. `Single' denotes the single-agent inference performance of the base models, and * indicates the stronger LLM within each group. For Debate and Idea3, the better-performing result is highlighted in bold.}
\label{tab:multi-agent_inference}
% \resizebox{\textwidth}{!}{%
\begin{center}
\begin{tabular}{@{}l|cc|cc|cc@{}}
\toprule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Inference\\ Paradigm\end{tabular}}& \multicolumn{2}{c|}{Llama-3.1-8B-Instruct} & \multicolumn{2}{c|}{Qwen2.5-7B-Instruct} & \multicolumn{2}{c}{Qwen2.5-14B-Instruct} \\
 &Group1 & Group2 & Group1* & Group3 & Group2* & Group3* \\ \midrule
Single &49.80&49.80&75.60&75.60&81.20&81.20\\
Debate &\textbf{64.00}&\textbf{66.20}&74.60&\textbf{80.00}&79.20&81.00\\
Idea3 (Ours) &63.40&62.00&\textbf{75.60}&77.80&\textbf{79.80}&\textbf{82.00}\\
\bottomrule
\end{tabular}%
% }
\end{center}
\vspace{-14pt}
\end{table}




\subsubsection{Effect of Interactive Type (Cooperation or Competition)}
\begin{wrapfigure}{l}{0cm}
\centering
\includegraphics[width=0.483\textwidth]{iclr2026/figures/ratio.pdf}
% \vspace{-10pt}
\caption{Average accuracy of Group1 under varying cooperation ratios. IRT is marked in red.}
% \vspace{-10pt}
\label{fig: different proportion of cooperation}
\end{wrapfigure}

In ILR training, we employ Item Response Theory (IRT) to dynamically determine interaction types, i.e., cooperation or competition. To further investigate the influence of cooperation, we vary the cooperation ratio ($p$) from 0.0 to 1.0 in increments of 0.2. Here, $p=0.0$ corresponds to full competition, $p=1.0$ to full cooperation, and intermediate values designate the first $p$-proportion of questions (ranked by difficulty) as cooperative, with the remainder treated competitively. Due to training costs, we restrict this study to Group1.

% Figure~\ref{fig: different proportion of cooperation} illustrates the performance trends, and the IRT result is highlighted in red. Two key findings emerge regarding dynamic interaction design: (1) \textbf{Suboptimality of Extreme Strategies}‌: Pure competition or cooperation proves suboptimal for ILR, underscoring the necessity of adaptive interaction when designing multi-agent learning. We attribute this as reasonable because for challenging problems, cooperation enables LLMs to complement each other's strengths and generate comprehensive solutions, while LLMs can yield advanced answers for overly simplistic tasks without cooperative emphasis. (2) \textbf{Configuration for $p$}: The optimal cooperation ratio $p$ demands meticulous design, either split additional subsets to manually adjust $p$, or use some approximate calculations like IRT to fit $p$. Although IRT's result is not optimal for Qwen2.5-7B-Instruct, it is acceptable considering the noise introduced in the approximation of question difficulty. Moreover, this approach significantly removes manual intervention requirements. This validates IRT's feasibility in integrating problem difficulty with LLM reasoning capabilities. Future research could investigate incorporating additional conditional parameters into IRT to obtain more robust results.

Figure~\ref{fig: different proportion of cooperation} shows the results, with IRT highlighted in red. Two key findings emerge for dynamic interaction design:
(1) \textbf{Suboptimality of Extreme Strategies.} Relying solely on competition or cooperation is suboptimal for ILR, underscoring the necessity of adaptive interaction in multi-agent learning. This is intuitive: for challenging problems, cooperation allows LLMs to leverage complementary strengths and produce more comprehensive solutions, whereas for simpler tasks that can be effectively solved independently, excessive cooperation provides little benefit and may even introduce noise into the final outputs.
(2) \textbf{Configuration of $p$.} The optimal cooperation ratio $p$ requires careful design. One option is to manually partition data into subsets and tune $p$, but this is costly. IRT offers a practical alternative by approximating problem difficulty and aligning it with model reasoning capability. Although not always optimal (e.g., for Qwen2.5-7B-Instruct), it achieves competitive results while eliminating manual intervention. This demonstrates the feasibility of IRT as a principled mechanism for integrating problem difficulty with LLM reasoning abilities. Future work may enhance robustness by incorporating additional conditional parameters into the IRT formulation.