\section{Conclusion}
% In this paper, we investigate whether interactive learning among multiple LLMs (\textit{multi-agent learning}) can achieve superior performance compared to self-learning (\textit{single-agent learning}). Unlike previous multi-agent learning approaches that conceptualize agents as complementary roles within a MAS to optimize overall system performance, we treat each agent as an independent entity and seek to optimize the capabilities of individual agents. Inspired by real-world human interaction, we propose ILR comprising two key components: Dynamic Interaction and Perception Calibration. The former first dynamically selects cooperation or competition modes based on the question's difficulty, and then lets LLMs exchange information via the Idea3 interaction (Idea Sharing, Idea Analysis, and Idea Fusion). The latter incorporates automated incentive signals into reward computations, enabling LLMs to perceive the quality of peers' answers and enhancing multi-agent learning. Experiments across different model series and scales demonstrate the efficacy of ILR, showing that interactive learning in LLMs yields greater performance improvements than self-learning. We further discover that Idea3 improves the robustness of advanced LLMs, and dynamic interaction types outperform pure cooperative or competitive strategies. These findings align with human learning patterns and provide insights into analyzing high-level human-like behaviors of LLMs.


In this paper, we investigate whether interactive learning among multiple LLMs (\textit{multi-agent learning}) can outperform traditional self-learning (\textit{single-agent learning}). Unlike prior multi-agent learning approaches that assign complementary roles to agents within a MAS to maximize system-level performance, we treat each agent as an independent problem solver and aim to enhance individual capabilities. Inspired by real-world human interaction, we propose ILR, a novel framework built on two key components: Dynamic Interaction and Perception Calibration. Dynamic Interaction adaptively selects between cooperation and competition based on question difficulty and model ability, then enables information exchange among LLMs through the Idea3 paradigm (Idea Sharing, Idea Analysis, and Idea Fusion). Perception Calibration incorporates automated incentive signals into reward computation, allowing LLMs to perceive the quality of peers' answers and thereby strengthening multi-agent learning. Extensive experiments across different model series and scales demonstrate the effectiveness of ILR, showing that interactive learning consistently yields greater performance improvements than self-learning. We further discover that Idea3 improves the robustness of stronger LLMs, while dynamic interaction outperforms purely cooperative or competitive strategies. These findings align with human learning patterns and provide valuable insights into analyzing high-level, human-like behaviors in LLMs.

\section{Ethics statement}
This study is conducted in strict accordance with the ethical guidelines outlined in the ICLR Code of Ethics and adheres to the principles of responsible AI research. Our research investigates whether multi-agent learning can enhance an LLM's individual problem-solving capacity through the proposed ILR (\textbf{I}nteractive \textbf{L}earning with \textbf{R}easoning) framework with Dynamic Interaction and Perception Calibration, which is inspired by human discussion patterns.

The research methodology is designed with ethical considerations, including rigorous bias mitigation protocols and fairness assessments throughout the experimental pipeline. For example, no content within our ILR is designed to target or disadvantage any demographic group, and we actively avoid discrimination by concentrating on both standard mathematical, competition-level mathematical, and code benchmarks that provide balanced coverage.

Importantly, this research does not involve any personal data or sensitive information. All training and evaluation benchmarks utilized in this study, such as GSM8K, MATH-500, Minerva Math, Olympiad Bench, AIME24\&25, and MBPP, are publicly accessible, widely recognized within the AI community, and devoid of personally identifiable information. Data usage strictly adheres to the respective licenses governing these datasets. All research outputs, including code, data processing pipelines, and experimental results, are maintained with the highest standards of scientific integrity, transparency, and reproducibility in mind.

\section{Reproducibility Statement}
Our experiment is based on the open-source framework OpenRLHF~\citep{hu2024openrlhf}. We attach our code, training data with a continuous difficulty level, and a README.md file with step-by-step instructions as supplementary materials to facilitate reproducibility. We also provide detailed information, such as self-ranking prompt, format prompt, Idea3 prompt, and evaluation prompt, in Appendix~\ref{appendix: prompt details}. Additionally, we clarify some important hyperparameters, training, and evaluation settings like approximation time and GPUs in Appendix~\ref{appendix: Implementation Details}. 

