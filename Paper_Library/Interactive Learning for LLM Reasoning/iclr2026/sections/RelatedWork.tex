 \section{Related Work}
Multi-agent learning~\citep{busoniu2006multi, han2024llm, li2024survey} first requires designing a multi-agent system that defines interaction paradigms among multiple agents, such as equi-level~\citep{chan2023chateval}, hierarchical~\citep{gronauer2022multi}, or nested structures~\citep{zhao2025sirius}. Then, within this architectural framework, distinct agents engage in interactive sampling to acquire experience, which subsequently undergoes optimization through learning algorithms. Therefore, we systematically review prior works from the following two perspectives: Multi-Agent Communication (interactive paradigms) and Multi-Agent Training (optimization methods).

\subsection{Multi-Agent Communication}
Traditionally, researchers employ recurrent neural networks (RNNs) as agents and utilize attention mechanisms to facilitate communication~\citep{yu2019review, ding2024learning, sun2024t2mac}. For instance, TarMAC leverages multi-head attention to enable agents to learn both message content and targeted recipient~\citep{das2019tarmac}. After the emergence of LLMs, researchers develop numerous explainable prompt-based multi-agent communication. Notable examples include Debate, where multiple agents articulate arguments culminating in a final answer through majority voting mechanisms~\citep{liang2023encouraging}, and Actor-Critic, where actor agents generate solutions subsequently evaluated by critic agents through iterative feedback processes~\citep{shinn2023reflexion, estornell2024acc, yuan2025reinforce}. SiriuS introduces role specialization, assigning agents different professional identities (e.g., physicists, mathematicians) to sequentially solve problems while maintaining correct responses for fine-tuning and augmenting incorrect ones via feedback, regeneration, and rephrasing~\citep{zhao2025sirius}. However, existing communication paradigms collectively conceptualize individual agents as components with optimization objectives centered on MAS performance. 

In contrast, our work conceptualizes each agent as an independent entity and examines whether multi-agent interactions can enhance individual reasoning abilities. We emulate human discussion dynamics through a novel Idea3 interaction, specifically designed to facilitate critical thinking communication among agents via its three-stage process: Idea Sharing, Idea Analysis, and Idea Fusion.

\subsection{Multi-Agent Training}
Conventional multi-agent training typically trains agents independently without awareness of other agents' states. Researchers employ multi-agent inference to collectively sample experiences, subsequently applying SFT or DPO to update individual agents independently, e.g., MALT~\citep{motwani2024malt}, Multiagent-FT~\citep{subramaniam2025multiagent}, and DEBATUNE~\citep{li2024can}. However, this static one-time sampling fundamentally compromises the dynamic nature of multi-agent interactions, wherein agents should engage in real time. To address this limitation, recent advances in Multi-Agent Reinforcement Learning (MARL) have enabled continuous, real-time interaction sampling among agents~\citep{ma2024coevolving, chen2025improving, liao2025marft}. For example, \citet{liu2025llm} introduce Multi-Agent GRPO for training LLMs in multi-turn conversational settings. ReMA~\citep{wan2025rema} proposes a turn-level ratio mechanism to alternately train high- and low-tier agents with multi-turn GRPO. MAPoRL~\citep{park2025maporl} implements a multi-agent proximal policy optimization algorithm, defining the agent state as the concatenation of interaction histories and incorporating manually predefined hyperparameters into rewards to incentivize collaboration.

Building on their design, we extend GRPO by introducing a fully automated reward calibration, where we incorporate distributional characteristics of rewards derived from agents' responses to the same question, enabling automatic peer perception without the need for manual intervention.