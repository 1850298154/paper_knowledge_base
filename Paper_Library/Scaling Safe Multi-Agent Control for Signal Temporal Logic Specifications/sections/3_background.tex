\section{Background}
% \ab{WRITE 1/2 LINES HERE INTRODUCING THIS SECTION}
\paragraph{Multi Agent Systems with Partial Observability}

We represent a multi-agent system with $N$ agents $\{1, 2, \ldots N \}$ where each agent has a state dimension $n$ and action dimension $m$. Each agent has its own state $\state[t]{i} \in \mathcal{S}_i \subset \mathcal{R}^n$, can take an action $\action[t]{i} \in \mathcal{U}_i \subset \mathcal{R}^m$, and the collective behavior of the agents is governed by a dynamics function $\state[t+1]{i} = f_i(\state[t]{i}, \action[t]{i})$. 
For simplicity, we assume all agents have the same
dynamics function $f_i = f$, 
state space $\mathcal{S}_i = \mathcal{S}$,
and action space $\mathcal{U}_i = \mathcal{U}$.
A trajectory $\tau$ is a sequence of states $\tau = (\sbar[0], \sbar[1], \ldots, \sbar[T_h])$
where $T_h$ is the time horizon,  
$\sbar[t] = (s_1(t), \ldots, s_N(t))$, $\ubar[t] = (u_1(t), \ldots, u_N(t))$ and a policy $\pi_i$ is a function that maps the state of agent $i$ to an action $u_i = \pi_i(s_i)$. The state of the system is partially observable, meaning that each agent can only observe its own state and the states of other agents within its sensing range.

\paragraph{Signal Temporal Logic}
\label{bg-stl}
Signal Temporal Logic (STL) integrates both first-order logic and time-dependent modifications of linear temporal logic operators. The essential logical operators include $\land$ (and), $\lnot$ (not), $\lor$ (or), and $\implies$ (implies). Time-dependent operators are $\E{a}{b}$ (eventually between times $a$ and $b$), $\G{a}{b}$ (globally between times $a$ and $b$), and $\U{a}{b}$ (until between times $a$ and $b$). STL formulas are defined as:
\[
\phi := \mathcal{P} \mid \lnot \phi \mid \phi \land \psi \mid \phi \lor \psi \mid \phi \implies \psi 
        %  \mid \X\phi % No next operator in our work
		 \mid \E{a}{b} \phi \mid \G{a}{b} \phi \mid \phi \U{a}{b} \psi,
\]
where $\mathcal{P}$ is a predicate function mapping states to real values. Quantitative semantics \citep{maler2004monitoring, LeungArechigaEtAl2021} of STL evaluate a robustness value, $\rho(\phi, \tau)$, which measures how strongly a state trace $\tau$ satisfies or violates $\phi$. This robustness metric is differentiable, allowing for direct optimization of STL formulas through differentiable planners like neural networks. 

% Signal Temporal Logic (STL) includes first-order logic operators $\land$ (and), $\lnot$ (not), $\lor$ (or), $\implies$ (implies) and extends Linear temporal logic operators $\X$ (next), $\LTLE$ (eventually), $\LTLG$ (globally), $\LTLU$ (until) and into time-dependant versions $\E{a}{b}$ (eventually in time $a$ and $b$), $\G{a}{b}$ (globally between times $a$ and $b$), $\U{a}{b}$ (until in time $a$ and $b$). The syntax of STL is recursively defined via the following grammar:
% \begin{equation}
%     \begin{aligned}
%         \phi := & \mathcal{P} \mid \lnot \phi \mid \phi \land \psi \mid \phi \lor \psi \mid \phi \implies \psi 
%                  \mid \X\phi \mid \E{a}{b} \phi \mid \G{a}{b} \phi \mid \phi \U{a}{b} \psi, 
%     \end{aligned}
% \end{equation}
% where $\mathcal{P}: \mathbb{R}^n \rightarrow \mathbb{R}$ represents a predicate function that maps a state to a real value, the quantitative semantics of STL evaluate the robustness value ($\rho(\phi, \tau): \Phi \times \mathcal{S}^T \rightarrow \mathbb{R}$) of an STL formula based on a state trace $\tau \in \mathcal{S}^T$ \citep{maler2004monitoring, LeungArechigaEtAl2021}, with $T$ representing the length of the trace $\tau$. The full set of quantitative semantics, detailed in the appendix, systematically maps any STL formula and its corresponding traces to a robustness value. The quantitative semantics is fully differentiable. Given differentiable predicates $\mathcal{P}$, one can directly define a differentiable objective function with the full expressivity of STL \citep{LeungArechigaEtAl2021}. By maximizing the objective function (robustness value), one can optimize on the trace $\tau$, or any differentiable planner generating $\tau$ (e.g., a neural network planner) directly.  

%  Typically, a robustness value greater than zero ($\rho(\phi, \tau) > 0$) indicates that the trace $\tau$ satisfies the STL formula $\phi$. Conversely, a value less than zero ($\rho(\phi, \tau) < 0$) suggests that $\tau$ does not satisfy $\phi$. For instance, consider the STL formula $\phi = \LTLE \mathtt{reach_goal}$ (eventually reach a goal): if path $\tau_1$ includes a state where the goal is reached, then $\rho(\phi, \tau_1) > 0$. If path $\tau_2$ never reaches the goal, then $\rho(\phi, \tau_2) < 0$. T

% \joe{define value on a trajectory in appendix}

\paragraph{Multi-agent Specification}
% \joe{No OR specs among agents, only individual specs and satisfying MA constraints. Remove \mastl~defn.}
With regards to multi-agent systems with $N$ agents, the \mastl~specification $\Psi$ is composed from $N$ individual STL specifications 
% logical And over all pi specs
$\bigwedge_{i=1}^N \phi_i$  where 
$\phi_i$ associates an STL specification for a single agent with index $i$.  
The \mastl~specification $\Psi$ is satisfied if all individual STL specifications are satisfied and the agents do not collide.

% We consider the case where the agents are independent and do not have to satisfy global coordination or synchronization of objectives and all agents are required to satisfy the same STL specification.

% Similar to \citet{Sun2022}, \mastl~is merely a subset of all STL specifications defined on the joint state space yet captures many intended multi-agent behaviors.


% \zk{Introductory Example}
% \subsection{Planning for STL Specifications}
% \joe{Cover relevant ICRA24 work, MILP tools, plan formats generated}
% % 
% \zk{What is plan, what is output of planner}

\paragraph{Graphical Representation for Multi-Agent Systems}
\label{sec:bg-gnn}
% \citep{yu2022learning}
% \zk{Introduce notation, GNN functions}

Graph Neural Networks (GNNs) are adept at modeling multi-agent systems by representing agents and obstacles as vertices within a graph \( \graph = (\vertices, \edges) \). Each vertex  \( \vertice{} \in \vertices = \vertices_a \cup \vertices_o \) corresponds to either an agent ($\vertices_a$) or a static obstacle ($\vertices_o$). Edges \( E \) encapsulate direct interactions between vertices, with specific emphasis on agent-to-agent and agent-to-obstacle connections. We adopt a distance-based adjacency criterion where an edge \( (\vertice{i}, \vertice{j}) \in \edges \) 
exists if the Euclidean distance between vertices \( \vertice{i} \) and \( \vertice{j} \) does not exceed a predefined threshold \( R \), for capturing the local topology of agents within this range \citep{zhang_neural_2023}.
A GNN processes the graph to produce a global embedding representing the collective state of the system. This global state is further processed through specialized readout functions \( r_i \),  tailored to extract and map the global embedding to a specific set of actions \( \action{i} \) for each agent \citep{yu2022learning, zhang_gcbf_2024, zhang_neural_2023}. 

\paragraph{Barrier Certificates}

Barrier certificates \citep{ames_bf_2017} are a useful technique to avoid robot collisions in MA systems \citep{wang_safety_2017} by forcing the state of the entire system to stay within the 
safe region. For a state space $\mathcal{S}\subset \mathbb{R}^n$, let $\mathcal{S}_u\subset\mathcal{S}$ be the unsafe set and $\mathcal{S}_s=\mathcal{S}\backslash\mathcal{S}_u$ the safe set, which contains the set of initial conditions $S_0 \subset S_s$. Also, define the space of control actions as $\mathcal{U} \subset \mathbb{R}^m$. For a dynamic system  $\dot{s}(t) = f(s(t), u(t))$, a control barrier function $h: \mathbb{R}^n \mapsto \mathbb{R}$ satisfies:
% \begin{equation}\label{eq:CBF}
%   \begin{aligned}
% &\forall~ s \in {\mathcal{S}_0},&h(s) \ge 0\\
% &\forall~ s \in {\mathcal{S}_d},&h(s) < 0\\
% &\forall~ s \in \left\{ {s\mid h(s) \ge 0} \right\}, &\nabla_s h \cdot f(s, u) + \alpha \left( h \right) \ge 0
% \end{aligned}
% \end{equation}
% \joe{May remove in favor of GCBF definition} 
\small
\begin{equation}
	\begin{aligned}
		h(s) &\ge 0 && \forall s \in \mathcal{S}_0, \
		h(s) &< 0 && \forall s \in \mathcal{S}_u, \
		\nabla_s h \cdot f(s,u) + \alpha(h(s)) &\ge 0 && \forall s \in {s \mid h(s) \ge 0}.
	\end{aligned}
\end{equation}
\normalsize
For a control policy ( $\pi: \mathcal{S} \to \mathcal{U}$ ) and CBF ($h$), if ($s(0) \in {s \mid h(s) \ge 0}$) and the above conditions are satisfied with ($u = \pi(x)$), then ($s(t) \in {s \mid h(s) \ge 0}$) for all $t \in [0, \infty)$. This implies that the state never enters the unsafe set ($\mathcal{S}_u$) under $\pi$ (see \cite{ames_bf_2017}).

Learning-based approaches for barrier certificates \citep{qin2021learning, yu2022learning, zhang_neural_2023, zhang_gcbf_2024} have been shown to scale in the number of agents beyond existing methods for known systems. 
Notably, a graphical perspective of the agents and their interactions can be used to model the system in a scalable manner (Sec. \ref{sec:app-gcbf+}).

% \subsection{Reinforcement Learning}
% \subsection{Goal-Conditioned State Space}

% \joe{Add single line explanation of goal conditioned controller, Consider moving gcbf+ defn from approach to background}
