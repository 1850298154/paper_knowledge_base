\section{Introduction}

Learning-based methods have shown promise in multi-agent systems (MAS) for tasks such as collision avoidance, path planning, and task allocation \citep{garg_learning_2023, Huang_Koenig_Dilkina_2021, 9366340_Damani_Luo_Wenzel_Sartoretti_2021_primal2,  10246420_li2023task}.
Extensions have also been developed to handle complex temporal tasks that may be described using 
% natural language \citep{min2022film, chen2023autotamp} or 
formal languages such as Signal Temporal Logic (STL) \citep{wang2023multi,forsberg2024multiagent} 
and other temporal logics \citep{pmlr-v168-zhang22b, hammond_multi-agent_2021, Eappen2022}; unfortunately, these methods have well-known limitations 
in terms of scalability and performance.

Signal Temporal Logic (STL) is a formal language for specifying complex temporal tasks that can be used to describe the behavior of agents in a multi-agent system.
In many settings, including autonomous vehicles \citep{Tuncali2019RequirementsDrivenTG},  drones \citep{Pant2018FlybyLogicCO}, and robotic swarms \citep{Yan2019SwarmST}, it is essential to ensure that the agents satisfy complex temporal tasks such as sequentially visiting a series of locations while avoiding collisions with each other and the environment.
Once the user has specified the task in STL, the task can be synthesized using formal methods \citep{maler2004monitoring, KR2021-30-Rational-Verification-for-Probabilistic-Systems, 10.1016/j.automatica.2016.04.006} in certain environments; however, these methods often struggle to scale to 
% large numbers of agents as well as
complex specifications and environments.
% which necessitates the use of optimization-based planners.
% sample-based or learning-based methods.
% \zk{The flow here is a bit strange. At the end of the last paragraph, it talks about "necessitates the use of sample-based or learning-based methods", so my expectation was that we would introduce these methods here, but MILP is an optimization-based approach.}
In response to these challenges, 
% \zk{If these challenges refer to scalability, we should not mention MILP here, because it does not solve the scalability issue. }
% , \zk{MILP appeared in the abstract before this} 
Mixed Integer Linear Programming (MILP)-based planners \citep{Sun2022, kurtz2022mixed} have been developed that can be used to plan over a range of STL specifications but still encounter difficulties with collision avoidance when a modest number such as 5 agents 
% \zk{We also only show 5 in the real drone experiment, can we just remove "such as 5"? Also should we say " difficulties with collision avoidance", because MILP also has limitations in solving the complex spec. } 
are considered (Table \ref{tab:disj_time_or_space}).

Inspired by recent progress in learning-based planners \citep{das2023odesolvers,xiong2023colearning,nawaz2024learning}, we propose a novel approach to planning for multi-agent systems with STL specifications 
that can scale beyond these limitations 
demonstrated on up to 32 agents.  
%\ab{THIS IS TOO BROAD OF A STATEMENT, GIVE SPECIFIC METRICS EVEN IN SIMPLE WORDS WHERE YOU ARE BETTER; IT'S MORE CONFUSING BECAUSE THE LAST EXAMPLE YOU GAVE IN THE PREVIOUS LINE IS A POSITIVE STATEMENT ABOUT THAT METHOD}
% \zk{It looks like the scalability is the only limitation. Should we be more specific about what are ``these limitations''?}
%of existing Mixed Integer Linear Programming (MILP)-based \citep{Sun2022, kurtz2022mixed} planners (Sec. \ref{sec:ps-stl}) which are exacerbated in the multi-agent (MA) setting.
% and single-agent learning-based methods \citep{kurtz2022mixed,xiong2023colearning}.
More specifically, we introduce a Graph Neural Network (GNN) based planner using Neural Ordinary Difference Equations (ODEs) \citep{das2023odesolvers} (Fig. \ref{fig:planner-overview}, Sec. \ref{sec:gnn-plan}) trained end-to-end on an STL objective to generate safe and robust plans for multiple agents 
that can be realized using a learnable MA collision avoidance controller (\citep{zhang_gcbf_2024}, Sec. \ref{sec:app-gcbf+}).
To scale up, we use the ODE-based component to plan general paths that satisfy the given task while using a GNN to model agent interactions in a scalable manner to achieve coordination between the agents as they determine which ODE-generated goal trajectory to follow. 
% \zk{Do we need to be a bit more explicit here? I.e., ODE is for avoiding gradient vanish in backpropagation, GNN is to model agent interaction. }
Our loss components (Sec. \ref{sec:app-diff-stl-planning}) allow the planner to find paths that satisfy the STL objective while also being achievable in the presence of collision avoidance maneuvers and agent-to-agent interactions.

%Our underlying collision avoidance mechanism is built upon \gcbfp~ \citep{zhang_gcbf_2024} (Sec. \ref{sec:app-gcbf+}) a robust technique 
%% built in JAX \citep{jax2018github} 
%that can reach arbitrary static goals given a model of the environment.


%As demonstrated in our work, in a setting with complex temporal specifications, we note a %deficiency in the objective-achieving capabilities of \gcbfp-based controllers trained on single %goal tasks when using a general STL planner.
%\gcbfp-based collision avoidance considers an egocentric graph-based view of the agent and its nearby agents upon which a goal-conditioned policy network and barrier function is trained on a small group of agents (say $N$).
% The learned policy network is then deployed on a larger number agents (say $N_d > N$) using the ability of GNN models to handle arbitrary graph topologies which can represent a different number of agents than during training. 
% In a similar way, using GNNs, our \planner~planner can be deployed on a larger number of agents than during training to realize safe and scalable specification-guided plans.

% This has been shown to be scalable in terms of the number of agents and fits our requirement of a differentiable approach to collision avoidance.
% Thus we choose \gcbfp~as our primary safety mechanism during execution of MA system and incorporate our planner to provide a goal for use by the 
% controller.
Our contributions are as follows:
1) We propose a novel scalable GNN-based planner (\planner) trained on an STL objective to generate safe and achievable plans for multiple agents.
% \joe{defined achievability in Defn 1 and replaced two instances of the term 'realizable'}
% \ab{WHAT DO YOU MEAN MY REALIZABLE? HAVE YOU DEFINED IT? DO YOU MEAN ACCURATE/OPTIMAL? YOU RARELY USE THE WORD AGAIN EXCEPT IN THE CONCLUSION SECTION LATER. ONLY USE WORDS WHICH HAVE A PRECISE ESTABLISHED UNDERSTANDING OR DEFINE A NEW ONE}
%method that combines a multi-agent collision avoidance controller with a 
2) We demonstrate the effectiveness of our approach on a range of STL specifications and show that our method can scale to a large number of agents and complex specifications beyond existing methods that use a state-of-the-art MILP-based planner with an average 65\% improved success rate.
%and a 69\% impact to completion times.

%in terms of scalability and performance.\ab{CAN YOU PUT SOME NUMBERS HERE? SAY HOW METHOD IS X\% BETTER THAN SOTA WITH XYZ METRICS}
% \end{itemize}

\begin{figure}[t!] % [t!] forces the figure to the top
	\centering 
	\begin{minipage}{.4\textwidth}
		\centering
		% \import{img/}{planner.pdf_tex}
		\includegraphics[width=\linewidth]{img/framework/framework_no_emb.pdf} % Replace with your image
	\end{minipage}%
	\begin{minipage}{.3\textwidth}
		\hfill
		\includegraphics[width=0.95\linewidth]{img/realrobot/g66_line.png}
	\end{minipage}\begin{minipage}{.3\textwidth}
		\hfill
		\frame{\resizebox{0.9\linewidth}{!}{\input{img/demo.pgf}}}
	\end{minipage}
	\caption{
		\textbf{(Left)} GNN-ODE Planner Architecture for Multi-Agent Systems with STL Specifications. 
  The planner $\plannn{i}$ generates a sequence of goals for agent $i$ given the initial state of the system $\graph[0]{}$. 
		The safety controller $\controlnn{i}$ ensures that the agents do not collide while following the generated goals.
		A GNN encodes the graph 
  % to produce 
  % a fixed global embedding $\emb$ 
  representing the 
  collective initial state of the system to yield an initial goal $\goal[0]{i}$ ({\color{red}red}) 
		for each agent $i$. 
		This goal $\goal[0]{i}$ is fed into a Multi-Layer Perceptron (MLP) network to generate a new goal $\goal[1]{i}$ ({\color{blue}blue}) which is fed back into the MLP network in a feedback loop. 
		This is repeated for $T-1$ steps to generate a sequence of goals for the agent. 
  The losses $\lossstl$ and $\lossacheivable$ are detailed in Sec. \ref{sec:app-diff-stl-planning} and are used to update our planner.
  \textbf{(Middle)} Real world experiments on $N=5$ drones.
		\textbf{(Right)} An example trajectory for $N=8$ agents for a \seq~spec requiring agents to visit A then B and finally C in order. 
  % \zk{The legend in the leftmost figure is not visible. Should we just remove it or make it bigger?} 
  % \zk{TODO: I can the paths for these real drones. }
%		\SJ{Nothing in the figure  or description refers to specifications}
		% \zk{Perhaps we should highlight that GNN provides a decentralized view here, since it is the key to scaling up.} 
		%    \zk{We should add a real-world demo here if possible. Perhaps only a simple swarm photo will be enough. If you can get one before the deadline, I can help to edit it. }
		% \zk{The GNN embedding only encodes the initial state. This is a weakness since it does not consider the following interactions between agents. Discussing this here exposes it to a significant place. I would suggest only mentioning that the GNN encodes the agent graph and leaving the details in Sec.~\ref{sec:gnn-plan}. }
	}
	\label{fig:planner-overview}
 \vspace{-0.5em}
\end{figure}

%\begin{figure}
%	\resizebox{0.5\textwidth}{!}{\input{img/demo.pgf}}
%\end{figure}
%\vspace{-1.5em}