\section{Implementation Details}
\label{app:implementation}
% final
\paragraph{Node features and edge features }
% \joe{Modify to match the new approach}
Following \citet{zhang_gcbf_2024}, the node features $v_i\in \mathbb R^{\rho_v}$ encode information specific to each node in our graph observation. 
Here, we set $\rho_v = 3$ and use the node features $v_i$ to one-hot encode the type of the node as either an agent node, goal node or LiDAR ray hitting point node.
The edge features $e_{ij}\in \mathbb R^{\rho_e}$, where $\rho_e > 0$ is the edge dimension, are defined as the information shared from node $j$ to the agent at node $i$, which depends on the states of the nodes $i$ and $j$. 
Since the safety objective depends on the relative positions, one component of the edge features is $p_{ij} = p_j - p_i$. 
The remaining edge features can be set depending on system dynamics, such as, relative velocities for double integrator dynamics. 

\paragraph{Computation Resources}
All training procedures were ran on an AWS \verb|g4dn.xlarge| instance or equivalent with 4 Intel Xeon-based CPU Cores and 16 GB of RAM with an Nvidia T4 GPU.

\paragraph{Evaluation Details}
Since we consider objectives that require agents to navigate close to one another at/near termination subsequently blocking the goal locations (A,B,C,D in Table \ref{tab:stl-specs-formal}), safety rates were reported until the point an agent had completed their plan.
This can be thought of as an alternative to the agents navigating to a `safe' position upon completing their specification/plan.
In a drone setting, we captured this behavior by landing the drones at an agent-specific location upon completing their specification.

\paragraph{Planner Details}
For all plans, at any time step $t$, planning step $t'$, each agent $i$ proceeded to the next waypoint $\goal[t'+1]{i}$ only when they reached goal $\goal[t']{i}$ within some threshold distance $r_{goal} = 0.3$ at a time $t \ge k(t'+1)$ where $k$ is the goal sampling interval (Sec. \ref{sec:app-diff-stl-planning}).
This allowed all agents to reach the waypoints in the plan without a strict time restriction on the plan duration.
The asynchronous nature of our plans (among agents) fits our problem description (Defn. \ref{def:ps-STL-MA}), specifically the STL Satisfaction criteria.
We leave the setting where agents follow a synchronized plan to future work.
%The MILP planner (STLPY \citep{kurtz2022mixed}) used Single Integrator dynamics to generate the position waypoints.
For a given plan of length $T$ with goal sample interval $k$ (values in Table \ref{tab:stl-specs-formal}), the maximum trajectory length horizon during evaluation $T_h$ was $5kT$.

\subsection{Environment Details}
\label{app:env-details}

Here, we provide the details of each experiment environment as taken from \citet{zhang_gcbf_2024}. We used a common simulation time step $\delta t=0.03$ across all three environments.

\textbf{SingleIntegrator} ~ We use single integrator dynamics as the base environment to verify the correctness of the implementation and to show the performance of the methods when there are no control input limits. 
The dynamics is given as $\dot x_i = v_i$, where $x_i=[p^x_i, p^y_i]^\top\in \mathbb R^2$ is the position of the $i$-th agent and $v_i =[v^x_i, v^y_i]^\top$ its velocity.  
In this environment, we use $e_{ij}=x_j-x_i$ as the edge information.

%We use the following reward function weights for training InforMARL.
%\begin{equation}
%	\lambda_{\text{nom}} = 0.1, \quad \lambda_{\text{goal}} = 0.1, \quad
%	\lambda_{\text{col}} = 5.0\,.
%\end{equation}

\textbf{DoubleIntegrator}~ We use double integrator dynamics for this environment. The state of agent $i$ is given by $x_i=[p^x_i,p^y_i,v^x_i,v^y_i]^\top$, where $[p^x_i,p^y_i]^\top$ is the position of the agent, and $[v^x_i,v^y_i]^\top$ is the velocity. The action of agent $i$ is given by $u_i=[a^x_i,a^y_i]^\top$, i.e., the acceleration. 
The dynamics function is given by:
\begin{equation}
	\dot x_i = \left[v^x_i, v^y_i, a^x_i, a^y_i\right]^\top
\end{equation}
 In this environment, we use $e_{ij}=x_j-x_i$ as the edge information.
%We use the following reward function weights for training InforMARL without obstacles.
%\begin{equation}
%	\lambda_{\text{nom}} = 0.1, \quad \lambda_{\text{goal}} = 0.1, \quad
%	\lambda_{\text{col}} = 2.0\,.
%\end{equation}
%and the following reward function weights with obstacles.
%\begin{equation}
%	\lambda_{\text{nom}} = 0.1, \quad \lambda_{\text{goal}} = 0.1, \quad
%	\lambda_{\text{col}} = 5.0\,.
%\end{equation}
%For the hand-crafted CBFs, we use $\alpha_0=10$. 

\textbf{DubinsCar}~ We use the standard Dubin's car model in this environment. The state of agent $i$ is given by $x_i=[p^x_i,p^y_i,\theta_i,v_i]^\top$, where $[p^x_i,p^y_i]^\top$ is the position of the agent, $\theta_i$ is the heading, and $v_i$ is the speed. The action of agent $i$ is given by $u_i=[\omega_i,a_i]^\top$ containing angular velocity and acceleration magnitude. The dynamics function is given by:
\begin{equation}
	\dot x_i = \left[v_i \cos(\theta_i), v_i \sin(\theta_i), \omega_i, a_i\right]^\top
\end{equation}
We use $e_{ij}=e_j(x_j)-e_i(x_i)$ as the edge information, where $e_i(x_i)=[p^x_i,p^y_i,v_i \cos(\theta_i), v_i \sin(\theta_i)]^\top$. 
%We use the following reward function weights for training InforMARL.
%\begin{equation}
%	\lambda_{\text{nom}} = 0.1, \quad \lambda_{\text{goal}} = 0.1, \quad
%	\lambda_{\text{col}} = 1.0\,.
%\end{equation}
%For the hand-crafted CBFs, we use $\alpha_0=5$. 