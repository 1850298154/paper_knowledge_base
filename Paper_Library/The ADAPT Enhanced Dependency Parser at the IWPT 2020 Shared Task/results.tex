% section: results

\input{tables/decision_custom}

Table~\ref{devresults:decision_custom} compares the semantic parser against the heuristic approach on the ELAS F1 metric.
The evaluation script was run without connecting fragmented graphs and format validation.
For all but two treebanks, the semantic parser performs better than the
best
heuristic approach.
For some languages, the difference in performance is large.
For \texttt{et\_ewt}, which does not have a development set,
we suspect that we overfitted our semantic parser on the
\texttt{et\_ewt} training data
by allowing it to train for 75 epochs.

\input{tables/fixes_test_custom}

Table~\ref{testresults_custom} shows test set ELAS obtained on the shared task
submission site for
\textit{(a)} our submission fully relying on the organiser's
             \texttt{quick-fix} tool to fix issues in the output of
             our system,
\textit{(b)} the same predictions post-processed by our own
             fragment connector that aims to minimise the
             number of root edges added, and
\textit{(c)} a re-run of our pipeline using the same models
             for system components as before but with all
             bugs fixed during development applied to all
             predictions and new decisions which models
             to apply to the test sets.
While the \texttt{quick-fix} tool enabled us to make a valid submission
in time, its
approach of adding edges from the root node to
all unreachable tokens
has a strong negative impact on 
precision, \eg 62.26 ELAS precision on the Czech CAC development set
\vs 87.37 without post-processing.
Our own post-competition fix avoids this
and would have brought us to the top half of the competition.

% eof