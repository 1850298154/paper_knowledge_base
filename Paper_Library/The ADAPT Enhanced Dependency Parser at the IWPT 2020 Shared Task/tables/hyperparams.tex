\begin{table}[t!]
\begin{center}
%\small
\begin{tabular}{ll}
\toprule
\multicolumn{2}{c}{\bf Semantic Parser Details } \\
\bf Parameter        & \bf Value \\
\midrule
Char-BiLSTM layers & 2 \\
BiLSTM layers      & 3 \\
BiLSTM size        & 400 \\
Char-BiLSTM size        & 64 \\
Arc MLP size     & 500 \\
Label MLP size & 100 \\
Dropout LSTMs      & 0.33 \\
Dropout MLP        & 0.33 \\
Dropout embeddings & 0.33 \\
Nonlinear act. (MLP) & ELU \\
Edge prediction threshold & 0.5 \\
%\midrule
%\multicolumn{2}{c}{\bf Embeddings } \\
%\bf Parameter &\bf Size \\
%\midrule
BERT word-piece embedding       & 768 \\
Char embedding       & 64 \\
Tag embedding (all tags)      & 50 \\
%\midrule
%\hline
%\hline
%\multicolumn{2}{c}{\bf Training } \\
%\bf{Parameter}        & \bf Value \\
%\midrule
Optimizer          & Adam\\
Learning rate      & 0.001\\
%Adam epsilon       & 1e-08\\
beta1           & 0.9\\
beta2              & 0.9\\
Num. epochs              & 75 \\
Patience              & 10 \\
Batch size              & 16 \\
\bottomrule
\end{tabular}
\end{center}
\caption{\label{table:hyper-params} Chosen hyperparameters for our semantic parser.
For the tag embedding, we use the same size embedding for all features (lemma, POS, morphological features, head-information and label embeddings) and concatenate them.}
\end{table}