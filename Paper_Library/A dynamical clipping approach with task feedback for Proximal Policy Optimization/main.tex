%File: anonymous-submission-latex-2025.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{amsmath}
\let\proof\relax
\let\endproof\relax
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tcolorbox}
\usepackage{url}
\definecolor{ccr}{RGB}{10,110,150}  % self-defined color
\definecolor{ccr2}{RGB}{153,0,76}  % self-defined color
%\hypersetup{colorlinks=true,
%	   linkcolor=ccr,
%	   urlcolor=red,
%	   citecolor=ccr2,}
\usepackage{multirow}
\usepackage{amssymb}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{definition}{Definition}
%\newtheorem{theorem}{Theorem}[section]
\newcommand{\bs}{\mathbf{s}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\btau}{\mathbf{\tau}}
\newcommand{\ie}{\textit{i}.\textit{e}.}
\newcommand{\viz}{{\emph{viz.}},\xspace}
\newcommand{\eg}{{\emph{e.g.}},\xspace}
\newcommand{\etc}{etc.}
\usepackage{amsthm}
%\theoremstyle{plain}
%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{corollary}[theorem]{Corollary}
%\theoremstyle{definition}
\newtheorem{definition}{Definition}
%\newtheorem{assumption}{Theorem}{Assumption}
\newtheorem{theorem}{Theorem}%[section]
\newtheorem{lemma}{Lemma}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
%\newtheorem{theorem}{Theorem}%[section]
%\newtheorem{lemma}[theorem]{Lemma}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{A Dynamical Clipping Approach with Task Feedback\\ for Proximal Policy Optimization}
\author {
    % Authors
    Ziqi Zhang\textsuperscript{\rm 2}\equalcontrib,
    Jingzehua Xu\textsuperscript{\rm 2}\equalcontrib,
    Zifeng Zhuang\textsuperscript{\rm 1}
    Hongyin Zhang\textsuperscript{\rm 1}
    Jinxin Liu\textsuperscript{\rm 1}\\
    Donglin Wang\textsuperscript{\rm 1}
    Shuai Zhang\textsuperscript{\rm 3}
    Shangke Lyu\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}School of Engineering, WestLake University\\
    \textsuperscript{\rm 2}MicroMasters Program in Statistics and Data Science, Massachusetts Institute of Technology, USA\\
    \textsuperscript{\rm 3}Department of Data Science, New Jersey Institute of Technology, USA\\
stevezhangz@163.com, xjzh23@mails.tsinghua.edu.cn,\\wangdonglin@westlake.edu.cn, lyushangke@westlake.edu.cn, sz457@njit.edu
}



% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
Proximal Policy Optimization (PPO) has been broadly applied to robotics learning, showcasing stable training performance. However, the fixed clipping bound setting may limit the performance of PPO. Specifically, there is no theoretical proof that the optimal clipping bound remains consistent throughout the entire training process. Meanwhile, previous researches suggest that a fixed clipping bound restricts the policy's ability to explore. Therefore, many past studies have aimed to dynamically adjust the PPO clipping bound to enhance PPO's performance. However, the objective of these approaches are not directly aligned with the objective of reinforcement learning (RL) tasks, which is to maximize the cumulative Return. Unlike previous clipping approaches, we propose a bi-level proximal policy optimization objective that can dynamically adjust the clipping bound to better reflect the preference (maximizing Return) of these RL tasks. Based on this bi-level proximal policy optimization paradigm, we introduce a new algorithm named Preference based Proximal Policy Optimization (Pb-PPO). Pb-PPO utilizes a multi-armed bandit approach to refelect RL preference, recommending the clipping bound for PPO that can maximizes the current Return. Therefore, Pb-PPO results in greater stability and improved performance compared to PPO with a fixed clipping bound. We test Pb-PPO on locomotion benchmarks across multiple environments, including Gym-Mujoco and legged-gym. Additionally, we validate Pb-PPO on customized navigation tasks. Meanwhile, we conducted comparisons with PPO using various fixed clipping bounds and various of clipping approaches. The experimental results indicate that Pb-PPO demonstrates superior training performance compared to PPO and its variants. \footnote{Our codebase has been released at \url{https://github.com/stevezhangzA/pb_ppo}}
\end{abstract}

% Uncomment the following to link to your code, datasets, an extended version or similar.
%
% \begin{links}
%     \link{Code}{https://aaai.org/example/code}
%     \link{Datasets}{https://aaai.org/example/datasets}
%     \link{Extended version}{https://aaai.org/example/extended-version}
% \end{links}
Typically, there are primarily two common paradigms in reinforcement learning (RL). The first involves alternating between learning Q-networks and using them to update the policy network~\citep{mnih2013playing,Mnih2015HumanlevelCT}. The second paradigm, based on gradient methods~\citep{lillicrap2019continuous}, directly updates the policy. The second paradigm is applicable in environments with high-dimensional action space and exhibits high converge speed. But, gradient-based paradigms are generally on-policy methods, meaning that the current policy may not utilize the previously collected dataset. 

To make full use of pre-collected dataset and improve algorithm's sample efficiency, we can utilize importance sampling to approximately transform on-policy algorithms into off-policy ones. However, during policy updates, a crucial challenge arises in determining the updating step size, where the new policy update may deviate too large from the old policy, compromising training stability. TRPO~\citep{schulman2017trust} addresses this concern by incorporating importance sampling and utilizing the Kullback-Leibler (KL) divergence to restrict the distance between the old policy and the new policy. This prevents excessive deviations caused by overly large updating step sizes. Subsequently, PPO~\citep{schulman2017proximal} introduces a clipped surrogate objective, which is also a KL divergence term, but limits the policy update within a $\epsilon$ surrogate trust region. With this surrogate KL term, PPO achieves higher training efficiency and stability, and increasing its real-world applicability. 

Specifically, PPO~\cite{schulman2017proximal} has been widely used in the robotics learning domain due to its stable training performance and theoretically monotonic improvement, which naturally align with the requirements of robotics learning~\cite{brohan2023rt1,brohan2023rt2,bhargava2020nonlinear}. However, PPO's performance is limited by the fixed setting of clipping bound. Because, a fixed clipping approach may impact the policy's exploratory~\cite{2022Upper}, training stability~\cite{chen2018adaptive}, and further impact the training results. Therefore, researching a better clipping approach to replace with the fixed clipping approach can be quite beneficial for further improving PPO's performance, and robotics learning.

Currently, PPO is improved from two major perspectives: 1) Modification of the advantage function~\citep{schulman2018highdimensional} to facilitate stable gradient descent or agent's exploration~\citep{Xie2022UpperCB}. 2) Introducing dynamic clipping approaches to enhance the policy's exploratory capabilities~\citep{wang2019trust} or ensure optimization within the boundary of optimal value~\citep{chen2018adaptive}. Regarding approach 2, most methods are replaced with dynamical clipping approach to enhance the policy's exploratory capabilities~\citep{wang2019trust} or ensure the performance of optimized policy within the boundary of optimal performance~\citep{chen2018adaptive}. Regarding the modification/improvement of PPO's clipping approach, most of which are not directly aligned with the goal of RL tasks, $\ie$ maximizing expected Return, therefore, these method may not directly contribute to policy improvement. 

In order to align the goal of sampling clipping bound with RL objective, we propose utilizing a multi-armed bandits approach (an almost parameter-free algorithm) and regarding PPO's evaluated Return as the RL task's feedback to dynamically recommend the clpping bound that can bring the highest return during the training process of PPO. Meanwhile, in order to improve the exploitation of candidate clipping bounds, we utilize the Upper Confidence Bound (UCB) value to guide PPO in exploring and exploiting the optimal clipping bounds throughout the entire online training stages. Our approach have several advantages over PPO and various PPO variants: 1) Compared to past clipping approaches, Pb-PPO's method of adjusting the clipping bound aligns directly with RL preference. Therefore, adjusting the clipping bound in Pb-PPO can directly enhance PPO's performance. 2) Through extensive robot experiments, we discovered that Pb-PPO's advantages over PPO extend not only to the training process but also to the model testing phase. Specifically, policies trained with Pb-PPO exhibit a more stable response to given instructions in robot tasks, showcasing the stability of Pb-PPO. To summerize, our contributions can be outlined as follows (\textbf{We recognize that while some modification of PPO have achieved competitive performance: ~\citeauthor{tao2024reverseforwardcurriculumlearning,XU202394,Song2020V-MPO:} \etc, our modification differs from these methods in that we focus on the clipping approach. Therefore, PPO could be further improved by combining these approaches with our method, and our majority baselines are previous clipping approaches.}):%tao2024reverseforwardcurriculumlearning
\begin{itemize}
%\item We proposing a bi-level proximal policy optimization paradigm, jointly optimizing the sampling of clipping bound and PPO, both of which aligns with the goal of RL tasks' preference (\textit{maximizing evaluated Return}).
\item We propose an algorithm called Pb-PPO, which dynamically adjusts the PPO clipping bound to align with RL preference. Compared to PPO and PPO variants, Pb-PPO demonstrates more stable training performance and achieves better training results. 
\item We test Pb-PPO in quadruped simulation environments designed for robot deployment and find that policies trained with Pb-PPO exhibit a more stable response to given instructions. This indicates that the improvements brought by Pb-PPO also include increased stability in policies.
\end{itemize}
\section{Related Work}  
\paragraph{Proximal Policy Optimization (PPO).}\label{clipping_work} Originally designed as an online algorithm, Proximal Policy Optimization (PPO)~\citep{schulman2017proximal} aimed to enhance the applicability of Trust Region Policy Optimization (TRPO)~\citep{schulman2017trust}. PPO introduced a clipping approach for efficient training, making it widely applicable across various domains, especially in robotics learning~\citep{hoeller2023anymal,Jenelten_2024}. Notably, PPO has recently been extended to the offline Reinforcement Learning (RL) setting~\citep{zhuang2023behavior} and multi-agent systems~\citep{yu2022surprising}. Our study focuses on augmenting PPO, emphasizing improvements in training stability and performance within online RL settings. Recent improvements related to PPO modifications include 1) enhancing the estimation of the advantage function to ensure stable gradient descent~\citep{schulman2018highdimensional} or agent's exploration~\citep{2022Upper}, and 2) introducing adaptive clipping approaches~\citep{chen2018adaptive} to optimize policy within the boundary of its optimal performance and introducing dynamical clipping approaches~\citep{wang2019trust} to address the limitation of fixed clipping bound that suffer from limit exploration. AAdditionally, PPO has been improved from other perspectives such as gradient updating, divergence, and exploration~\cite{tao2024reverseforwardcurriculumlearning,XU202394,Song2020V-MPO:}.
\paragraph{Preference Based RL (PbRL).} 
Preference-based Reinforcement Learning (PbRL) is an approach used for learning from preference or feedback, extensively employed to capture and reflect human preference across diverse domains~\citep{arumugam2019deep, christiano2023deep, ibarz2018reward, warnell2018deep, lee2021pebble, ouyang2022training}. Typically, the majority of approaches involve pre-training a reward model to reflect human preference, followed by optimization based on this pre-trained reward model, yielding notable improvements, such as enhancing Large Language Models (LLM)~\citep{ouyang2022training}. Apart from learning from human feedback, PbRL also encompasses learning from task feedback in domains like Natural Language Processing (NLP) or Computer Vision (CV)~\citep{pinto2023tuning,liu2022rlet}. Recently,~\citeauthor{rafailov2023direct} proposed direct optimization from preference showcasing strong performance in LLM optimization. On the other hand, the primary distinction between the aforementioned approaches and Pb-PPO lies in Pb-PPO utilizing RL tasks' feedback to tune RL policy hyper-parameters rather than directly adjusting the network's parameters. Therefore, our approach is closely related to Automatic Machine Learning (AutoML)~\citep{pmlr-v32-hutter14,Fawcett2015AnalysingDB,van_Rijn_2018}.
\section{Preliminary}
\paragraph{Reinforcement Learning.}
We formulate RL as a Markov Decision Process (MDP) tuple $\ie$ $\mathcal{M}:=(\mathcal{A},\mathcal{S},r,\gamma,d_{\mathcal{M}},p_{0},\bs_0)$. Specifically, $\mathcal{A}$ denotes action space, $\mathcal{S}$ denotes state space, $\ba\in \mathcal{A}$ denotes action, $\bs\in\mathcal{S}$ denotes observation (state), $r(\bs,\ba):\mathcal{A}\times\mathcal{S}\rightarrow \mathbb{R}$ denotes the reward function, $\gamma \in [0,1]$ denotes the discount factor, $d_{\mathcal{M}}(\bs_{t+1}|\bs_{t},\ba_{t}):\mathcal{S}\times\mathcal{A}\rightarrow \mathcal{S}$ denotes the transition function (dynamics), $p_{0}$ denotes the distribution of initial state, and $\bs_{0}\sim p_{0}$ denotes the initial state. The goal of RL is to find the optimal policy $\pi^*$ that can maximize the accumulated Return $\ie$ $\pi^*:=\arg\max_{\pi}\mathbb{E}_{\tau\sim \pi(\tau)}[R(\tau)]$, where $\tau:=\big\{\bs_0,\ba_0,\cdots,\bs_t,\ba_t,\cdots \bs_T,\ba_T|\bs_{t+1}\sim d_{\mathcal{M}}(\cdot|\bs_t,\ba_t),\ba_{t}=\pi(\cdot|\bs_t),s_0\sim p_0\big\}$ is the roll-out trajectory, $T$ denotes the time horizon, and $R(\tau)=\sum_{t=0}^{t=T}\gamma^t r(\bs_t,\ba_t)$ denotes the accumulated Return. In order to optimize policy to attain the maximum Return, off-policy algorithms iteratively estimate the expected Return of given state-action pairs $(\bs_{t}, \ba_{t})$ by training a $Q$ network $\ie$ $Q(\bs_t,\ba_t)=\mathbb{E}_{(\bs_t,\ba_t)\sim\pi(\tau)}[\sum^{t'=t}_{t'=0}\gamma^{t'}r(\bs_{t'},\ba_{t'})|\bs_{0}=\bs_t,\ba_{0}=\ba_t]$, and optimize the policy by maximizing $Q$ $\ie$ $\max_{\pi}\mathcal{J}(\pi)=\max_{\pi}\mathbb{E}_{\bs_t \sim \pi(\tau)} [Q(\bs_t,\pi(\cdot|\bs_{t}))]$. In particular, $(\bs_t,\ba_t)$ can be sampled from the dataset collected across entire online training process. 

Unlike off-policy algorithms, on-policy algorithms typically optimize $\pi$ using policy gradient approaches, $\ie$ $\max_{\pi}\mathcal{J}(\pi)=\max_{\pi}\mathbb{E}_{(\bs_t,\ba_t)\sim\pi(\tau)}(\cdot)\log \pi(\bs_t,\ba_t)$, on the dataset collected by the current policy. Here, $(\cdot)$ encompasses various alternatives, such as Q value $Q(\bs_t,\ba_t)$, advantage value $A(\bs_t,\ba_t)=Q(\bs_t,\ba_t)-V(\bs_t)$, where $V(\bs_t)$ is a value network used to estimate the expectation of $Q(\bs_t,\ba_t)$, $\ie$ $V(\bs_t)=\mathbb{E}_{\ba\sim \mathcal{A}}[Q(\bs_t,\ba)]$. However, on-policy algorithms cannot utilize datasets collected from another policy $\ie$ $\pi'\in\Pi, \pi!=\pi$ to update the target policy $\pi$, which limits their sample efficiency. To enhance the on policy algorithms' efficiency of data utilization, we can introduce importance sampling $\ie$ Equation~\ref{importance_sampling}, approximating on-policy algorithms as off-policy algorithms. This allows us to leverage datasets collected by other policies to train the current policy.
\begin{equation}
\label{importance_sampling}
\begin{split}
\mathcal{J}_{\pi_{\rm old}}(\pi_{\rm new})=\mathbb{E}_{\tau\sim \pi_{\rm old}(\tau)}\bigg[\frac{\pi_{\rm new}(\tau)}{\pi_{\rm old}(\tau)}A^{\pi_{\rm old}}(\bs_t,\ba_t)\bigg],
\end{split}
\end{equation}
however, directly optimize Equation~\ref{importance_sampling} may lead to the new policy diverging from the old policy, making it challenging to reach the optimal solution. Therefore, it's necessary to further minimize the KL divergence between the new and old policies, thus~\citeauthor{schulman2017trust} propose Trust Region Optimization (TRPO) $\ie$ Equation~\ref{TRPO}.
\begin{equation}
\label{TRPO}
\begin{split}
\mathcal{J}_{\pi_{\rm old}}=\mathbb{E}_{\tau\sim \pi_{\rm old}(\tau)}\bigg[\frac{\pi_{\rm new}(\tau)}{\pi_{\rm old}(\tau)}A^{\pi_{\rm old}}(\bs_t,\ba_t)\bigg]\\+D_{\rm KL}[\pi_{\rm new}(\cdot|\bs_t)||\pi_{\rm old}(\cdot|\bs_{t})].
\end{split}
\end{equation}
\paragraph{Proximal Policy Optimization (PPO).} Directly computing KL divergence is computationally inefficient. Therefore,~\citeauthor{schulman2017proximal} proposes the surrogate trust region optimization objective called PPO, as seen in Equation~\ref{PPO-clip}, which enhances the computational efficiency of PPO by truncating the KL divergence within a fixed region.

\begin{align}
\label{PPO-clip}
\begin{split}
\mathcal{J}_{\pi_{\rm old}}(\pi_{\rm new})&=\mathbb{E}_{\tau\sim\pi_{\rm old}(\tau) }\bigg[\min(\frac{\pi_{\rm new}(\tau)}{\pi_{\rm old}(\tau)}A^{\pi_{\rm old}}(\bs_{t},\ba_{t}),\\&{\rm clip}(\frac{\pi_{\rm new}(\tau)}{\pi_{\rm old}(\tau)},1-\epsilon,1+\epsilon)A^{\pi_{\rm old}}(\bs_t,\ba_t))\bigg],
\end{split}
\end{align}

where $\epsilon \in (0,1)$ is the clipping threshold. In the section Introduction, we have initially mentioned the limitations of using fixed surrogate trust region to constrain policy updating. Therefore, we introduce Preference based PPO (Pb-PPO), which dynamically samples the clipping bound to truncate the KL divergence, and such clipping approach is aligned with the maximization of RL return. To begin with introducing Pb-PPO we first introduce multi-armed bandit and Upper Confidence Bound. 

\paragraph{Multi-armed bandit and Upper Confidence Bound (UCB).} Given n independent variables $\zeta=\{\epsilon_0,\epsilon_1,\cdots,\epsilon_i,\cdots,\epsilon_n\}$ with equal distribution, we treat the process of sampling these variables according to the expectation as a multi-armed bandit game. Specifically, when sample the i-th arm $\epsilon_i$, we can obtain a immediate reward $r_{t=N_{\epsilon_i}}$, where $N_{\epsilon_i}$ denotes the total times access to $\epsilon_i$, and we estimate the expected Return brought by sampling $\epsilon_i$ as $\mathbb{E}[R_{\epsilon_i}|\epsilon_i]$ $\ie$ Equation~\ref{R_arm}.
\begin{equation}
\label{R_arm}
    \begin{split}
        U(\epsilon_i)=\mathbb{E}[R_{\epsilon_i}|\epsilon_i]=\sum_{t=0}^{t=N_{\epsilon_i}}\gamma^t r_{t}(\epsilon_i).
    \end{split}
\end{equation}
The objective of this game is to sample the variable to achieve the highest Return. Additionally, during process of sampling from $\zeta$ and updating $\mathbb{E}[R_{\epsilon_i}|\epsilon_i]$, if we greedily sample variables (sampling with the max expected Return), this kind of sampling is termed exploitation, otherwise, it is referred to as exploration. However, if we update the estimation of expected Return without exploration (only using greedy strategy), we may fail to identify the optimal clipping bound, leading to overestimating the confidence of sub-optimal clipping bounds. To address this overestimation issue, it is crucial to strike a balance between exploitation and exploration, necessitating the introduction of uncertainty.

UCB is a strategy function defined as $U^{UCB}(\epsilon_i)=U(\epsilon_i)+\hat U(\epsilon_i)$ utilized to balance the exploration and exploitation of bandit arms by introducing uncertainty $\hat U(\epsilon_i)$, where the uncertainty value of i-th variable $\epsilon_{i}$ can be formulated as Equation~\ref{ucb}. Therefore, we can sample the variable with the highest UCB value $\ie$ $\epsilon^*:=\arg\max_{\epsilon_i} \{U^{UCB}(\epsilon_i)|\epsilon_i\in\zeta\}$ to efficiently balance exploration with exploitation of candidate variables.
\begin{align}
    \begin{split}
       \hat U(\epsilon_i)=(R^{\rm max}_{\epsilon_i}-R^{\rm min}_{\epsilon_i})\sqrt{\frac{1}{2}\ln \frac{2}{\sigma}}.
    \end{split}
\label{ucb}
\end{align}
\textbf{\textit{Proof}} of Equation~\ref{ucb} see Appendix, where $\sigma$ denotes uncertainty factor.
\begin{algorithm*}[ht]
\small
\caption{Pb-PPO} 
\label{Pb_ppo_alg}
\begin{algorithmic}
\STATE $\textbf{Require:}$ PPO modules ($\pi_{\rm new}$, $V_{\phi}$, $\mathcal{D}_{\rm online}$); candidate clipping bounds: $\zeta$=$\{\epsilon_0, \epsilon_1, \cdots, \epsilon_n \}$. The counter $N^{\rm bandit}$ of total visitations, the counter $N^{\rm bandit}_{\epsilon_0}$ of each bandit arm. \\
\STATE $\textbf{Initialize parameters of multi-armed bandits: }$ tabular Return $\{\mathbb{E}[R_0|\epsilon_0]=0,\cdots,\mathbb{E}[R_n|\epsilon_n]=0\}$, total visitations $N^{\rm bandit}=0$ for all arms, and the visitation counter of each arm $N^{\rm arm}=\{N^{\rm arm}_{\epsilon_0}=0,N^{\rm arm}_{\epsilon_1}=0,\cdots,N^{\rm arm}_{\epsilon_n}=0\}$, discount factor $\gamma_{bandit}\in [0,1]$ for bandit arms, global step counter $N_{\rm step}$\\
\STATE$\textbf{Initialize PPO parameters:}$ $\pi_{\rm new}$ and $V_{\phi}$ \\
\STATE$\textbf{Initialize RL hype-parameter:}$ Global online step as $N_{\rm step}$, online replay buffer as $\mathcal{D}_{\rm online}$. \\
\WHILE{$N_{\rm step}<10^6$} 
\STATE Interacting $\pi_{\rm new}$ with environment to collect new trajectory $\tau=\{\bs_0,\ba_0,r_0,\cdots,\bs_T,\ba_T,r_T\}$, then appending $\tau$ to $\mathcal{D}_{\rm online}$ \\
\STATE Update online steps $N_{\rm step}\leftarrow N_{\rm step}+len(\tau)$\\
\STATE Alternate the new policy to old policy: $\pi_{\rm old} \leftarrow \pi_{\rm new}$\\
\STATE Computing UCB values $\{U^{\rm UCB}(\epsilon_i)\}=\{\mathbb{E}[R_0|\epsilon_0]+\lambda\hat{U}(\epsilon_0),\cdots,\mathbb{E}[R_n|\epsilon_n]+\lambda\hat{U}(\epsilon_n)\}$.\\
\STATE Sampling a clipping bound $\epsilon_i$ according to the maximum of UCB values as the optimal clipping bound $\epsilon^*$ $\ie$ $\epsilon^*=\arg\max_{\epsilon_i}\{U^{\rm UCB}(\epsilon_i)\},$ then utilize $\epsilon^*$ to train PPO.\\
\FOR{j$\in$ ${\rm range}$(updating steps)}
\STATE Rewrite Equation~\ref{PPO-clip} as:
$\mathcal{J}_{\pi_{\rm old}}(\pi_{\rm new},\epsilon^*)= \mathbb{E}_{\tau\sim \pi_{\rm old}}[\min(\frac{\pi_{\rm new}(\tau)}{\pi_{\rm old}(\tau)}A^{\pi_{\rm old}}(\bs_t,\ba_t),{\rm clip}(\frac{\pi_{\rm new}(\tau)}{\pi_{\rm old}(\tau)},1-\epsilon^*,1+\epsilon^*)A^{\pi_{\rm old}}(\bs_t,\ba_t))]$ 
\STATE Updating $\pi_{\rm new}$ with $\mathcal{J}_{\pi_{\rm old}}(\pi_{\rm new},\epsilon^*)$.
\ENDFOR
\STATE Evaluating $\pi_{\rm new}$ k episodes and compute un-normalized expectation $R^{\rm bandit}_{\epsilon^*}$ for sampled clipping bound via Equation~\ref{R_arm}.\\
\STATE Updating the total Return, $\ie$ \textcolor{blue}{$R^{\rm bandit}\leftarrow R^{\rm bandit}+\gamma_{bandit}*R^{\rm bandit}_{\epsilon^*}$}, update the expectation $\mathbb{E}[R^{\rm bandit}_{\epsilon^i}|\epsilon^i]$ by $\mathbb{E}[R^{\rm bandit}_{\epsilon^i}|\epsilon^i]=\gamma_{bandit}\mathbb{E}[R^{\rm bandit}_{\epsilon^i}|\epsilon^i]+R^{\rm bandit}_{\epsilon^i}.$ 
\STATE Updating the total Visitation counter by $N^{\rm bandit}\leftarrow N^{\rm bandt}+1$, and bandit visitation counter by $N^{\rm bandit}_{\epsilon_i}\leftarrow N^{\rm bandit}_{\epsilon_i}+1$; \\
\STATE Updating the normalized expectation for each bandits via Equation~\ref{expectation_update}.\\
\ENDWHILE
%\end{algorithmic}
\end{algorithmic}
\end{algorithm*}

\section{Preference based Proximal Policy Optimization (Pb-PPO)} 
\paragraph{Bi-level Proximal Policy Optimization.} We cast proximal policy optimization as a bi-level optimization problem with two primary objectives: \textit{Objective 1):} Proximal Policy Optimization~\cite{schulman2017proximal}, which aims to maximize the expected Return, $\ie$ $\max\mathbb{E}_{\tau \sim \pi_{\rm new}(\tau)}[ \gamma^t\cdot r(\bs_t,\ba_t)]$, and \textit{Objective 2):} Updating the UCB values $\big\{\mathbb{E}[R_0|\epsilon_0]+\hat U(\epsilon_0),\cdots,\mathbb{E}[R_n|\epsilon_n]+\hat U(\epsilon_n)\big\}$ of candidate clipping bounds $\zeta=\{\epsilon_0,\epsilon_1,\cdots,\epsilon_n\}$ during the updating phases. This ensures the sampling of the optimal clipping bound, thereby optimizing PPO to reach maximum RL tasks' preference. 

Specifically, In the problem setting of bi-level proximal policy optimization, $\mathbb{E}[R_{\epsilon_i}|\epsilon_i]$ represents the expected Return obtained by rolling out a policy trained with clipping bound $\epsilon_i$ $\ie$ Equation~\ref{R_arm}. Specifically, we consider the Return of the $n_{\pi}$-th updated policy $\pi^{n_{\pi}}_{\rm new}$ when utilizing $\epsilon_n$ as reward, denoted as $r_t=r_{N_{\epsilon_i}}$. We then update the expected Return of $\epsilon_n$ using Equation~\ref{R_arm}. Consequently, we further define the bi-level objective of $n_{\pi}$ policy iteration of PPO as $\mathcal{J}(\pi^{n_{\pi}},\zeta)$,
\begin{equation}
\begin{split}
\max_{U(\epsilon^*),\mathcal{J}(\pi^{n_{\pi}},\epsilon^*)} \mathcal{J}(\pi^{n_{\pi}},\epsilon^*)\\ s.t.\,\, \epsilon^* \leftarrow\arg\max_{\epsilon_i}\{U^{UCB}(\epsilon_i)|\epsilon_i \in \zeta\}.  
\end{split}
\label{bi_level_optimization}
\end{equation}
Equation~\ref{bi_level_optimization} implies that the objective of bi-level proximal optimization is the upper bound of $\mathcal{J}(\pi^{n_{\pi}})$, and we can obtain the best training performance by jointly sampling the optimal clipping bound $\ie$ $\epsilon^*:=\arg \max_{\epsilon_i} \big\{\mathcal{J}(\pi^{n_{\pi}},\epsilon_i)|\epsilon_i\in \zeta\}$, and optimizing PPO with $\epsilon^*$. Subsequently, we propose Pb-PPO. 

\paragraph{Preference based Proximal Policy Optimization (Pb-PPO).} Pb-PPO is based on the bi-level proximal policy optimization. In addition to updating the policy through proximal policy optimization, it is also necessary, in each training epoch, to select the optimal clipping bound based on RL tasks' feedback. This ensures that the current update step length is optimized to algin with maximizing accumulated Return. In the following sections, we first introduce the implementation of \textit{Objective 2)}. Subsequently, in section~\ref{method_Pb_PPO}, we will provide a comprehensive overview of the implementation process of Pb-PPO.

\subsection{Implementation of \textit{Objective 2)}} 
\begin{figure*}[ht]
\begin{center}
\hspace{-3.9cm}\includegraphics[scale=0.23]{other_alg_compare_legend.pdf}\\
\includegraphics[scale=0.172]{Ant_v3.pdf}
\includegraphics[scale=0.172]{hopper_v3.pdf}
\includegraphics[scale=0.172]{walker2d_v3.pdf}
\includegraphics[scale=0.172]{cheetah_v3.pdf}
\caption{Pb-PPO (task feedback) on locomotion tasks. Each solid curve in these figures represent the average experimental results across multiple seeds, and the shadowed area corresponds to the fluctuation of Return curves.}
\label{fig_locomotion}
\end{center}
\vspace{-10pt}
\end{figure*}
\begin{table*}[ht]
\centering
\caption{Performance comparision of the last training outcomes. We average the last training results of PPO with diverse clipping bounds and Pb-PPO across multiple seeds, we also compare with the experimental results of TRPO, DDPG, official PPO, TRGPPO, etc. are directly quoted from~\citep{fujimoto2018addressing}, and PPO-$\lambda$ is quoted from~\citep{chen2018adaptive}.}
\label{tab_final_res}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccccccccc}
\toprule
\textbf{Task}& PPO ($\epsilon$=0.02)	&PPO ($\epsilon$=0.15)	&PPO ($\epsilon$=0.23)&PPO (official)& TRPO & DDPG &PPO-$\lambda$&TRGPPO	& Pb-PPO\\
\midrule
$\texttt{Ant-v3}$	&1686$\pm$276	&2458$\pm$476	&2457.9$\pm$506.6&1083.2&-75.85&1005.3&-&-&\textbf{3151.8$\pm$545.9} \\
$\texttt{Halfcheetah-v3}$	&1284$\pm$81	&1608$\pm$68	&1568$\pm$65&1795.4&-15.6	&3305.6	&-&4986.1&2781.4$\pm$1188.0\\
$\texttt{Hopper-v3}$	&2858$\pm$275	&2274$\pm$591	&2327$\pm$523&2164.7&2471.3	&1843.9	&1762.3&3200.5&\textbf{3414.9$\pm$216.9}\\
$\texttt{Walker2d-v3}$ &1887$\pm$229	&2542$\pm$304	&2340$\pm$304&3317.7&2321.5	&1843.9	&2312.1&3886.8&\textbf{3913.1$\pm$699.4}\\
\midrule
\textbf{Avg.}	&1623.8	&1869.6	&1836.4&2090.3&	1175.3&1999.7&-&-&3315.3\\
\bottomrule
\end{tabular}}
\vspace{-10pt}
\end{table*}
\begin{figure*}[ht]
\centering
\includegraphics[scale=0.32]{routine_assign/3.pdf}
\includegraphics[scale=0.32]{routine_assign/9.pdf}
\includegraphics[scale=0.32]{routine_assign/10.pdf}
\includegraphics[scale=0.32]{routine_assign/11.pdf}\\
\hspace*{-3.1cm}\includegraphics[scale=0.29]{fig2_legend/Average_Return.pdf}\hspace*{2.9cm}
\includegraphics[scale=0.29]{fig2_legend/Policy_Trajectory.pdf}
\vspace{-10pt}
\caption{\textbf{Pb-PPO on AUV navigation tasks across different difficulty levels.} (a) Average Return curve in the hard navigation task. (b) From left to right are trajectories of the AUV in the easy, medium, hard environment in turn, we introduce the environment setting in Appendix.}
\label{auto_car}
\end{figure*}
\begin{figure*}[ht]
\centering
\hspace{-6pt}
\includegraphics[scale=0.26]{Flat_terrian.pdf}
%\hspace{-10pt}
\includegraphics[scale=0.15]{quad_robot_plane/legged_gym.pdf}
\includegraphics[scale=0.12]{quad_robot_plane/base_vel_x.pdf}
\includegraphics[scale=0.12]{quad_robot_plane/base_vel_y.pdf}
\includegraphics[scale=0.12]{quad_robot_plane/base_vel_yaw}\\
\includegraphics[scale=0.26]{flat-discrete.pdf}
\,\,\,\,\,\,\,\,\hspace{-6pt}\includegraphics[scale=0.32]{flat_terrian.png}
\,\,\,\,\,\,
\includegraphics[scale=0.16]{discrete_terrian/legged_gym_discrete.pdf}
\includegraphics[scale=0.12]{discrete_terrian/base_vel_x.pdf}
\includegraphics[scale=0.12]{discrete_terrian/base_vel_y.pdf}
%\includegraphics[scale=0.10]{quad_robot/base_vel_yaw}
%\includegraphics[scale=0.10]{quad_robot/base_vel_x.pdf}
%\includegraphics[scale=0.10]{quad_robot/base_vel_y.pdf}
%\includegraphics[scale=0.10]{quad_robot/base_vel_yaw}
\caption{(a) Performance of Pb-PPO on flat terrain. The first image visualizes the training curve of Pb-PPO, showing the average returns in a parallel environment of multiple robots. The remaining images (depicting linear velocities in the x and y directions, and angular velocity) visualize the physical values and corresponding commands during the evaluation process using the pre-trained policy to initialize 50 robots. (b) Performance of Pb-PPO on complex terrain. We used the policy trained in a complex environment, testing them in the flat-discrete terrain environment shown in the first image. The remaining images are similar to the content shown in (a).}
\label{real_simulation}
\vspace{-10pt}
\end{figure*}
\paragraph{Notations.} We first define crucial symbols. Specifically, we define $N_{\rm step}$ as the number of policy updating iterations, $N_{\epsilon_i}$ as the visitation counter for the bandit clipping bound (bandit arm) $\epsilon_i$, $R^{\rm bandit}$ as the total accumulated Return, which is the discounted sum of all evaluated Returns after updating the old policy across all training iterations (illustrated in line 24 of Algorithm~\ref{Pb_ppo_alg}, labeled by blue), and $R^{\rm bandit}_{\epsilon_i}$ as the discounted sum of all evaluated Returns after updating the old policy when choosing $\epsilon_i$.

\paragraph{Sampling clipping bound with alternate uncertainty term.} 
We sample the clipping bound with the highest UCB value, $\ie$, $\epsilon_i=\arg \max_{\epsilon}\{U^{\rm UCB}(\epsilon_i)=U(\epsilon_i)+\lambda\hat U(\epsilon_i)|\epsilon_i \in \zeta\}$. Despite Equation~\ref{ucb} strictly adhering to UCB theory, a concern we may encounter is the inherent fluctuation in RL training curves. If Equation~\ref{ucb} is directly employed, several issues may arise, including encountering local minima, especially when initially rolling out a trajectory with poor performance leading to a sustained large value for $\hat U$.

To address this concern, we utilize Equation~\ref{ucb_computing} instead to compute the uncertainty factor. Specifically, given the sampling times $N_{\epsilon_i}^{\rm bandit}$ of $\epsilon_{i}$ and total sampling times $N^{\rm bandit}=\sum_{\epsilon_i\in \zeta} N^{\rm bandit}_{\epsilon_i}$. If a certain clipping bound is sampled infrequently, resulting in a lower $N_{\epsilon_i}^{\rm bandit}$, it will correspondingly yield a higher $\frac{N^{\rm bandit}}{N^{\rm bandit}_{\epsilon_i}}$, leading to a larger $U^{\rm UCB}$. This encourages the exploitation of such clipping bounds. Additionally, ${\rm eps}$ is a very small float number set to prevent value overflow.
\begin{equation}
\begin{split}
    \hat U(\epsilon_i)=\sqrt{\frac{N^{\rm bandit}}{N^{\rm bandit}_{\epsilon_i}+{\rm eps}}}.
\end{split}    
\label{ucb_computing}
\end{equation}
\paragraph{Connection between Equation~\ref{ucb_computing} and Equation~\ref{ucb}.} Equation~\ref{ucb_computing} represents an modification or real implementation over Equation~\ref{ucb} based on some empirical analysis. From Equation~\ref{ucb_computing}, we can observe that for $\pi_{\epsilon}$ with relatively fewer samples, it will obtain a larger value of $\frac{N^{\rm bandit}}{N^{\rm bandit}_{\epsilon_i}+{\rm eps}}$, which further leads to a larger $\hat U(\epsilon_i)$. This encourages more exploration of such $\epsilon$. Meanwhile, the less explored $\epsilon$ is used less frequently, which may result in a larger variance in performance, corresponding to a larger  $R^{\rm max}_{\epsilon_i}-R^{\rm min}_{\epsilon_i}$, further we will have a larger $\hat U(\epsilon_i)$.  Therefore, it implies that $\epsilon$ that is fewer exploited will receive a larger $\hat U(\epsilon_i)$, which aligns with the pattern observed in Equation~\ref{ucb_computing}. As mentioned earlier, Equation~\ref{ucb_computing} can mitigate the impact of excessive fluctuations in the RL evaluation scores from Equation~\ref{ucb}. Therefore, we choose Equation~\ref{ucb_computing} to implement Pb-PPO.

\paragraph{Estimation of candidate clipping bounds' expected Return.} In this section, we discuss updating the expectation of arm $\epsilon_i$, $\ie$, updating $\mathbb{E}[R_{\epsilon_i}|\epsilon_i]$. Specifically, for each sampled arm $\epsilon_i$, we first update the policy $\pi_{\rm old}$ with the clipping bound $\epsilon_i$. We then evaluate this updated policy $\pi_{\rm new}$ k times, and calculate the average evaluated return $R^{\rm bandit}_{\epsilon_i}$, which serves as the reward $r_{N_{\epsilon_i}}$ for arm $\epsilon_i$. Subsequently, we update the expected return of sampling $\epsilon_i$, $\ie$, $\mathbb{E}[R_{\epsilon_i}|\epsilon_i]$, using Equation~\ref{R_arm}. Meanwhile, we can normalize the expected return of arm $\epsilon_i$ by calculating the advantage, as shown in Equation~\ref{expectation_update}.
\begin{equation}
    {\rm norm} (\mathbb{E}[R_{\epsilon_i}|\epsilon_i])=\mathbb{E}[R_{\epsilon_i}|\epsilon_i]-{\bar R^{\rm bandit}}.
    \label{expectation_update}
\end{equation}
\subsection{Practical Implementation of Pb-PPO}
\label{method_Pb_PPO}
Our codebase is constructed based on \texttt{stable\_baselines3}~\footnote{\url{https://github.com/DLR-RM/stable-baselines3}}, and the implementation has been detailed in Algorithm~\ref{Pb_ppo_alg}. In our practical implementation for Gym-Mujoco domain, we compute $R^{\rm bandit}_{\epsilon_i}$ by averaging the rollout performance over k=2 or 10 iterations, achieving stable performance with the weight of the random item $\lambda$ set to 5. Additionally, we generate candidate clipping bounds by uniformly sampling from a specified range. For example, when setting up 10 bounds with the minimum clipping bound as 0 and the maximum clipping bound as 1, we obtain the following 10 candidate clipping bounds: $\{0, 0.1, 0.2, 0.4, \cdots, 1\}$. Furthermore, we propose using Equation~\ref{expectation_update} to normalize the expected Return of the sampled bound $\epsilon_i$. This equation reflects the advantage of the current sampled clipping bound compared to the average expected Return across all candidate clipping bounds. We denote this setting as \textit{Pb-PPO (wi-ad)}. Additionally, we validate Pb-PPO without normalization, denoted as \textit{Pb-PPO (wo-ad)}. For access to our code, please visit: . 
For the settings of tasks sourced from legged-gym and AUV navigation domains, please refer to Appendix.


\section{Evaluation}
\paragraph{Our benchmarks.} 
To compare the performance of Pb-PPO with various PPO variants, we conducted evaluations on locomotion tasks in the Gym-Mujoco domain~\cite{1606.01540}. Additionally, to validate Pb-PPO's performance on robotic tasks, we test it in legged-gym and navigation tasks. Specifically, our Gym-Mujoco benchmarks include various of locomotion tasks such as \texttt{Walker2d-V3}, \texttt{Hopper-V3}, \texttt{HalfCheetah-V3}, and \texttt{Ant-V3}. For the legged-gym tasks~\cite{rudin2022learning}, we evaluated a quadruped (
The robot tag is \texttt{anymal-c}) robot's performance on both flat and complex terrains (\textit{In the legged gym environment, the training objective is a goal-conditioned policy, meaning that a policy is given a goal and makes decisions based on this goal.}). The navigation tasks are designed based on the ROS-Gazebo~\footnote{\url{https://gazebosim.org/docs/garden/ros_installation}} environment for autonomous vehicles. In this environment, the vehicle needs to navigate around various obstacles and reach three predefined targets sequentially. Based on the arrangement of obstacles and targets, the navigation environment can be divided into three levels: \texttt{easy}, \texttt{medium}, and \texttt{hard}, meanwhile, detailed environment settings can be found in Appendix.
\paragraph{Baselines.}
Our baseline models primarily consist of PPO variants with different clipping bounds. Additionally, we compare the final training results of Pb-PPO with Trust Region Policy Optimization (TRPO)~\citep{schulman2017trust}, and various on-policy algorithms including Deep Deterministic Policy Gradient (DDPG)~\citep{lillicrap2019continuous}, Trust Region-Guided PPO (TRGPPO)~\citep{wang2019trust}, and PPO-$\lambda$~\citep{chen2018adaptive}. 
\subsection{Experimental Results on Gym-Mujoco domain.} Our main experimental results are illustrated in Figures~\ref{fig_locomotion} and table~\ref{tab_final_res}. Overall, both settings of Pb-PPO demonstrate favorable performance in the selected locomotion tasks. Specifically, in our chosen locomotion tasks, Pb-PPO exhibits a advantage in the \texttt{Walker2d}, \texttt{HalfCheetah}, and \texttt{Ant} tasks, and it also demonstrates faster convergence in the \texttt{Hopper} tasks. Meanwhile, we summarize the final training results of PPO baseline and various on-policy algorithms in Table~\ref{tab_final_res}, where Pb-PPO outperforms the most of selected on-policy algorithms. Apart from its advantages in algorithm performance and convergence speed, Pb-PPO also shows consistently stable training curves across various locomotion tasks (without significant gaps).
\begin{figure*}[ht]
\begin{center}
\small
\includegraphics[scale=0.033]{demo/pb_ppo_turn.jpg}
\includegraphics[scale=0.033]{demo/ppo_turn.jpg}
\caption{The walking states of quadruped robots (\texttt{anymal-c}) on complex terrains. The upper figure shows the quadruped robot trained by Pb-PPO, and the lower figure shows the quadruped robot trained by PPO. In general, Pb-PPO exhibits a more stable gait when climbing stairs.} 
\label{fig_abili}
\end{center}
\vspace{-20pt}
\end{figure*} 
\begin{figure}[t]
\begin{center}
\small
\includegraphics[scale=0.16]{abili.pdf}
\includegraphics[scale=0.16]{improve_rate.pdf}
\caption{(a) Return of Pb-PPO across different ${\rm num}(\zeta)$. (b) The success rate of policy improvement.} 
\label{fig_abili}
\end{center}
\vspace{-20pt}
\end{figure}
Our main experimental results are illustrated in Figures~\ref{fig_locomotion} and table~\ref{tab_final_res}. Overall, both settings of Pb-PPO demonstrate favorable performance in the selected locomotion tasks. Specifically, in our chosen locomotion tasks, Pb-PPO exhibits a advantage in the \texttt{Walker2d}, \texttt{HalfCheetah}, and \texttt{Ant} tasks, and it also demonstrates faster convergence in the \texttt{Hopper} tasks. Meanwhile, we summarize the final training results of PPO baseline and various on-policy algorithms in Table~\ref{tab_final_res}, where Pb-PPO outperforms the most of selected on-policy algorithms. Apart from its advantages in algorithm performance and convergence speed, Pb-PPO also shows consistently stable training curves across various locomotion tasks (without significant gaps).
\subsection{Pb-PPO on robotic tasks} 
We further test Pb-PPO's performance in multiple simulation environments designed for real-world deployment, including autonomous vehicle and legged gym tasks.
\paragraph{Pb-PPO can complete navigation tasks of varying difficulty levels.} We tested Pb-PPO in a simulated environment for obstacle avoidance and navigation tasks with an autonomous vehicle. As shown in Figure~\ref{auto_car} (a), Pb-PPO has better Return curve that surpass the majority online algorithms including PPO and Soft Actor Critic (SAC)~\cite{haarnoja2018soft} on the hard navigation task. Furthermore, as shown in Figure~\ref{auto_car} (b), the policies trained with Pb-PPO can consistently navigate around all obstacles and reach the preset targets in nearly the shortest path, demonstrating Pb-PPO's stable evaluation performance in multi-target obstacle avoidance tasks. (\textit{Additionally, we supplement Pb-PPO in underwater obstacle avoidance tasks in the Appendix, where Pb-PPO still exhibits stable performance.})

\paragraph{Pb-PPO demonstrates more stable performance on quadruped robots compared to PPO.} We also evaluate Pb-PPO on quadruped robots in the legged-gym environment. As shown in Figure~\ref{real_simulation}(a), we train and test Pb-PPO on flat terrain in parallel. Compared to PPO, Pb-PPO demonstrate consistently better training curves. Meanwhile, we conducted parallel tests with 50 quadruped robots on flat terrain and found that Pb-PPO's policy more closely adhered to the given commands, thereby validating its superior stability. Furthermore, as illustrated in Figure~\ref{real_simulation}(b), we used pre-trained policies on discrete, flat 1:1 terrain and initialized 50 robots to test on flat-discrete mixed terrain. Pb-PPO exhibits a better Return curve, and stronger response to commands than PPO. We believe the advantage of Pb-PPO is due to its ability to dynamically adjust the clipping bound based on task feedback, achieving a stable balance between model updates and exploration. This advantage is reflected not only in the Return curves but also in the stability of the test tasks. However, in these quadruped robot tasks, the superiority of Pb-PPO over PPO in terms of Return curves was less pronounced compared to single-environment tests (Gym locomotion, navigation). We attribute this to the reward in parallel environments reflecting the overall task performance rather than setting the most appropriate bound for each policy with fine granularity. Therefore, Pb-PPO's performance in parallel environments should have to be further improved.

\paragraph{Actual motion states of policies trained by Pb-PPO.} Currently, in Figure~\ref{real_simulation}, we have not shown the motion states of more agents. Therefore, we further supplement more motion states in Figure~\ref{fig_abili}. As shown in Figure~\ref{fig_abili}, the policy trained by Pb-PPO exhibits higher stability when climbing stairs compared to PPO. Additionally, we have provided videos in the supplementary materials for reviewers to conduct systematic evaluations.

\section{Ablation}
\paragraph{Variations in the performance of Pb-PPO as influenced by different numbers of bandit arms {\rm num}$(\zeta)$.} In this section, we further validate the effectiveness of our approach by changing the number of bandit arms. Illustrated in Figure~\ref{fig_abili}, the training performance, averaged across $\texttt{Walker2d}$, $\texttt{Hopper}$, and $\texttt{Humanoid}$ for Pb-PPO (wi-ad), exhibits sensitivity to the number of bandits. Notably, the performance of Pb-PPO demonstrates improvement with an increasing ${\rm num}(\zeta)$ (across 3, 6, 12).  
\paragraph{Rendering the success rate of Pb-PPO's policy improvement.} We count the times that new policy is better than old policy as $N_{\rm success}$, and compute the success ratio by dividing the whole iterations $N$, $\ie$ $\frac{N_{\rm success}}{N}$. If  $\frac{N_{\rm success}(\texttt{Pb-PPO})}{N(\texttt{Pb-PPO})}$ is higher than $\frac{N_{\rm success}(\texttt{PPO})}{N(\texttt{PPO})}$, the feasibility of Pb-PPO can be further validated. As shown in Figure~\ref{fig_abili}, we average the training results across $\texttt{Hopper}$, $\texttt{Walker2d}$, $\texttt{Ant}$, $\texttt{HalfCheetah}$, our Pb-PPO achives $5.0\%$ success rate which is the best across all selected baselines.
\section{Conclusion and Limitation}
In this study, we propose Pb-PPO which utilizing bandit algorithm to dynamically adjust the clipping bound during the process of proximal policy optimization. Pb-PPO showcase better performance than PPO and PPO's variants, even can solve all tasks with single set of hyper-parameters. However, Pb-PPO's performance in parallel environments should have to be further improved. In the future, we will scale our approach to the setting reflecting human preference.
%\bibliographystyle{corl_2024}
\bibliography{aaai25}
\newpage
\onecolumn
\section{Mathematic Proof of Equation 5}
According to Hoeffdings inequality, we know that: 
\begin{equation}
\begin{split}
|\frac{R_{\epsilon_i}-\mathbb{E}[R|\epsilon_i]}{n}|\leq \frac{R^{\rm max}_{\epsilon_i}-R^{\rm min}_{\epsilon_i}}{n}\sqrt{\frac{1}{2}\ln \frac{2}{\sigma}} \\
\rightarrow R_{\epsilon_i}- \mathbb{E}[R|\epsilon_i]\leq (R^{\rm max}_{\epsilon_i}-R^{\rm min}_{\epsilon_i})\sqrt{\frac{1}{2}\ln \frac{2}{\sigma}} \\
\rightarrow R_{\epsilon_i}\leq \mathbb{E}[R|\epsilon_i]+(R^{\rm max}_{\epsilon_i}-R^{\rm min}_{\epsilon_i})\sqrt{\frac{1}{2}\ln \frac{2}{\sigma}},
\end{split}
\end{equation}

\section{Social Impact} In this research, we propose Pb-PPO, an enhanced version of PPO that achieves superior performance on robotic tasks. Additionally, Pb-PPO demonstrates stable evaluation results, making it promising for fields requiring consistent training. In the future, we plan to explore how to scale Pb-PPO to domains that reflect human preferences.
\section{Computing Resources}
We run each task multiple times. Our experiment are running on computing clusters with 16$\times$4 core cpu (Intel(R) Xeon(R) CPU E5-2637 v4 @ 3.50GHz), and 16$\times$RTX2080 Ti GPUs

\section{Scaled Experiments}
\begin{figure}[ht]
\centering
\includegraphics[scale=0.6]{AUV_sea.pdf}
\caption{Policy trajectory in AUV navigation system.}
\label{appendix_auv_simulation}
\vspace{-20pt}
\end{figure}
\paragraph{Underwater AUV simulation setting.} \label{water_auv}As shwon in Figure~\ref{appendix_auv_simulation}, the simulation is carried out on a 40m$\times$40m area with a water depth of -200m, on which the obstacles are randomly distributed. At the beginning, the position of the AUV is randomly distributed, and the AUV knows its own position. The area boundaries act as obstacles to restrict the AUVs in the specified area.
\section{Environments and hyper-parameters}
\label{navigation}
In the main context, we test Pb-PPO on costumed AUV navigation environment and legged-gym these two environments. Since AUV navigation environment is designed by our-self, we carefully detailed the modeling of this environment. In terms of legged-gym our training configuration is the same as the original set up of legged-gym, we additionally provide the command during the evaluation process. Our environment codebase is implemented based on this project (related to ROS-Gazebo):~\url{https://gitee.com/fangxiaosheng666/PPO-SAC-DQN-DDPG}
\label{navigation}
\paragraph{AUV Navigation.\label{environment_setup:auv_environment}} We describe the AUV navigation problem as a Markov decision process (MDP), which can be defined by a quintuple, i.e.
\begin{equation}\label{eq:10}
\mathcal{M} = (\mathcal{S},\mathcal{A},d_{\mathcal{M}}(\cdot \mid \bs_t,\ba_{t}),\mathcal{R},\gamma),
\end{equation}
where $\mathcal{S}$ and $\mathcal{A}$ denotes the state and action space of the AUV, respectively. Moreover, $\gamma$ is the discount factor, while $d_{\mathcal{M}}(\cdot\mid \bs_t,\ba_{t})$ represents the state transition probability function. To be intuitive, at time $t$, AUV selects the action $\ba_{t}\in\mathcal{A}$ according to its policy $\pi_\theta$ by observing the current state $\bs_t\in\mathcal{S}$, and transitions to the next state $\bs_{t+1}\sim d_{\mathcal{M}}(\cdot \mid \bs_t,\ba_{t})$ and gets the reward $r(t)\in\mathcal{R}$. The details are as follows:

{\bf State space:} In the navigation task, the observation space of AUV at time $t$ is $\bs_t$, which can be defined as
\begin{equation}\label{eq:11}
\begin{aligned}
\bs_t=[{l}(t), l_{A\leftrightarrow T}(t), \theta(t), \phi_{A\leftrightarrow T}(t), \chi(t)],
\end{aligned}
\end{equation}
where ${l}(t)$ contains the distances detected by sonar between the AUV and various obstacles, while $l_{A\leftrightarrow T}(t)$ represents the distance between the AUV and the target point. $\theta(t)$ and $\phi_{A\leftrightarrow T}(t)$ respectively indicate the orientation angle (yaw angle) of the AUV and the angle between the AUV and the target point. Furthermore, $\chi(t)\in\left\{0,1 \right\}$, and $\chi(t)=1$ indicates the current training episode has concluded, while vice versa. 

{\bf Action space:}
In the process of navigation task, the AUV makes action $\ba_{t}$ at time $t$ by observing the state $\bs_t$ and action $\ba_{t}$, which can be given by
\begin{equation}\label{eq:12}
\ba_{t}=\left [v(t), \omega\ba_{t}\right],
\end{equation}
where $\left\|v(t)\right\|=\sqrt{v_{x}(t)^2+v_{y}(t)^2}$ and $\left\|\omega\ba_{t}\right\|$ indicate the linear and angular velocity of the AUV, respectively. And the AUV can adjust its own motion state by changing its linear and angular velocity.

{\bf Reward function:}
We need to design the corresponding reward function to guide the AUV to make reasonable decisions in the complex environment to optimize the navigation trajectory to safely complete the navigation task. The rewards received by the AUV at time $t$ consist of the following parts
\begin{equation}\label{eq:13}
r_{c}(t)=-500 \operatorname{ceil}\left(l_{\rm safe} / \min \left({l}(t)\right)\right),
\end{equation}
\begin{equation}\label{eq:14}
r_{g}(t)=1000 \operatorname{ceil}\left(l^{\max }_{A \leftrightarrow T} / l_{A \leftrightarrow T}(t)\right),
\end{equation}
\begin{equation}\label{eq:15}
r_{e}(t)=-0.2+5(l_{A \leftrightarrow T}(t-1)-l_{A \leftrightarrow T}(t))+2(\phi_{A\leftrightarrow T}(t-1)-\phi_{A\leftrightarrow T}(t)),
\end{equation}
where $r_{c}(t)$ is a penalty term used to prevent the AUV from colliding with the obstacles, while $l_{\rm safe}$ is the safe distance between the AUV and the obstacles, and ceil($x$) is the binary function, which means that ceil($x$) equals to 1 when $x\geq1$, and equals to 0 when $x\leq1$. Additionally, when the AUV detects the target point for the first time, it receives a reward $r_{g}(t)$. In addition, we use the reward item $r_{e}(t)$ to encourage the AUV to get closer to the target point. Therefore, the total reward available for AUV at time $t$ can be weighted by
\begin{equation}\label{eq:16}
r(t)=\delta_c r_{c}(t)+\delta_{g} r_{g}(t)+\delta_{e} r_{e}(t),
\end{equation}
where $\delta_c$, $\delta_{g}$ and $\delta_{e}$ are the weights of each reward or penalty item, respectively, which can be adjusted according to the application needs. 

Based on the above analysis, we summarize several engineering constraints that need to be considered during the actual navigation process, and formulate a constraint optimization problem whose goal is to optimize the policy of the AUV to maximize the total expected return. The constrained optimization problem can be expressed as
\begin{subequations}
\begin{align}
\max _{\pi_{\theta}} J\left(\theta\right)=\max _{\pi_{\theta}} &\mathbb{E}\left[\sum_{t^{\prime}= t}^{T=\infty} \gamma^{t^{\prime}- t} r_{t^{\prime}}\left({s}(t), \pi_{\theta}\left({a}(t) \mid {s}(t)\right)\right)\right],\label{eq:17a}\\
&s.t.~ \min \left({l}(t)\right) \geq l_{\rm safe},\label{eq:17b}\\
&s.t.~ l_{A\leftrightarrow T}(t) \leq l^{\max }_{A \leftrightarrow T},\label{eq:17c}\\
v_{\min } \leq\|{v}(t)\|& \leq v_{\max }, \omega_{\min } \leq\left\|\omega\ba_{t}\right\| \leq \omega_{\max },\label{eq:17d}
\end{align}
\end{subequations}
where Equation~\ref{eq:17a} denotes the optimization objective, and Inequality~\ref{eq:17b} represents the constraint that prevents the AUV from colliding with obstacles. Moreover, Inequality~\ref{eq:17c} stands for the constraint that ensures the AUV to get to the target point, while Inequality~\ref{eq:17d} restricts the velocity and angular velocity range of the AUV. 

\paragraph{Legged Gym.} We utilize the default configuration of \texttt{"anymal-c"} in legged-gym for policy training and evaluation. In particular, the default configuration of \texttt{"anymal-c"} can be referred to \url{https://github.com/leggedrobotics/legged_gym/tree/master/legged_gym/envs/anymal_c}, which includes plat and complex terrain these two different kinds of settings. Additionally, our complex terrain are composed of 1:1 flat and discrete sub-terrains. During the evaluation stage, we set up the command as shown in table~\ref{legged_command}, while maintaining the original parameters as: \url{https://github.com/leggedrobotics/legged_gym/blob/master/legged_gym/scripts/play.py}
\begin{table}[ht]
\centering
\vspace{-10pt}
\caption{Command setup when conducting evaluation.}
\begin{tabular}{c|c}
\toprule
Physical standard&range \\
\midrule
x-axis v& [-1.0 m/s, 1.0 m/s] \\
y-axis v& [-1.0 m/s, 1.0 m/s] \\
angular v& [-0.01 rad/s, 0.01 rad/s]\\
heading&[-3.14, 3.14]\\
\bottomrule
\end{tabular}
\label{legged_command}
\end{table}
\label{leg_g}

\end{document}

%===============================================================================

\clearpage
% The acknowledgments are automatically included only in the final and preprint versions of the paper.
\acknowledgments{If a paper is accepted, the final camera-ready version will (and probably should) include acknowledgments. All acknowledgments go at the end of the paper, including thanks to reviewers who gave useful comments, to colleagues who contributed to the ideas, and to funding agencies and corporate sponsors that provided financial support.}

%===============================================================================

% no \bibliographystyle is required, since the corl style is automatically used.
\bibliography{example}  % .bib

\end{document}