\begin{figure}[h!]
    \centering
    \includegraphics[width=0.90\textwidth]{images/cover_image_corl.pdf}
    \caption{
    ANYmal-D hardware experiments in indoor/outdoor terrains (stairs, random blocks, high obstacles). The white arrow marks the direction of movement toward the specified goal position. The robot employs an animal-like gait to traverse uneven ground and avoid obstacles, demonstrating our policyâ€™s transfer of flat-ground motion priors to complex environments. (See supplementary videos at \url{https://anymalprior.github.io/})
    }
    \label{fig:result_cover}
    \vspace{-0.6cm}
\end{figure}
\section{Introduction} \label{sec:intro}
\vspace{-0.2cm}
Legged locomotion remains one of the most challenging problems in robotics, and reinforcement learning (RL) has recently emerged as a promising approach to tackle this complexity.
Contemporary RL locomotion controllers have successfully enabled robots to achieve smooth and stable motion while accurately tracking base velocity~\citep{doi:10.1126/scirobotics.aau5872, doi:10.1126/scirobotics.abk2822,zhuang2024humanoid} or goal position commands~\cite{cheng2023parkour,zhang2024learningagilelocomotionrisky,ren2025vbcomlearningvisionblindcomposite} across various terrains.
Additionally, position-based locomotion policies have demonstrated the capability to perform effective local navigation without relying on an external high-level planning module.
Despite these advancements and their successful deployment on hardware platforms, both velocity-based and position-based locomotion controllers heavily depend on meticulous reward design and struggle to learn expressive, animal- or human-like motions.  

Motion imitation has emerged as an efficient alternative to alleviate these challenges by guiding RL with reference trajectories obtained from teleoperation~\citep{fu2024mobile, zhao2023learningfinegrainedbimanualmanipulation}, motion capture~\citep{he2025asap, Han_2024}, or trajectory optimization~\citep{vollenweider2022advancedskillsmultipleadversarial}. 
Such motion imitation-based policies have enabled natural and agile maneuvers, such as jumping~\citep{Han_2024, li2023versatile} and backflips~\citep{li2023learning}, which is particularly challenging to learn from scratch due to the extensive reward design and tuning.

However, motion imitation-based method is not without its limitations. 
Their performance is inherently bounded by the fidelity and specificity of the reference data.
When a policy is strictly forced to mimic a fixed set of demonstrations, it tends to overfit to the specific characteristics how the reference data are recorded. 
As a result, when deployed in an environment with different dynamics (e.g. terrain types), the policy often fails to reproduce the intended motion style consistently.
This issue arises due to covariate shift, the mismatch between the distribution of states encountered during training (from the demonstration data) and those encountered in the new environment, which can lead to degraded performance and style inconsistency.
Although recent works have sought to improve adaptation beyond the environment where demonstration data are collected~\citep{Han_2024,wu2023RAL,smith2023learningadaptingagilelocomotion}, these approaches either rely on reference trajectories through trajectory optimization method or are confined to setups with only minimal environmental differences. 
Adaptation in more challenging and diverse scenarios using readily accessible, minimally processed references, such as raw retargeted animal motion data, remains underexplored.

In this work, we propose a hierarchical reinforcement learning pipeline that addresses motion imitation challenges.
Our framework begins by pre-training motion priors using motion imitation-based RL on animal datasets collected exclusively from flat terrain. 
These priors subsequently facilitate the training of a high-level goal-reaching policy within a position-based formulation, enabling effective adaptation to complex, non-flat environments and accomplishing perceptive locomotion and local navigation.
During deployment, our policy navigates to target goals while preserving the natural animal-like motion style and executing smooth obstacle avoidance behaviors.

Our contributions are: \textbf{(a)} a hierarchical RL framework that combines pre-training of motion priors from flat-terrain animal data with a task training stage that learns residual corrections for improved adaptability; \textbf{(b)} a unified pipeline achieving both perceptive locomotion and local navigation in a smooth, animal-like gait; and \textbf{(c)} a comprehensive analysis of how motion priors and learned residuals influence overall task and locomotion performance.

Simulation and hardware experiments demonstrate that our framework not only retains the training efficiency of motion imitation but also effectively generalizes to challenging, real-world locomotion and local navigation tasks.
\vspace{-0.3cm}