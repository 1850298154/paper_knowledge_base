\section{Related Work} \label{sec:related_work}
\vspace{-0.2cm}
\subsection{Motion Imitation with Reinforcement Learning}
\vspace{-0.2cm}
Motion imitation via RL has emerged as a reliable strategy for acquiring diverse motor skills directly from reference data. 
In legged locomotion, such approaches have been effective at transferring demonstrative behaviors to real-world systems.
For instance, \citet{RoboImitationPeng20} extend Deepmimic framework~\citep{2018-TOG-deepMimic} to real quadrupeds by using retargeted animal motion data and reward defined by the tracking error between the reference and actual states.
Similarly, \citet{he2025asap} have shown that humanoid robots can achieve agile, long-range movements leveraging comparable techniques.
Complementary studies by \citet{li2024fld} and \citet{watanabe2025dfmdeepfouriermimic} further streamline the imitation pipeline by automating motion representation and phase labeling for Deepmimic-based methods.  

An alternative line of research employs adversarial motion priors (AMP), where a GAN-style discriminator learns the distribution of state trajectories from demonstration data~\citep{li2023versatile,li2023learning,2021-TOG-AMP,Escontrela22arXiv_AMP_in_real}. 
Nevertheless, AMP-based approaches still face inherent adversarial learning challenges, including instability during discriminator training and mode collapse, especially when the dataset contains diverse motions~\citep{Peng:EECS-2021-267}. 
In our work, we adopt the Deepmimic-based approach from \citet{li2024fld} due to its ability to yield refined motion representations and training tractability compared to the AMP-based methods.
\vspace{-0.2cm}
\subsection{Task Adaptation with Motion Priors}
\vspace{-0.2cm}
Motion imitation-based RL policies often serve as low-level motion priors that underpin more complex, high-level tasks.
These motion skills can be encoded into a low-dimensional latent space and reused by high-level task policies through hierarchical conditioning~\citep{Han_2024,luo2024universal,2022-TOG-ASE}.
Alternatively, some methods directly incorporate style rewards into task training~\citep{wu2023RAL,Escontrela22arXiv_AMP_in_real}.

For example, \citet{Han_2024} propose a two-stage training strategy where a low-level imitation policy based on animal motions is first acquired and then used to train high-level policies that adapt to various tasks on rough terrains. 
Although their method successfully generates animal-like motions across multiple tasks on various terrains, it remains dependent on training with motion data from uneven terrains and on manually calibrating the simulation environment to match the specific environmental setups under which the training data are collected for the imitation performance.

In contrast, \citet{wu2023RAL} achieve stable locomotion on complex terrains using motion priors learned solely from flat-terrain data, by integrating an AMP-based reward into the task training.
However, this approach relies on carefully crafted trajectories obtained via trajectory optimization algorithms and typically captures a trotting gait.
Extending the generalization of motion priors derived from raw retargetted animal data, including other gait patterns such as walking, pacing, or cantering into novel terrains, remains underexplored.
Our approach addresses this gap by learning joint residuals on top of low-level priors which is only trained on flat terrain, thereby enhancing adaptability to diverse and challenging environments beyond where the motion data are recorded.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{images/framework_overview.pdf}
    \caption{
    Overview of our training framework for quadruped locomotion and local navigation: (1) retarget animal mocap data based on the robotâ€™s configuration;(2) pre-train an FLD encoder and low-level policy on flat terrain for motion priors; (3) train a high-level teacher to output latent commands and joint residuals for rough-terrain adaptation; (4) distill into a student policy for sim-to-real using only noisy observations. Green dots: downsampled Velodyne-LiDAR scans; blue dots: elevation scans around the robot's feet.
    }
    \label{fig:framework}
    \vspace{-0.55cm}
\end{figure*}
\vspace{-0.2cm}
\subsection{Local Navigation at Locomotion Level}
\vspace{-0.2cm}
In autonomous legged robotics, local navigation is commonly managed by a high-level planning module that continuously outputs waypoints~\citep{anymalparkour,lorenz2021navigation,yang2023iplannerimperativepathplanning} or velocity commands~\citep{zhang2024navigation,lee2024navigation} for a low-level locomotion controller.
However, developing an integrated, end-to-end policy that simultaneously handles locomotion and navigation is desirable, as it leverages the full capabilities of the robot to determine optimal actions.

Prior work using position-based task rewards~\citep{ren2025vbcomlearningvisionblindcomposite,rudin2022advancedskillslearninglocomotion} has demonstrated emergent local navigation behavior within a single learned locomotion policy.
Yet, these approaches generally necessitate extensive reward shaping to produce smooth motions and are predominantly applied to local navigation tasks in near-flat terrains.

Building on these developments, our aim is to improve the efficiency and performance of local navigation training at the locomotion level.
By integrating motion priors derived from animal demonstrations (originally collected on flat ground) into a position-based locomotion framework, our pipeline produces a smooth, animal-like gait that empowers the robot to traverse varied terrains and efficiently circumvent local obstacles en route to its goal.
\vspace{-0.2cm}