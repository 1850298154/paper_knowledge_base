\section{Experiments} \label{sec:results}
\vspace{-0.2cm}
\subsection{Experiment Setup}
\vspace{-0.2cm}
We select ANYmal-D as our robot platform and introduce several experiments to demonstrate and verify the effectiveness of our framework. 
Our experiments aim to \textbf{(a)} test whether a policy enriched with latent motion priors and joint residual corrections can reliably navigate to target goals and avoid local obstacles on complex terrains; \textbf{(b)} evaluate how different penalties on residual actions affect locomotion and goal-reaching performance; and \textbf{(c)} examine the motion regularization performance improvement over a baseline RL controller trained from scratch under similar reward conditions.
\begin{figure*}[t!]
    \centering
        \includegraphics[clip, trim=0cm 0cm 0cm 0cm,width=0.90\textwidth]{images/joint_residual_rugged_slope_stairs_HFE_two_corl.pdf }
        \vspace{-0.1cm}
    \caption{Comparison of high-level joint residuals for the HFE joint on the left front (LF) and right front (RF) leg across two terrain types: (a) pyramid stairs and (b) pyramid slopes with a rugged surface (see \figref{fig:residual_terrain_levels}). Each terrain type is divided into five difficulty levels (displayed at the bottom of each subplot), with difficulty increasing from left (easy) to right (hard).}
    \label{fig:residual_low_level_twoterrains}
    \vspace{-0.2cm}
\end{figure*}
\begin{figure}[t!]
    \centering
    \includegraphics[clip, trim=0cm 0cm 0cm 0cm,width=0.91\textwidth]{images/variance_residual_stairs_slope_corl_new.pdf}
    \vspace{-0.1cm}
    \caption{Variance in joint residuals of LF-HFE joint, as observed in the experiment depicted in \figref{fig:residual_terrain_levels}. Darker colors denote higher involvement of joint residuals on specific terrain types.}
    \label{fig:var_joint_res}
    \vspace{-0.45cm}
\end{figure}
\vspace{-0.2cm}
\subsection{Simulation Results}
\vspace{-0.2cm}
\subsubsection{Low-level policy}
\vspace{-0.2cm}
We train and test a low-level policy that can demonstrate multiple animal-like motions imitated in simulation. 
As mentioned in \secref{sec:method}, the low-level policy mimics motions that include walking, pacing, and cantering gaits, and each is at different forward velocities. 
Each skill can be performed on flat terrain and also transition smoothly between each other by commanding different latent embeddings. \Figref{fig:dof_pos_transition} highlights the strong imitation performance of the low-level policy by comparing the reference positions with the actual positions of all joints.
\Figref{fig:foot_sequence_low_level} also shows the footfall sequence of individual motion skills learned in the low-level policy. 
\vspace{-0.2cm}
\subsubsection{High-Level Policy} \label{sec:highlevel}
\vspace{-0.2cm}
We test our high-level student policy in simulation under exteroceptive noise conditions that resemble real-world scenarios. 
The evaluation is carried out on multiple terrains, including stairs, boxes of varying heights, slopes with rugged surfaces, and high obstacles. 
Examples of these terrains can be found in \suppref{sec:hyper_low_high_detail}. 
In all terrain types tested, our high-level policy achieves a high success rate in reaching target positions. 
The policy effectively learns to generate appropriate residuals on top of the low-level motions. 
\blue{By integrating the animal-style smoothness derived from low-level motion priors with adaptive high-level residuals, } the robot exhibits natural gait behaviors and bypassed local obstacles directly through perception, eliminating the need for a separate navigation module to output waypoints. 
Refer to \suppref{sec:eval_student_policy_sim} for a detailed analysis of the performance.

As shown in \figref{fig:residual_low_level_twoterrains} and \figref{fig:var_joint_res}, we analyze the residual changes with the increasing difficulty levels of the terrains on the pyramid stairs and the pyramid slopes (\figref{fig:residual_terrain_levels}). 
Based on the results in \figref{fig:var_joint_res}, we observe an increasing trend in the variance of the joint residuals in all joints as the terrain on which the robot is walking becomes more challenging, suggesting a growing contribution of residuals. 
For a detailed analysis, we select the joint residuals of the joint LF-HFE and RF-HFE as a representative. 
According to the plots, the high-level policy continuously adjusts the joint residual based on the current terrain situation. 
Peaks with continuously increasing height can be observed in \figref{fig:residual_low_level_twoterrains} (a) and (b). 
The variation in the joint residual likely reflect changes in the robot's base orientation and the challenges of traversing steps of varying heights or slopes.
Since the low-level motions alone cannot fully address these conditions, an adaptive residual component is needed to maintain effective locomotion.
These results demonstrate that the high-level policy can successfully leverage joint residuals to generalize to non-flat and complex terrains.

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/stair_success_rate_corl_new.pdf}
        \label{fig:stairs_success_rate}
    \end{subfigure}
    \vspace{-0.5cm}
    \begin{subfigure}[b]{0.53\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/slope_success_rate_corl_new.pdf}
        \label{fig:slope_success_rate}
    \end{subfigure}
    \vspace{-0.10cm}
    \caption{Goal-reaching success rate on stairs and sloped terrains across different difficulty levels, with the left subplots representing ascent and the right ones representing descent. Each terrain type is evaluated over 100 trials, with randomized initial robot poses in each experiment.}
    \label{fig:success_rate_comparison}
    \vspace{-0.2cm}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.65\linewidth]{images/error_joint_penalty_flat_corl.pdf}
    \vspace{-0.25cm}
    \caption{Average deviation of actual joint positions from reconstructed reference trajectories with FLD decoder over five seconds of forward walking on flat ground, evaluated at different residual penalty weights. 
    Lower deviations indicate gait behavior that more closely matches the animal motions. }
    \label{fig:error_joint_flat}
    \vspace{-0.45cm}
\end{figure}

\vspace{-0.2cm}
\subsubsection{Impact of Penalty on Residual Actions}
\vspace{-0.2cm}
\blue{We find that training with decreasing values of the joint residual penalty results in stronger adaptation ability but produces less-regularized motions.} 
We train six high-level teacher policies by selecting six penalty weights of the joint residuals ranging from -100 to 0, and investigate how it would affect the performance of locomotion and task completion. 
As shown in \figref{fig:success_rate_comparison}, all policies achieve a high success rate in near-flat terrain. 
However, an excessively high penalty (the weight larger than -10) reduces the efficiency of tackling higher steps and steeper slopes. 
\blue{By contrast an overly low penalty (the weight less than -0.001) may allow residuals to grow without bound, yielding high success rate but resulting in gaits that no longer follow the regularized motions provided by the motion priors, even on flat and near-flat terrain (see~\figref{fig:error_joint_flat} and supplementary videos).
This illustrates a tradeoff between preserving the motion style imparted by the priors and boosting the policy’s adaptability across varied terrains.
}

The performance of our high-level policy underscores the effective regularization achieved by the motion priors embedded in the low-level policy.
Rather than engaging in extensive tuning of multiple reward weights, our approach hinges on a single primary penalty term that constrains the sum of the squared residuals.
Our comparative experiments further confirm that including this penalty is crucial for balancing task-completion rewards with deviations from the motion priors, thereby improving the learned policy’s overall performance.
\vspace{-0.2cm}
\subsubsection{Comparison with Baseline RL Model} \label{sec:com_baseline}
\vspace{-0.2cm}
To evaluate the impact of incorporating motion priors in task-specific training, we compare our high-level teacher policy with a baseline RL model that is trained entirely from scratch without leveraging any low-level motion priors.
Both models are provided with the same observation space configuration except for latent encodings. 
However, the baseline model’s action space is limited to a 12-dimensional vector of joint actions.
\blue{The results display that the baseline policy, trained under the same reward setup without additional tuning and exploration techniques, tends to exhibit a jumping gait despite its ability to navigate challenging terrains (see supplementary videos). 
Incorporating additional regularization terms tuning on base acceleration or vertical velocity, for example, could encourage more natural locomotion. 
In contrast, our model, which integrates motion priors, achieves animal-like gait under identical reward conditions only by including the penalty term for joint residuals. 
This indicates that the learned motion priors inherently enforce the natural movement style learned from animal data, enabling us to cut back on extra regularization and streamline the tuning process. 
}

\begin{figure*}[t!]
    \centering
        \includegraphics[clip, trim=0cm 0cm 0cm 0cm,width=0.98\textwidth]{images/hardware_experiment.pdf }
    \caption{Real world deployment of the high-level student policy. The sequence of images illustrates the ability of the robot to reach the goal position (blue map pin) on complex terrain while avoiding the local obstacles.
    }
    \label{fig:hardware_exp}
    \vspace{-0.4cm}
\end{figure*}
 
% [NOTE]: ask
\vspace{-0.2cm}
\subsection{Evaluation in Real World}
\vspace{-0.2cm}
In the final part, we evaluate the high-level student policy on the ANYmal-D robot in real-world environments (see \figref{fig:result_cover} and \figref{fig:hardware_exp}). 
The test area comprises random steps, stairs, and various obstacles that the robot must avoid. 
With a fixed goal provided, the robot navigate to the target while maintaining a smooth animal-style gait without requiring explicit waypoint generation for obstacle bypassing. 
We encourage readers to refer to supplementary videos for further details. 
These results further validate the locomotion and local navigation capabilities of our system in real-world scenarios. 
\vspace{-0.3cm}