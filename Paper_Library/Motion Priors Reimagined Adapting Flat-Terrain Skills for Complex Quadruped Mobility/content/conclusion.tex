\section{Conclusion} 
\label{sec:conclusion}
\vspace{-0.2cm}
We presented a hierarchical RL framework that fused low-level animal motion priors with high-level residual learning for goal-directed locomotion across complex terrains. 
This design reduced reward-tuning effort for motion regularization and improved the robustness and adaptability of flat-terrain skills. 
In simulation, we showed that learned joint residuals achieved a tradeoff between adhering to motion priors and adapting to novel terrains, and that incorporating these priors produced more natural animal-style gaits than baseline RL controllers under similar reward conditions.
Finally, hardware experiments with ANYmal-D confirmed its capability for perceptive locomotion and obstacle-aware local navigation across diverse terrains.