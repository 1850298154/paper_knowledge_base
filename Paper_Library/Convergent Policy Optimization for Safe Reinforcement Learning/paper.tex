\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
%\usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{subfigure}
\usepackage{color}

\usepackage[numbers]{natbib} 

\usepackage{mathtools}
\mathtoolsset{showonlyrefs=true}

\usepackage{xargs}    
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}



\title{Convergent Policy Optimization for Safe Reinforcement Learning}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

%\author{%
  %David S.~Hippocampus\thanks{Use footnote for providing further information
  %  about author (webpage, alternative address)---\emph{not} for acknowledging
  %  funding agencies.} \\
  %Department of Computer Science\\
  %Cranberry-Lemon University\\
  %Pittsburgh, PA 15213 \\
  %\texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
%}


\author{
  Ming Yu \thanks{The University of Chicago Booth School of Business, Chicago, IL. Email: \texttt{ming93@uchicago.edu}. }\\
  \And
  Zhuoran Yang \thanks{Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ.}\\
  \And
  Mladen Kolar \thanks{The University of Chicago Booth School of Business, Chicago, IL.}\\
  \And
  Zhaoran Wang \thanks{Department of Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL.} 
  }


\usepackage{mystyle}
\graphicspath{{figs/}}

\begin{document}

\maketitle

\begin{abstract}%
  We study the safe reinforcement learning problem with nonlinear
  function approximation, where policy optimization is formulated as a
  constrained optimization problem with both the objective and the
  constraint being nonconvex functions. For such a problem, we
  construct a sequence of surrogate convex constrained optimization
  problems by replacing the nonconvex functions locally with convex
  quadratic functions obtained from policy gradient estimators.  We
  prove that the solutions to these surrogate problems converge to a
  stationary point of the original nonconvex problem. Furthermore, to
  extend our theoretical results, we apply our algorithm to examples
  of optimal control and multi-agent reinforcement learning with
  safety constraints.
\end{abstract}


\input{Introduction}

\input{Background}

\input{Algorithm}

\input{Theoretical}

\input{Application}

\input{Experiment}


\clearpage
\bibliographystyle{plain}
\bibliography{paper}

\clearpage
\appendix


%\input{Experiment}

\input{other_applications}

\input{Proof}


\end{document}
