\section{Proof of Theorem \ref{thm:main}}
\label{sec:proof}

According to the choice of the surrogate function
\eqref{eq:quadratic_appriximation_J} and Assumption
\ref{assumption:bounded}, it is straightforward to verify that the
function $\overline J^{(k)} (\theta)$ defined in \eqref{eq:J_bar} is
uniformly strongly convex in $\theta$ for each iteration
$t$. Moreover, both $\overline J^{(k)} (\theta)$ and
$\nabla_{\theta} \overline J^{(k)} (\theta)$ are Lipschitz continuous
functions.


From Lemma 1 in \cite{ruszczynski1980feasible} we have
\begin{equation*}
\lim_{t \to \infty} \Big| \overline J^{(k)} (\theta) - \EE \big[ \tilde J(\theta, \theta_k, \tau ) \big] \Big| = 0.  
\end{equation*}
Since the function $\EE \big[ \tilde J(\theta, \theta_k, \tau ) \big]$
is Lipschitz continuous in $\theta_k$, we obtain that
\[
\Big| \overline J^{(k_1)} (\theta) - \overline J^{(k_2)} (\theta) \Big| \leq L_0 \cdot \| \theta_{k_1} - \theta_{k_2} \| + \epsilon,
\]
for some constant $L_0$ and the error term $\epsilon$ that goes to
$0$ as $k_1,k_2$ go to infinity. This shows that the function
sequence $\overline J^{(k_j)} (\theta)$ is equicontinuous.  Since
$\Theta$ is compact and the discounted cumulative reward function is
bounded by $r_{\max}/(1-\gamma)$, we can apply Arzela-Ascoli theorem
\cite{dunford1958linear, kelley2017general} to prove existence of
$\hat J(\theta)$ that converges uniformly. Moreover, since we apply
the same operations on the constraint function $D(\theta)$ as to the
reward function $J(\theta)$ in Algorithm~\ref{algo}, the above
properties also hold for $D(\theta)$.


The rest of the proof follows in a similar way as the proof of Theorem
1 in \cite{liu2018stochastic}.  Under
Assumptions~\ref{assumption:step_size} - \ref{assumption:feasible},
the technical conditions in \cite{liu2018stochastic} are satisfied by
the choice of the surrogate functions
\eqref{eq:quadratic_appriximation_J} and
\eqref{eq:quadratic_appriximation_D}. According to Lemma 2 in
\cite{liu2018stochastic}, with probability one we have
\[
\mathop{\lim\sup}_{k \to \infty} D(\theta_k) \leq D_0.
\]
This shows that, although in some of the iterations the convex
relaxation problem \eqref{eq:QCQP} is infeasible, and we have to solve
the alternative problem \eqref{eq:QCQP_alternative}, the iterates
$\{\theta_k\}$ converge to the feasible region of the original
problem \eqref{eq:constrained} with probability one. Furthermore, with
probability one, the convergent point $\tilde \theta$ is the optimal
solution to the following problem
\begin{equation}
\begin{aligned}
\label{eq:problem_converged}
\mathop{\textrm{minimize}}_{\theta \in \Theta} 
~~ \hat J(\theta)  \qquad
\text{subject to} \qquad
 \hat D(\theta)   \leq  D_0.
\end{aligned}
\end{equation}
The KKT conditions for \eqref{eq:problem_converged} together with the
Slater condition show that the KKT conditions of the original problem
\eqref{eq:constrained} are also satisfied at $\tilde \theta$. This
shows that $\tilde \theta$ is a stationary point of the original
problem almost surely.





%%% Local Variables:
%%% TeX-master: "paper"
%%% End:

