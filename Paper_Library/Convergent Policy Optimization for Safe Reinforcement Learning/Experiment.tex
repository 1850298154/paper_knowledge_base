

\section{Experiment}
\label{sec:experiment}

We verify the effectiveness of the proposed algorithm through
experiments. We focus on the LQR setting with a random initial state as
discussed in Section \ref{sec:application_LQR}. In this experiment we
set $x \in \RR^{15}$ and $u \in \RR^{8}$. The initial state
distribution is uniform on the unit cube:
$x_0 \sim \cD = \text{Uniform} \big( [-1,1]^{15} \big)$. Each element
of $A$ and $B$ is sampled independently from the standard normal
distribution and scaled such that the eigenvalues of $A$ are within
the range $(-1,1)$. We initialize $F_0$ as an all-zero matrix, and the
choice of the constraint function and the value $D_0$ are such that
(1) the constrained problem is feasible; (2) the solution of the
unconstrained problem does not satisfy the constraint, i.e., the
problem is not trivial; (3) the initial value $F_0$ is not feasible.
The learning rates are set as $\eta_k = \frac{2}{3}k^{-3/4} $ and
$\rho_k = \frac{2}{3}k^{-2/3} $. The conservative choice of step size
is to avoid the situation where an eigenvalue of $A-BF$ runs out of
the range $(-1,1)$, and so the system is stable. 
\footnote{The code is available at \url{https://github.com/ming93/Safe_reinforcement_learning}}

Figure \ref{fig:LQR_1_constraint} and \ref{fig:LQR_1_objective} show
the constraint and objective value in each iteration, respectively.
The red horizontal line in Figure \ref{fig:LQR_1_constraint} is for
$D_0$, while the horizontal line in Figure \ref{fig:LQR_1_objective}
is for the unconstrained minimum objective value.  We can see from
Figure \ref{fig:LQR_1_constraint} that we start from an infeasible
point, and the problem becomes feasible after about 100
iterations. The objective value is in general decreasing after
becoming feasible, but never lower than the unconstrained minimum, as
shown in Figure \ref{fig:LQR_1_objective}.


\paragraph{Comparison with the Lagrangian method.} 
We  compare our proposed method with the usual Lagrangian method.
For the Lagrangian method, we follow the algorithm proposed in
\cite{chow2017risk} for safe reinforcement learning, which iteratively
applies gradient descent on the parameter $F$ and gradient ascent on
the Lagrangian multiplier $\lambda$ for the Lagrangian function until
convergence.  

Table \ref{table:experiment_compare_our_Lagrangian} reports the
comparison results with mean and standard deviation based on 50
replicates.  In the second and third columns, we compare the minimum
objective value and the number of iterations to achieve it.  We also
consider an approximate version, where we are satisfied with the
result if the objective value exceeds less than 0.02\% of the minimum
value. The fourth and fifth columns show the comparison results for
this approximate version.  We can see that both methods achieve
similar minimum objective values, but ours requires less number of
policy updates, for both minimum and approximate minimum version.




%\begin{figure*}[htbp]
%\begin{minipage}[t]{0.45\linewidth}
%\centering
%\includegraphics[width=0.9\textwidth]{LQR_1_constraint}
%\caption{Constraint value $D(\theta_k)$ in each iteration.}
%\label{LQR_1_constraint}
%\end{minipage}
%\begin{minipage}[t]{0.45\linewidth}
%\centering
%\includegraphics[width=0.9\textwidth]{LQR_1_objective}
%\caption{Objective value $J(\theta_k)$ in each iteration}
%\label{LQR_1_objective}
%\end{minipage}
%\end{figure*}



\begin{figure}
    \centering
    \subfigure[Constraint value $D(\theta_k)$ in each iteration.]
    {
        \includegraphics[width=0.46 \textwidth]{LQR_1_constraint.pdf}\hfill
        \label{fig:LQR_1_constraint}
    }
    \,
    \subfigure[Objective value $J(\theta_k)$ in each iteration.]
    {
        \includegraphics[width=0.46\textwidth]{LQR_1_objective.pdf}\hfill
        \label{fig:LQR_1_objective}
    }
    \caption{An experiment on constrained LQR problem. The iterate starts from an infeasible point and then becomes feasible and eventually converges.}
    \label{fig:LQR}
\end{figure}



\begin{table}
\begin{center}
\begin{tabular}{c|cc|cc}
\hline
& min value & \verb|#| iterations & approx. min value & approx. \verb|#| iterations   \\\hline
Our method &   30.689 $\pm$ 0.114 & 2001 $\pm$ 1172 & 30.694 $\pm$ 0.114 & 604.3 $\pm$ 722.4\\ 
Lagrangian &    30.693 $\pm$ 0.113 & 7492 $\pm$ 1780 & 30.699 $\pm$ 0.113 & 5464 $\pm$ 2116  \\\hline
\end{tabular}
\end{center}
\caption{Comparison of our method with Lagrangian method}
\label{table:experiment_compare_our_Lagrangian}
\end{table}%




%\begin{figure}
%    \centering
%    \subfigure[Constraint value $D(\theta_k)$ in each iteration for our method.]
%    {
%        \includegraphics[width=0.4\textwidth]{compare_our_constraint}\hfill
%        \label{fig:compare_our_constraint}
%    }
%    \,\,\,
%    \subfigure[Objective value $J(\theta_k)$ in each iteration for our method.]
%    {
%        \includegraphics[width=0.4\textwidth]{compare_our_objective}\hfill
%        \label{fig:compare_our_objective}
%    }
%    \\
%    \subfigure[Constraint value $D(\theta_k)$ in each iteration for the Lagrangian method.]
%    {
%        \includegraphics[width=0.4\textwidth]{compare_L_constraint}\hfill
%        \label{fig:compare_L_constraint}
%    }
%    \,\,\,
%    \subfigure[Objective value $J(\theta_k)$ in each iteration for the Lagrangian method.]
%    {
%        \includegraphics[width=0.4\textwidth]{compare_L_objective}\hfill
%        \label{fig:compare_L_objective}
%    }
%    \caption{A comparison of our method and the Lagrangian method on constrained LQR problem.}
%    \label{fig:comparison_our_Lagrangian}
%\end{figure}









%%% Local Variables:
%%% TeX-master: "paper"
%%% End: