%!TEX root =paper.tex



\vspace{-1mm}
\section{Background}
\label{sec:background}

A Markov decision process is denoted by
$(\cS, \cA, P, \gamma, r, \mu)$, where $\cS$ is the state space, $\cA$
is the action space, $P$ is the transition probability distribution,
$\gamma \in (0,1)$ is the discount factor,
$r\colon \cS \times \cA \rightarrow \RR$ is the reward function, and
$\mu \in \cP(\cS)$ is the distribution of the initial state
$s_0\in \cS$, where we denote $\cP(\cX)$ as the set of probability
distributions over $\cX$ for any $\cX$.  A policy is a mapping
$\pi: \cS \rightarrow \cP(\cA)$ that specifies the action that an agent
will take when it is at state $s$.

\vspace{-1mm}
\paragraph{Policy gradient method.}

Let $\{ \pi_{\theta} \colon \cS \rightarrow \cP(\cA) \}$ be a
parametrized policy class, where $\theta \in \Theta$ is the parameter
defined on a compact set $\Theta$. This parameterization transfers the
original infinite dimensional policy class to a finite dimensional
vector space and enables gradient based methods to be used to maximize
\eqref{eq:expected_reward}.  For example, the most popular Gaussian
policy can be written as
$\pi(\cdot | s, \theta) = \cN \big(\mu(s, \theta), \sigma(s, \theta)
\big)$, where the state dependent mean $\mu(s, \theta)$ and standard
deviation $\sigma(s, \theta)$ can be further parameterized as
$\mu(s, \theta) = \theta_{\mu}^\top \cdot x(s)$ and
$\sigma(s, \theta) = \exp\big(\theta_{\sigma}^\top \cdot x(s)\big)$
with $x(s)$ being a state feature vector.  The goal of an agent is to
maximize the expected cumulative reward
\begin{equation}
  \label{eq:expected_reward}
  R(\theta) = \EE_\pi
  \biggl [ \sum_{t\geq 0} \gamma ^t \cdot r(s_t, a_t) \biggr ],  
\end{equation}
where $s_0 \sim \mu$, and for all $t\geq 0$, we have
$s_{t+1} \sim P(\cdot \given s_t, a_t)$ and
$a_t \sim \pi(\cdot \given s_t)$.
Given a policy $\pi(\theta)$, we define the state- and action-value functions of $\pi_{\theta}$, respectively, as 
\#
\label{eq:value_funcs}
V^{\theta} (s) = \EE_{\pi_{\theta} } \biggl [ \sum_{t\geq 0} \gamma ^t r(s_t, a_t) \bigggiven s_0 = s \biggr ],  \
Q^{\theta} (s,a) = \EE_{\pi_{\theta} } \biggl [ \sum_{t\geq 0} \gamma ^t r(s_t, a_t) \bigggiven s_0 = s, a_0 = a\biggr ].
\#
The policy gradient method updates the parameter $\theta$ through gradient ascent
\#
\theta_{k+1} = \theta_{k} + \eta \cdot \hat{\nabla}_{\theta} R(\theta_k),
\#
where $\hat{\nabla}_{\theta} R(\theta_k)$ is a stochastic estimate of
the gradient $\nabla _{\theta} R(\theta_k)$ at $k$-th iteration.  
Policy gradient method, as well as its variants (e.g. policy gradient with baseline \cite{sutton2018reinforcement}, neural policy gradient \cite{wang2019neural, liu2019neural, cai2019neural}) is widely used in reinforcement learning.
The gradient
$\nabla _{\theta} R(\theta)$ can be 
estimated according to the policy gradient
theorem \citep{sutton2000policy},
\#
\label{eq:pg_theorem}
\nabla_{\theta} R(\theta) = \EE \Big[ \nabla_{\theta} \log \pi_{\theta}(s,a) \cdot Q^{\theta}(s,a) \Big].
\#


\vspace{-1mm}
\paragraph{Actor-critic method.}


To further reduce the variance of the policy gradient method, we could
estimate both the policy parameter and value function
simultaneously. This kind of method is called actor-critic algorithm
\cite{konda2000actor}, which is widely used in reinforcement
learning. Specifically, in the value function evaluation
(\emph{critic}) step we estimate the action-value function
$Q^{\theta}(s,a)$ using, for example, the temporal difference method
TD(0) \cite{dann2014policy}. The policy parameter update
(\emph{actor}) step is implemented as before by the Monte-Carlo method
according to the policy gradient theorem \eqref{eq:pg_theorem} with
the action-value $Q^{\theta}(s,a)$ replaced by the estimated value in
the policy evaluation step. 


\vspace{-1mm}
\paragraph{Constrained MDP.}

In this work, we consider an MDP problem with an additional constraint
on the model parameter $\theta$. Specifically, when taking action at
some state we incur some cost value. The constraint is such that the
expected cumulative cost cannot exceed some pre-defined constant.  A
constrained Markov decision process (CMDP) is denoted by
$(\cS, \cA, P, \gamma, r, d, \mu)$, where
$d\colon \cS \times \cA \rightarrow \RR$ is the cost function and the
other parameters are as before.  The goal of an the agent in CMDP is
to solve the following constrained problem
\begin{equation}
\begin{aligned}
\label{eq:constrained}
&\mathop{\textrm{minimize}}_{\theta \in \Theta} 
~~J(\theta) =    \EE_{\pi_{\theta} } \biggl [ - \sum_{t\geq 0} \gamma ^t \cdot r(s_t, a_t) \biggr ] , \\
&\text{subject to}
~~D(\theta) =   \EE_{\pi_{\theta} } \biggl [  \sum_{t\geq 0} \gamma ^t \cdot  d(s_t, a_t) \biggr ]  \leq  D_0,
\end{aligned}
\end{equation}
where $D_0 $ is a fixed constant. We consider only one constraint
$D(\theta) \leq D_0$, noting that it is straightforward to generalize
to multiple constraints. Throughout this paper, we assume that both
the reward and cost value functions are bounded:
$\big|r(s_t, a_t)\big| \leq r_{\max}$ and
$\big|d(s_t, a_t)\big| \leq d_{\max}$. Also, the parameter space
$\Theta$ is assumed to be compact.







%%% Local Variables:
%%% TeX-master: "paper"
%%% End: