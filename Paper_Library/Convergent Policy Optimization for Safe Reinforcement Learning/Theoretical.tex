%!TEX root =paper.tex

\section{Theoretical Result}
\label{sec:theoretical}

In this section, we show almost sure convergence of the iterates
obtained by our algorithm to a stationary point.  We start by stating
some mild assumptions on the original problem \eqref{eq:constrained}
and the choice of some parameters in Algorithm \ref{algo}.

\begin{assumption}
\label{assumption:step_size}
The choice of $\{ \eta_k \}$ and $\{ \rho_k \}$ satisfy 
$\lim_{k \to \infty} \sum_k \eta_k = \infty$, $\lim_{k \to \infty} \sum_k \rho_k = \infty$
and
$\lim_{k \to \infty} \sum_k \eta_k^2 + \rho_k^2 < \infty$.
Furthermore, we have $\lim_{k \to \infty} \eta_k / \rho_k  = 0$ and $\eta_k$ is decreasing.
\end{assumption}

\begin{assumption}
\label{assumption:bounded}
For any realization, $J^*(\theta)$ and $D^*(\theta)$ are continuously
differentiable as functions of $\theta$.  Moreover, $J^*(\theta)$,
$D^*(\theta)$, and their derivatives are uniformly Lipschitz
continuous.
\end{assumption}

Assumption \ref{assumption:step_size} allows us to specify the
learning rates.  A practical choice would be $\eta_k = k^{-c_1}$ and
$\rho_k = k^{-c_2}$ with $0.5 < c_2 < c_1 < 1$. This assumption is
standard for gradient-based algorithms.  Assumption
\ref{assumption:bounded} is also standard and is known to hold for a
number of models.  It ensures that the reward and cost functions are
sufficiently regular.  In fact, it can be relaxed such that each
realization is Lipschitz (not uniformly), and the event that we keep
generating realizations with monotonically increasing Lipschitz
constant is an event with probability 0.  See condition iv) in
\cite{yang2016parallel} and the discussion thereafter.  Also, see
\cite{pirotta2015policy} for sufficient conditions such that both the
expected cumulative reward function and the gradient of it are
Lipschitz.


The following Assumption \ref{assumption:feasible} is useful only when
we initialize with an infeasible point. We first state it here and we
will discuss this assumption after the statement of the main theorem.

\begin{assumption}
\label{assumption:feasible}
Suppose $(\theta_S, \alpha_S)$ is a stationary point of the optimization problem
\begin{equation}
\begin{aligned}
\label{eq:relaxation_infeasible}
 \mathop{\textrm{minimize}}_{\theta, \alpha} ~~ \alpha \qquad
 \text{subject to} \qquad D(\theta) \leq D_0 + \alpha.
\end{aligned}
\end{equation}
We have that $\theta_S$ is a feasible point of the original problem
\eqref{eq:constrained}, i.e. $D(\theta_S) \leq D_0$.
\end{assumption}


We are now ready to state the main theorem. 
\begin{theorem}
\label{thm:main}
Suppose the Assumptions \ref{assumption:step_size} and
\ref{assumption:bounded} are satisfied with small enough initial step size $\eta_0$.
Suppose also that, either $\theta_0$ is a feasible point, or
Assumption \ref{assumption:feasible} is satisfied.  If there is a
subsequence $\{ \theta_{k_j} \}$ of $\{ \theta_k \}$ that converges to
some $\tilde\theta$, then there exist uniformly continuous functions
$\hat J(\theta)$ and $\hat D(\theta)$ satisfying
\begin{equation*}
  \lim_{j \to
  \infty} \overline J^{(k_j)} (\theta) = \hat J(\theta) \qquad
\text{and}\qquad \lim_{j \to \infty} \overline D^{(k_j)} (\theta) =
\hat D(\theta).
\end{equation*}
Furthermore, suppose there exists $\theta$ such
that $\hat D(\theta) < D_0$ (i.e. the Slater's condition holds), then
$\tilde\theta$ is a stationary point of the original problem
\eqref{eq:constrained} almost surely.
\end{theorem}

The proof of Theorem \ref{thm:main} is provided in the supplementary
material.

Note that Assumption \ref{assumption:feasible} is not necessary if we
start from a feasible point, or we reach a feasible point in the
iterates, which could be viewed as an initializer.  Assumption
\ref{assumption:feasible} makes sure that the iterates in Algorithm
\ref{algo} keep making progress without getting stuck at any
infeasible stationary point. A similar condition is assumed in
\cite{liu2018stochastic} for an infeasible initializer.  If it turns
out that $\theta_0$ is infeasible and Assumption
\ref{assumption:feasible} is violated, then the convergent point may
be an infeasible stationary point of \eqref{eq:relaxation_infeasible}.
In practice, if we can find a feasible point of the original problem,
then we proceed with that point. Alternatively, we could generate
multiple initializers and obtain iterates for all of them. As long as
there is a feasible point in one of the iterates, we can view this
feasible point as the initializer and Theorem \ref{thm:main} follows
without Assumption \ref{assumption:feasible}.  In our later
experiments, for every single replicate, we could reach a feasible
point, and therefore Assumption \ref{assumption:feasible} is not
necessary.




Our algorithm does not guarantee safe exploration during the training phase. 
Ensuring safety during learning is a more challenging problem. 
Sometimes even finding a feasible point is not straightforward, 
otherwise Assumption \ref{assumption:feasible} is not necessary.




Our proposed algorithm is inspired by \cite{liu2018stochastic}.
Compared to \cite{liu2018stochastic} which deals with an optimization
problem, solving the safe reinforcement learning problem is more
challenging.  We need to verify that the Lipschitz condition is
satisfied, and also the policy gradient has to be estimated (instead
of directly evaluated as in a standard optimization problem).  The
usage of the Actor-Critic algorithm reduces the variance of the sampling,
which is unique to Reinforcement learning.








%%% Local Variables:
%%% TeX-master: "paper"
%%% End: