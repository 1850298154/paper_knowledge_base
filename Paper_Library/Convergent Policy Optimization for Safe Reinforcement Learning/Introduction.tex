%!TEX root =paper.tex

\section{Introduction}
\label{sec:introduction}

Reinforcement learning \citep{sutton2018reinforcement} has achieved
tremendous success in video games \citep{mnih2015human,
  peng2017multiagent, sun2018tstarbots, lee2018modular,xu2018macro}
and board games, such as chess and Go \citep{silver2016mastering,
  silver2017mastering, silver2018general}, in part due to powerful
simulators \citep{bellemare2013arcade, vinyals2017starcraft}.  In
contrast, due to physical limitations, real-world applications of
reinforcement learning methods often need to take into consideration
the safety of the agent \citep{amodei2016concrete,
  garcia2015comprehensive}.  For instance, in expensive robotic and
autonomous driving platforms, it is pivotal to avoid damages and
collisions \citep{fisac2018general, berkenkamp2017safe}.
In medical applications, we need to consider the switching cost \cite{bai2019provably}.
 
A popular model of safe reinforcement learning is the constrained
Markov decision process (CMDP), which generalizes the Markov decision
process by allowing for inclusion of constraints that model the
concept of safety \citep{altman1999constrained}.  In a CMDP, the cost is
associated with each state and action experienced by the agent, and
safety is ensured only if the expected cumulative cost is below a
certain threshold.  Intuitively, if the agent takes an unsafe action
at some state, it will receive a huge cost that punishes risky
attempts.  Moreover, by considering the cumulative cost, the notion of
safety is defined for the whole trajectory enabling us to examine the
long-term safety of the agent, instead of focusing on individual
state-action pairs. For a CMDP, the goal is to take sequential
decisions to achieve the expected cumulative reward under the safety
constraint.

Solving a CMDP can be written as a linear program
\cite{altman1999constrained}, with the number of variables being the
same as the size of the state and action spaces. Therefore, such an
approach is only feasible for the tabular setting, where we can
enumerate all the state-action pairs. For large-scale reinforcement
learning problems, where function approximation is applied, both the
objective and constraint of the CMDP are nonconvex functions of the
policy parameter.  One common method for solving CMDP is to formulate
an unconstrained saddle-point optimization problem via Lagrangian
multipliers and solve it using policy optimization algorithms
\cite{chow2017risk, tessler2018reward}. Such an approach suffers the
following two drawbacks:

First, for each fixed Lagrangian multiplier, the inner minimization
problem itself can be viewed as solving a new reinforcement learning
problem. From the computational point of view, solving the
saddle-point optimization problem requires solving a sequence of MDPs
with different reward functions. For a large scale problem, even
solving a single MDP requires huge computational resources, making
such an approach computationally infeasible.

Second, from a theoretical perspective, the performance of the
saddle-point approach hinges on solving the inner problem optimally.
Existing theory only provides convergence to a stationary point where
the gradient with respect to the policy parameter is zero
\citep{grondman2012survey, li2017deep}.  Moreover, the objective, as a
bivariate function of the Lagrangian multiplier and the policy
parameter, is not convex-concave and, therefore, first-order iterative
algorithms can be unstable \citep{goodfellow2014generative}.

In contrast, we tackle the nonconvex constrained optimization problem
of the CMDP directly. We propose a novel policy optimization
algorithm, inspired by \cite{liu2018stochastic}. Specifically, in each
iteration, we replace both the objective and constraint by quadratic
surrogate functions and update the policy parameter by solving the new
constrained optimization problem.  The two surrogate functions can be
viewed as first-order Taylor-expansions of the expected reward and
cost functions where the gradients are estimated using policy gradient
methods \citep{sutton2000policy}.  Additionally, they can be viewed as
convex relaxations of the original nonconvex reward and cost
functions. In \S\ref{sec:theoretical} we show that, as the algorithm
proceeds, we obtain a sequence of convex relaxations that gradually
converge to a smooth function. More importantly, the sequence of
policy parameters converges almost surely to a stationary point of the
nonconvex constrained optimization problem.


\paragraph{Related work.} 
Our work is pertinent to the line of research on CMDP
\citep{altman1999constrained}. For CMDPs with large state and action
spaces, \cite{chow2018lyapunov} proposed an iterative algorithm based
on a novel construction of Lyapunov functions. However, their theory
only holds for the tabular setting.  Using Lagrangian multipliers,
\citep{prashanth2016variance,chow2017risk,achiam2017constrained,
  tessler2018reward} proposed policy gradient
\citep{sutton2000policy}, actor-critic \citep{konda2000actor}, or
trust region policy optimization \citep{schulman2015trust} methods for
CMDP or constrained risk-sensitive reinforcement learning
\citep{garcia2015comprehensive}.  These algorithms either do not have
convergence guarantees or are shown to converge to saddle-points of
the Lagrangian using two-time-scale stochastic approximations
\citep{borkar1997stochastic}. However, due to the projection on the
Lagrangian multiplier, the saddle-point achieved by these approaches
might not be the stationary point of the original CMDP problem. In
addition, \cite{wen2018constrained} proposed a cross-entropy-based
stochastic optimization algorithm, and proved the asymptotic behavior
using ordinary differential equations.  In contrast, our algorithm and
the theoretical analysis focus on the discrete time CMDP.  Outside of
the CMDP setting, \citep{huang2018learning, lacotte2018risk} studied
safe reinforcement learning with demonstration data,
\cite{turchetta2016safe} studied the safe exploration problem with
different safety constraints, and \cite{ammar2015safe} studied
multi-task safe reinforcement learning.


\paragraph{Our contribution.} 
Our contribution is three-fold.  First, for the CMDP policy
optimization problem where both the objective and constraint function
are nonconvex, we propose to optimize a sequence of convex relaxation
problems using convex quadratic functions. Solving these surrogate
problems yields a sequence of policy parameters that converge almost
surely to a stationary point of the original policy optimization
problem. Second, to reduce the variance in the gradient estimator that
is used to construct the surrogate functions, we propose an online
actor-critic algorithm.  Finally, as concrete applications, our
algorithms are also applied to optimal control (in
\S\ref{sec:application_LQR}) and parallel and multi-agent
reinforcement learning problems with safety constraints (in
supplementary material).






%%% Local Variables:
%%% TeX-master: "paper"
%%% End: