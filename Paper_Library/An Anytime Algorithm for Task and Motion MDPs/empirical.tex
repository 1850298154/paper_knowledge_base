
\section{Empirical Evaluation}
\label{sec:empirical}
\begin{figure*}[t]
  \centering
  \includegraphics[width=2.2in]{figures/n05-h10}
  \includegraphics[width=2.2in]{figures/n10-h10}
  \includegraphics[width=2.2in]{figures/n20-h10}
\caption{\small Performance of our anytime algorithm for solving MDPs using
  dynamic abstractions. The plots from left to right corresponds to
  formulation of the problem with 5\%, 10\%, and 20\% rates of failure
  of the abstract actions described in the text. The blue lines (red
  lines) plot the probability mass of possible outcomes (proportion of
  nodes in the policy graph) that is covered by the partially computed
  policy as computation time (x axis, in seconds) evolves. }\label{fig:results_mdp}
\end{figure*}

We implemented the algorithms presented in Sec.\,\ref{sec:alg} using an implementation of LAO*~\citep{hansen01_lao} as the SSP solver. We used the OpenRAVE~\citep{diankov10_openrave} system for modeling and visualizing test environments and its collision checkers and RRT~\citep{lavalle2000rapidly} implementation for motion planning. Since there has been very little research on the task and motion planning problem in stochastic settings, there are no standardized benchmarks.
%with discount factor $\gamma=1$
%Variations among robot platforms and substantial diversity in research approaches and their corresponding input requirements have made it difficult to create benchmarks for task and motion planning even in deterministic settings~\cite{lagriffoul15_benchmarks}.  
We evaluated our algorithms by creating a hangar model in OpenRAVE for the aircraft inspection problem (Fig.\,\ref{fig:scenario}). UAV actions in this domain include actions for moving to various components of the aircraft, such as the left and right wings, nacelles, fuselage, etc. Each such action could result in the UAV reaching the specified component or a region around the component. The inspection action for a component had the stochastic effect of localizing a fault's location. The environment included docking stations that the UAV could reach and recharge on reserve battery power. Generators for concretizing all actions except the inspect action uniformly sampled  poses in the target regions. Some of these poses naturally lead to shorter trajectories and therefore lower battery usage, depending on the UAV's current pose. However, we used uniform-random samples to evaluate the performance of the algorithm while avoiding domain-specific enhancements. The generator for \emph{inspect($s$)} simulated an inspection pattern by randomly sampling five waypoint poses in an envelope around $s$ and ordering them along the medial axis of the component. We used a linear function of the trajectories to keep track of battery usage at the low level and to report insufficient battery as the \emph{failureReason} when infeasibility was detected. This function was used to provide failure reasons to the high-level when the battery level was found to be insufficient.


 Fig.\,\ref{fig:results_mdp} shows the performance of our
 approach for producing execution strategies with motion planning
 refinements as a function of the time for which the algorithm is
 allowed to run. The red lines show the number of nodes in the
 high-level policy that have been evaluated, refined, and potentially
 replaced with updated policies that permit low-level plans. The blue
 lines show the probability with which the policy available at any
 time during the algorithm's computation will be able to handle all
 possible execution-time outcomes. The different plots show how these relations change as we increase the level of uncertainty in the domain. The horizon is fixed at ten high-level decision epochs (each of which can involve arbitrarily long movements) and the number of parts with faults is fixed at two. The policy generated by LAO* is unrolled into a tree prior to the start of refinement. The reported times include the time taken for unrolling.


Our main result is that that our anytime algorithm balances complexity of computing task and motion policies with time very well and produces desirable concave anytime peformance profiles. Fig.\,\ref{fig:results_mdp} shows that when noise in the agent's actuators
and sensors is set at $5\%$, with $10\%$ of computation our algorithm computes an executable
policy that misses only the least likely $10\%$ of the possible execution outcomes. This policy is computed in less
than 10 seconds. In the worst case, with a 20\% error rate in actuators and sensors (sensors used in practice are much more reliable), we miss only about 20\% of the execution trajectories with 40\% of the computation.

 


