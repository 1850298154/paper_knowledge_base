\section{Introduction}

In order to be truly helpful, robots will need to be able to accept commands from humans at high-levels of abstraction, and autonomously execute them.
Consider the problem of inspecting an aircraft (Fig.\,\ref{fig:scenario}). 
%Today, this task requires extensive manual involvement. The time required and risks involved have compelled several organizations to enlist tele-operated UAVs to assist with the process. Such UAVs exhibit a limited level of autonomy: typically, they are able to compute their own motion plans for moving from one user-provided waypoint to another. Although this mode of operation reduces risk, it does not quite deliver the advances in efficiency that autonomous robot assistants have been widely imagined to provide. 
In order to autonomously plan and execute such a task, the robots (UAVs in this case) will need to be able to make  high-level inspection decisions on their own, while satisfying low-level constraints that arise from environment geometries and the limited capabilities of the UAVs. High-level decisions can include selecting where to go next, with whom to communicate, and what to inspect.  These decisions need to take into account the uncertainty in the UAV's actions.% and need to be made in a manner consistent with the geometries of the environment and the capabilities of the UAV.

\begin{figure}
  \includegraphics[height=1in]{figures/scenario_3.png}
  \includegraphics[height=1in]{figures/scenario_2.png}
\caption{The aircraft inspection scenario}\label{fig:scenario}
\end{figure}

For instance, at the start of an aircraft's inspection, one may know that the left wing has a structural problem, but the location of the fault may not be known precisely. When a UAV inspects the left wing, its sensors may succeed with probability 0.9, and so on. In order to solve this task autonomously, the UAV needs to select  which pose to fly to next, which trajectory to use in order to do so, and  the order in which to carry out inspections while making sure that it always has sufficient battery to return to the docking station and that it does not collide with any object in the environment. The feasibility of a high-level strategy for inspection therefore depends on the battery power required for each high-level operation such as ``move to left wing"; ``inspect left wing", etc., which in turn depends on the low-level motion plan selected, which in turn depends on the hangar's geometric layout and the physical geometry of the UAV. Throughout this paper, we will use the term ``high-level'' to represent a discrete MDP and ``low-level'' to refer to a motion planning problem.


The framework of Markov decision processes (MDPs) can express discrete sequential decision making (SDM) problems. Numerous advances have been made in solving MDPs~\citep{russell15_robust}. However, the scalability of these approaches relies upon a few key properties, including a bounded branching factor (or the set of possible actions) and the ability to express a problem accurately using discrete state variables. Both of these properties fail to hold in problems such as those described above. Recent work on deterministic, integrated task and motion planning~\citep{kaelbling11_hierarchical,erdem11_tmp,srivastava14_tmp,dantam16_incremental} shows that hierarchical approaches are useful for such problems. 

Computing task and motion policies for MDPs presents a new set of challenges not encountered in computing  task and motion  plans for deterministic scenarios. In particular, selecting an action for a state while ensuring a feasible refinement requires knowing the history of actions used to reach that state, since effects on properties that were abstracted away (such as battery usage) cannot be modeled accurately  at the high level. A direct application of classical task and motion planning techniques  is further limited by the number of possible high-level action paths that can be taken during an execution. Indeed, the task and motion planning literature makes it clear that computing a single high-level sequence of actions that is feasible with low-level constraints is a challenge; the extension to MDPs expands the problem to computing a feasible high-level sequence of actions for every possible stochastic outcome of a high-level action. 

In this paper, we investigate the problem of computing task and motion policies and show that principles of abstraction can be used to effectively model the problem, as well as to solve it by dynamically refining the abstraction used. We address the problem of computational complexity by developing an anytime algorithm that rapidly produces feasible policies for a high likelihood of scenarios that may be encountered during execution. Our methods can therefore be used to start the execution before the complete problem is solved; computation could continue during execution. The continual policy computation reports the probability of encountering situations which have not been resolved yet. This can be used to select the point at which execution is started in a manner appropriate to the application. In the worst case, if an unlikely event is encountered before the ongoing policy computation resolves it, execution could be brought to a safe state; in situations where this is not possible, one could wait for the entire policy to be computed with motion plans. In this way our approach offers a trade-off between pre-execution guarantees and pre-computation time requirements.

The rest of this paper is organized as follows. Sec.\,\ref{sec:background} introduces the main concepts that we draw upon from prior work. Sec.\,\ref{sec:formal} presents our formalization of abstractions and representations. This is followed by a description of our algorithms (Sec.\,\ref{sec:alg}). Sec.\,\ref{sec:empirical} presents an empirical evaluation of our approach in a test scenario that we created using open-source 3D models of aircraft and various hangar components. Sec.\,\ref{sec:related} discusses the relationship of the presented work and contributions with prior work. 
