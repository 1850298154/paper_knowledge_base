\section{Background}
\label{sec:background}
A Markov decision process (MDP) $\langle S, A, T, R, \gamma \rangle$
is defined by a set of states $S$, a set of actions $A$, a transition
function $T:S\times A \rightarrow \mu S$ that gives the probability
distribution over result states upon the application of an action on a
state; a reward function $R: S \rightarrow \mathbb{R}$; and a
discounting factor $\gamma \le 1$. We will use $T(s,a)$ as a function that maps a state to its probability. 
We will be particularly interested in
MDPs with absorbing states and $\gamma=1$, or, stochastic shortest path
problems~\citep{bertsekas91_ssp}. In this class of MDPs, the reward
function yields negative values (action costs) for states except the
absorbing states $G$. Absorbing states give zero rewards; once the
agent reaches an absorbing state, it stays in it: $\forall a \in A,
g\in G, R(g)=0; T(g,a)(g)=1$. We will consider SSPs that have a
known initial state $s_0$ and a finite time horizon, $h$, which
represents an uppper bound on the number of discrete decision making
steps available to the agent.

Solutions to MDPs are represented as policies. A policy $\pi: S \times
\set{1,\ldots, h} \rightarrow A$ maps a state and the timestep at
which it is encountered, to the action that the agent should execute
while following $\pi$. Given an MDP, the optimal ``policy" of the agent is defined as one that maximizes the expected long-term reward $\sum_{i=1}^h r_i$, where $r_i$ is the reward obtained at timestep $i$ following the function $R$. Our notion of policies includes non-stationary policies since the optimal policy in a finite horizon MDP need not be stationary. In principle, dynamic programming can be used to compute the optimal policy in this setting just as in the infinite horizon setting:

\begin{eqnarray}
V^0(s) &=& R(s)\\
V^i(s) &=& R(s) + max_a \sum_{s'} T(s,a)(s')V^{i-1}(s')
\end{eqnarray}

Here $V^i$ is the $i$-step-to-go value function. Since we are given the initial state $s_0$,
non-stationary policies can be expressed as finite state machines (FSMs). We will
consider policies that are represented as tree-structured FSMs, also
known as contingent plans. Several algorithms have been developed to
solve SSPs. The LAO* algorithm~\citep{hansen01_lao} was developed to
incorporate heuristics while computing solution policies for SSPs. %Kolobov et al.~
\cite{kolobov11_ssp} developed general methods for solving SSPs in
the presence of dead-ends. 




Specifying real-world sequential decision making problems as MDPs
using explicitly enumerated state lists usually results in large,
unweildy formulations that are difficult to modify, maintain, or
understand. Lifted, or parameterized representations for MDPs such as
FOMDPs~\citep{sanner09_fomdp}, RDDL ~\citep{sanner10_rddl} and
PPDDL~\citep{younes04_ppddl} have been developed for overcoming these
limitations. Such languages separate an MDP \emph{domain},
constituting parameterized actions, functions and predicate
vocabularies, from an MDP \emph{problem}, which expresses the
specific objects of each type and a reward function. We refer to
\cite{helmert09_pddl_grounding} for a general introduction to these
concepts. W.l.o.g, we consider the vocabulary to consist of predicates
alone, since functions can be represented as special predicates.  A
grounded predicate is a predicate whose parameters have been
substituted by the objects in an MDP problem. For instance, Boolean
valuations of the grounded predicate \emph{faultLocated(LeftWing)}
express whether %or not
the LeftWing's fault's precise location was %has been
identified.
In our framework, states are defined as valuations of grounded
predicates in a given problem.  Although this framework usually
expresses \emph{discrete} properties, it %the framework
can be extended
naturally to model actions that have continuous action arguments and
depend on and affect geometric properties of the environment.

\begin{figure}[t]
        \begin{small}
                \begin{tabular}{r|p{4in}}\multicolumn{2}{p{4in}}
                  {Action:
                    \emph{inspect(Structure $s$, Trajectory $tr$)}}\\
                  precond & $\emph{batterySufficient}(tr) \land $  \emph{inspects}($tr, s$)$\land$ collisionFree($tr$)\\
                  effect & \emph{faultLocated}$(s) \qquad 0.8$ \\
                  & $\lnot$\emph{faultLocated}$(s) \qquad 0.2$ \\
                  & \emph{decrease(batteryLevel($c(tr))$)}\\
                \end{tabular}
        \end{small}
        \caption{Specification of a  stochastic action model}\label{fig:inspect_concrete}
\end{figure}

\begin{example}
  \label{eg:inspect_concrete}
  Fig.\,\ref{fig:inspect_concrete} shows the specification for an
  \emph{inspect} action in the aircraft inspection domain in a
  language similar to PPDDL (some syntactic elements have been
  simplified for readability).  This action models the act of
  inspecting a structure $s$ while following the path $tr$. We use \emph{batterySufficient}$(tr)$ as an
  abbreviation for \emph{batteryRemaining}$ -
  $\emph{batteryRequired($tr$)}. Intuitively, the specification states that if this
  action is executed in a state where the  battery is sufficient and the selected trajectory satisfies constraints for being an inspection trajectory (the
  \emph{precondition} is satisfied),  it will result in locating
  the fault with the probability 0.8. In any case, the battery's
  charge will be depleted by an amount depending on the trajectory
  used for inspection $c(tr)$.  The \emph{inspects(c, tr)} predicate is true if the trajectory $tr$ ``covers" the given structure. Different interpretations for such predicates would result in different classes of coverage patterns.
\end{example}




