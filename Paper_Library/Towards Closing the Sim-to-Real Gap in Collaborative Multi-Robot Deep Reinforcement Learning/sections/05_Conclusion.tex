
\section{Conclusion and Future Work}\label{sec:conclusion}

Adversarial agents and closing the simulation-to-reality gap are among the key challenges preventing wider adoption of reinforcement learning in real-world applications. In this paper, we have addressed the latter one from the perspective of the former: by introducing adversarial conditions inspired by real-world perturbances to a subset of agents in a multi-robot system during a collaborative reinforcement learning process, we have been able to identify points where the robustness of distributed multi-agent DRL algorithms needs to be improved. In this paper, we have considered multiple robotic arms in a simulation environment collaborating towards learning a common policy to reach an object. In order to emulate more realistic conditions and understand how perturbances in the environment affect the learning process, we have considered variability across the agents in terms of their ability to sense and actuate accurately. We have shown how different types of disturbances in the model's input (sensing) and output (actuation) affect the robustness and ability to converge towards an effective policy. We have seen how variable perturbances have the most effect on the ability of the network to converge, while disturbances in the ability of the robots to actuate properly have had a comparatively worse effect than those in their ability to sense the position of the object accurately.

The conclusions of this work serve as a starting point towards the design and development of more robust methods able to identify and take into account these disturbances in the environment that do not occur across all robots equally. This will be the subject of our future work, as well as the study of other types or combinations of disturbances in the environment. We will also work towards modeling more accurately real-world errors for RL simulation environments.