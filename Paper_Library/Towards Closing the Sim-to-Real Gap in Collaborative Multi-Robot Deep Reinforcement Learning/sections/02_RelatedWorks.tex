
\section{Related Works}
\label{sec:related}

In this work, we study adversarial conditions in a simulation environment to emulate real-world conditions in terms of variability of the environment across a set of multiple agents collaborating in learning the same task. With most of the literature in simulation-to-reality transfer aiming at specific applications or adaptation to different environments~\cite{balaji2019deepracer, matas2018sim, arndt2019meta}, in this section we focus instead on previous works analyzing the effect of adversarial of byzantine effects in multi-agent reinforcement learning, as well as considering other perturbations in the environment to better emulate real-world conditions. The literature in adversarial conditions for collaborative multi-agent learning is, nonetheless, sparse.

Adversarial RL has been a topic of extensive study over the past years. Multiple deep learning algorithms have been shown to be vulnerable when subject to adversarial input perturbations, being able to induce certain policies~\cite{behzadan2017vulnerability}. This is a general problem of reinforcement learning that affects different types of algorithms and scenarios. In multi-agent environments, the ability of an attacker to create adversarial observations increases significantly~\cite{gleave2019adversarial}. A comprehensive survey on the main challenges and potential solutions for adversarial attacks on DRL is available in~\cite{ilahi2020challenges}. The authors classify attacks in four categories: attacks targeting (i) rewards, (ii) policies, (iii) observations, and (iv) the environment. Among these, those targeting observations and the environment are the most relevant within the scope of this survey. In most of these cases, however, the literature only considers single-agent learning (or multiple agents being affected in the same way). Moreover, previous works focus on malicious perturbations aimed at decreasing the performance of the learning agent. In this paper, nonetheless, we induce perturbations that are inspired by real-world issues including changes in accuracy or calibration errors.

Other authors have explored the effects of having noisy rewards in RL. In this direction, Wang et al. presented an analysis of perturbed rewards for different RL algorithms, including PPO but also DQN and DDPG, among others~\cite{wang2020reinforcement}. Compared to their approach, we also consider perturbances on the RL process but focus on those that model real-world noises and errors. Moreover, we specifically put an emphasis on multi-robot collaborative learning, and consider situations in which the perturbances that affect different robots are also different. We also focus on the PPO algorithm as the state-of-the-art in three-dimensional locomotion. In fact, PPO has been identified as one of the most robust approaches against reward perturbances in~\cite{wang2020reinforcement}. Also within the study of noisy rewards, a method to improve performance in such scenarios is proposed in~\cite{kumar2019enhancing}.

In general, we see a gap in the literature in the study of noisy or perturbed environments that do not affect equally across multiple agents collaborating towards learning the same task. This paper thus tries to address this issue with an initial assessment of how perturbations in the environment influencing a subset of agents affect a global common model where experiences from different agents are aggregated.