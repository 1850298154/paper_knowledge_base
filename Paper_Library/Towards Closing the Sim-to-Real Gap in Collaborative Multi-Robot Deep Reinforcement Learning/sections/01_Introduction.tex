
\section{Introduction}

Reinforcement learning (RL) algorithms for robotics and cyber-physical systems have seen an increasing adoption across multiple domains over the past decade~\cite{arulkumaran2017brief, nguyen2020deep}. Deep reinforcement learning (DRL) enables agents to be trained in realistic environments without the need for large amounts of data to be gathered and labeled a priori. Specifically, reinforcement learning has enjoyed significant success in robotic control tasks involving manipulation~\cite{mnih2016asynchronous, rajeswaran2017learning, matas2018sim}. Motivated by the way humans and animals learn, DRL algorithms work on a \textit{trial and error} basis, where an agent interacts with its environment and receives a reward based on its performance. When complex agents or environments are involved, the learning process can be relatively slow. This has motivated the design and development of multi-agent DRL algorithms. In this paper, we are interested in exploring some of the challenges remaining in multi-robot collaborative DRL.

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{fig/concept_v3.pdf}
    \caption{Conceptual view of the proposed scenario, where multiple robotic agents are collaboratively learning the same task. While the task is common, and the agents are a priori identical, we study how different alterations in the agents or their environments affects the performance of the collaborative learning process.}%The results of such study have potential for enabling more robust simulation-to-reality methods in deep reinforcement learning.}
    \label{fig:concept}
    \vspace{-1em}
\end{figure}

Reinforcement learning applied to multi-agent systems has two dimensions: DRL algorithms that model policies for multi-agent control and interaction, and DRL approaches that rely on multiple agents to parallelize the learning process or explore a wider variety of experiences. Within the former category, we can find examples of DRL for formation control~\cite{conde2017time}, obstacle and collision avoidance~\cite{chen2017decentralized, long2018towards}, collaborative assembly~\cite{schwung2017application}, or cooperative multi-agent control in general~\cite{gupta2017cooperative}. In the latter category, most existing approaches refer to the utilization of multiple agents to learn in parallel, but from the point of view of a multi-process or multi-threaded application~\cite{mnih2016asynchronous}. We are interested in works exploring the possibilities of using multiple robotic agents that collaborate on learning the same task. This has been identified as one of the future paradigms with 5G-and-beyond connectivity and edge computing~\cite{queralta2020enhancing, queralta2020blockchain}. For instance, in~\cite{gu2017deep} an asynchronous method for off-policy updates between robots was presented. Other works also consider network conditions and propose frameworks for multi-agent collaborative DRL over imperfect network channels~\cite{yu2020multi}. This type of scenario is illustrated in Fig.~\ref{fig:concept}, where three robotic arms are collaboratively learning the same task and sharing their experiences to update a common policy. Hereinafter, we refer to these types of scenarios as multi-agent or multi-robot collaborative RL tasks, where multiple agents collaborate to learn the same task but might be exposed to different environments, or work under different conditions.

Among the multiple challenges in DRL, recent years have seen a growing research interest in closing the simulation-to-reality gap~\cite{matas2018sim, balaji2019deepracer}, and on the design and development of robust algorithms with resilience against adversarial conditions~\cite{behzadan2017vulnerability, gleave2019adversarial, wang2020reinforcement}. This latter topic is also of paramount relevance in distributed or multi-agent DRL, where adversarial agents can hinder the collaborative learning process~\cite{song2018multi}. When multiple agents are learning a collaborative or coordinated behavior, byzantine agents can significantly reduce the performance of the system as a whole.

We aim at studying how adversarial conditions can help to bridge the simulation-to-reality gap. In~\cite{matas2018sim} and~\cite{balaji2019deepracer}, the authors analyze perturbances in the rewards towards the applicability of DRL in real-world applications. In~\cite{matas2018sim}, the focus is on learning how to manipulate deformable objects, with agents trained in a simulation environment but directly deployable in the real-world. In~\cite{arndt2019meta}, the authors present a meta-learning approach for domain adaption in simulation-to-reality transfers. Our objective in this paper is not to design a specific sim-to-real method for a given algorithm or task, but instead to analyze the performance of collaborative multi-robot DRL in the presence of disturbances in the environment as a step towards more effective sim-to-real transfers where real noises, errors or perturbances are accounted for also in the simulation environment. This includes variability in the operation of the robots, as robots might be operating in slightly different environments, or operate in different ways under the same environment. In particular, we are interested in studying how exposing multiple collaborative robots to different environments from the point of view of sensing and actuation can affect the joint learning effort.

In this paper, therefore, we focus on introducing perturbances inspired by real-world cases in a multi-agent DRL simulation. We expose different subsets of agents to slightly modified environments and study how different types of disturbances affect the collaborative learning process and the ability of the multi-robot system to converge to a common policy. The main contribution of this paper is the analysis of how input and output disturbances affect a collaborative deep reinforcement learning process with multiple robot arms. In particular, we simulate real-world perturbations that can occur on robotic arms, from the sensing and actuation perspectives. This is, to the best of our knowledge, the first study to consider the evaluation of both sensing and actuation disturbances in a multi-robot collaborative learning scenario, with different robots being exposed to different environments.

The remainder of this document is organized as follows. In Section~\ref{sec:related} we review the literature in distributed RL, adversarial RL, and robust multi-agent RL in the presence of byzantine agents. Then, Section~\ref{sec:methodology} introduces the DRL algorithm, and the methodology and simulation environment utilized in our experiments. The agent training methods and environment disturbances introduced to emulate real-world operational variability, together with the simulations results, are presented in Section~\ref{sec:results}. Section~\ref{sec:conclusion} concludes the work. % and outlines our future research directions.

