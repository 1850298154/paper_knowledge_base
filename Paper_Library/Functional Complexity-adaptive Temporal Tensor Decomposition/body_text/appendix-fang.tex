
\appendix
\onecolumn
\textbf{\Large Appendix}
\section{Details of derivations}
\label{ap:derivation}
\subsection{Log-marginal likelihood}
\label{ap:lml}
\begin{equation}
\begin{split}
    \ln p(\mathcal{D}) &= \int q(\mathcal{U}, \boldsymbol{\lambda}, \tau)\ln p(\mathcal{D}) d\mathcal{U}d \boldsymbol{\lambda}d\tau = \int q(\mathcal{U}, \boldsymbol{\lambda}, \tau) \ln \frac{p(\mathcal{U}, \boldsymbol{\lambda}, \tau, \mathcal{D})}{p( \mathcal{U}, \boldsymbol{\lambda}, \tau|\mathcal{D})}d\mathcal{U}d \boldsymbol{\lambda}d\tau \\
    &=\int q(\mathcal{U}, \boldsymbol{\lambda}, \tau) \ln \frac{p(\mathcal{U}, \boldsymbol{\lambda}, \tau, \mathcal{D})q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}{p( \mathcal{U}, \boldsymbol{\lambda}, \tau|\mathcal{D})q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}d\mathcal{U}d \boldsymbol{\lambda}d\tau \\
    &=\int q(\mathcal{U}, \boldsymbol{\lambda}, \tau) \ln \frac{p(\mathcal{U}, \boldsymbol{\lambda}, \tau, \mathcal{D})}{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}d\mathcal{U}d \boldsymbol{\lambda}d\tau - \int q(\mathcal{U}, \boldsymbol{\lambda}, \tau) \ln \frac{p( \mathcal{U}, \boldsymbol{\lambda}, \tau|\mathcal{D})}{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}d\mathcal{U}d \boldsymbol{\lambda}d\tau \\
    & = \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln \frac{p(\mathcal{U}, \boldsymbol{\lambda}, \tau, \mathcal{D})}{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}] - \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln \frac{p(\mathcal{U}, \boldsymbol{\lambda}, \tau)|\mathcal{D}}{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}]\\
    &=\mathcal{L}(q) + \text{KL}(q(\mathcal{U}, \boldsymbol{\lambda}, \tau)\|p(\mathcal{U}, \boldsymbol{\lambda}, \tau|\mathcal{D})).
    \end{split}
\end{equation}
\subsection{Lower bound of log-marginal likelihood}
\label{ap:A.2}
\begin{equation}
\begin{split}
    &\mathcal{L}(q) = \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln \frac{p(\mathcal{U}, \boldsymbol{\lambda}, \tau, \mathcal{D})}{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}] = \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln p(\mathcal{U}, \boldsymbol{\lambda}, \tau, \mathcal{D})] - \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln q(\mathcal{U}, \boldsymbol{\lambda}, \tau)]\\
    &=\mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln p(\mathcal{D}|\mathcal{U}, \boldsymbol{\lambda}, \tau)] + \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln p(\mathcal{U}, \boldsymbol{\lambda}, \tau)] - \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln q(\mathcal{U}, \boldsymbol{\lambda}, \tau)]\\
    &=\mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln p(\mathcal{D}|\mathcal{U}, \boldsymbol{\lambda}, \tau)]+  \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln 
    \frac{p(\mathcal{U}|\boldsymbol{\lambda})}{q(\mathcal{U})} + \ln 
    \frac{p(\boldsymbol{\lambda})}{q(\boldsymbol{\lambda})} + \ln 
    \frac{p(\tau)}{q(\tau)}]\\
    &=\mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln p(\mathcal{D}|\mathcal{U}, \boldsymbol{\lambda}, \tau)]+\mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda})}[\ln 
    \frac{p(\mathcal{U}|\boldsymbol{\lambda})}{q(\mathcal{U})}]  - \text{KL}(q(\boldsymbol{\lambda})\|p(\boldsymbol{\lambda})) - \text{KL}(q(\tau)\|p(\tau)).
\end{split}
\end{equation}
The first term of evidence lower bound (posterior expectation of log-likelihood) can be written as:
\begin{equation}
    \begin{split}
        \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}&[\ln p(\mathcal{D}|\mathcal{U}, \boldsymbol{\lambda}, \tau)] = -\frac{N}{2}\ln(2\pi) + \frac{N}{2}\mb{E}_q[\ln\tau]-\frac{1}{2}\sum_{n=1}^{N}\mb{E}_q[\tau]\mb{E}_q[(y_n-\boldsymbol{1}^{\T}\underset{k}{\circledast} \mf{u}^k(i_k^n,t_n))^2]\\
        &=-\frac{N}{2}\ln(2\pi) + \frac{N}{2}(\psi(\rho)-\ln\iota)-\frac{1}{2}\sum_{n=1}^{N}\frac{\rho}{\iota}\mb{E}_q[(y_n-\boldsymbol{1}^{\T}\underset{k}{\circledast} \mf{u}^k(i_k^n,t_n))^2],
    \end{split}
\end{equation}
where $\psi(\cdot)$ is the digamma function.
The posterior expectation of model error is:
\begin{equation}
    \begin{split}
&\mb{E}_q[(y_n-\boldsymbol{1}^{\T}\underset{k}{\circledast} \mf{u}^k(i_k^n,t_n))^2]=y_n^2 -2y_n\boldsymbol{1}^{\T}\underset{k}{\circledast} \mathbf{g}^k(i_k^n,t_n) + \boldsymbol{1}^{\T}\underset{k}{\circledast} \text{vec}(\mathbf{g}^{k}(i_k^{n},t_n)\mathbf{g}^{k}(i_k^{n},t_n)^{\T}+\sigma^2\mf{I}).
    \end{split}
    \label{eq:model_err}
\end{equation}
If $\sigma=0$, then posterior expectation of model error becomes  $(y_n-\boldsymbol{1}^{\T}\underset{k}{\circledast} \mf{u}^k(i_k^n,t_n))^2$.

The second term of evidence lower bound can be written as\footnote{The KL divergence between two Gaussian distributions $p(x)\sim \mathcal{N}(x|\mu_1, \sigma_1^2)$ and $q(x)\sim \mathcal{N}(x|\mu_2, \sigma_2^2)$ can be computed using $\text{KL}(p\|q)=\frac{1}{2}[\ln(\frac{\sigma_2^2}{\sigma_1^2})+ \frac{\sigma_1^2+(\mu_1-\mu_2)^2}{\sigma_2^2}-1]$. Detailed derivation can be found in Appendix \ref{GaussianKL}. }:
\begin{equation}
    \begin{split}
    \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda})}[\ln 
    \frac{p(\mathcal{U}|\boldsymbol{\lambda})}{q(\mathcal{U})}]&= \int\int q(\mathcal{U}, \boldsymbol{\lambda})\ln 
    \frac{p(\mathcal{U}|\boldsymbol{\lambda})}{q(\mathcal{U})} d\mathcal{U}d\boldsymbol{\lambda} = \int\int q(\mathcal{U}, \boldsymbol{\lambda})\ln p(\mathcal{U}|\boldsymbol{\lambda})d\mathcal{U}d\boldsymbol{\lambda} - \int q(\mathcal{U})\ln q(\mathcal{U})d\mathcal{U}\\
    &=\int q(\mathcal{U})\ln p(\mathcal{U}|\boldsymbol{\lambda}=\mb{E}_q(\boldsymbol{\lambda}))d\mathcal{U} - \int q(\mathcal{U})\ln q(\mathcal{U})d\mathcal{U} \\
    &= -\text{KL}(q(\mathcal{U}) \| p(\mathcal{U}|\boldsymbol{\lambda}=\mb{E}_q(\boldsymbol{\lambda}))) = -\sum_{n=1}^N \sum_{k=1}^K\sum_{r=1}^R \frac{1}{2}[\ln(\frac{\beta_r}{\alpha_r\sigma^2})+ \frac{\alpha_r}{\beta_r}(\sigma^2+(\mathbf{g}^k_r(i_k^n, t_n))^2)-1],
    \end{split}
\end{equation}
where $\mathbf{g}^k_r(i_k^n, t_n)$ is the $r$-$th$ element of $\mathbf{g}^k(i_k^n, t_n)$.
%\frac{a_r^0}{\beta_r}

The third term of evidence lower bound can be written as\footnote{The KL divergence between two Gamma distributions $p(x) \sim \text{Gamma}(x|a_1, b_1)$ and $q(x)\sim \text{Gamma}(x|a_2, b_2)$ can be computed using $\text{KL}(p \parallel q) = a_2 \ln \frac{b_1}{b_2} - \ln\frac{\Gamma(a_2)}{\Gamma(a_1)}+(a_1 - a_2)\psi(a_1)-(b_2 - b_1)  \frac{a_1}{b_1}$. Detailed derivation can be found in \ref{GammKL}.}:
\begin{equation}
    \begin{split}
        \text{KL}(q(\boldsymbol{\lambda})\|p(\boldsymbol{\lambda}))= \sum_{r=1}^{R}
        a_0 \ln \frac{\beta_r}{b_0} - \ln\frac{\Gamma(a_0)}{\Gamma(\alpha_r)}+(\alpha_r - a_0)\psi(\alpha_r)-(b_0 - b_1)  \frac{\alpha_r}{\beta_r}.
        \label{eq:kl1}
    \end{split}
\end{equation}

The fourth term of evidence lower bound can be written as:
\begin{equation}
    \begin{split}
        \text{KL}(q(\tau)\|p(\tau))= c_0 \ln \frac{\iota}{d_0} - \ln\frac{\Gamma(c_0)}{\Gamma(\rho)}+(\rho - c_0)\psi(\rho)-(d_0 - \iota)  \frac{\rho}{\iota}.
        \label{eq:kl2}
    \end{split}
\end{equation}

\subsection{KL divergence of two Gaussian distribution}
\label{GaussianKL}
The Kullback-Leibler (KL) Divergence between two probability distributions \(p\) and \(q\) is defined as:
\[
\text{KL}(p \| q) = \int_{-\infty}^{\infty} p(x) \ln \left( \frac{p(x)}{q(x)} \right) dx.
\]
 Let \( p \sim \mathcal{N}(\mu_1, \sigma_1^2) \) and \( q \sim \mathcal{N}(\mu_2, \sigma_2^2) \), where the probability density functions (PDFs) are given by:
\[
p(x) = \frac{1}{\sqrt{2\pi \sigma_1^2}} \exp \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} \right),
\]
\[
q(x) = \frac{1}{\sqrt{2\pi \sigma_2^2}} \exp \left( -\frac{(x - \mu_2)^2}{2 \sigma_2^2} \right).
\]
Substitute the Gaussian PDFs into the definition of KL divergence:
\[
\text{KL}(p \| q) = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi \sigma_1^2}} \exp \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} \right) \ln \left( \frac{\frac{1}{\sqrt{2\pi \sigma_1^2}} \exp \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} \right)}{\frac{1}{\sqrt{2\pi \sigma_2^2}} \exp \left( -\frac{(x - \mu_2)^2}{2 \sigma_2^2} \right)} \right) dx.
\]
Simplify the logarithmic term:
\[
\ln \left( \frac{p(x)}{q(x)} \right) = \ln \left( \frac{\frac{1}{\sqrt{2\pi \sigma_1^2}}}{\frac{1}{\sqrt{2\pi \sigma_2^2}}} \right) + \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} + \frac{(x - \mu_2)^2}{2 \sigma_2^2} \right)
\]
\[
= \ln \left( \frac{\sigma_2}{\sigma_1} \right) + \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} + \frac{(x - \mu_2)^2}{2 \sigma_2^2} \right).
\]
Thus, the integral for KL divergence becomes:
\[
\text{KL}(p \| q) = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi \sigma_1^2}} \exp \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} \right) \left( \ln \left( \frac{\sigma_2}{\sigma_1} \right) + \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} + \frac{(x - \mu_2)^2}{2 \sigma_2^2} \right) \right) dx.
\]
Simplifying the Integral: We can now break the integral into two parts:
1. The constant term:
\[
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi \sigma_1^2}} \exp \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} \right) \ln \left( \frac{\sigma_2}{\sigma_1} \right) dx.
\]
Since \( \frac{1}{\sqrt{2\pi \sigma_1^2}} \exp \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} \right) \) is the PDF of a Gaussian distribution, its integral is 1, so this term evaluates to:
\[
\ln \left( \frac{\sigma_2}{\sigma_1} \right).
\]
2. The difference of squared terms:
\[
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi \sigma_1^2}} \exp \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} \right) \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} + \frac{(x - \mu_2)^2}{2 \sigma_2^2} \right) dx.
\]
This term can be split into two parts:
- The first part involves \( \mu_1 \), and after calculation, it simplifies to:
\[
\frac{\sigma_1^2}{2 \sigma_2^2} - \frac{1}{2}.
\]
The second part involves the difference between \( \mu_1 \) and \( \mu_2 \), and after calculation, it simplifies to:
\[
\frac{(\mu_1 - \mu_2)^2}{2 \sigma_2^2}.
\]
Combining all parts, the KL divergence between two Gaussian distributions is:
\[
\text{KL}(p \| q) = \ln \left( \frac{\sigma_2}{\sigma_1} \right) + \frac{\sigma_1^2}{2 \sigma_2^2} - \frac{1}{2} + \frac{(\mu_1 - \mu_2)^2}{2 \sigma_2^2}.
\]


\subsection{KL divergence of two Gamma distribution}
\label{GammKL}
The Kullback-Leibler (KL) Divergence between two probability distributions \( p \) and \( q \) is defined as:
\[
\text{KL}(p \parallel q) = \int_0^\infty p(x) \ln \left( \frac{p(x)}{q(x)} \right) dx.
\]
Let \( p \sim \text{Gamma}(a_1, b_1) \) and \( q \sim \text{Gamma}(a_2, b_2) \), where the probability density functions (PDFs) with rate parameters are given by:
\[
p(x) = \frac{b_1^{a_1} x^{a_1 - 1} \exp\left( -b_1 x \right)}{\Gamma(a_1)}, \quad x \geq 0,
\]
\[
q(x) = \frac{b_2^{a_2} x^{a_2 - 1} \exp\left( -b_2 x \right)}{\Gamma(a_2)}, \quad x \geq 0.
\]
Substitute the PDFs of the Gamma distributions into the definition of KL divergence:
\[
\text{KL}(p \parallel q) = \int_0^\infty \frac{b_1^{a_1} x^{a_1 - 1} \exp\left( -b_1 x \right)}{\Gamma(a_1)} \ln \left( \frac{\frac{b_1^{a_1} x^{a_1 - 1} \exp\left( -b_1 x \right)}{\Gamma(a_1)}}{\frac{b_2^{a_2} x^{a_2 - 1} \exp\left( -b_2 x \right)}{\Gamma(a_2)}} \right) dx.
\]
Simplify the logarithmic term:
\[
\ln \left( \frac{p(x)}{q(x)} \right) = \ln \left( \frac{b_1^{a_1} x^{a_1 - 1} \exp\left( -b_1 x \right)}{b_2^{a_2} x^{a_2 - 1} \exp\left( -b_2 x \right)} \cdot \frac{\Gamma(a_2)}{\Gamma(a_1)} \right)
\]
\[
= (a_1 - a_2) \ln(x) + \left( -b_1 x + b_2 x \right) + \ln \left( \frac{\Gamma(a_2)}{\Gamma(a_1)} \right) + (a_1 \ln(b_1) - a_2 \ln(b_2)).
\]
Thus, the integral becomes:
\[
\text{KL}(p \parallel q) = \int_0^\infty \frac{b_1^{a_1} x^{a_1 - 1} \exp\left( -b_1 x \right)}{\Gamma(a_1)} \left[ (a_1 - a_2) \ln(x) + (b_2 - b_1) x + \ln \left( \frac{\Gamma(a_2)}{\Gamma(a_1)} \right) + (a_1 \ln(b_1) - a_2 \ln(b_2)) \right] dx.
\]
We now break this into four separate integrals.
\[
I_1 = \int_0^\infty \frac{b_1^{a_1} x^{a_1 - 1} \exp\left( -b_1 x \right)}{\Gamma(a_1)} (a_1 - a_2) \ln(x) dx.
\]
This integral can be solved using the properties of the Gamma distribution and the digamma function \( \psi(a) \):
\[
I_1 = (a_2 -  a_1) \left( \ln(b_1) - \psi(a_1) \right),
\]
where \( \psi(a) \) is the digamma function, the derivative of the logarithm of the Gamma function.
\[
I_2 = \int_0^\infty \frac{b_1^{a_1} x^{a_1} \exp\left( -b_1 x \right)}{\Gamma(a_1)} (b_2 - b_1) dx.
\]
After performing the integration, we obtain:
\[
I_2 = (b_2 - b_1) \frac{\Gamma(a_1 + 1)}{\Gamma(a_1)} \cdot \frac{1}{b_1} = (b_2 - b_1) a_1 \frac{1}{b_1},
\]
\[
I_3 = \int_0^\infty \frac{b_1^{a_1} x^{a_1 - 1} \exp\left( -b_1 x \right)}{\Gamma(a_1)} \ln \left( \frac{\Gamma(a_2)}{\Gamma(a_1)} \right) dx.
\]
Since this is a constant term, we can immediately evaluate it:
\[
I_3 = \ln \left( \frac{\Gamma(a_2)}{\Gamma(a_1)} \right),
\]
\[
I_4 = \int_0^\infty \frac{b_1^{a_1} x^{a_1 - 1} \exp\left( -b_1 x \right)}{\Gamma(a_1)} (a_1 \ln(b_1) - a_2 \ln(b_2)) dx.
\]
This integral simplifies to:
\[
I_4 = a_1 \ln(b_1) - a_2 \ln(b_2).
\]
Combining all the parts, the KL divergence between two Gamma distributions with rate parameters is:
\[
\text{KL}(p \parallel q) = (a_2 - a_1) \left( \ln(b_1) - \psi(a_1) \right) + (b_2 - b_1) a_1 \frac{1}{b_1} + \ln \left( \frac{\Gamma(a_2)}{\Gamma(a_1)} \right) + a_1 \ln(b_1) - a_2 \ln(b_2).
\]

%\subsection{Algorithm}
%\label{ap:algorithm}
%%\begin{figure}[h]
%%    \centering
%%    \includegraphics[width=1\linewidth]{figure/algo1.png}
%%    %\caption{Enter Caption}
%%    \label{alg:1}
%%\end{figure}
%
% \begin{algorithm}[H]
%   \SetAlgoLined
% \KwIn{Training data $\mathcal{D}=\{y_n, \mf{i}_n, t_n\}_{n=1}^{N}$ }
% Collect all possible $I_k$ indexes for $K$ modes and $T$ possible timestamps.
%  Initialize  $\{\boldsymbol{\omega}_k\}_{k=1}^K, \{\alpha_r, \beta_r\}_{r=1}^{R}, \sigma, \rho, \iota$.
% 
%   \While{not convergence}{
% Construct a set of initial ODE state tables $\mathcal{Z}_0$ using  Fourier features and Encoder, encompassing all possible indexes.
%
% \For{$i = 1, 2, \cdots, T$}{
%     $\mathcal{Z}(t_i)$ = \text{ODESolve}($\mathcal{Z}(t_{i-1})$, 
%     $\{h_{\boldsymbol{\theta}_k}\}_{k=1}^{K}$,$(t_{i-1},t_i))$
%    
%     Compute necessary $\mathbf{g}^k(i_k,t_i)$ from $\mathcal{Z}(t)$ using \eqref{eq:latent-ode3}.  
%     }
% Take gradient step on \eqref{eq:loss}.
%     }
%   \caption{Training process of \MODEL}
%   \label{ap:algorithm1}
% \end{algorithm}
%
%

\subsection{Predictive distribution}
\label{pred_distr}

Through minimizing the negative log-marginal likelihood with observed training data, we can infer the distributions of the latent variables $q$, with which a predictive distribution  can be derived. Given index set $\{i^p_1,\cdots, i^p_k, t_p\}$, we are to predict the corresponding value, we have:
\begin{equation}
    \begin{split}
        p(y_p|\mathcal{D}) &\simeq \int p(y_p|\{\mf{u}^{k}_{i_k^p, t_p}\}_{k=1}^K, \tau)q(\{\mf{u}^{k}_{i_k^p, t_p}\}_{k=1}^K)q(\tau)d(\{\mf{u}^{k}_{i_k^p, t_p}\}_{k=1}^K) d\tau\\
        &=\int\int \mathcal{N}(y_p|\boldsymbol{1}^{\T}(\mf{u}^{1}_{i_1^p, t_p} \circledast \cdots \circledast \mf{u}^{k}_{i_k^p, t_p}),\tau^{-1}) \prod_{k=1}^K
        q(\mf{u}^{k}_{i_k^p, t_p})d(\mf{u}^{K}_{i_K^p, t_p})q(\tau)d\tau\\
        &=\int\int \mathcal{N}(y_p|\boldsymbol{1}^{\T}(\mf{u}^{1}_{i_1^p, t_p} \circledast \cdots \circledast \mf{u}^{K}_{i_K^p, t_p}),\tau^{-1}) \prod_{k=1}^K \mathcal{N}(\mf{u}^{k}_{i_k^p, t_p}|\mathbf{g}^k(i_k^p, t_p),\sigma^2\mf{I})d(\mf{u}^{k}_{i_k^p, t_p}) \text{Gamma}(\tau|\rho, \iota)d\tau\\
        &=\int\int \mathcal{N}(y_p|\boldsymbol{1}^{\T}(\mf{u}^{1}_{i_1^p, t_p} \circledast \cdots \circledast \mf{u}^{K}_{i_K^p, t_p}),\tau^{-1})\mathcal{N}(\mf{u}^{1}_{i_1^p, t_p}|\mathbf{g}^1(i_1^p, t_p),\sigma^2\mf{I})d(\mf{u}^{1}_{i_1^p, t_p}) \\& \qquad \qquad \prod_{k\ne 1} \mathcal{N}(\mf{u}^{k}_{i_k^p, t_p}|\mathbf{g}^k(i_k^p, t_p),\sigma^2\mf{I})d(\mf{u}^{k}_{i_k^p, t_p}) \text{Gamma}(\tau|\rho, \iota)d\tau \\
        &= \int\int \mathcal{N}(y_p|(\underset{k\ne 1}{\circledast} 
 \mf{u}^{k}_{i_k^p, t_p})^{\T}\mf{u}^{1}_{i_1^p, t_p} ,\tau^{-1})\mathcal{N}(\mf{u}^{1}_{i_1^p, t_p}|\mathbf{g}^1(i_1^p, t_p),\sigma^2\mf{I})d(\mf{u}^{1}_{i_1^p, t_p}) \\& \qquad \qquad\prod_{k\ne 1} \mathcal{N}(\mf{u}^{k}_{i_k^p, t_p}|\mathbf{g}^k(i_k^p, t_p),\sigma^2\mf{I})d(\mf{u}^{k}_{i_k^p, t_p}) \text{Gamma}(\tau|\rho, \iota)d\tau\\
 & = \int\int \mathcal{N}(y_p|(\underset{k\ne 1}{\circledast} 
 \mf{u}^{k}_{i_k^p, t_p})^{\T}\mathbf{g}^1(i_1^p, t_p) ,\tau^{-1}+\sigma^2(\underset{k\ne 1}{\circledast} 
 \mf{u}^{k}_{i_k^p, t_p})^{\T}(\underset{k\ne 1}{\circledast} 
 \mf{u}^{k}_{i_k^p, t_p}) )\\& \qquad \qquad\prod_{k\ne 1} \mathcal{N}(\mf{u}^{k}_{i_k^p, t_p}|\mathbf{g}^k(i_k^p, t_p),\sigma^2\mf{I})d(\mf{u}^{k}_{i_k^p, t_p}) \text{Gamma}(\tau|\rho, \iota)d\tau\\
 &\qquad \vdots \\
 &=\int \mathcal{N}\big(y_p|\boldsymbol{1}^{\T}\underset{k}{\circledast} 
 \mathbf{g}^k(i_k^p, t_p), \tau^{-1}+ \sigma^2\sum_{j=1}^K(\underset{k\ne j}{\circledast} 
 \mathbf{g}^k(i_k^p, t_p))^{\T}(\underset{k\ne j}{\circledast} 
 \mathbf{g}^k(i_k^p, t_p)\big) \text{Gamma}(\tau|\rho, \iota)d\tau \\
 &= \mathcal{T}\big(y_p|\boldsymbol{1}^{\T}\underset{k}{\circledast} 
 \mathbf{g}^k(i_k^p, t_p),  \{\frac{\iota}{\rho}+\sigma^2\sum_{j=1}^K(\underset{k\ne j}{\circledast} 
 \mathbf{g}^k(i_k^p, t_p))^{\T}(\underset{k\ne j}{\circledast} 
 \mathbf{g}^k(i_k^p, t_p))\}^{-1}, 2\rho\big).
    \end{split}
\end{equation}
We found the prediction distribution follows the student's-t distribution.


\subsection{Closed Form of Predictive Distribution}
\label{app:closed_distri}
After obtaining the variational posteriors of the latent variables, we can further derive the predictive distribution of the new data with arbitrary indexes. Given the index set $\{i^p_1,\cdots, i^p_k, t_p\}$ for prediction, we can obtain the variational predictive posterior distribution, which follows a Student's t-distribution (See Appendix \ref{pred_distr} for more details):
\begin{equation}
	\begin{split}
		&p(y_p|\mathcal{D}) \sim \mathcal{T}(y_p|\mu_p, s_p, \nu_p),\\
		&\mu_p =  \boldsymbol{1}^{\T}[\underset{k}{\circledast} 
		\mf{g}^k(i_k^p, t_p)], \quad \nu_p = 2\rho,\\
		&s_p =\big\{\frac{\iota}{\rho}+\sigma^2\sum_{j=1}^K[\underset{k\ne j}{\circledast} 
		\mf{g}^k(i_k^p, t_p)]^{\T}[\underset{k\ne j}{\circledast} 
		\mf{g}^k(i_k^p, t_p)]\big\}^{-1},
	\end{split}
\end{equation}
where $\mu_p$, $s_p$, $\mu_p$ is the mean, scale parameter and degree of freedom of the Student's t-distribution, respectively. The closed-form  predictive distribution is a great advantage for the prediction process, as it allows us to do the probabilistic reconstruction and prediction with uncertainty quantification over the arbitrary continuous indexes.



\newpage
\section{Additional experiment results}
\label{ap:exper_res}
\subsection{Experiment settings: synthetic data}
\label{ap:setting1}
The \MODEL  was implemented with PyTorch \cite{paszke2019pytorch} and $\texttt{torchdiffeq}$ library (\url{https://github.com/rtqichen/torchdiffeq}). We employed a single hidden-layer neural network (NN) to parameterize the encoder. Additionally, we used two  NNs, each with two hidden layers, for derivative learning and for parameterizing the decoder, respectively. Each layer in all networks contains 100 neurons.
  We set the dimension of Fourier feature $M=32$, the  ODE state $J=5$ and the initial number of components  of the latent factor trajectories $R=5$.  The \MODEL  was trained  using Adam \cite{kingma2014adam} optimizer with the learning rate set as $5e^{-3}$. The hyperparamters $\{a_r^0, b_r^0\}_{r=1}^{R},c^0, d^0$ and initial values of learnable parameters $\{\alpha_r, \beta_r\}_{r=1}^{R},\rho, \sigma^2,  \iota$ are  set to $1e^{-6}$ (so that all the initial posterior means of $\{\lambda_r\}_{r=1}^{R}$ equal $1$). We ran 2000 epochs, which is sufficient for convergence.
  
\subsection{Experiment settings: real-world data}
\label{ap:setting2}
For THIS-ODE, we used a two-layer network with the layer width  chosen from $\{50, 100\}$. For DEMOTE, we used two hidden layers for both the reaction process and entry value prediction, with the layer width chosen from $\{50, 100\}$. For LRTFR, we  used two hidden layers with $100$ neurons to  parameterize the latent function of each mode.
We varied $R$  from $\{3,5,7\}$ for all baselines.
 For deep-learning based methods, we all used  $\texttt{tanh}$ activations. For FunBaT, we varied \text{Matérn Kernel} $\{1/2, 3/2\}$ along the kernel parameters for optimal performance for different datasets.
We use ADAM optimizer with the learning rate tuned from  $\{5e^{-4}, 1e^{-3}, 5e^{-3}, 1e^{-2}\}$. 


\subsection{Rank learning curves of different datasets}
\label{ap:learning_curve}
\begin{figure*}[h]
    \centering
    \begin{minipage}{0.41\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/syn_uv_sum1.png}
        \caption*{(a) Power of learned factor trajectories.}
    \end{minipage}
    \begin{minipage}{0.41\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/syn_uv_sum2.png}
        \caption*{(b) Posterior means of $\boldsymbol{\lambda}$.}
    \end{minipage}
    \caption{Rank learning curves of the synthetic data.}
    \label{fig:learning_curve_syn}
\end{figure*}

\begin{figure*}[h]
    \centering
    \begin{minipage}{0.41\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/traffic_uv_sum1.png}
        \caption*{(a) Power  of learned factor trajectories (CA traffic).}
    \end{minipage}
    \begin{minipage}{0.41\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/traffic_uv_sum2.png}
        \caption*{(b) Posterior means of $\boldsymbol{\lambda}$ (CA traffic).}
    \end{minipage}
    
    \vspace{2mm}
        \begin{minipage}{0.41\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/server_uv_sum1.png}
        \caption*{(c) Power  of learned factor trajectories (Server).}
    \end{minipage}
    \begin{minipage}{0.41\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/server_uv_sum2.png}
        \caption*{(d) Posterior means of $\boldsymbol{\lambda}$ (Server).}
    \end{minipage}
    
        \vspace{2mm}
        \begin{minipage}{0.41\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/ssf_uv_sum1.png}
        \caption*{(e) Power  of learned factor trajectories (SSF).}
    \end{minipage}
    \begin{minipage}{0.41\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/ssf_uv_sum2.png}
        \caption*{(f) Posterior means of $\boldsymbol{\lambda}$ (SSF).}
    \end{minipage}
    \caption{Rank learning curves of three datasets.}
    \label{fig:learning_curve_three}
\end{figure*}

Fig.~\ref{fig:learning_curve_syn}  plots the rank-learning curves during the gradient descent iterations of the synthetic data. That is, the evolutions of (a) the power of $R$ components of the estimated posterior mean of the factor trajectories \footnote{We define the power  of $r$-th component of factor trajectories as: $\sum_{k=1}^{K}\int_{i_k}\int_{t}(\mathbf{g}^k_r(i_k,t))^2di_kdt$, which represents  the contribution of the $r$-th component to the final output. The power can be approximated using $\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{g}^k_r(i_k^n,t_n))^2$.}
and (b) the values of estimated posterior mean of $\{\lambda_r\}_{r=1}^{R}$. Note that the power of $r$-$th$ component of factor trajectories is conditioned by $\lambda_r$ (as shown \eqref{term2} and \textit{Remark 1}), and we plot the pair with the same color. One can see that as the epoch increases, the power of 4 components of the factor trajectories are forced to 0, which  aligns with the increments of 4 corresponding $\lambda_r$s. And we can manually exclude the four components, which will not affect the final prediction results\footnote{Our criterion for excluding the $r$-$th$ rank is when $\mathbb{E}(\lambda_r)$ is large and power $\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{g}^k_r(i_k^n,t_n))^2$ is relatively small.}. Only the third component ($r=3$) is activated after convergence and $\lambda_3$ settles at a small value correspondingly. This indicates that our method successfully identifies the true underlying rank (i.e., 1) of the synthetic data while effectively pruning the other four components. Fig.~(\ref{fig:learning_curve_three}) plots the rank-learning curves of the CA traffic, Server and SSF datasets respectively. In the same sense, we can infer that the revealed ranks of these three datasets are 5,7,7 respectively.
\label{ap:b}





\subsection{Noise robustness}
\label{ap:noise_rob}

Here, we show the robustness of our proposed FARD against the noise on the CA traffic dataset by comparing with the baselines with varying ranks on  Tab.~\ref{Table:as1} and Tab.~\ref{Table:as2}. Then, we show the robustness of \MODEL against varing levels and types of noises on Tab.~\ref{Table:as3}. The results shows that \MODEL do not show significant performance degrade with varing levels of noises and even non-Gaussian noises, demonstating its robustness.

\begin{table*}[h!]
\small
\centering
\renewcommand{\arraystretch}{1.05}
\begin{tabular}{|c|c|c|c|c|}
\hline 
\textbf{} & \MODEL  \quad & \MODEL w.o. FARD & 
 LRTFR (R=5)   & DEMOTE (R=5) \\ \hline
Noise Variance & \multicolumn{4}{c|}{\textbf{MAE}}  \\ \hline
% 0.3 &\textbf{0.299 $\pm$ 0.026} &0.339 $\pm$ 0.017 &0.542 $\pm$ 0.008 &0.463 $\pm$ 0.004 & 0.399 $\pm$ 0.001 \\ \hline
0.5 & \textbf{0.134 $\pm$ 0.001}&0.169 $\pm$ 0.022 & 0.213 $\pm$ 0.007
   &0.148 $\pm$ 0.003
 \\ \hline
1 &\textbf{0.182 $\pm$ 0.006} & 0.219 $\pm$ 0.006 &0.305 $\pm$ 0.009
 &0.219 $\pm$ 0.034
 \\ \hline
\end{tabular}
\caption{Extra experimental results  on the robustness of functional automatic rank determination mechanism against the noise on the CA traffic dataset. The results were averaged over five runs.}
\label{Table:as1}
\end{table*}
\begin{table*}[h!]
\small
\centering
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{1pt}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
\textbf{} & LRTFR (R=7)   & DEMOTE (R=7) & 
 LRTFR (R=10)   & DEMOTE (R=10)& 
 LRTFR (R=15)   & DEMOTE (R=15) \\ \hline
 Noise Variance & \multicolumn{6}{c|}{\textbf{RMSE}}  \\ \hline
0.5 & 0.484 $\pm$ 0.023&0,423 $\pm$ 0.019 &0.469 $\pm$ 0.053 &0.4375 $\pm$ 0.005 &0.475 $\pm$ 0.018 &0.455 $\pm$ 0.005\\ \hline
1   & 0.5475 $\pm$ 0.011 &0.517 $\pm$ 0.020 &0.621 $\pm$ 0.038&0.547 $\pm$ 0.024&0.708 $\pm$ 0.013&0.552 $\pm$ 0.003\\ \hline
Noise Variance & \multicolumn{6}{c|}{\textbf{MAE}}  \\ \hline
0.5& 0.224 $\pm$ 0.019& 0.157 $\pm$ 0.006 &0.232 $\pm$ 0.012&0.148 $\pm$ 0.001&0.245 $\pm$ 0.011&0.140 $\pm$ 0.002\\ \hline
1   & 03335 $\pm$ 0.018&0.209 $\pm$ 0.020 &0.402 $\pm$ 0.032 &0.312 $\pm$ 0.013&0.469 $\pm$ 0.0135&0.319 $\pm$ 0.0025\\ \hline
\end{tabular}
\caption{Extra experimental results  on the robustness of functional automatic rank determination mechanism against the noise on the CA traffic dataset. The results were averaged over five runs.}
\label{Table:as2}
\end{table*}

\begin{table}[h!]
	\small
	\centering
	\renewcommand{\arraystretch}{1.2}
	\begin{tabular}{|c|c|c|}
		\hline 
		Gaussion noise & \textbf{RMSE}  & \textbf{MAE}  \\ \hline
		$\sigma^2$=0.05 &0.056 $\pm$ 0.004 &0.045 $\pm$ 0.003    \\ \hline
		$\sigma^2$=0.10 &0.090 $\pm$ 0.005 &0.068 $\pm$ 0.005  \\ \hline
			$\sigma^2$=0.15 &0.112 $\pm$ 0.006 &0.086 $\pm$ 0.005 \\ \hline
				Laplacsian noise & \textbf{RMSE}  & \textbf{MAE}  \\ \hline
		$\sigma^2$=0.05 &0.065 $\pm$ 0.006 &0.052 $\pm$ 0.006    \\ \hline
		$\sigma^2$=0.10 &0.094 $\pm$ 0.008 &0.069 $\pm$ 0.006  \\ \hline
		$\sigma^2$=0.15 &0.117 $\pm$ 0.07 &0.089 $\pm$ 0.007
		 \\ \hline
				Poisson noise & \textbf{RMSE}  & \textbf{MAE}  \\ \hline
		$\sigma^2$=0.05 &0.064 $\pm$ 0.004 &0.049 $\pm$ 0.005    \\ \hline
		$\sigma^2$=0.10 &0.096 $\pm$ 0.010 &0.073 $\pm$ 0.009  \\ \hline
		$\sigma^2$=0.15 &0.115 $\pm$ 0.012 &0.081 $\pm$ 0.01
		 \\ \hline
	\end{tabular}
	\vspace{2mm}
	\caption{Extra experimental results  on the robustness of functional automatic rank determination mechanism against various  noise on the synthetic dataset. The results were averaged over five runs.}
	\label{Table:as3}
\end{table}


\newpage
\subsection{Running time}
Here we provide the comparisions of running time of different methods in Tab.\ref{Tab:runtime}.
\label{ap:running}
\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{c|c|c|c}
\hline
 \textbf{} & \textbf{CA traffic} & \textbf{Server Room} & \textbf{SSF}  \\
\hline
\textbf{THIS-ODE} & 283.9 & 144.8 & 158.9  \\
\textbf{NONFAT} & 0.331 & 0.81 & 0.67  \\
\textbf{DEMOTE} & 0.84 & 7.25 & 1.08  \\
\textbf{FunBaT-CP} & 0.059 & 0.027 & 0.075  \\
\textbf{FunBaT-Tucker} & 2.13 & 3.20 & 2.72  \\
\textbf{LRTFR} & 0.098 & 0.178 & 0.120  \\
\textbf{CATTE} & 0.227 & 0.83 & 0.247  \\
\hline
\end{tabular}
\caption{Per-epoch/iteration running time of different methods (in seconds).}
\label{Tab:runtime}
\end{table}

\subsection{Hyperparameter Analysis}
\label{app:hyper}
We first test the influnence of the dimension of ODE latent state $J$ in Tab.~\ref{Table:J}. We observe that the final results are robust to the selection of $J$.
\begin{table}[h!]
	\small
	\centering
	\renewcommand{\arraystretch}{1.1}
	\begin{tabular}{|c|c|c|c|c|}
		\hline 
		$J$ & 6  & 8 &
		10 & 
		16  \\ \hline
		RMSE &\textbf{0.279}&0.294 & 0.284  &0.290 \\ \hline
		MAE &0.090 & 0.088 & \textbf{0.085} & 0.088 \\ \hline
	\end{tabular}
	\vspace{2mm}
	\caption{Performance of \MODEL under different $J$ on the CA traffic dataset. The results were averaged over five runs.}
	\label{Table:J}
\end{table}

Then, we test  influnence of the hyper-parameters $a_r^0$ and $b_r^0$ in Tab.~\ref{Table:J}. We observe that the final results are robust to the selection of $a_r^0$ and $b_r^0$.
\begin{table}[h!]
	\small
	\centering
	\renewcommand{\arraystretch}{1.2}
	\begin{tabular}{|c|c|c|c|}
		\hline 
		 & $a_r^0 = b_r^0 = 1e^{-6}$  & $a_r^0 = b_r^0 = 1e^{-4}$ & 
		$a_r^0 = 1e^{-6},  b_r^0 = 2e^{-4}$  \\ \hline
		RMSE &0.056 $\pm$ 0.004 &0.056 $\pm$ 0.003 & 0.054 $\pm$ 0.005   \\ \hline
		MAE &0.045 $\pm$ 0.003 & 0.044 $\pm$ 0.003 & 0.044 $\pm$ 0.004  \\ \hline
	\end{tabular}
\vspace{2mm}
	\caption{Performance of \MODEL under different $a_r^0$ and $b_r^0$ on the synthetic data experiment. The results were averaged over five runs.}
	\label{Table:a_b}
\end{table}


\subsection{Additional visualization results with other baselines}
\label{app:more_visual}
Here, we show more visualization results of predictions on different methods and datasets on Fig.~\ref{fig:traffic_extra}, Fig.~\ref{fig:server_extra} and Fig.~\ref{fig:ssf_extra}. Our method yields qualitatively better visual results compared to the baselines.

\begin{figure*}[h!]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figure/Traffic_pred1.png}
		\caption*{(a) (1,4,13)}
	\end{minipage}
	\hspace{-0.1cm} % 调整两张图片之间的间距
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figure/Traffic_pred2.png}
		\caption*{(b) (5,3,12)}
	\end{minipage}    

	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figure/Traffic_pred3.png}
		\caption*{(c) (1,3,1)}
	\end{minipage}
	\hspace{-0.1cm} % 调整两张图片之间的间距
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figure/Traffic_pred4.png}
		\caption*{(d) (0,16,6)}
	\end{minipage}
	\caption{Additional visualization results on the \textit{CA Traffic} dataset at different coordinates.}
	\label{fig:traffic_extra}
	\vspace{-12pt}
\end{figure*}
\begin{figure*}[h!]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figure/Server_pred1.png}
		\caption*{(a) (6,2,5)}
	\end{minipage}
	\hspace{-0.1cm} % 调整两张图片之间的间距
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figure/Server_pred2.png}
		\caption*{(b) (1,1,1)}
	\end{minipage}    
	
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figure/Server_pred3.png}
		\caption*{(c) (5,6,2)}
	\end{minipage}
	\hspace{-0.1cm} % 调整两张图片之间的间距
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figure/Server_pred4.png}
		\caption*{(d) (2,6,3)}
	\end{minipage}
	\caption{Additional visualization results on the \textit{Server Room} dataset at different coordinates.}
	\label{fig:server_extra}
	\vspace{-12pt}
\end{figure*}
\begin{figure*}[h!]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figure/SSF_pred1.png}
		\caption*{(a) (1,1,1)}
	\end{minipage}
	\hspace{-0.1cm} % 调整两张图片之间的间距
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figure/SSF_pred2.png}
		\caption*{(b) (5,1,10)}
	\end{minipage}    
	
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figure/SSF_pred3.png}
		\caption*{(c) (3,2,7)}
	\end{minipage}
	\hspace{-0.1cm} % 调整两张图片之间的间距
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figure/SSF_pred4.png}
		\caption*{(d) (7,10,15)}
	\end{minipage}
	\caption{Additional visualization results on the \textit{SSF} dataset at different coordinates.}
	\label{fig:ssf_extra}
	\vspace{-12pt}
\end{figure*}



\qquad 

\subsection{Additional  results on  scalability}
\begin{table}[t]
	\centering
	\renewcommand{\arraystretch}{1.1}
	\begin{tabular}{c|c|c|c|c|c}
		\hline
		\textbf{timestamps \ spatial resolution} & $50 \times 50$ &  $100 \times 50$&  $200 \times 50$ & $100 \times 100$ & $200 \times 200$  \\
		\hline
		$T=100$ & 0.398 & 0.673 & 1.011 & 1.016 & 2.392 \\
		$T=200$ & 0.814 & 1.088 & 1.670& 1.714& 5.081 \\
		$T=500$ &1.688 & 2.162 & 3.755& 3.721& 10.79  \\
		\hline
	\end{tabular}
	\vspace{2mm}
	\caption{Per-epoch/iteration running time on different size of tensors(in seconds).}
	\label{Tab:scal1}
\end{table}
\label{ap:more_scal}
To further demonstrate the scalability of \MODEL, we conducted synthetic experiments across varying configurations of timestamps$\times$spatial points. All experiments employed the same network architecture, with  $5\%$
of the total data points randomly selected for training. Therefore, the size of the training dataset ranges from $10^{5}$ to $10^{7}$. We hereby report the average training time (in seconds) per epoch in Table~\ref{Tab:scal1}.


One can see that our method also scales well on the spatial mode since our method decouple each mode and convert the expanding spatial resulotion to the increase of the ODE states.  For example, for the spatial resolution $50 \times 50$, we only need to solve  $50+50=100$ ODE states. Likelywise, for the spatial resolution $200 \times 200$, we only need to solve $200+200=400$ ODE states. We will supplement the results in the latest version.

\quad 
\newpage
\section{More Comments on the functional automatic rank determination mechanism}
\label{app:difference}
Our proposed functional automatic rank determination mechanism is different from previous method\cite{morup2009automatic_ARD, cheng2022towards, zhao2015bayesianCP},  which can not be directly applied for functional tensors. The differences are:

\textbf{a) Derivation of ELBO:} Previous methods predefine separate latent factors and other latent variables (e.g., $\boldsymbol{\lambda}$) as variational parameters and then derive the ELBO. However, maintaining these parameters quickly consume memory as the dataset grows. Instead, we characterize the variational posterior mean of the latent factors using newly proposed continuous-indexed latent ODEs. This not only distinguishes our ELBO derivation from earlier methods but also overcomes scalability issues when handling large datasets.

\textbf{b) Optimization of ELBO:} In previous methods, seperate variational parameters are updated iteratively—adjusting one variable at a time while keeping the others fixed— which limits the ability to leverage distributed computing resources. In contrast, we use functional parameterization of the posterior distribution and derive a closed-form ELBO that can be efficiently optimized using gradient descent. This allows all variational parameters to be updated simultaneously, making our method highly parallelizable and well-suited for GPUs.

Furthermore, previous methods struggle to infer out-of-bound data points while our model can handle them effectively.

Overall, \MODEL integrates the strengths of deep learning—scalability, powerful representations, and flexible architectures—with the benefits of Bayesian learning, including robustness to noise, uncertainty quantification, and adaptive model complexity, to offers an elegant solution for generalized temporal tensor decomposition.



\section{Impact Statement}
\label{ap:ImSt}
This paper focuses on advancing temporal tensor decomposition techniques to push the boundaries of
tensor decomposition. We are mindful of the broader ethical implications associated with technological progress in this field. Although immediate societal impacts may not be evident, we recognize the
importance of maintaining ongoing vigilance regarding the ethical use of these advancements. It is
crucial to continuously evaluate and address potential implications to ensure responsible development
and application in diverse scenarios.

