
\section{Experiment}
\subsection{Synthetic Data}
\label{syt_data}
%\begin{figure}[t]
%    \centering
%    \begin{minipage}{0.15\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{figure/Syn1_1.png}
%        \caption*{(a) $\bc{Y}(0.152, 0.823,  t)$}
%    \end{minipage}
%    \hspace{-0.22cm} % 调整两张图片之间的间距
%    % \begin{minipage}{0.242\textwidth}
%    %     \centering
%    %     \includegraphics[width=\textwidth]{figure/Syn1_2.png}
%    %     \caption*{(b) $\bc{Y}(0.406, 0.133,  t)$}
%    % \end{minipage}
%    % \vspace{-0.02cm}
%    % \begin{minipage}{0.242\textwidth}
%    %     \centering
%    %     \includegraphics[width=\textwidth]{figure/Syn1_3.png}
%    %     \caption*{(c) $\bc{Y}(0.679, 0.553,  t)$}
%    % \end{minipage}
%    % \hspace{-0.22cm} % 调整两张图片之间的间距
%    \begin{minipage}{0.15\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{figure/Syn1_4.png}
%        \caption*{(b) $\bc{Y}(0.992, 0.982,  t)$}
%    \end{minipage}
%    
%\end{figure}

%\begin{figure}[htb]
%    \centering
%    \includegraphics[width=1\linewidth]{figure/Syn2.png}
%    \caption{Visualizations of learned factor trajectories at timestamp $t=0.235$. Only the $3_{\text{rd}}$ component is revealed to be informative and others are pruned to be zero.}
%    \label{fig:s2}
%    \vspace{-0.05in}
%\end{figure}










% \begin{table*}[h]
% \small
% \centering
% \renewcommand{\arraystretch}{1}
% \begin{tabular}{|c|c|c|c|c|c|}
% \hline 
% \textbf{} & \MODEL  \quad & \MODEL w.o. FARD & 
%  FunBat-CP (R=7) & 
%  NONFAT (R=7)  & DEMOTE (R=7) \\ \hline
% Noise Variance & \multicolumn{5}{c|}{\textbf{RMSE}}  \\ \hline
% % 0.3 &\textbf{0.299 $\pm$ 0.026} &0.339 $\pm$ 0.017 &0.542 $\pm$ 0.008 &0.463 $\pm$ 0.004 & 0.399 $\pm$ 0.001 \\ \hline
% 0.5 & \textbf{0.305 $\pm$ 0.005}&0.356 $\pm$ 0.010 & 0.554 $\pm$ 0.007 &0.495 $\pm$ 0.018 &0.423 $\pm$ 0.019\\ \hline
% 1 &\textbf{0.406 $\pm$ 0.007} & 0.461 $\pm$ 0.015 &0.563 $\pm$ 0.006 & 0.536 $\pm$ 0.006 & 0.547 $\pm$ 0.001\\ \hline
% % \textbf{} & \multicolumn{5}{c|}{\textbf{MAE}}  \\ \hline
% % % 0.3 &\textbf{0.115 $\pm$ 0.009} & 0.117 $\pm$ 0.005& 0.217 $\pm$ 0.036& 0.143 $\pm$ 0.002 &0.118 $\pm$ 0.001 \\ \hline
% % 0.5 &\textbf{0.134 $\pm$ 0.001} &0.169 $\pm$ 0.022& 0.216 $\pm$ 0.034&0.148 $\pm$ 0.004 & 0.157 $\pm$ 0.006 \\ \hline
% % 1 &\textbf{0.182 $\pm$ 0.006} &0.219 $\pm$ 0.006& 0.239 $\pm$ 0.025&0.193 $\pm$ 0.004&0.268 $\pm$ 0.003\\ \hline
% \end{tabular}
% \caption{Experiments  on the robustness of automatic rank determination mechanism against the noise on the CA traffic dataset. The results were averaged over five runs.}
% \label{Table:as}
% \end{table*}
% \vspace{-2mm}
 

We first evaluated \MODEL  on a synthetic  task.
We generated a two-mode temporal tensor,  and each entry is defined as: 
\begin{equation}
\begin{split}
    	\bc{Y}(i_1,  i_2,  t) =\boldsymbol{1}^{\T} & [\mf{u}^{1}(i_1, t)\circledast \mf{u}^{2}(i_2, t)], \\
      \text{where} \quad \mf{u}^{1}(i_1, t)= -\cos^{3}(2\pi t + 2.5\pi i_1 &); \quad \mf{u}^{2}(i_2, t)= \sin(3\pi t+3.5\pi i_2).
\end{split}
\label{eq:syn}
\end{equation}

We randomly sampled $25\times 25 \times 50$ off-grid indexes entries from interval $ [0, 1] \times [0, 1] \times [0, 1]$.
We added  Gaussian noise $\boldsymbol{\epsilon} \sim \mathcal{N}(0,  0.05) $ to the  generated data.
We  randomly selected $20\%$ of the data (6250 points in total) as the training data. Detailed model settings can be found in Appendix~\ref{ap:setting1}.
In Figure~\ref{fig:syn1},  we showed  the predictive trajectories of entry value  indexed in different coordinates. 
The dotted line represents the ground truth and the full line represents the  the predictive mean  learned by our model. The cross symbols represent the training points. The shaded region represents  the predictive   uncertainty region.
\begin{wrapfigure}{r}{0.45\textwidth}     
	\centering
	\begin{subfigure}{0.22\textwidth}        
		\includegraphics[width=\linewidth]{figure/Syn1_1.png}
		\caption{$\bc{Y}(0.152, 0.823,  t)$}
	\end{subfigure}
	\hfill                                  
	\begin{subfigure}{0.22\textwidth}
		\includegraphics[width=\linewidth]{figure/Syn1_4.png}
		\caption{ $\bc{Y}(0.992, 0.982,  t)$}
	\end{subfigure}
	\caption{Prediction results on different coordinates.}
	\label{fig:syn1}
	\vspace{-5mm}
\end{wrapfigure}

One can see that although the training points are sparse and noisy,   \MODEL  accurately recovered the ground truth,  demonstrating that it has effectively captured the  temporal dynamics. 
Figure~\ref{fig:s2}  depicts $R$ components of the learned  factor trajectories   at timestamp $t=0.235$.
 One can see that \MODEL  identifies the underlying rank (i.e.,  1) through uniquely  recovering the real mode functions and other four components are learned to be zero. More detailed interpretations on the rank revealing   process  were   provided  in Appendix \ref{ap:learning_curve}.




\subsection{Real-world Data}
 \textbf{Datasets:} We examined \MODEL  on three real-world benchmark datasets. (1) CA traffic,   lane-blocked records  in California. We extracted a three-mode temporal tensor between 5 severity levels,  20 latitudes and 16 longitudes. We collected 10K entry values and their timestamps.\url{(https://smoosavi.org/dataset s/lstw}; (2) Server Room,  temperature logs of Poznan Supercomputing and Networking Center. We extracted a three-mode temporal tensor between 3 air conditioning modes ($24^{\circ}$,  $27^{\circ}$ and $30^{\circ}$),  3 power usage levels $(50\%,  75\%,  100\%)$ and 34 locations. We collected 10K entry values and their timestamps.(\url{https://zenodo.org/record/3610078#%23.Y8SYt3bMJGi}); (3) SSF,  sound speed field measurements in the pacific ocean covering the region between latitudes $17^{\circ}$N $\sim 20^{\circ}$N,  longitude $114.7^{\circ}$E $\sim 117.7^{\circ}$E and depth $0$m $\sim200$m. 
We extracted a three-mode continuous-indexed temporal tensor data contains 10K observations across 10 latitudes,  20 longitudes,  10 depths  and 34 timestamps over 4 days. (\url{https://ncss.hycom.org/thredds/ncss/grid/GLBy0.08/expt_93.0/ts3z/dataset.html}).
\begin{wrapfigure}{r}{0.45\textwidth}
	\centering
	\includegraphics[width=0.44\textwidth]{figure/Syn2.png}
	\caption{Visualizations of learned factor trajectories at timestamp $t=0.235$. Only the $3_{\text{rd}}$ component is revealed to be informative and others are pruned to be zero.}
	\label{fig:s2}
	%\vspace{-5mm}
\end{wrapfigure}
\textbf{Baselines and Settings:} We compared \MODEL with state-of-the-art temporal and functional tensor methods: (1) THIS-ODE~\citep{thisode},  a continuous-time decomposition using a neural ODE to estimate tensor entries from static factors and time; (2) NONFAT~\citep{NONFAT},  a bi-level latent GP model that estimates dynamic factors  with  Fourier bases; (3) DEMOTE~\citep{wang2023dynamicdemote},  a neural diffusion-reaction process model for learning dynamic factors in tensor decomposition; (4) FunBaT~\citep{fang2023functional},  a Bayesian method using GPs as functional priors for continuous-indexed tensor data;  (5) LRTFR~\citep{luo2023lowrank},  a low-rank functional Tucker model that uses factorized neural representations for decomposition. 
We followed~\citep{wang2023dynamicdemote, fang2023functional} to randomly draw $80\%$ of observed entries for training  and the rest for testing. 
The performance metrics include the root-mean-square error (RMSE) and the  mean absolute error (MAE).
Each experiment was conducted five times (5‑fold cross‑validation) and we reported the average test errors  with their standard deviations. For \textsc{\MODEL},  we set  the ODE state dimension $J=10$ and  the initial number
of components of the factor trajectories $R = 10$. We provided more detailed baseline settings  in Appendix~\ref{ap:setting2}.



\begin{table*}[t]
	\scriptsize
	%\footnotesize
	\centering
	\renewcommand{\arraystretch}{1.05}
	\setlength{\tabcolsep}{4pt}
	\begin{tabular}{c|c c c| c c c}
		\hline & \multicolumn{3}{c|}{\textbf{RMSE}} & \multicolumn{3}{c}{\textbf{MAE}} \\ \quad 
		\text{Datasets} & \textit{CA Traffic} & \textit{Server Room} & \textit{SSF} & \textit{CA Traffic} & \textit{Server Room} & \textit{SSF}\\ \hline
		\multicolumn{7}{c}{$R=3$} \\ \hline
		%P-Tucker  & 0.358 $\pm$ 0.012 & 0.677 $\pm$ 0.129 & 1.446 $\pm$ 0.007 & 0.207 $\pm$ 0.035 & 0.323 $\pm$ 0.053 & 1.089 $\pm$ 0.007 \\ 
		
		
		THIS-ODE & 0.672 $\pm$ 0.002  & 0.132 $\pm$ 0.002 & 2.097 $\pm$ 0.003 & 0.587 $\pm$ 0.002 & 0.083 $\pm$ 0.002 & 2.084 $\pm$ 0.003\\ 
		NONFAT & 0.504 $\pm$ 0.010 &  0.129 $\pm$ 0.002 & 9.796 $\pm$ 0.010 & 0.167 $\pm$ 0.009 & 0.078 $\pm$ 0.001  & 8.771 $\pm$ 0.043 \\
		DEMOTE & 0.447 $\pm$ 0.001 &  0.131 $\pm$ 0.001 & 9.789 $\pm$ 0.001 & 0.118 $\pm$ 0.002 & 0.090 $\pm$ 0.0015  & 8.757 $\pm$ 0.001 \\ 
		
		FunBaT-CP & 0.563 $\pm$ 0.025 & 0.425 $\pm$ 0.003 & 0.696 $\pm$ 0.047 & 0.244 $\pm$ 0.025 & 0.308 $\pm$ 0.001 & 0.549 $\pm$ 0.038 \\ 
		FunBaT-Tucker & 0.584 $\pm$ 0.009 & 0.498 $\pm$ 0.058 & 0.730 $\pm$ 0.201 & 0.189 $\pm$ 0.014 & 0.381 $\pm$ 0.053 & 0.614 $\pm$ 0.128 \\ 
		LRTFR & 0.379 $\pm$ 0.042 & 0.151 $\pm$ 0.004 & 0.595 $\pm$ 0.018 & 0.187 $\pm$ 0.022 & 0.110 $\pm$ 0.002 & 0.464 $\pm$ 0.0165 \\ \hline
		
		
		\multicolumn{7}{c}{$R=5$} \\ \hline
		%P-Tucker  & 0.582 $\pm$ 0.001 & 0.458 $\pm$ 0.039 & 1.728 $\pm$ 0.003 & 0.307 $\pm$ 0.001 & 0.259 $\pm$ 0.007 & 1.378 $\pm$ 0.004 \\ 
		
		
		THIS-ODE & 0.632 $\pm$ 0.002 & 0.132 $\pm$ 0.003 & 1.039 $\pm$ 0.015 & 0.552 $\pm$ 0.001 &   0.083 $\pm$ 0.002 &1.032 $\pm$ 0.002\\ 
		NONFAT & 0.501 $\pm$ 0.002 &  0.117 $\pm$ 0.006 & 9.801 $\pm$ 0.014 & 0.152 $\pm$ 0.001 & 0.071 $\pm$ 0.004  & 8.744 $\pm$ 0.035 \\ 
		DEMOTE & 0.421 $\pm$ 0.002 &  0.105 $\pm$ 0.003 & 9.788 $\pm$ 0.001 & 0.103 $\pm$ 0.001 & 0.068 $\pm$ 0.003  & 8.757 $\pm$ 0.001 \\ 
		
		
		FunBaT-CP & 0.547 $\pm$ 0.025& 0.422 $\pm$ 0.001 & 0.675 $\pm$ 0.061 & 0.204 $\pm$ 0.052 & 0.307 $\pm$ 0.002 &  0.531 $\pm$ 0.051 \\ 
		FunBaT-Tucker & 0.578 $\pm$ 0.005 & 0.521$\pm$ 0.114 & 0.702 $\pm$ 0.054& 0.181 $\pm$ 0.005 & 0.391 $\pm$ 0.097& 0.557 $\pm$ 0.041 \\
		
		LRTFR & 0.376 $\pm$ 0.016 & 0.167 $\pm$ 0.006 & 0.532 $\pm$ 0.036 & 0.182 $\pm$ 0.012 & 0.121 $\pm$ 0.005 & 0.418 $\pm$ 0.003 \\ \hline
		
		\multicolumn{7}{c}{$R=7$} \\ \hline
		%P-Tucker  & 0.594 $\pm$ 0.001 & 0.665 $\pm$ 0.103 & 1.468 $\pm$ 0.001 & 0.283 $\pm$ 0.001 & 0.305 $\pm$ 0.042 & 1.139 $\pm$ 0.002 \\ 
		
		
		
		THIS-ODE & 0.628 $\pm$ 0.007 & 0.154 $\pm$ 0.016 & 1.685 $\pm$ 0.009 & 0.548 $\pm$ 0.006 & 0.089 $\pm$ 0.002 & 1.674 $\pm$ 0.008\\ 
		NONFAT & 0.421 $\pm$ 0.016 &  0.128 $\pm$ 0.002 & 9.773 $\pm$ 0.015 & 0.137 $\pm$ 0.006 & 0.077 $\pm$ 0.002  & 8.718 $\pm$ 0.035 \\ 
		DEMOTE & 0.389 $\pm$ 0.005 &  0.094 $\pm$ 0.006 & 9.790 $\pm$ 0.002 & 0.091 $\pm$ 0.001 & 0.062 $\pm$ 0.006  & 8.753 $\pm$ 0.006 \\ 
		
		
		FunBaT-CP & 0.545 $\pm$ 0.009& 0.426 $\pm$ 0.001 & 0.685 $\pm$ 0.049 & 0.204 $\pm$ 0.037 & 0.307 $\pm$ 0.001 & 0.541 $\pm$ 0.039 \\ 
		FunBaT-Tucker  &0.587 $\pm$ 0.011 & 0.450 $\pm$ 0.041 & 0.642 $\pm$ 0.037& 0.195 $\pm$ 0.022 & 0.330 $\pm$ 0.026 & 0.507 $\pm$ 0.029 \\ 
		LRTFR & 0.365 $\pm$ 0.042 &  0.156 $\pm$ 0.012 & 0.502 $\pm$ 0.033 & 0.161 $\pm$ 0.014 & 0.118 $\pm$ 0.009  & 0.392 $\pm$ 0.028 \\ \hline
		
		
		
		\multicolumn{7}{c}{Functional Automatic Rank Determination} \\ \hline
		\MODEL(Ours)  &\textbf{ 0.284 $\pm$ 0.016} & \textbf{0.078 $\pm$ 0.001} & \textbf{0.373 $\pm$ 0.003} & \textbf{0.085 $\pm$ 0.004} & \textbf{0.047 $\pm$ 0.003} & \textbf{0.288 $\pm$ 0.003} \\ \hline
		\MODEL w.o. FARD  &{ 0.301 $\pm$ 0.020} & {0.091 $\pm$ 0.008} & {0.402 $\pm$ 0.013} & {0.094 $\pm$ 0.010} & {0.0657 $\pm$ 0.005} & {0.310 $\pm$ 0.010} \\ \hline
	\end{tabular}
	\caption{Predictive errors and standard deviation. The results were averaged over five runs.}
	\label{Tab:results}
	\vspace{-20pt}
\end{table*}
\textbf{Prediction Performance: }
Table~\ref{Tab:results} shows that \MODEL\ consistently outperforms all baselines by a substantial margin, without requiring manual rank tuning. Moreover, the integration of FARD leads to significant performance gains, highlighting its effectiveness.
The learned ranks of the CA traffic,  Server Room and SSF datasets are $5,  7,  7$ respectively. 
We illustrated  their rank-learning curves  of three datasets in Figure~\ref{fig:learning_curve_three} in  Appendix \ref{ap:b}.
We observed that methods which do not consider the continuously indexed mode (e.g.,  NONFAT,  DEMOTE,  THIS-ODE) perform poorly on the SSF dataset. In contrast,  approaches that leverage this continuity achieve significantly better results. This is because the SSF dataset exhibits strong continuity across three modes,  and methods that fail to incorporate this information struggle to deliver satisfactory reconstructions. Additional visualization results of the predictions from various methods and datasets are presented in Fig.~\ref{fig:traffic_extra}, Fig.~\ref{fig:server_extra} and Fig.~\ref{fig:ssf_extra}, and  in Appendix~\ref{app:more_visual}.




\textbf{Revealed Rank Analysis and Interpretability:}
We analyzed the revealed rank and  the learned factor  trajectories of the SSF dataset. 
  Figure~\ref{fig:last}(a) shows the posterior mean of  the variance of the learned  factor trajectories (i.e.,  $\mathbb{E}_{q}(\frac{1}{\boldsymbol{\lambda}})$),  which governs the fluctuations of their corresponding $R=10$ components of factor  trajectories.
One can see that  $\mathbb{E}_q(\frac{1}{\lambda_3}), \mathbb{E}_q(\frac{1}{\lambda_{4}})$ and $\mathbb{E}_q(\frac{1}{\lambda_{8}})$ are small, indicating that these components concentrate around zero and  can be pruned without affecting the final predictions. Thus,  our method revealed the rank of the SSF dataset to be  7.
Additionally,   
$\mathbb{E}_q(\frac{1}{\lambda_1})$ and $\mathbb{E}_q(\frac{1}{\lambda_{10}})$  dominate,  indicating that the corresponding $1_{\text{st}}$ and $10_{\text{th}}$ components of the  factor trajectories form the primary structure of the data. 
To illustrate this,  we plotted these two components of the depth-mode factor trajectories  at depth 30m in Figure~\ref{fig:last}(b).
 As we can see,  the trajectories show periodic patterns,  which are  influenced by the the day-night cycle of ocean temperature.
We also compared the predicted curves of \MODEL and LRTFR for an entry  
 located at  $17^{\circ}$N,  $114.7^{\circ}$E and a depth of 30m.  \MODEL outperforms LRTFR,  providing more accurate predictions with uncertainty quantification,  even outside the training region (right to the dashed vertical line).
  This suggests that our model holds promise for extrapolation tasks.
The results collectively highlighted the advantage of \MODEL  in capturing continuous-indexed multidimensional dynamics,  which is crucial for analyzing real-world temporal data and performing predictive tasks.



\begin{figure*}[t]
	\centering
	\begin{minipage}{0.242\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figure/ssf_variance_power.png}
		\caption*{(a) Posterior mean of $\frac{1}{\boldsymbol{\lambda}}$}
	\end{minipage}
	\hspace{-0.1cm} % 调整两张图片之间的间距
	\begin{minipage}{0.242\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figure/ssf_lf1.png}
		\caption*{(b) Factor trajectories}
	\end{minipage}    
	\begin{minipage}{0.242\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figure/ssf_visual1.png}
		\caption*{(c) Entry predictions}
	\end{minipage}
	\hspace{-0.1cm} % 调整两张图片之间的间距
	\begin{minipage}{0.242\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figure/syn_scability.png}
		\caption*{(d) Scalability}
	\end{minipage}
	\caption{Illustrations on (a) the 
		posterior mean of the variance 
		%(i.e.,  $1/\mathbb{E}_{q}(\boldsymbol{\lambda})$)
		from the SSF dataset ($R=10$),  (b) the dominant components of learned depth-mode factor trajectories  from the SSF dataset,  (c) the entry value predictions indexed in ($17^{\circ}$N,  $114.7^{\circ}$E,  30m) of the SSF dataset,  (d) the scalability over the length of time series on  synthetic dataset.}
	\label{fig:last}
\end{figure*}

\begin{table*}[h!]
	\scriptsize
	\centering
	\renewcommand{\arraystretch}{1.06}
	\begin{tabular}{|c|c|c|c|c|}
		\hline 
		\textbf{} & \MODEL  \quad & \MODEL w.o. FARD & 
		LRTFR (R=5)   & DEMOTE (R=5) \\ \hline
		Noise Variance & \multicolumn{4}{c|}{\textbf{RMSE}}  \\ \hline
		% 0.3 &\textbf{0.299 $\pm$ 0.026} &0.339 $\pm$ 0.017 &0.542 $\pm$ 0.008 &0.463 $\pm$ 0.004 & 0.399 $\pm$ 0.001 \\ \hline
		0.5 & \textbf{0.305 $\pm$ 0.005}&0.356 $\pm$ 0.010 & 0.434 $\pm$ 0.047 &0.430 $\pm$ 0.010\\ \hline
		1 &\textbf{0.406 $\pm$ 0.007} & 0.461 $\pm$ 0.015 &0.516 $\pm$ 0.034 &0.552 $\pm$ 0.009\\ \hline
		% \textbf{} & \multicolumn{5}{c|}{\textbf{MAE}}  \\ \hline
		% % 0.3 &\textbf{0.115 $\pm$ 0.009} & 0.117 $\pm$ 0.005& 0.217 $\pm$ 0.036& 0.143 $\pm$ 0.002 &0.118 $\pm$ 0.001 \\ \hline
		% 0.5 &\textbf{0.134 $\pm$ 0.001} &0.169 $\pm$ 0.022& 0.216 $\pm$ 0.034&0.148 $\pm$ 0.004 & 0.157 $\pm$ 0.006 \\ \hline
		% 1 &\textbf{0.182 $\pm$ 0.006} &0.219 $\pm$ 0.006& 0.239 $\pm$ 0.025&0.193 $\pm$ 0.004&0.268 $\pm$ 0.003\\ \hline
	\end{tabular}
	\caption{Experiments  on the robustness of functional automatic rank determination mechanism against the noise on the CA traffic dataset. The results were averaged over five runs.}
	\label{Table:as}
\end{table*}


\textbf{Robustness against Noise:}
The incorporated FARD mechanism can reveal the underlying rank of the functional temporal data and prune the unnecessary components of the  factor trajectories, enhancing noise robustness. To evaluate its performance,  we  added Gaussian noise with varying  variance levels to the training set of CA traffic dataset and compared the results of different methods,  as  summarized in Table~\ref{Table:as}. 
We disabled FARD by using a simple RMSE criterion as the objective function to constitute an ablation study.  More detailed results of the baselines with varying ranks in Table~\ref{Table:as1} and Table~\ref{Table:as2} in Appendix~\ref{ap:noise_rob}. \MODEL  achieves lower prediction errors than \MODEL w.o. FARD,  , confirming FARD’s benefit.  We also demonstrate the robustness of \MODEL\ aganist Laplacian and Poisson noise  in Table~\ref{Table:as3} (Appendix~\ref{ap:noise_rob}).


\begin{table}[h!]
	\scriptsize
	\centering
	\renewcommand{\arraystretch}{1.1}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Tensor size} & $20\times20\times20\times25$ ($4_{\text{th}}$-oder) &$20\times20\times20\times20\times25$ ($5_{\text{th}}$-oder) & $20\times20\times20\times20\times20\times25$ ($6_{\text{th}}$-oder) \\
		\hline
		\textbf{Time(s)} & 0.433 & 1.101 & 3.062 \\
		\hline
	\end{tabular}
\vspace{2mm}
	\caption{Per-epoch/iteration running time on different orders of tensors(in seconds).}
	\label{Tab:scal2}
\end{table}

\textbf{Sensitivity and Scalability:}
We  examined the sensitivity of \MODEL with respect to the variational hyperparamter $a_r^0, b_r^0$ and the dimensionality of ODE state $J$ on the CA traffic dataset. The results were given in Table~\ref{Table:a_b} and  Table~\ref{Table:J} respectively. Empirically, \MODEL performs consistently well across different $a_r^0, b_r^0$ and $J$.
We further evaluated the scalability of \MODEL  with respect to the  length of  the time series $T$ and   $J$. For $T$,    we randomly sampled $25 \times 25 \times T$ off-grid index entries from the interval $ [0, 1] \times [0, 1] \times [0, 1]$ using
Eq.~\eqref{eq:syn} for training. We varied $T$ across $\{50,  200,  500,  1000\}$ and $J$ across $\{5,  25,  30\}$.   To better quantify the scalability,  we  employed the simple Euler scheme with fixed step size~\citep{platen2010numerical} for network training.  The results  shown in Figure~\ref{fig:last}(d)  indicate that the running time of \MODEL  grows linearly with $T$  and  is insensitive to   $J$,  demonstrating its suitability for large-scale applications.


In addition, we conducted experiments to assess scalability with respect to tensor order.  
We generated synthetic datasets with varying orders while fixing the temporal dimension to $25$ and the non-temporal dimensions to $20$.  
From each dataset, we randomly extracted $1\%$ of the total data points for training.  
The average training time per epoch is reported in Table~\ref{Tab:scal2}.  
The results show that \MODEL scales well with tensor order.  
By decoupling each mode, \MODEL translates the addition of modes into an increase in the number of ODE states under our proposed efficiency-driven approach (as discussed in Section~\ref{sec:Latent-ODE-Flow}).  
For example, for a 4th-order tensor of size $20 \times 20 \times 20 \times 25$, we only need to solve $20 + 20 + 20 = 60$ ODE states, which can be updated simultaneously with a single ODE solver.  Additional results on the scalability of \MODEL are provided in Appendix~\ref{ap:more_scal}.






\textbf{Computational Efficiency:}
We compared per-iteration runtimes of \MODEL\ and baselines on a system with an NVIDIA RTX 4070 GPU, Intel i9-13900H CPU, 32 GB RAM, and 1 TB SSD (Table~\ref{Tab:runtime}, Appendix~\ref{ap:running}). \MODEL\ is slightly faster than NONFAT and DEMOTE, and much faster than THIS-ODE.





