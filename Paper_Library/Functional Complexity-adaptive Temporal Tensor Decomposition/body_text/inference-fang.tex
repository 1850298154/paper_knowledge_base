\section{Model Inference}
 \begin{algorithm}[t]
	\SetAlgoLined
	\KwIn{Training data $\mathcal{D}=\{y_n, \mf{i}_n, t_n\}_{n=1}^{N}$ }
	Collect all possible $I_k$ indexes for $K$ modes and $T$ possible timestamps.
	Initialize  $\{\boldsymbol{\omega}_k\}_{k=1}^K, \{\alpha_r, \beta_r\}_{r=1}^{R}, \sigma, \rho, \iota$.
	
	\While{not convergence}{
		Construct a set of initial ODE state tables $\mathcal{Z}_0$ using  Fourier features and Encoder, encompassing all possible indexes.
		
		\For{$i = 1, 2, \cdots, T$}{
			$\mathcal{Z}(t_i)$ = \text{ODESolve}($\mathcal{Z}(t_{i-1})$, 
			$\{h_{\boldsymbol{\theta}_k}\}_{k=1}^{K}$,$(t_{i-1},t_i))$
			
			Compute necessary $\mathbf{g}^k(i_k,t_i)$ from $\mathcal{Z}(t)$ using \eqref{eq:latent-ode3}.  
		}
		Take gradient step on \eqref{eq:loss}.
	}
	\caption{Training process of \MODEL}
	\label{ap:algorithm1}
\end{algorithm}


\subsection{Factorized Functional Posterior and Analytical Evidence Lower Bound}
It is intractable to compute the full posterior of latent variables in \eqref{eq:joint} due to the high-dimensional integral and complex form of likelihood. We take a workaround to construct a variational distribution $q(\mathcal{U}, \boldsymbol{\lambda}, \tau)$ to approximate the exact posterior $p(\mathcal{U}, \boldsymbol{\lambda}, \tau|\mathcal{D})$. 
Similar to the widely-used mean-field assumption, we  design the approximate posterior in a fully factorized form: $q(\mathcal{U}, \boldsymbol{\lambda}, \tau) = q(\mathcal{U})q(\boldsymbol{\lambda})q(\tau)$.

Specifically, the conditional conjugate property of Gaussian-Gamma distribution motivates us to formulate the corresponding variational posteriors as follows:
\begin{equation}
    \begin{split}
        &q(\mathcal{U}) = \prod_{n=1}^{N}\prod_{k=1}^{K} \mathcal{N}(\mf{u}^k(i_k^n,t_n)|\mf{g}^k(i_k^n,t_n), \sigma^2\mf{I}),
        \label{eq:q_u} 
    \end{split}
\end{equation}
\vspace{-1mm}
where $\mf{g}^k(\cdot, \cdot)$  is the mode-wise latent temporal representations parameterized by $\boldsymbol{\omega}_k$, as we mentioned in Section \ref{sec:Latent-ODE-Flow}, and $\sigma$ is the variational variance shared by all $\mf{u}^k$.
Similarly, we formulate $q(\boldsymbol{\lambda}), q(\tau)$ as:
\vspace{-1mm}
\begin{equation}
        q(\boldsymbol{\lambda}) =  \prod_{r=1}^{R} \text{Gamma}(\lambda_r|\alpha_r, \beta_r),
        q(\tau) = \text{Gamma}(\tau|\rho, \iota),\label{eq:q_tau}
\end{equation}
where $\{\alpha_r, \beta_r\}_{r=1}^{R}, \rho, \iota$ are the variational parameters to characterize the approximated posteriors. 
Our goal is to estimate the latent ODE parameters $\boldsymbol{\omega}_k$ and variational parameters $\{ \{\alpha_r, \beta_r\}_{r=1}^{R}, \sigma, \rho, \iota \}$ in \eqref{eq:q_u}  \eqref{eq:q_tau} to make the approximated posterior $q(\mathcal{U}, \boldsymbol{\lambda}, \tau)$ as close as possible to the true posterior $p(\mathcal{U}, \boldsymbol{\lambda}, \tau|\mathcal{D})$. To do so, we follow the variational inference framework~\citep{variational_inference} and construct the following objective function by minimizing the Kullback-Leibler (KL) divergence between the approximated posterior and the true posterior $\text{KL}(q(\mathcal{U}, \boldsymbol{\lambda}, \tau)\|p(\mathcal{U}, \boldsymbol{\lambda}, \tau|\mathcal{D}))$, which  leads to the maximization of the evidence lower bound (ELBO): 
\vspace{-1mm}
\begin{equation}
        \text{ELBO} = \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln p(\mathcal{D}|\mathcal{U}, \boldsymbol{\lambda}, \tau)] +\mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda})}[\ln 
        \frac{p(\mathcal{U}|\boldsymbol{\lambda})}{q(\mathcal{U})}] 
         - \text{KL}(q(\boldsymbol{\lambda})\|p(\boldsymbol{\lambda})) - \text{KL}(q(\tau)\|p(\tau)) \label{eq:elbo}.
\end{equation}
The ELBO is consist of four terms. The first term is posterior expectation of log-likelihood  while the last three are KL terms. Usually, the first term is intractable if the likelihood model is complicated and requires the costly  sampling-based approximation to handle the integrals in the expectation ~\citep{doersch2016tutorialVAE, NONFAT}. Fortunately, by leveraging the well-designed conjugate priors and factorized structure of the posterior, we make an endeavor to derive its analytical expression:
\vspace{-1mm}
\begin{align}
        &\mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln p(\mathcal{D}|\mathcal{U}, \boldsymbol{\lambda}, \tau)] =-\frac{N}{2}\ln(2\pi) + \frac{N}{2}(\psi(\rho)-\ln\iota) 
        \nonumber\\  &-\frac{1}{2}\sum_{n=1}^{N}\frac{\rho}{\iota}\big\{ y_n^2 -2y_n\{\boldsymbol{1}^{\T}[\underset{k}{\circledast} \mf{g}^k(i_k^n,t_n)]\}
  +\boldsymbol{1}^{\T}[\underset{k}{\circledast} \text{vec}(\mf{g}^{k}(i_k^{n},t_n)\mf{g}^{k}(i_k^{n},t_n)^{\T}+\sigma^2\mf{I})]\big\},
\end{align}
where $\mf{g}^k_r(i_k^n, t_n)$ is the $r$-th element of the $k$-th mode's latent temporal representation $\mf{g}^k(i_k^n, t_n)$. We provide  the detailed derivation in Appendix \ref{ap:A.2}. 
%We note that the first term computes the posterior expectation of the log-likelihood of the noisy observation.
%, which approximates  the MAE criterion.
The second term of \eqref{eq:elbo} computes the KL divergence between prior and posterior of factor trajectories, which is also with a closed from:
\vspace{-1mm}
\begin{align}
    &\mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda})}[\ln 
    \frac{p(\mathcal{U}|\boldsymbol{\lambda})}{q(\mathcal{U})}]= -\text{KL}(q(\mathcal{U}) \| p(\mathcal{U}|\boldsymbol{\lambda}=\mb{E}_q(\boldsymbol{\lambda})))=\nonumber \\
    &-\sum_{n=1}^N \sum_{k=1}^K\sum_{r=1}^R \frac{1}{2}\big\{\ln(\frac{\beta_r}{\alpha_r\sigma^2})+ \frac{\alpha_r}{\beta_r}\{\sigma^2+[\mf{g}^k_r(i_k^n, t_n)]^2\}-1\big\}, \label{term2}
\end{align}
where $\mb{E}_q(\boldsymbol{\lambda})=[\mb{E}_q({\lambda_1}),\ldots,\mb{E}_q({\lambda_R})]^{\T}=[{\alpha_1}/{\beta_1},\ldots,{\alpha_R}/{\beta_R}]^{\T}$.
This term encourages rank reduction,  as it drives the 
posterior mean of $\lambda_r$  to be large, thereby forcing the corresponding $r$-th component of $K$ factor trajectories $\{\mf{g}^{k}_r(\cdot, \cdot)\}_{k=1}^K$ to be zero. The combination of the above two terms enables an automatic rank determination mechanism by striking a balance between capacity of representation and model complexity. 
As the prior and posterior of $\boldsymbol{\lambda}$ and $\tau$ are both Gamma distribution, the last two KL terms in the ELBO are analytically computable, as shown in \eqref{eq:kl1}\eqref{eq:kl2} in Appendix~\ref{ap:A.2}.





\textit{We highlight that each term in \eqref{eq:elbo} admits a closed‐form expression, eliminating the need for sampling‐based approximations.} Consequently, during training we can compute the ELBO’s gradient with respect to the variational parameters exactly, allowing us to employ standard gradient‐based optimization methods.  We present the differences from previous methods in  Appendix~\ref{app:difference}.  Our proposed method scales efficiently to jointly optimize both the variational and latent ODE parameters:
\begin{equation} 
    \text{argmax}_{\{\boldsymbol{\omega}_k\}_{k=1}^K, \{\alpha_r, \beta_r\}_{r=1}^{R}, \sigma, \rho, \iota} \text{ELBO}.
    \label{eq:loss}
\end{equation}
We summarize the inference algorithm in Algorithm 1. 
When $N$ is large, we can use the mini-batch gradient descent method to accelerate the optimization process.
After obtaining the variational posteriors of the latent variables, we can further derive the predictive distribution for new data at arbitrary indices. More details can be found in Appendix~\ref{app:closed_distri}. To distinguish our method from previous approaches, we refer to it as a functional automatic rank determination mechanism (FARD).







%\subsection{Closed Form of Predictive Distribution}
%After obtaining the variational posteriors of the latent variables, we can further derive the predictive distribution of the new data with arbitrary indexes. Given the index set $\{i^p_1,\cdots, i^p_k, t_p\}$ for prediction, we can obtain the variational predictive posterior distribution, which follows a Student's t-distribution (See Appendix \ref{pred_distr} for more details):
%\begin{equation}
%    \begin{split}
%        &p(y_p|\mathcal{D}) \sim \mathcal{T}(y_p|\mu_p, s_p, \nu_p),\\
%       &\mu_p =  \boldsymbol{1}^{\T}[\underset{k}{\circledast} 
% \mf{g}^k(i_k^p, t_p)], \quad \nu_p = 2\rho,\\
%        &s_p =\big\{\frac{\iota}{\rho}+\sigma^2\sum_{j=1}^K[\underset{k\ne j}{\circledast} 
% \mf{g}^k(i_k^p, t_p)]^{\T}[\underset{k\ne j}{\circledast} 
% \mf{g}^k(i_k^p, t_p)]\big\}^{-1},
%    \end{split}
%\end{equation}
%where $\mu_p$, $s_p$, $\mu_p$ is the mean, scale parameter and degree of freedom of the Student's t-distribution, respectively. The closed-form  predictive distribution is a great advantage for the prediction process, as it allows us to do the probabilistic reconstruction and prediction with uncertainty quantification over the arbitrary continuous indexes.


% % Algorithm
% \begin{algorithm}[H]
% \SetAlgoLined
% \caption{Training process of \MODEL}
% \KwIn{Training data $\mathcal{D}=\{y_n, \mf{i}_n, t_n\}_{n=1}^{N}$ }
% Collect all possible $I_k$ indexes for $K$ modes and construct $K$ initial time embedding tables $\{\mf{Z}_0^k\}_{k=1}^{K}$ with Encoder.
% \BlankLine
% % Steps
% \REPEAT{
% \For{$i = 1, 2, \cdots$}{
%     $\{\mf{Z}^k(t)\}_{k=1}^{K}$ = \text{ODESolve}($\{\mf{Z}^k_0\}_{k=1}^{K}$, 
%     $\{h_{\boldsymbol{\theta}}\}_{k=1}^{K}$,$t_{i-1},t_i)$
%     }
%     }
% Return $Y$\;
% \end{algorithm}




%\textit{Interpretation of Evidence Lower Bound:}

