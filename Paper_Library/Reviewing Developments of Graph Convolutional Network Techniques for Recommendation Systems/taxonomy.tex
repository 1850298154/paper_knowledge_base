\section{GCN-based Recommendation Models}

\subsection{GCN Outlooks}

There are primarily three types of tasks on graphs: classification, prediction, and regression, occurring at three levelsâ€”node, edge, and subgraph. Despite the task diversity, a standard optimization procedure exists. Embeddings are mapped along with labels to formulate the loss function, and common optimizers are employed for model learning. Various mapping functions (\textit{e.g.}, MLP, inner product) and loss functions (\textit{e.g.}, pair-wise, point-wise) can be chosen based on specific tasks. For instance, in pair-wise loss functions like Bayesian Personalized Ranking (BPR)~\cite{rendle2009bpr}, discrimination between positive and negative samples is encouraged, and the formulation is as follows:

\begin{equation}
	\mathcal{L} = \sum_{p, n} -\textrm{ln}\sigma(s(p) - s(n)),
\end{equation} 

where $\sigma(\cdot)$ is the sigmoid function, $p$ and $n$ denote positive and negative samples, and $s(\cdot)$ measures the samples. In contrast, point-wise loss functions include mean square error (MSE) loss, cross-entropy loss, and others.

To illustrate GNN model optimization in link prediction and node classification, consider link prediction. The likelihood of an edge existing between nodes $i$ and $j$ is calculated based on the similarity with node embeddings in each propagation layer:

\begin{equation}
	s\left(i, j\right) = f(\{\mathbf{h}^l_i\}, \{\mathbf{h}^l_j\}),
\end{equation}

where $f(\cdot)$ denotes the mapping function. Training data $\mathcal{O} = \{(i, j, k)\}$ consists of observed positive and randomly-selected negative samples, $(i, j)$ and $(i, k)$, respectively. For node classification, node embeddings are transformed into a probability distribution representing its class:

\begin{equation}
	\mathbf{p}_i = f(\{\mathbf{h}_i^l\}),
\end{equation}

where $\mathbf{p}_i\in \mathbf{R}^{C\times 1}$ is the distribution, and $C$ is the number of classes. Training data $\mathcal{O} = \{(i, \mathbf{y}_i)\}$ is structured such that $\mathbf{y}_i\in \mathbf{R}^{C\times 1}$, and $i$ belonging to class $c$ is denoted by $\mathbf{y}_{ic} = 1$; otherwise, $\mathbf{y}_{ic} = 0$. For classification tasks, the point-wise loss function, like cross-entropy loss, is typically chosen:

\begin{equation}
	\mathcal{L} = -\sum_{(i, \mathbf{y}_i)\in \mathcal{O}} \mathbf{y}_i^T \log{\mathbf{p}_i}.
\end{equation}


\subsection{User-item Interaction with Collaborative Filtering}
A line of research~\cite{TrustWalker,BiRank,HOP-rec,RippleNet} exploits higher-order connectivity information between users and items to infer user preferences. For example, TrustWalker~\cite{TrustWalker} utilizes random walks to directly propagate preference scores. However, none of these approaches leverage such information in the embedding space.

Another relevant research avenue involves exploiting the user-item graph structure for recommendation. Previous efforts, like ItemRank~\cite{ItemRank}, employ label propagation to directly disseminate user preference scores across the graph, encouraging connected nodes to have similar labels. Recently, GRMF~\cite{rao2015collaborative} and HOP-Rec~\cite{HOP-rec} smooth node embeddings between one-hop and high-hop neighbors by introducing additional loss terms. These methods incorporate graph structure at the objective function level, distinguishing themselves from SVD++ which encodes neighborhood information in the predictive model formulation.

Apart from utilizing on-graph representation ability, another category of collaborative filtering (CF) methods considers historical items to profile a user, enriching user representations. Early works like FISM~\cite{FISM} construct a user representation via the average or weighted sum of ID embeddings of historical items. Later, SVD++ integrates such representations with the user's ID embedding as a final representation, achieving success in rating prediction. Nevertheless, historical items contribute differently to shaping a user's preference, necessitating adaptive learning of their weights. Recent works such as ACF~\cite{ACF}, NAIS~\cite{NAIS}, and DeepICF~\cite{DeepICF} introduce attention mechanisms to specify varying importance of historical items, achieving improved embeddings.

When revisiting historical interactions as a user-item bipartite graph, these improvements are attributed to encoding a user's ego network, i.e., her one-hop neighbors, into the embedding learning. Recently emerged graph neural networks (GNNs) have gained prominence in modeling graph structure, especially high-hop neighbors, to guide embedding learning~\cite{GCN,GraphSAGE}. GNNs were initially proposed for node classification on attributed graphs, where each node is described by rich input features (e.g., attributes, contents). The basic idea of GNN is to encode graph structure, as well as input features, into a better representation of each node. Early studies defined graph convolution in the spectral domain, such as Laplacian eigen-decomposition~\cite{DBLP:journals/corr/BrunaZSL13} and Chebyshev polynomials~\cite{FirstGCN}, which are computationally expensive. Later on, GraphSage~\cite{GraphSAGE} and GCN~\cite{GCN} redefined graph convolution in the spatial domain, aggregating the embeddings of neighbors to refine the target node's embedding. Due to its interpretability and efficiency, this formulation quickly became prevalent in GNNs and is widely used~\cite{DeepInf,Feng2019TOIS,zhao2019cross}.

Motivated by the strength of graph convolution, recent efforts like NGCF~\cite{NGCF}, GC-MC~\cite{GC-MC}, and PinSage~\cite{PinSage} adapted GCN to the user-item interaction graph, capturing CF signals in high-hop neighbors for recommendation.

It is worth mentioning that several recent efforts provide deep insights into GNNs~\cite{DeepInsights,ICLR19-APPNP,SGCN}. 
In particular, Wu et al.~\cite{SGCN} argue for the unnecessary complexity of GCN, developing a simplified GCN (SGCN) model by removing nonlinearities and collapsing multiple weight matrices into one. 
Another work conducted concurrently~\cite{LR-GCCF} also finds that nonlinearity is unnecessary in NGCF and develops a linear GCN model for CF. 
Recently, a light-weighted GCN architecture namely LightGCN attracts great attention as it removes unnecessary modules, such as removing all redundant parameters and retaining only the ID embeddings, making the model as simple as matrix factorization (MF).



\subsection{User-user Soical Regularization}

One of the earliest instances of a social recommender system dates back to 1997. 
In recent years, an abundance of social media platforms, such as Facebook and Twitter, has emerged, providing individuals with convenient means to communicate and express themselves. The widespread adoption of social media has resulted in an unprecedented volume of social information.
For instance, Facebook, the largest social networking site, has fostered a staggering 35 billion online friendships. 
Similarly, the most popular user on Twitter, the leading microblogging site, boasts an impressive 37,974,138 followers. The exponential growth of social media has significantly expedited the advancement of social recommender systems.


In recent years, there are lots of works exploiting user's social relations for improving the recommender system~\cite{wu2018social_collaborative, tang2013exploiting, tang2016recommendations}. 
Most of them assume that users' preference is similar to or influenced by their friends, which can be suggested by social theories such as social homophily~\cite{mcpherson2001birds} and social influence~\cite{marsden1993network}. 
According to the assumptions above, social regularization has been proposed to restrain the user embedding learning process in the latent factor based models~\cite{ma2011recommender, jamali2010matrix, ma2008sorec}. 
And TrustMF~\cite{yang2016social} model is proposed to model the mutual influence between users by mapping users into two low-dimensional space: truster space and trustee space and factorize the social trust matrix. 
By treating the social neighbors' opinion as the auxiliary implicit feedbacks of the targeted user, TrustSVD~\cite{guo2015trustsvd} is proposed to incorporate the social influence from social neighbors on top of SVD++~\cite{koren2008factorization}. 
Generally, this technique extends traditional matrix factorization methods by incorporating social regularization terms into the objective function. Social regularization encourages the learned user and item embeddings to respect the social relationships, making the recommendation model aware of the influence of social connections.

Moreover, some recent studies like~\cite{wang2017item, fan2018deep, chen2019social} and~\cite{fan2019deep, chen2019efficient, krishnan2019modular} leverage deep neural network and transfer learning or adversarial learning approach respectively, to learn a more complex representation or model the shared knowledge between social domain and item domain. 
GCNs have gained popularity in social regularized recommendation. They capture the complex relationships in a social network by learning node embeddings through iterative information aggregation from neighboring nodes. The learned embeddings can then be used for improved recommendation accuracy.

Social regularized recommendation enhances the personalization of recommendations by considering the influence of social connections. It acknowledges that users with similar social ties may share common preferences, leading to more accurate and personalized recommendations.
By incorporating social connections, the recommendation system can introduce diversity in recommendations. It can avoid the "filter bubble" problem where users are only exposed to a narrow set of items, thus promoting serendipitous discovery.
In summary, social regularized recommendation techniques offer promising avenues to enhance recommendation systems by incorporating social network information. While they bring advantages in terms of personalization and diversity, addressing challenges related to data sparsity, scalability, and privacy is crucial for their widespread and ethical adoption.

\subsection{Item-item Side Information}

Exploring Knowledge Graphs (KGs) as a form of supplementary information has garnered interest in various applications, particularly in recommender systems. Existing recommender models that incorporate KGs fall into three primary categories: path-based~\cite{Hete-cf, HINRec, MCRec, RuleRec}, embedding-based~\cite{ckbe, DKN, IKSR}, and hybrid methods~\cite{wang2018ripplenet, kgat, ckan, chen2022modeling, wang2021learning}.

Path-based methods delve into various connecting patterns among items in KGs, such as meta-paths or meta-graphs, to offer additional guidance for recommendations. The generation of these patterns relies either on path generation algorithms~\cite{HERec} or manual creation~\cite{MCRec}. While path-based methods naturally introduce interpretability and explanation into recommendations, designing such patterns can be challenging, particularly for large-scale and complex KGs. The exhaustive retrieval and generation of paths become impractical, and the selection of paths significantly impacts the final recommendation performance.

Embedding-based methods employ knowledge graph embedding (KGE) algorithms~\cite{KGE_survey} to directly utilize semantic information in KGs and enhance the representations of users and items. For instance, DKN~\cite{DKN} utilizes TransD~\cite{TransD} to jointly process KGs and learn item embeddings. However, the limitations of embedding-based methods lie in emphasizing rigorous semantic relations and neglecting user-item interactions in recommender systems. Moreover, most embedding-based methods lack support for end-to-end training.

Hybrid methods integrate path-based and embedding-based techniques, aiming to achieve state-of-the-art performance. These methods typically employ iterative information propagation under a graph neural network framework to generate entity representations for information enrichment. For example, CKAN~\cite{ckan} utilizes a heterogeneous propagation strategy along multi-hop links to encode knowledge associations for users and items.

Two models, MetaHIN and MetaKG~\cite{metahin, metakg}, leverage the power of the meta-learning paradigm, treating preference learning for each user as a single meta-learning task. However, a significant challenge arises in the computational cost associated with optimizing individual meta-learners for each user from the interaction records. Another recent model, KGPL~\cite{KGPL}, adopts graph semi-supervised learning to employ pseudo-labeling via random walk, simulating a graph to increase interaction density. Nonetheless, starting with structure exploration may be influenced by the initial state of interaction sparsity, potentially perturbing and destabilizing the model training for recommendation. These models are included in experiments for performance comparison.

