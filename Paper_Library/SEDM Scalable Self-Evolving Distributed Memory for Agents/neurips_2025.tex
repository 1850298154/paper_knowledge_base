\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
    % \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025

% The authors should use one of these tracks.
% Before accepting by the NeurIPS conference, select one of the options below.
% 0. "default" for submission
\usepackage{amsmath}
 \usepackage[preprint]{neurips_2025}
% the "default" option is equal to the "main" option, which is used for the Main Track with double-blind reviewing.
% 1. "main" option is used for the Main Track
%  \usepackage[main]{neurips_2025}
% 2. "position" option is used for the Position Paper Track
%  \usepackage[position]{neurips_2025}
% 3. "dandb" option is used for the Datasets & Benchmarks Track
 % \usepackage[dandb]{neurips_2025}
% 4. "creativeai" option is used for the Creative AI Track
%  \usepackage[creativeai]{neurips_2025}
% 5. "sglblindworkshop" option is used for the Workshop with single-blind reviewing
 % \usepackage[sglblindworkshop]{neurips_2025}
% 6. "dblblindworkshop" option is used for the Workshop with double-blind reviewing
 % \usepackage[dblblindworkshop]{neurips_2025}

% After being accepted, the authors should add "final" behind the track to compile a camera-ready version.
% 1. Main Track
 % \usepackage[main, final]{neurips_2025}
% 2. Position Paper Track
%  \usepackage[position, final]{neurips_2025}
% 3. Datasets & Benchmarks Track
 % \usepackage[dandb, final]{neurips_2025}
% 4. Creative AI Track
%  \usepackage[creativeai, final]{neurips_2025}
% 5. Workshop with single-blind reviewing
%  \usepackage[sglblindworkshop, final]{neurips_2025}
% 6. Workshop with double-blind reviewing
 % \usepackage[dblblindworkshop, final]{neurips_2025}
% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote.
% For workshops (5., 6.), the authors should add the name of the workshop, "\workshoptitle" command is used to set the workshop title.
% \workshoptitle{WORKSHOP TITLE}

% "preprint" option is used for arXiv or other preprint submissions
 % \usepackage[preprint]{neurips_2025}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{authblk}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{enumitem}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{tabularx}
\usepackage{makecell} % 支持多行单元格
\usepackage{array}    % 支持 m{} 列类型垂直居中
\usepackage{graphicx}
\usepackage{multirow}
% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote. 
\title{SEDM: Scalable Self-Evolving Distributed Memory for Agents}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\newcommand{\equalcontrib}{\textsuperscript{*}}

\begin{document}
% 手动插入共一脚注
\renewcommand{\thefootnote}{\fnsymbol{footnote}}   % 用符号做脚注编号
\setcounter{footnote}{1}                            % 把 * 对应到脚注 1
\footnotetext{Equal contribution.}

% 恢复阿拉伯数字脚注（对应作者）
\renewcommand{\thefootnote}{\arabic{footnote}}
\setcounter{footnote}{1}

% ============= 作者列表 =============
\author[1,2]{Haoran Xu\equalcontrib}
\author[1,3]{Jiacong Hu\equalcontrib}
\author[1,4]{Ke Zhang}
\author[1,5]{Lei Yu}
\author[1,6]{Yuxin Tang}
\author[1,7]{Xinyuan Song}
\author[1,8]{Yiqun Duan}
\author[1]{Lynn Ai}
\author[1]{Bill Shi\thanks{Corresponding Author: \texttt{tianyu@gradient.network}}\textsuperscript{1}}

% ============= 机构 =============
\affil[1]{Gradient} 
\affil[2]{Zhejiang University}

\affil[3]{South China University of Technology}
\affil[4]{Waseda University}
\affil[5]{University of Toronto}
\affil[6]{Rice University}
\affil[7]{Emory University}
\affil[8]{University of Technology Sydney}

\maketitle


\begin{abstract}
  In long-term multi-agent systems, the accumulation of trajectories and historical interactions makes efficient memory management a particularly challenging task, with significant implications for both performance and scalability. Existing memory management methods typically depend on vector retrieval and hierarchical storage, yet they are prone to noise accumulation, uncontrolled memory expansion, and limited generalization across domains. To address these challenges, we present \textbf{SEDM} (Self-Evolving Distributed Memory), a verifiable and adaptive framework that transforms memory from a passive repository into an active, self-optimizing component. SEDM integrates verifiable write admission based on reproducible replay, a self-scheduling memory controller that dynamically ranks and consolidates entries according to empirical utility, and cross-domain knowledge diffusion that abstracts reusable insights to support transfer across heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM improves reasoning accuracy while reducing token overhead compared with strong memory baselines, and further enables knowledge distilled from fact verification to enhance multi-hop reasoning. The results highlight SEDM as a scalable and sustainable memory management design for open-ended multi-agent collaboration. The code will be released upon acceptance of this paper.
\end{abstract}

\vspace{-0.5em}
\section{Introduction}
\vspace{-0.5em}
In recent years, the rapid development of large-scale multi-agent systems (MAS)~\cite{Wooldridge2009MAS, Lowe2017MADDPG, Foerster2016Comm, cite2, Busoniu2008Survey} has expanded their application in diverse domains, including collaborative reasoning, decision-making, and autonomous planning~\cite{cite1}. A fundamental challenge in open-ended, long-term tasks lies in enabling agents to effectively manage, interpret, and reutilize information accumulated through continuous interactions with both peers and their environment~\cite{cite2}. In the absence of effective memory management design, the sheer scale of historical interactions can easily overwhelm computational resources and compromise decision quality~\cite{cite3}.

In open-ended and long-term multi-agent tasks, each agent relies on its past memories, the observed states of other agents, and the current environment to make decisions for subsequent actions or responses~\cite{cite4}. During continuous interaction between agents and their environment, the MAS gradually accumulates extensive logs of interactions, invocation trajectories, and high-level policy memories~\cite{openai2024gpt4}. Such overwhelming amounts of information directly impact the efficiency and cost of decision-making, often leading to higher monetary costs and longer contextual requirements for inference~\cite{cite6}. Therefore, designing an efficient and sustainable memory mechanism has become a critical issue for modern long-term multi-agent systems.

Current methods primarily adopt vector retrieval and hierarchical memory structures to manage storage and retrieval efficiently~\cite{cite8}. Vector retrieval~\cite{Johnson2019FAISS, cite34, cite33, cite14, Guo2016DRMM} leverages semantic similarity to identify relevant entries, while hierarchical organization arranges information in layered structures according to abstraction levels~\cite{cite9}. These approaches have shown promise in improving retrieval accuracy and managing memory scalability~\cite{cite10}. However, in complex collaborative multi-agent tasks, their effectiveness diminishes, as the underlying assumptions of stability and linear growth do not hold~\cite{cite11}. This gap between theoretical promise and practical performance highlights several critical limitations that hinder their long-term applicability.

One major challenge is the inevitable accumulation of noise, which severely degrades retrieval quality~\cite{cite12}. As the memory size expands without constraint, the system faces exponentially increasing computational costs in both retrieval and context construction~\cite{cite33}. This not only reduces overall efficiency but also amplifies the interference caused by redundant information~\cite{cite14}. In particular, the presence of low-value or semantically irrelevant entries dilutes the contribution of high-quality information in retrieval results, impairing downstream task performance and leading to measurable declines in metrics~\cite{cite15}. In addition, the cumulative noise effect increases response latency and accelerates the nonlinear consumption of computational and storage resources~\cite{cite34}, ultimately threatening both scalability and stability in long-term MAS operations~\cite{cite17}.

To overcome these limitations, we introduce Scalable Self-Evolving Distributed Memory (\textbf{SEDM}), a framework that transforms memory from a passive repository into an adaptive, self-optimizing, and verifiable component for multi-agent systems. Unlike conventional designs that treat memory as a static store, SEDM continually refines knowledge to enhance learning and decision-making efficiency in dynamic task environments. It operationalizes memory as an active mechanism by integrating verifiability and continuous self-improvement into the memory lifecycle. At its core, memory items undergo a rigorous admission process based on self-contained execution contexts (SCECs), such as Docker and ReproZip~\cite{Merkel2014Docker, chirigati2016reprozip}, which package all necessary information for environment-free replay and offline validation. This mechanism provides empirical evidence for utility at write time, ensuring that only useful, high-quality experiences enter the memory repository. Once admitted, memory items are dynamically managed by a self-scheduling controller and enhanced through cross-domain knowledge diffusion. The controller leverages admission-derived weights, combined with semantic similarity, to schedule retrieval-time usage without costly reranking, while consolidation and progressive evolution continuously refine the repository by promoting stable items, merging redundancies, and pruning harmful ones. Beyond single-task settings, SEDM abstracts reusable insights into general forms, enabling knowledge distilled in one domain to be safely transferred and re-validated in others. Together, these components establish a scalable and auditable memory mechanism that enhances reasoning accuracy, reduces overhead, and supports sustainable long-term multi-agent collaboration.

We evaluate SEDM on two representative benchmarks, FEVER~\cite{thorne-etal-2018-fever} for fact verification and HotpotQA~\cite{yang2018hotpotqadatasetdiverseexplainable} for multi-hop reasoning, comparing against no-memory and G-Memory baselines~\cite{zhang2025gmemorytracinghierarchicalmemory}. The results show that SEDM consistently improves task accuracy while significantly reducing token overhead, thereby achieving a better balance between performance and efficiency. Ablation studies confirm that both the verifiable admission mechanism and the self-scheduling controller contribute progressively to this gain, with the latter playing a key role in constraining prompt growth without sacrificing accuracy. Furthermore, cross-domain evaluation demonstrates that memory distilled from one dataset can transfer to another, with factual knowledge from FEVER notably boosting performance on HotpotQA. These findings highlight SEDM as a scalable, adaptive, and generalizable memory framework for long-term multi-agent reasoning.

Our contributions are summarized as follows: 
\begin{itemize}[left = 0em] 
    \item We propose \textbf{Self-Evolving Distributed Memory (SEDM)}, a novel framework that transforms memory from a passive repository into an adaptive, verifiable, and continuously improving component, introducing self-contained execution contexts (SCECs) for reproducible admission and utility-based memory weighting.  

    \item We design a \textbf{self-scheduling memory controller} that selectively manages memory at retrieval time and continuously refines the repository through consolidation, redundancy suppression, and progressive evolution, thereby balancing accuracy and efficiency.  

    \item We conduct extensive evaluations on LoCoMo, FEVER, and HotpotQA benchmarks, demonstrating that SEDM consistently improves task accuracy while significantly reducing token overhead. 
\end{itemize}

\vspace{-0.5em}
\section{Related Work}
\vspace{-0.5em}
\paragraph{Self-Evolving Agents.}  
Recent efforts in building self-evolving agents have focused on enabling systems to improve their reasoning or behavior over time without explicit retraining. Approaches such as Reflexion~\cite{shinn2023reflexion} and Voyager~\cite{wang2023voyager} allow agents to iteratively refine their strategies by leveraging self-reflection and accumulated trajectories. Similarly, MEMIT~\cite{meng2022memit} demonstrates the feasibility of localized knowledge editing within large language models, suggesting a pathway for agents to evolve by continuously updating their internal representations. These studies highlight the importance of mechanisms that support autonomous adaptation and progressive self-improvement in dynamic environments.

\paragraph{Agent Memory.}  
In parallel, research on agent memory has investigated how to store, retrieve, and utilize knowledge efficiently across long-horizon interactions. Episodic memory systems, such as those proposed by Park et al.~\cite{park2023generativeagents}, emulate human-like memory consolidation to support consistent long-term behavior in simulated social environments. Memory-augmented neural networks~\cite{graves2016hybrid} and differentiable neural dictionaries~\cite{kaiser2017learning} further demonstrate how structured memory access can enhance reasoning and generalization. More recently, retrieval-augmented generation frameworks tailored for interactive agents~\cite{mialon2023augmented} have shown that dynamically grounding responses in curated external memories improves both interpretability and task success. Together, these works underscore the need for memory systems that are not only scalable but also adaptive to the agent’s evolving operational context.
\vspace{-0.5em}
\section{Methodology}
\vspace{-0.5em}

\subsection{System Overview}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{sedm-intro.pdf}
    \caption{Illustration of different memory strategies. 
\textbf{No Memory}: the agent interacts with the environment without retaining past information. 
\textbf{Fixed Memory}: the agent retrieves from a static memory pool, which may grow excessively. 
\textbf{SEDM}: introduces verifiable write admission, parallel simulation, and adaptive scheduling to build high-quality, self-evolving memory that supports efficient and transferable knowledge use.}
    \label{fig:sedm}
\end{figure}

Figure~\ref{fig:sedm} illustrates the differences between no memory, fixed memory, and our proposed SEDM framework, highlighting how SEDM achieves verifiable admission, adaptive scheduling, and sustainable knowledge evolution.

Figure~\ref{fig:sedm-arch} gives an end-to-end view of \textbf{SEDM}. The system introduces verifiability and self-improvement into the memory life cycle and consists of three tightly integrated modules. 
(i) \emph{SCEC-based Verifiable Write Admission} packages each run into a Self-Contained Execution Context (SCEC) and performs environment-free A/B replay to estimate the marginal utility of a candidate memory item; only items with positive evidence are admitted and assigned an initial weight. 
(ii) \emph{Self-Scheduling in the Memory Controller} uses admission-derived weights together with semantic similarity to score candidates at retrieval time, while also maintaining the repository by updating weights from observed outcomes, merging near duplicates, and pruning harmful entries. 
(iii) \emph{Cross-Domain Knowledge Diffusion} abstracts admitted items into conservative general forms and re-validates them in other tasks, allowing knowledge to transfer safely across domains.


\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{SEDM.pdf}
    \caption{\textbf{SEDM architecture}. 
    \emph{Left}: task execution generates traces that are packaged into a Self-Contained Execution Context (SCEC) with inputs, outputs, tool summaries, seeds, and hashes. 
    \emph{Bottom}: from each SCEC, a candidate memory is extracted and evaluated via paired A/B replay (\emph{Original} vs.\ \emph{Injected}); distributed verification computes $\Delta$Reward, $\Delta$Latency, and $\Delta$Tokens, and an admission gate accepts the item and assigns its initial weight if the score is positive, else discards it. 
    \emph{Right}: the memory controller performs (a) \emph{memory scheduling} using $s(q,m)=\operatorname{sim}(q,m)\times w(m)$ for retrieval and injection, (b) \emph{consolidation and evolution} by updating weights from outcomes and merging near-duplicate items ($m_{\mathrm{merged}}=\mathrm{Merge}(m_i,m_j)$), and (c) \emph{knowledge diffusion} by abstracting reusable insights ($m_{\mathrm{general}}=\mathrm{Abstract}(m_{\mathrm{specific}})$). 
    Linked trajectory, query, and insight graphs track the vertical evolution of memory and preserve provenance. 
    The dashed loop indicates retrieval and injection during inference, closing the self-improving cycle.}
    \label{fig:sedm-arch}
\end{figure}


\subsection{SCEC-based Verifiable Write Admission}

We formulate write admission as a verifiable, environment-free procedure that assigns an initial utility weight to each candidate memory item before it enters the repository. The process is based on a \emph{Self-Contained Execution Context (SCEC)}, a minimal and standardized package that enables validation, parallel replay, and offline auditing. By placing admission behind paired A/B evaluations within SCECs, the system produces reproducible evidence for weight initialization while filtering out negative or noisy experiences.  

\subsubsection{Self-Contained Packaging and Distributed Replay}

Each task execution is encapsulated into an SCEC to support reproducible validation and analysis without requiring the original environment. An SCEC includes all necessary inputs, outputs, tool summaries, seeds, and configuration hashes, ensuring (i) self-contained representation, (ii) environment-free replay by summarizing external tool calls, (iii) deterministic reproduction across model versions and seeds, and (iv) minimal sufficiency by storing only essential information.  

Treating an SCEC as an independent job enables large-scale distributed A/B replay on arbitrary workers. Only aggregated statistics, along with integrity hashes and version stamps, are uploaded, preserving auditability while controlling cost. This environment-free design eliminates the need to reconstruct complex environments or interact with real agents during validation, thereby allowing memory effectiveness to be tested through parallel replay at scale. As a result, admission decisions can be made rapidly and consistently, significantly reducing computational overhead while ensuring that only high-quality experiences enter the memory repository.


\subsubsection{SCEC-grounded A/B Test for Memory Item Initialization}

From each SCEC, we extract one candidate memory item $m$, represented as a concise, independently injectable snippet. The extraction process identifies decisive reasoning or corrective steps, performs deduplication and canonicalization, and attaches provenance information.  

To evaluate its utility, we conduct a paired A/B test within the same SCEC. The control condition (A) uses the original prompt, while the treatment condition (B) augments the prompt with the candidate memory $m$. This setup isolates the marginal effect of $m$ and provides empirical evidence for its contribution. For a query $q$, the constructed prompts are defined as
\begin{equation}
I_A = f(q), 
\qquad 
I_B = f(q; m),
\end{equation}
where $f(\cdot)$ denotes prompt construction and $I_B$ injects $m$ into the SCEC’s dedicated slot together with summarized tool feedback. The model execution inside the SCEC is denoted by $\mathcal{F}$:
\begin{equation}
o_A = \mathcal{F}(I_A), 
\qquad 
o_B = \mathcal{F}(I_B).
\end{equation}

We then measure the deltas in reward, latency, and token usage:
\begin{equation}
\Delta R = R(o_B) - R(o_A), \quad
\Delta L = L(o_B) - L(o_A), \quad
\Delta T = T(o_B) - T(o_A),
\end{equation}
where $R(\cdot)$ is the task-specific reward, and $L(\cdot)$ and $T(\cdot)$ denote latency and token overhead, respectively. A composite admission score balances utility and cost:
\begin{equation}
S = \Delta R  -  \lambda_L \Delta L  -  \lambda_T \Delta T,
\end{equation}
with $\lambda_L,\lambda_T \geq 0$ controlling the trade-offs.  

The admission decision and initial weight are then defined as
\begin{equation}
\mathrm{accept}(m)  \Longleftrightarrow  S \ge \eta, 
\qquad 
w_0(m) = \max\{0,  S\},
\end{equation}
where $\eta$ is the acceptance threshold. Multiple runs may be averaged to mitigate variance.  

Accepted items are stored together with their initial weights and full provenance (hashes, seeds, versions, and A/B fingerprints), while rejected or ambiguous items are excluded. This procedure yields a compact, auditable admission signal that can be validated offline and efficiently executed in parallel without dependence on the original environment.

\subsection{Self-Scheduling in the Memory Controller}

The memory controller manages and optimizes the repository through a self-scheduling policy. Unlike traditional systems that depend on costly per-query reranking~\cite{nogueira2019passage, ren2021rocketqav2, ni2022large}, our approach establishes an evidence-based mechanism for both selecting memory items during retrieval and continuously refining the repository. It comprises two core functions: \emph{retrieval-time scheduling}, which determines how to use memories effectively for an incoming query, and \emph{consolidation and progressive evolution}, which curates a compact, high-quality memory set. Together, these components ensure that memory usage is grounded in verifiable utility signals, improving both efficiency and performance.  

\subsubsection{Retrieval-time Scheduling}

The controller’s scheduling policy relies on a ranking signal aligned with realized utility, avoiding the instability and computational cost of on-the-fly large language model reranking~\cite{wu2023llmrerank}. Prior approaches typically use vector similarity or ad-hoc prompt-based scoring, but semantic similarity alone does not guarantee actual task benefit, and repeated reranking adds latency and variance~\cite{cite34, cite33} or ad-hoc prompt-based scoring~\cite{nogueira2019passage, wu2023llmrerank}.  

In our design, we incorporate evidence collected at write time via A/B validation on Self-Contained Execution Contexts (SCECs). These statistics, particularly the measured changes in reward and latency, are mapped into a stable admission-derived weight $w(m)$ for each memory item. At retrieval time, this weight is combined with semantic similarity to form a utility-aligned score:
\begin{equation}
s(q, m) = \operatorname{sim}(q, m) \times w(m),
\end{equation}
where $\operatorname{sim}(q, m)$ denotes the semantic similarity between query $q$ and memory $m$, and $w(m)$ reflects its empirically validated utility.  

This coupling of semantic relevance with admission-grounded evidence stabilizes selection and reduces overhead, ensuring that memory items are injected into prompts not only because they are similar, but because they have demonstrated measurable benefit. As a result, retrieval decisions are both efficient and aligned with the system’s long-term objectives.

\subsubsection{Consolidation and Progressive Evolution}

The consolidation and progressive evolution module maintains a compact yet effective memory repository by suppressing redundancy, preserving items with stable gains, and eliminating or recycling items that show conflicts or sustained negative contributions. While retrieval-time scheduling focuses on selecting memories for specific queries, consolidation and evolution aim to improve the repository itself so that subsequent scheduling operates on a cleaner and more reliable basis.  

Progressive evolution is achieved by tracking usage and outcome signals and applying conservative updates to utility weights over time. Items that are rarely retrieved or consistently fail to provide positive utility are gradually decayed, reducing their influence in future selections. Conversely, items that repeatedly yield positive gains across related contexts are promoted. Promotion increases their weights and, when consistency is observed across multiple items, may trigger abstraction into higher-level insights. Such abstractions enable representative entries to replace families of consistent low-level experiences~\cite{goyal2019abstraction,parisotto2019stabilizing}. If observed outcomes diverge significantly from admission-time evidence, items are demoted or queued for cleanup. All updates are logged with provenance to ensure the evolution process remains auditable~\cite{buneman2001provenance, groth2012provenance, chirigati2016reprozip}.  

The weight update for a memory item $m$ depends on its current weight $w(m)$, usage frequency $f_{use}(m)$, and average realized utility $\bar{U}(m)$ since the last update:
\begin{equation}
w_{t+1}(m) = w_t(m) + \alpha \cdot \bar{U}_t(m) - \beta \cdot f_{\mathrm{use},t}(m),
\end{equation}
where $\alpha$ and $\beta$ control the influence of observed utility and usage. This ensures weights evolve from admission-derived values toward refined, usage-aware estimates.  

Conflict detection is integral to this loop. A memory is marked as conflicting when repeated injections consistently reduce task reward or when its implications contradict other rules. Such items undergo progressive weight reduction, and if their weight falls below a threshold, they are demoted or removed. All decisions retain version traces and evidence chains to support rollback and inspection.  

Semantic consolidation addresses redundancy among items from different SCECs. When two or more items, $m_i$ and $m_j$, show high semantic similarity without conflicting applicability, they are merged:
\begin{equation}
m_{\text{merged}} = \text{Merge}(m_i, m_j).
\end{equation}
The merged entry preserves essential content while aggregating evidence from the originals, and its weight $w(m_{\text{merged}})$ is reconciled to reflect combined support without double counting. Contributing items are archived or soft-deleted, preserving provenance if needed. By collapsing near-duplicates into single representatives, the repository reduces retrieval noise and strengthens utility signals~\cite{broder1997resemblance, manku2007detecting, charikar2002simhash, zhang2019duplicate}.  

Through these routines, the controller maintains a concise but reliable set of memories. Redundant entries are consolidated~\cite{manku2007detecting}, stable positives are promoted~\cite{kaelbling1996rlsurvey}, abstractions generalize recurring insights~\cite{goyal2019abstraction}, and harmful items are isolated or removed~\cite{toneva2019catastrophic}. As a result, the repository remains aligned with realized utility, ensuring that retrieval decisions exploit empirical evidence rather than ad-hoc reranking.

\subsection{Cross-Domain Knowledge Diffusion}

This component exploits the environment-free and verifiable properties of the SCEC method, treating memory entries as portable and re-verifiable assets across domains. Tasks from diverse domains continuously supply evidence to refine memory weights, thereby improving both universality and robustness of the repository. The process follows a loop of \emph{migrate, re-validate, and re-incorporate}: transfer is initiated through retrieval and weighted injection; subsequent usage allows re-estimation of weights via SCEC-compatible procedures; and the updates inform later scheduling and admission decisions. Throughout, retrieval signals remain unchanged: similarity-based relevance, $\operatorname{sim}(q, m)$ for query $q$ and memory item $m$, combined with the admission-derived weight $w(m)$. This design avoids evidence-free cold starts and eliminates the need for additional scoring components at runtime.  

Immediately after admission, each specific entry $m_{\text{specific}}$ generates a conservative general form $m_{\text{general}}$ through a lightweight abstraction operator:
 \begin{equation}
m_{\text{general}} = \operatorname{Abstract}(m_{\text{specific}}).
 \end{equation}
This yields a dual-linked pair in which the general form strips domain-specific features while preserving transferable essence. The general form serves as a low-risk candidate for cross-domain retrieval, while the specific form remains primary within its source domain.  

The abstraction process is rule-governed and minimal. Entities and domain-specific terms are replaced with typed placeholders, retaining actionable task–action structures while removing non-essential detail~\cite{goyal2019abstraction, lake2015human, dong2019neural}. The result is a compact snippet suitable for direct injection and controlled comparisons. To minimize orchestration cost, abstraction is generated alongside the SCEC-based A/B assessment within the distributed pipeline~\cite{chirigati2016reprozip}.  


For weight inheritance, the general form is initialized conservatively as a scaled version of the specific weight, $w_{\text{general}} = \alpha \cdot w_{\text{specific}}$ with $\alpha < 1$. This prior encodes caution against over-abstraction while preserving provenance for later auditing and weight updates.  

At retrieval, both forms compete in the candidate set with a unified score:
\begin{equation}
s(q, m) = \operatorname{sim}(q, m) \times w(m).
\end{equation}
In-domain queries typically favor specific forms due to higher semantic match and weight, while cross-domain queries benefit from the more stable similarity of general forms. This mechanism enables knowledge diffusion across domains without introducing extra runtime complexity, while maintaining auditability, portability, and stability. Subsequent use further refines the weights, allowing knowledge to propagate adaptively across diverse tasks.


% \subsection{Self-evolving Memory Selection and Storage(SMSS)}
% Current research regards memory as a passive storage component and focuses on how to extract efficient memory from past queries. In contrast, self-evolving memory selection and storage~(SMSS) evolves memory into an active component through a structured capture, candidate extraction, and weight management in an offline process.\par
% \section{Scalable Self-Evolving Distributed Memory for Agents}

% In a Self-Evolving Distillation Memory (SEDM), memory serves not only as a repository for historical interaction information but also as an adaptive mechanism that continuously optimizes knowledge to improve learning and decision-making efficiency in dynamic task environments. Specifically, SEDM is composed of three key modules: self-evolving memory storage, which provides verifiable and trustworthy knowledge; automatic memory scheduling, which assigns dynamically updated weights to each knowledge entry; and cross-domain memory utilization, which enables adaptive transfer and integration of knowledge across different domains. 



% \subsection{Self-evolving Memory Selection and Storage (SMSS)}

% Current research regards memory as a passive storage component and focuses on how to extract efficient memory from past queries. In contrast, self-evolving memory selection and storage~(SMSS) evolves memory into an active component through a structured capture, candidate extraction, and weight management in an offline process.\par

% \textcolor{blue}{SMSS starts an online inference for each query $q$ to collect all underlying memory $\mathcal{M}$, such as the task prompt, environment feedback, injection information and execution trace summary into a memory graph, and then add it into a context package. Formally, the candidate memory set $M_q$ is defined as:}

% \begin{equation}
% M_q = \{ m \in \mathcal{M}  |  \operatorname{sim}(\phi(q), \phi(m)) \geq \tau \},
% \end{equation}

% \textcolor{blue}{where $\phi(\cdot)$ is the embedding function, $\operatorname{sim}(\cdot,\cdot)$ is a similarity metric (e.g., cosine similarity), and $\tau$ is a similarity threshold.} \par

% \textcolor{blue}{The contextpack block leverages an A/B validator to identify potential high-value memory items. Two versions of input are generated:}

% \begin{equation}
% I_A = f(q), \qquad I_B = \big[ f(q+m)  ;  \max_{m \in M_q^*} \omega_m \phi(m) \big],
% \end{equation}

% \textcolor{blue}{where $I_A$ is the input without memory injection, $I_B$ is the input with injection, $M_q^* \subseteq M_q$ is the selected subset, and $\omega_m$ is the weight of memory item $m$. The model then produces outputs:}

% \begin{equation}
% o_A = \mathcal{M}(I_A), \qquad o_B = \mathcal{M}(I_B).
% \end{equation}

% \textcolor{blue}{The validator compares the results in terms of reward gain and token overhead:}

% \begin{equation}
% \Delta R = R(o_B) - R(o_A), \qquad \Delta T = T(o_B) - T(o_A),
% \end{equation}

% \textcolor{blue}{where $R(\cdot)$ is a task-specific reward function and $T(\cdot)$ measures token usage or computational cost. To balance the trade-off, we define the scoring function:}

% \begin{equation}
% M_q^{*}=
% \begin{cases}
% M_q, & \text{if } \Delta R - \lambda \cdot \Delta T > 0,\\begin{equation}2pt]
% \emptyset, & \text{otherwise},
% \end{cases}
% \label{eq:smss_selection}
% \end{equation}
% \begin{equation}
% M \leftarrow M \cup M_q^{*}.
% \label{eq:smss_update}
% \end{equation}
% \textcolor{blue}{where $\lambda$ controls the trade-off between performance gain and efficiency. If $S(M_q^*)$ exceeds a threshold $\eta$, the corresponding memory entries are stored or updated in the memory bank.} \par

% %考虑这一部分开始引入公式,需要仔细阐述a/b validator的细节，和各种符号。穿插在方法中间, 包括图文对应

% According to the validation results, SMSS chooses appropriate context within higher score and update into memory storage. In subsequent query, the SEDM system can use this selected memory information for prioritization and injection control, prioritizing high-value knowledge while weakening or isolating redundant memory.
% \subsection{Automatic Memory Scheduling~(AMS)}
% Although the A/B validate in SMSS enable to complete candidate extraction and choose better memory, in a long-term agentic systems, the scaling memory still confuses retrieval efficiency and knowledge quality. Therefore, constructing a automatic scheduling to ensure select more efficient and useful item is critical to SEDM. To address this issue, this paper further designed a Autonomous Memory Scheduling (AMS) to automatically optimize and dynamically schedule the importance of selected memory.\par
% %这里需要开始写 自动权重配置的方法。

% \textcolor{blue}{In this paragraph, need to add detail about weight method}\par

% %这里需要开始写 自动权重配置的方法。
% In AMS, results within different weight of memory scheduling are written back to memory system, AMS automatically adjust the weight to enhance efficiency of memory scheduling. Through AMS mechanism, SEDM not only ensures a self-evolving memory selection and storage among the scaling historical memory, but also balance various relevant memory to enhance cross-domain adaptability. \par
% After self-evolving memory selection and storage (SMSS) and automatical periodic scheduling (AMS), how to effectively utilize these high-value memories across different tasks and agents becomes the key to self-evolving memory. 


% AMS maintains a compact, query-aware working set $\mathcal{M}_t\subseteq\mathcal{M}$ through a unified weighting–gating–consolidation process. At round $t$, each memory item $m\in\mathcal{M}$ is assigned an \emph{instantaneous utility} 
% \begin{equation}
% u_t(m) = \Delta R_t(m) - \lambda \Delta T_t(m),
% \end{equation}
% smoothed over time by an exponential moving average
% \begin{equation}
% \bar{u}_t(m) = \beta \bar{u}_{t-1}(m) + (1-\beta) u_t(m), \quad \bar{u}_0(m)=0.
% \end{equation}
% Its \emph{scheduled weight} is then determined by combining utility and query relevance,
% \begin{equation}
% \omega_t(m) = \sigma \bigl(\alpha \bar{u}_t(m) + \gamma I_t(m) - \kappa\bigr), \quad 
% I_t(m)=\mathbb{E}_{q\sim\mathcal{D}_t}[\operatorname{sim}(\phi(q),\phi(m))],
% \end{equation}
% where $\sigma(x)=(1+e^{-x})^{-1}$. A differentiable gating mechanism produces the working set
% \begin{equation}
% s_m = \operatorname{Sigmoid} \Bigl(\tfrac{\omega_t(m)-\mu}{\tau}\Bigr), \qquad
% \mathcal{M}_t = \{  m\in\mathcal{M}\mid s_m\ge \theta  \}.
% \end{equation}

% For items excluded from $\mathcal{M}_t$, AMS performs residual compression in embedding space,
% \begin{equation}
% \Delta\phi(m)=\phi(m)-\Pi_{\mathrm{span}(\mathcal{M}_t)}\phi(m),
% \end{equation}
% discarding those with $\|\Delta\phi(m)\|<\varepsilon$, and archiving the rest with decayed weight 
% \begin{equation}
% \omega_t(m)\leftarrow \delta \omega_t(m),\qquad 0<\delta<1.
% \end{equation}
% At inference time, if retrieval from $\mathcal{M}_t$ is insufficient, AMS falls back to the full bank $\mathcal{M}$ with a time-decayed resurrection bonus
% \begin{equation}
% \operatorname{bonus}(m)=\omega_t(m) e^{-\eta\Delta t_m},
% \end{equation}
% thus ensuring efficiency while preserving long-term knowledge availability.

% \subsection{Cross-domain Memory Utilization}

% %我们考虑智能体 在不同下游任务的迁移 +不同智能体之间的迁移。现在可以保证下游任务迁移，不同智能体迁移尚未实现？
% Cross-domain Memory Utilization(CMU) uses memory retrieval in SMSS and self-weighted injection in AMS to enable knowledge transfer in cross-task domains. When an agent encounters a new task from different domain, the CMU adaptively retrieves the most relevant entries from the current memory pool stored by previous AMS. Compared to only relying on in-task input, the CMU enables the agent to call historical memory from various prior tasks, significantly improving performance on zero-shot and few-shot tasks. 
% \par

% When encountering a new task from domain $\mathcal{D}^{\text{new}}$, CMU adaptively reuses past memory items stored by AMS. 
% Given a query $q\sim\mathcal{D}^{\text{new}}$, the retrieval score of each memory item $m\in\mathcal{M}$ is
% \begin{equation}
% r(q,m)  =  \operatorname{sim}\bigl(\phi(q),\phi(m)\bigr),
% \label{eq:cmu_retrieval}
% \end{equation}
% where $\phi(\cdot)$ denotes the embedding function.

% The cross-domain relevance weight combines retrieval similarity and AMS scheduling weight:
% \begin{equation}
% \theta_t(q,m)  =  \operatorname{softmax}_{m\in\mathcal{M}}
% \Bigl( r(q,m) \cdot \omega_t(m) \Bigr).
% \label{eq:cmu_weight}
% \end{equation}

% The cross-domain injected representation for query $q$ is then
% \begin{equation}
% \tilde{h}(q)  =  \sum_{m\in\mathcal{M}} \theta_t(q,m) \phi(m),
% \label{eq:cmu_injection}
% \end{equation}
% and the final task-aware representation is obtained by
% \begin{equation}
% h^{\text{CMU}}(q)  =  \phi(q)  +  \lambda_{\text{cmu}} \tilde{h}(q),
% \label{eq:cmu_final}
% \end{equation}
% where $\lambda_{\text{cmu}}$ controls the strength of cross-domain memory injection.

% In this way, CMU transforms the local memory $\mathcal{M}$ from passive storage into an active cross-domain knowledge base, enabling stronger generalization in zero-shot and few-shot tasks.



% SEDM transforms memory from passive storage into a self-evolving optimization process, providing continuous enhancement capabilities in efficient memory selection, memory importance scheduling and cross-domain application. In next section, this paper separately evaluates AMS, SEDM and CMU in three experiments

\vspace{-0.5em}
\section{Experiment}
\vspace{-0.5em}

\subsection{Experimental Setup}

\subsubsection{Dataset and Model}
Evaluation is conducted on the LoCoMo benchmark~\cite{maharana2024LoCoMo}, which consists of two components: 
(i) multi-turn dialogues (approximately 600 turns per dialogue, averaging 26,000 tokens) and 
(ii) question-answer (QA) pairs grounded in those dialogues. 
Each dialogue contains roughly 200 questions spanning \textit{single-hop}~\cite{rajpurkar2016squad}, 
\textit{multi-hop}~\cite{yang2018hotpotqa}, \textit{open-domain}~\cite{kwiatkowski2019natural}, 
and \textit{temporal reasoning}~\cite{chen2021timeqa}. 
All experiments are carried out with \texttt{gpt-4o-mini} to ensure consistency and comparability across evaluations.  

Performance is measured using two complementary metrics: Token-level F1 Score (F1)~\cite{rajpurkar2016squad}, 
which captures the overlap between predicted and ground-truth answers, 
and BLEU-1 (B1)~\cite{papineni2002bleu}, which evaluates unigram-level lexical similarity. 
\subsubsection{Baselines} 
To evaluate the effectiveness of our framework \textbf{SEDM}, we compare it with several representative baselines for multi-session dialogue reasoning: 
\textbf{LoCoMo}~\cite{maharana2024LoCoMo}, a benchmark for assessing long-range retrieval and reasoning in multi-session conversations; 
\textbf{ReadAgent}~\cite{lee2024human}, a human-in-the-loop agent for long-context reading; 
\textbf{MemoryBank}~\cite{zhong2024memorybank}, a memory-augmented retrieval model; 
\textbf{MemoryGPT}~\cite{packer2024memgptllmsoperatingsystems}, an LLM operating system with hierarchical memory modules; 
\textbf{A-Mem}~\cite{xu2025amemagenticmemoryllm}, a dynamic agentic memory system that creates, links, and updates structured memories; 
\textbf{Zep}~\cite{rasmussen2025zeptemporalknowledgegraph}, a retrieval-based agent with structured memory access for temporally extended queries; 
\textbf{LangMem}~\cite{wang2023augmentinglanguagemodelslongterm}, an open-source framework connecting memory chains across sessions; 
\textbf{Mem0}~\cite{chhikara2025mem0buildingproductionreadyai}, a modular memory system with explicit in-context memory operations; 
and \textbf{G-memory}~\cite{zhang2025gmemorytracinghierarchicalmemory}, which leverages hierarchical tracing for memory retrieval. 



% \subsection{Main results}
% \begin{table*}[!ht]
% \centering
% \renewcommand{\arraystretch}{1.2}

% \caption{Evaluation results on the \textbf{LoCoMo} benchmark dataset comparing SEDM with other memory-enabled systems. Models are evaluated on F1~\cite{rajpurkar2016squad} and BLEU-1 (B1)~\cite{papineni2002bleu} across \textbf{Single Hop}~\cite{rajpurkar2016squad}, \textbf{Multi-Hop}~\cite{yang2018hotpotqa}, \textbf{Open Domain}~\cite{kwiatkowski2019natural}, \textbf{Temporal}, and \textbf{Adversial} questions~\cite{chen2021timeqa}. Higher is better. The top two best results are marked in \textbf{Bold}.}\label{tab:LoCoMo_results}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{l|cc|cc|cc|cc|cc}
% \toprule
% \midrule
% \textbf{Method} & 
% \multicolumn{2}{c|}{\textbf{Single Hop}} & 
% \multicolumn{2}{c|}{\textbf{Multi-Hop}} & 
% \multicolumn{2}{c|}{\textbf{Open Domain}} & 
% \multicolumn{2}{c|}{\textbf{Temporal}} & 
% \multicolumn{2}{c}{\textbf{Adversial}} \\
%  & F1$\uparrow$ & B1$\uparrow$ & F1$\uparrow$ & B1$\uparrow$ & F1$\uparrow$ & B1$\uparrow$ & F1$\uparrow$ & B1$\uparrow$ & F1$\uparrow$ & B1$\uparrow$ \\
% \midrule
% LoCoMo~\cite{maharana2024LoCoMo} & 25.02 & 19.75 & 18.41 & 14.77 & 40.36 & 29.05 & 12.04 & 11.16 & 69.23 & 68.75 \\
% ReadAgent~\cite{lee2024human} & 9.15 & 6.48 & 12.60 & 8.87 & 9.67 & 7.66 & 5.31 & 5.12 & 9.81 & 9.02\\
% MemoryBank~\cite{zhong2024memorybank}& 5.00 & 4.77 & 9.68 &6.99 & 6.61 & 5.16 & 5.56 & 5.94 & 7.36 & 6.48\\ 
% MemoryGPT~\cite{packer2024memgptllmsoperatingsystems}& 26.65 & 17.72 & \textbf{25.52} & \textbf{19.44} & 41.04 & 34.34 & 9.15 & 7.44 & \textbf{43.29} & \textbf{42.73}\\
% A-Mem~\cite{xu2025amemagenticmemoryllm} & 27.02 & 20.09 & \textbf{45.85} & \textbf{36.67} & 44.65 & 37.06 & 12.14 & 12.00 & \textbf{50.03} & \textbf{49.47} \\
% Zep~\cite{rasmussen2025zeptemporalknowledgegraph} & 30.15 & 17.15 & 15.04 & 11.56 & 26.67 & 18.44 & 3.49 & 2.68 & 22.60 & 15.05 \\
% LangMem~\cite{langchain2024} & 22.40 & 15.21 & 18.65 & 16.03 & 31.62 & 23.85 & 27.75 & 21.53 & 28.34 & 21.31 \\
% Mem0~\cite{chhikara2025mem0buildingproductionreadyai} & 27.29 & 18.63 & 18.59 & 13.86 & 34.03 & 24.77 & 26.90 & 21.06 & 30.41 & 22.22 \\
% G-memory~\cite{zhang2025gmemorytracinghierarchicalmemory} & \textbf{34.6} & \textbf{26.6} & 9.05 & 7.2 &\textbf{53.5} & \textbf{44.0} & \textbf{32.4} & \textbf{25.6} & 11.3 & 9.3 \\
% \textbf{SEDM (Ours)} & \textbf{33.5} & \textbf{24.4} & 12.1 & 9.2 & \textbf{51.7} & \textbf{37.0} & \textbf{47.5}& \textbf{33.1} & 12.1 & 9.3 \\
% \midrule
% \bottomrule
% \end{tabular}}
% \end{table*}
\subsection{Main results}
\begin{table*}[!ht]
\centering
\renewcommand{\arraystretch}{1.2}

\caption{Evaluation results on the \textbf{LoCoMo} benchmark dataset comparing SEDM with other memory-enabled systems. Models are evaluated on F1~\cite{rajpurkar2016squad} and BLEU-1 (B1)~\cite{papineni2002bleu} across \textbf{Single Hop}~\cite{rajpurkar2016squad}, \textbf{Multi-Hop}~\cite{yang2018hotpotqa}, \textbf{Open Domain}~\cite{kwiatkowski2019natural}, \textbf{Temporal}, and \textbf{Adversial} questions~\cite{chen2021timeqa}. Higher is better. The top two best results are marked in \textbf{Bold}.}\label{tab:LoCoMo_results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|cc|cc|cc|cc|cc}
\toprule
\midrule
\textbf{Method} & 
\multicolumn{2}{c|}{\textbf{Single Hop}} & 
\multicolumn{2}{c|}{\textbf{Multi-Hop}} & 
\multicolumn{2}{c|}{\textbf{Open Domain}} & 
\multicolumn{2}{c|}{\textbf{Temporal}} & 
\multicolumn{2}{c}{\textbf{Adversial}} \\
 & F1$\uparrow$ & B1$\uparrow$ & F1$\uparrow$ & B1$\uparrow$ & F1$\uparrow$ & B1$\uparrow$ & F1$\uparrow$ & B1$\uparrow$ & F1$\uparrow$ & B1$\uparrow$ \\
\midrule
LoCoMo~\cite{maharana2024LoCoMo} & 25.0 & 19.8 & 18.4 & 14.8 & 40.4 & 29.1 & 12.0 & 11.2 & 69.2 & 68.8 \\
ReadAgent~\cite{lee2024human} & 9.2 & 6.5 & 12.6 & 8.9 & 9.7 & 7.7 & 5.3 & 5.1 & 9.8 & 9.0\\
MemoryBank~\cite{zhong2024memorybank}& 5.0 & 4.8 & 9.7 &7.0 & 6.6 & 5.2 & 5.6 & 5.9 & 7.4 & 6.5\\ 
MemoryGPT~\cite{packer2024memgptllmsoperatingsystems}& 26.7 & 17.7 & \textbf{25.5} & \textbf{19.4} & 41.0 & 34.3 & 9.2 & 7.4 & \textbf{43.2} & \textbf{42.7}\\
A-Mem~\cite{xu2025amemagenticmemoryllm} & 27.0 & 20.1 & \textbf{45.9} & \textbf{36.7} & 44.7 & 37.1 & 12.1 & 12.0 & \textbf{50.0} & \textbf{49.5} \\
Zep~\cite{rasmussen2025zeptemporalknowledgegraph} & 30.2 & 17.2 & 15.0 & 11.6 & 26.7 & 18.4 & 3.5 & 2.7 & 22.6 & 15.1 \\
LangMem~\cite{langchain2024} & 22.4 & 15.2 & 18.7 & 16.0 & 31.6 & 23.9 & 27.8 & 21.5 & 28.3 & 21.3 \\
Mem0~\cite{chhikara2025mem0buildingproductionreadyai} & 27.3 & 18.6 & 18.6 & 13.9 & 34.0 & 24.8 & 26.9 & 21.1 & 30.4 & 22.2 \\
G-memory~\cite{zhang2025gmemorytracinghierarchicalmemory} & \textbf{34.6} & \textbf{26.6} & 9.05 & 7.2 &\textbf{53.5} & \textbf{44.0} & \textbf{32.4} & \textbf{25.6} & 11.3 & 9.3 \\
\textbf{SEDM (Ours)} & \textbf{33.5} & \textbf{24.4} & 12.1 & 9.2 & \textbf{51.7} & \textbf{37.0} & \textbf{47.5}& \textbf{33.1} & 12.1 & 9.3 \\
\midrule
\bottomrule
\end{tabular}}
\end{table*}
Table~\ref{tab:LoCoMo_results} presents the results of \textbf{SEDM} compared with strong baselines, including LoCoMo, ReadAgent, MemoryBank, MemoryGPT, A-Mem, Zep, LangMem, Mem0, and G-memory, on the \textbf{LoCoMo} benchmark across \textit{Single Hop}, \textit{Multi-Hop}, \textit{Open Domain}, \textit{Temporal}, and \textit{Adversarial} reasoning tasks.  

Overall, the results show that \textbf{SEDM} delivers strong and consistent improvements in challenging reasoning scenarios. In particular, SEDM achieves the highest F1 and BLEU-1 scores on the \textbf{Open Domain} and \textbf{Temporal} settings, surpassing all other baselines by a substantial margin. For instance, SEDM improves Temporal reasoning by more than $15$ F1 points compared with the strongest baseline (G-memory), demonstrating its ability to capture and utilize temporally structured information effectively.  

On the \textbf{Single Hop} setting, SEDM remains highly competitive, ranking among the top two models alongside G-memory, while showing significantly better balance across different question types. Although A-Mem and MemoryGPT achieve strong results on \textbf{Multi-Hop} and \textbf{Adversarial} tasks respectively, their performance drops markedly on other reasoning categories, whereas SEDM maintains stable and robust performance across the board.  

These results highlight that the learned selective memory mechanism in SEDM provides clear advantages over heuristic, retrieval-based, and hierarchical memory systems. By selectively integrating context across sessions, SEDM enables LLMs to reason more effectively over long, multi-session dialogues and demonstrates strong generalization across diverse reasoning categories.  



\subsection{Efficient analysis on Fever and HotpotQA}

We further conduct Experiments on two widely used benchmark datasets: \textbf{FEVER}\citep{thorne-etal-2018-fever} and \textbf{HotpotQA}\citep{yang2018hotpotqadatasetdiverseexplainable}.HotpotQA is a large-scale question-answering benchmark designed to evaluate the ability of systems to perform multi-hop reasoning across diverse natural language inputs. 
FEVER is a fact-checking dataset that provides human-written claims about Wikipedia entities, each labeled as \emph{Supported}, \emph{Refuted}, or \emph{NotEnoughInfo}. Both datasets present significant challenges for testing long-term reasoning and memory utilization in language agents.

The proposed \textbf{SEDM} is compared with the following baselines:  
(1) \textbf{No Memory}: the model only relies on the query input without any memory augmentation, serving as the basic performance reference; 
(2) \textbf{G-Memory}\citep{zhang2025gmemorytracinghierarchicalmemory}: a memory-augmented method that stores all past information in a global memory pool and retrieves by similarity search. 
Although effective, it incurs high inference cost due to the large number of prompt tokens;
(3) \textbf{SEDM (ours)}: our scalable self-evolving distributed memory, which introduces memory scheduling and selection mechanisms to maintain a compact and adaptive working set, thereby balancing performance and efficiency.

All experiments run on the same backbone LLM (GPT-4o-mini)~\cite{openai2024gpt4}.  
Dense retrieval is handled by ALL-MINILM-L6-V2~\cite{reimers2019sentencebert}, which embeds both knowledge snippets and queries for similarity search. On the evaluation side, we use FEVER accuracy for fact-checking and HotpotQA exact-match (EM) for multi-hop QA. Efficiency is tracked by counting prompt and completion tokens consumed during inference. To ensure fair comparisons, every method is granted the same memory budget; the proposed SEDM does not expand this budget, but instead adaptively schedules which entries are kept in memory.


The overall performance on FEVER and HotpotQA is summarized in Table~\ref{tab:fever_hotpotqa}. In the FEVER dataset, the baseline model achieved only 57 without memory, reflecting limited reasoning ability in the absence of external knowledge and prior memory. G-Memory improved the score to 62, but this gain came at the cost of a dramatic increase in the number of prompt tokens, leading to significantly higher inference costs. In contrast, SEDM achieved the highest score of 66 while consuming far fewer tokens than G-Memory. This demonstrates that our method successfully balances the trade-off between performance and efficiency through its memory selection and scheduling mechanisms. \par
In the HotpotQA dataset, the trend is similar to that observed in FEVER. The no-memory baseline scored only 34, while G-Memory increased the score to 38. SEDM further improved performance, reaching a score of 39 while simultaneously reducing computational overhead, confirming its effectiveness in multi-hop reasoning tasks.  

Moreover, we evaluate the transfer ability of SEDM between FEVER and HotpotQA, two distinct downstream tasks. Specifically, the agent collects experience on the HotpotQA task using SEDM and then evaluates it on FEVER to measure knowledge transfer and prompting effects. Under this setting, the score on FEVER reached 64. Compared with G-Memory, which scored 62, and the no-memory baseline, which scored 57, our results demonstrate that SEDM enables adaptive memory selection that leverages previously collected experiences to improve performance across tasks.

\begin{table}[ht]
\centering
\caption{Performance comparison on \textbf{FEVER} (fact verification) and \textbf{HotpotQA} (multi-hop reasoning). 
We report task accuracy (Score) along with efficiency metrics (Prompt Tokens and Completion Tokens). SEDM achieves the best accuracy on both benchmarks while substantially reducing token consumption compared with G-Memory, highlighting its ability to balance effectiveness and efficiency.}
\label{tab:fever_hotpotqa}
\begin{tabularx}{\textwidth}{
>{\centering\arraybackslash}m{2cm}|
>{\centering\arraybackslash}X
>{\centering\arraybackslash}X
>{\centering\arraybackslash}X|
>{\centering\arraybackslash}X
>{\centering\arraybackslash}X
>{\centering\arraybackslash}X}
\toprule
\midrule
\makecell{Method} & \multicolumn{3}{c|}{FEVER} & \multicolumn{3}{c}{HotpotQA} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & Score & Prompt Tokens & Completion Tokens & Score & Prompt Tokens & Completion Tokens \\
\midrule
No Memory & 57 & 1.65M & 24K & 34 & 2.46M & 29K \\
G-Memory  & 62 & 3.62M & 109K & 38 & 4.63M & 114K \\
SEDM (\textbf{Ours}) & \textbf{66} & 2.47M & 53K & \textbf{39} & 3.88M & 55K \\
\midrule
\bottomrule
\end{tabularx}
\end{table}




\begin{table}[t]
\centering
\caption{Ablation study on HotpotQA and FEVER, showing the progressive contribution of SEDM components.}
\label{tab:ablation_hotpotqa}
\begin{tabular}{lcccccc}
\toprule
\midrule
Dataset & \multicolumn{1}{c}{Setting} & Score & Prompt tokens & Completion tokens \\
\midrule
\multirow{3}{*}{HotpotQA}
 & No Memory          & 34 & 2.46M & 29K \\
 & + SCEC            & 37 & 3.52M & 52K \\
 & + SCEC + Self-Scheduling & \textbf{39} & 3.88M & 55K \\
\midrule
\multirow{3}{*}{FEVER}
 & No Memory          & 57 & 1.65M & 24K  \\
 & + SCEC           & 64 & 2.19M  & 53K\\
 & + SCEC + Self-Scheduling & \textbf{66} & 2.47M & 53K\\
 \midrule
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation Study}

To evaluate the contribution of individual SEDM components, we conduct ablation studies on both HotpotQA and FEVER. Table~\ref{tab:ablation_hotpotqa} reports results under three configurations: (i) the baseline without memory, (ii) the addition of SCEC-based verifiable write admission (\emph{+SCEC}), and (iii) the full SEDM with the memory controller’s self-scheduling mechanism (\emph{+SCEC + Self-Scheduling}). 

On HotpotQA, introducing \emph{+SCEC} improves the score from 34 to 37, but also increases prompt tokens by 43\% (2.46M $\rightarrow$ 3.52M) and completion tokens from 29K to 52K. With \emph{+Self-Scheduling}, the score further rises to 39, while prompt tokens grow only by 10\% (3.52M $\rightarrow$ 3.88M), showing that scheduling effectively controls token overhead relative to the accuracy gain. On FEVER, the baseline achieves 57. Adding \emph{+SCEC} raises the score to 64, accompanied by an increase in prompt tokens from 1.65M to 2.19M (+33\%) and completion tokens from 24K to 53K. With scheduling, performance improves to 66, while prompt tokens rise only to 2.47M (+13\%), and completion tokens remain unchanged, confirming that the controller filters relevant memory without inflating responses.

In summary, across both datasets, SCEC consistently yields substantial accuracy gains at the cost of increased token usage, while the self-scheduling mechanism provides further improvements with relatively minor overhead. This demonstrates that SEDM not only enhances reasoning accuracy but also achieves a more favorable trade-off between performance and efficiency.

\subsection{Cross-Domain Evaluation}

To further assess the generalization ability of SEDM across domains, we conduct a cross-domain experiment in which memory is collected on one dataset and evaluated on another. Table~\ref{tab:cross_domain} reports the results on FEVER, HotpotQA, and LoCoMo.

\begin{table}[ht]
\centering
\caption{Cross-domain evaluation of SEDM. Rows indicate the dataset used for memory collection, and columns indicate the dataset used for testing.}
\label{tab:cross_domain}
\begin{tabularx}{0.65\textwidth}{
>{\centering\arraybackslash}m{3cm}|
>{\centering\arraybackslash}X
>{\centering\arraybackslash}X
>{\centering\arraybackslash}X}
\toprule
\midrule
\textbf{Collect $\downarrow$ / Test $\rightarrow$} & \textbf{FEVER} & \textbf{HotpotQA} & \textbf{LoCoMo} \\
\midrule
FEVER      & 66 & 41 & 38.1 \\
HotpotQA   & 64 & 39 & 38.6 \\
LoCoMo     & 65 & 34 & 37.6 \\
\midrule
\bottomrule
\end{tabularx}
\end{table}

\textbf{FEVER as target:} The best score is achieved by FEVER itself (66); memory from HotpotQA and LoCoMo yields slightly lower scores (64 and 65), indicating that fact-verification benefits most from in-domain memory.

\textbf{HotpotQA as target:} The highest score is obtained by FEVER → HotpotQA (41), 2 points above the in-domain result (39). Memory from LoCoMo drops to 34, the lowest cross-domain score, suggesting that dialogue-grounded knowledge is least useful for multi-hop reasoning.

\textbf{LoCoMo as target:} All three sources perform within 1 point of each other (37.6–38.6). Thus, no single source dominates, and dialogue-grounded evaluation is remarkably robust to the origin of memory.

Overall, SEDM exhibits task-dependent transfer: factual-verification memory transfers surprisingly well to HotpotQA, whereas dialogue memory transfers poorly to the other two tasks. In-domain memory is not universally optimal, and the benefit of domain alignment varies significantly by task pair.


\vspace{-0.5em}
\section{Conclusion}
\vspace{-0.5em}
 This paper introduces \textbf{SEDM}, Scalable Self-Evolving Distributed Memory, which transforms memory in multi-agent systems from a passive repository into an adaptive and verifiable component by integrating SCEC-based admission, self-scheduling refinement, and cross-domain knowledge diffusion. Through this principled design, SEDM addresses the challenges of noise accumulation, uncontrolled growth, and weak generalization that limit existing methods. Experiments on LoCoMo, FEVER, and HotpotQA confirm that SEDM improves reasoning accuracy while reducing computational and token overhead, demonstrating its potential as a scalable and sustainable memory mechanism for long-term multi-agent collaboration.

% \section{Reference}
% \subsection{Citations within the text}


% The \verb+natbib+ package will be loaded for you by default.  Citations may be
% author/year or numeric, as long as you maintain internal consistency.  As to the
% format of the references themselves, any style is acceptable as long as it is
% used consistently.


% The documentation for \verb+natbib+ may be found at
% \begin{center}
%   \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
% \end{center}
% Of note is the command \verb+\citet+, which produces citations appropriate for
% use in inline text.  For example,
% \begin{verbatim}
%    \citet{hasselmo} investigated\dots
% \end{verbatim}
% produces
% \begin{quote}
%   Hasselmo, et al.\ (1995) investigated\dots
% \end{quote}


% If you wish to load the \verb+natbib+ package with options, you may add the
% following before loading the \verb+neurips_2025+ package:
% \begin{verbatim}
%    \PassOptionsToPackage{options}{natbib}
% \end{verbatim}


% If \verb+natbib+ clashes with another package you load, you can add the optional
% argument \verb+nonatbib+ when loading the style file:
% \begin{verbatim}
%    \usepackage[nonatbib]{neurips_2025}
% \end{verbatim}


% As submission is double blind, refer to your own published work in the third
% person. That is, use ``In the previous work of Jones et al.\ [4],'' not ``In our
% previous work [4].'' If you cite your other papers that are not widely available
% (e.g., a journal paper under review), use anonymous author names in the
% citation, e.g., an author of the form ``A.\ Anonymous'' and include a copy of the anonymized paper in the supplementary material.


% \subsection{Footnotes}


% Footnotes should be used sparingly.  If you do require a footnote, indicate
% footnotes with a number\footnote{Sample of the first footnote.} in the
% text. Place the footnotes at the bottom of the page on which they appear.
% Precede the footnote with a horizontal rule of 2~inches (12~picas).


% Note that footnotes are properly typeset \emph{after} punctuation
% marks.\footnote{As in this example.}


% \subsection{Figures}


% \begin{figure}
%   \centering
%   \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%   \caption{Sample figure caption.}
% \end{figure}


% All artwork must be neat, clean, and legible. Lines should be dark enough for
% purposes of reproduction. The figure number and caption always appear after the
% figure. Place one line space before the figure caption and one line space after
% the figure. The figure caption should be lower case (except for first word and
% proper nouns); figures are numbered consecutively.


% You may use color figures.  However, it is best for the figure captions and the
% paper body to be legible if the paper is printed in either black/white or in
% color.


% \subsection{Tables}


% All tables must be centered, neat, clean and legible.  The table number and
% title always appear before the table.  See Table~\ref{sample-table}.


% Place one line space before the table title, one line space after the
% table title, and one line space after the table. The table title must
% be lower case (except for first word and proper nouns); tables are
% numbered consecutively.


% Note that publication-quality tables \emph{do not contain vertical rules.} We
% strongly suggest the use of the \verb+booktabs+ package, which allows for
% typesetting high-quality, professional tables:
% \begin{center}
%   \url{https://www.ctan.org/pkg/booktabs}
% \end{center}
% This package was used to typeset Table~\ref{sample-table}.


% \begin{table}
%   \caption{Sample table title}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule(r){1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \subsection{Math}
% Note that display math in bare TeX commands will not create correct line numbers for submission. Please use LaTeX (or AMSTeX) commands for unnumbered display math. (You really shouldn't be using \$\$ anyway; see \url{https://tex.stackexchange.com/questions/503/why-is-preferable-to} and \url{https://tex.stackexchange.com/questions/40492/what-are-the-differences-between-align-equation-and-displaymath} for more information.)

% \subsection{Final instructions}

% Do not change any aspects of the formatting parameters in the style files.  In
% particular, do not modify the width or length of the rectangle the text should
% fit into, and do not change font sizes (except perhaps in the
% \textbf{References} section; see below). Please note that pages should be
% numbered.


% \section{Preparing PDF files}


% Please prepare submission files with paper size ``US Letter,'' and not, for
% example, ``A4.''


% Fonts were the main cause of problems in the past years. Your PDF file must only
% contain Type 1 or Embedded TrueType fonts. Here are a few instructions to
% achieve this.


% \begin{itemize}


% \item You should directly generate PDF files using \verb+pdflatex+.


% \item You can check which fonts a PDF files uses.  In Acrobat Reader, select the
%   menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
%   also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
%   available out-of-the-box on most Linux machines.


% \item \verb+xfig+ "patterned" shapes are implemented with bitmap fonts.  Use
%   "solid" shapes instead.


% \item The \verb+\bbold+ package almost always uses bitmap fonts.  You should use
%   the equivalent AMS Fonts:
% \begin{verbatim}
%    \usepackage{amsfonts}
% \end{verbatim}
% followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or \verb+\mathbb{C}+
% for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You can also use the following
% workaround for reals, natural and complex:
% \begin{verbatim}
%    \newcommand{\RR}{I  R} %real numbers
%    \newcommand{\Nat}{I  N} %natural numbers
%    \newcommand{\CC}{I    C} %complex numbers
% \end{verbatim}
% Note that \verb+amsfonts+ is automatically loaded by the \verb+amssymb+ package.


% \end{itemize}


% If your file contains type 3 fonts or non embedded TrueType fonts, we will ask
% you to fix it.


% \subsection{Margins in \LaTeX{}}


% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+ from the \verb+graphicx+ package. Always specify the
% figure width as a multiple of the line width as in the example below:
% \begin{verbatim}
%    \usepackage[pdftex]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.pdf}
% \end{verbatim}
% See Section 4.4 in the graphics bundle documentation
% (\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})


% A number of width problems arise when \LaTeX{} cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command when
% necessary.

% \begin{ack}
% Use unnumbered first level headings for the acknowledgments. All acknowledgments
% go at the end of the paper before the list of references. Moreover, you are required to declare
% funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).
% More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2025/PaperInformation/FundingDisclosure}.


% Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to automatically hide this section in the anonymized submission.
% \end{ack}

% \section*{References}


% References follow the acknowledgments in the camera-ready paper. Use unnumbered first-level heading for
% the references. Any choice of citation style is acceptable as long as you are
% consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
% when listing the references.
% Note that the Reference section does not count towards the page limit.
% \medskip


% {
% \small


% [1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
% connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
% (eds.), {\it Advances in Neural Information Processing Systems 7},
% pp.\ 609--616. Cambridge, MA: MIT Press.


% [2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
%   Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
% TELOS/Springer--Verlag.


% [3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
% recall at excitatory recurrent synapses and cholinergic modulation in rat
% hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
% }


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \appendix

% \section{Technical Appendices and Supplementary Material}
% Technical appendices with additional results, figures, graphs and proofs may be submitted with the paper submission before the full submission deadline (see above), or as a separate PDF in the ZIP file below before the supplementary material deadline. There is no page limit for the technical appendices.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%% Checklist Below%%%%%%%%%%%%%%%%%%
\bibliographystyle{plain}
\bibliography{references}

% \newpage
% \section*{NeurIPS Paper Checklist}

% %%% BEGIN INSTRUCTIONS %%%
% % The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: {\bf The papers not including the checklist will be desk rejected.} The checklist should follow the references and follow the (optional) supplemental material.  The checklist does NOT count towards the page
% % limit. 

% % Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:
% % \begin{itemize}
% %     \item You should answer \answerYes{}, \answerNo{}, or \answerNA{}.
% %     \item \answerNA{} means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
% %     \item Please provide a short (1–2 sentence) justification right after your answer (even for NA). 
% %    % \item {\bf The papers not including the checklist will be desk rejected.}
% % \end{itemize}

% % {\bf The checklist answers are an integral part of your paper submission.} They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

% % The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "\answerYes{}" is generally preferable to "\answerNo{}", it is perfectly acceptable to answer "\answerNo{}" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "\answerNo{}" or "\answerNA{}" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer \answerYes{} to a question, in the justification please point to the section(s) where related material for the question can be found.

% % IMPORTANT, please:
% % \begin{itemize}
% %     \item {\bf Delete this instruction block, but keep the section heading ``NeurIPS Paper Checklist"},
% %     \item  {\bf Keep the checklist subsection headings, questions/answers and guidelines below.}
% %     \item {\bf Do not modify the questions and only use the provided macros for your answers}.
% % \end{itemize} 
 

% %%% END INSTRUCTIONS %%%


% \begin{enumerate}

% \item {\bf Claims}
%     \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
%     \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: The abstract and introduction clearly state the paper's main contributions, including a verifiable write admission procedure, a self-scheduling memory controller, and a cross-domain knowledge diffusion mechanism. These claims are directly supported by the methodology and experimental results presented in the paper.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the abstract and introduction do not include the claims made in the paper.
%         \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
%         \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
%         \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
%     \end{itemize}

% \item {\bf Limitations}
%     \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
%     \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: The paper discusses the key limitations and boundary conditions of our approach within the relevant sections of the text. 
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
%         \item The authors are encouraged to create a separate "Limitations" section in their paper.
%         \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
%         \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
%         \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
%         \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
%         \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
%         \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
%     \end{itemize}

% \item {\bf Theory assumptions and proofs}
%     \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
%     \item[] Answer:  \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: The paper presents a novel system design and a series of empirical experiments. It does not contain any formal theoretical results or proofs.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include theoretical results. 
%         \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
%         \item All assumptions should be clearly stated or referenced in the statement of any theorems.
%         \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
%         \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
%         \item Theorems and Lemmas that the proof relies upon should be properly referenced. 
%     \end{itemize}

%     \item {\bf Experimental result reproducibility}
%     \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
%     \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: Our methodology is a novel algorithm, and the paper fully describes its components, from the SCEC framework to the memory controller's self-scheduling and evolution policies. All key design choices and parameters are detailed in the main text, ensuring that a skilled researcher can replicate our core findings.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include experiments.
%         \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
%         \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
%         \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
%         \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
%         \begin{enumerate}
%             \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
%             \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
%             \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
%             \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
%         \end{enumerate}
%     \end{itemize}


% \item {\bf Open access to data and code}
%     \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
%     \item[] Answer: \answerNo{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: While the paper's core contribution is a novel methodology, we do not provide code or data at this time to maintain anonymity for the double-blind review process. However, as noted above, our paper includes detailed instructions and descriptions of the experimental setup to enable faithful reproduction of the results.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that paper does not include experiments requiring code.
%         \item Please see the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
%         \item While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
%         \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
%         \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
%         \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
%         \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
%         \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
%     \end{itemize}


% \item {\bf Experimental setting/details}
%     \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
%     \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: The experimental setup and key hyperparameters are described in the methodology and experimental sections.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include experiments.
%         \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
%         \item The full details can be provided either with the code, in appendix, or as supplemental material.
%     \end{itemize}

% \item {\bf Experiment statistical significance}
%     \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
%     \item[] Answer: \answerNo{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: We do not report error bars for the primary results. However, our methodology is designed to ensure statistical robustness through repeated A/B measurements, which is discussed in the paper's methods section.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include experiments.
%         \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
%         \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
%         \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
%         \item The assumptions made should be given (e.g., Normally distributed errors).
%         \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.
%         \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.
%         \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
%         \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
%     \end{itemize}

% \item {\bf Experiments compute resources}
%     \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
%     \item[] Answer:  \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: We provide an overview of the compute resources used for our experiments in the paper. 
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include experiments.
%         \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
%         \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
%         \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
%     \end{itemize}
    
% \item {\bf Code of ethics}
%     \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
%     \item[] Answer:  \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: The research fully adheres to the NeurIPS Code of Ethics. It does not involve human subjects, sensitive data, or any applications with immediate ethical concerns. All experiments use publicly available benchmark datasets and focus on methodological advancements in a controlled research setting.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
%         \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
%         \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
%     \end{itemize}


% \item {\bf Broader impacts}
%     \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
%     \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: The paper discusses the broader societal impacts in a dedicated section. We highlight the positive impacts of our memory system, such as improving the efficiency and reliability of large language models for complex tasks, which could benefit areas like scientific research and education. We also acknowledge potential negative impacts, such as the technology's possible use in generating misleading information, and discuss how our core mechanisms for verification and auditable weight assignment can serve as a form of mitigation.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that there is no societal impact of the work performed.
%         \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
%         \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
%         \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
%         \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
%         \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
%     \end{itemize}
    
% \item {\bf Safeguards}
%     \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
%     \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: The paper focuses on a methodological contribution and does not release a new model or dataset. It utilizes existing, well-established public benchmarks that are not considered high-risk for misuse.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper poses no such risks.
%         \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
%         \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
%         \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
%     \end{itemize}

% \item {\bf Licenses for existing assets}
%     \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
%     \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: All benchmark datasets used in this study are widely used in the research community and are properly cited in the paper. The code is a novel contribution from the authors, and its license will be specified upon public release.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not use existing assets.
%         \item The authors should cite the original paper that produced the code package or dataset.
%         \item The authors should state which version of the asset is used and, if possible, include a URL.
%         \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.
%         \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
%         \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, \url{paperswithcode.com/datasets} has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
%         \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
%         \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.
%     \end{itemize}

% \item {\bf New assets}
%     \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
%     \item[] Answer: \answerNo{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: This paper does not introduce any new datasets or models. The core contribution is a novel methodology, and all experiments were conducted on existing, publicly available benchmark datasets.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not release new assets.
%         \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
%         \item The paper should discuss whether and how consent was obtained from people whose asset is used.
%         \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
%     \end{itemize}

% \item {\bf Crowdsourcing and research with human subjects}
%     \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? 
%     \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: This research does not involve crowdsourcing or any form of research with human subjects. All experiments are based on a computational method applied to pre-existing, publicly available benchmark datasets.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
%         \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
%         \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
%     \end{itemize}

% \item {\bf Institutional review board (IRB) approvals or equivalent for research with human subjects}
%     \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
%     \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: This research does not involve crowdsourcing, human subjects, or any data collection from human participants. All experiments are conducted on publicly available benchmark datasets that do not contain personally identifiable information.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
%         \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
%         \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
%         \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
%     \end{itemize}

% \item {\bf Declaration of LLM usage}
%     \item[] Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.
%     %this research? 
%     \item[] Answer:  \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: The proposed method, Self-Evolving Distillation Memory (SEDM), is a memory system designed to augment Large Language Models (LLMs). The core components, including verifiable write admission, memory scheduling, and knowledge diffusion, are all based on using LLMs as the underlying agent for knowledge distillation and reasoning. As such, LLMs are a central component of our methodology.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components.
%         \item Please refer to our LLM policy (\url{https://neurips.cc/Conferences/2025/LLM}) for what should or should not be described.
%     \end{itemize}

% \end{enumerate}


\end{document}