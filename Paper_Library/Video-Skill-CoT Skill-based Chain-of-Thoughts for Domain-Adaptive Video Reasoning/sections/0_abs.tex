\begin{abstract}
Recent advances in Chain-of-Thought (CoT) reasoning have improved complex video understanding, but existing methods often struggle to adapt to domain-specific skills (e.g., event detection, spatial relation understanding, emotion understanding) over various video content.
To address this, we propose
\textsc{Video-Skill-CoT} (a.k.a. \textsc{Video-SkoT})
a framework that automatically constructs and leverages skill-aware CoT supervisions for domain-adaptive video reasoning.
First, we construct skill-based CoT annotations: We extract domain-relevant reasoning skills from training questions, cluster them into a shared skill taxonomy, and create detailed multi-step CoT rationale tailored to each video-question pair for training.
Second, we introduce a skill-specific expert learning framework. Each expert module specializes in a subset of reasoning skills and is trained with lightweight adapters using the collected CoT supervision. 
We demonstrate the effectiveness of the proposed approach on three video understanding benchmarks, where
\ours consistently outperforms strong baselines. 
We also provide in-depth analyses on comparing different CoT annotation pipelines
and learned skills over multiple video domains.
\end{abstract} 