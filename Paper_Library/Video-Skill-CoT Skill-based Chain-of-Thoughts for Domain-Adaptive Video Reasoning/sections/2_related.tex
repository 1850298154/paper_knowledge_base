\section{Related Work}
\label{sec:related_work}



\paragraph{Video Understanding with MLLMs.}
Prior video understanding models focused on pretraining strategies~\cite{sun2019videobertjointmodelvideo,li2020herohierarchicalencodervideolanguage,lei2021less}.
Recent work incorporates CoT reasoning~\cite{kojima2022large,wei2023chainofthoughtpromptingelicitsreasoning} from the NLP domain and studies how to collect and learn to generate such CoT reasoning for different video understanding tasks~\cite{fei2024video,li2025videochat,liu2025videomind,zhi2025videoagent2}.
Unlike these methods, which often struggle with comprehending videos without explicit skill-specific guidance, our approach introduces a skill-aware reasoning framework incorporating question-adaptive skill selection and skill-guided CoT supervision.

\paragraph{Skill-specific Expert Learning.}
Modular and expert-based architectures have been widely explored to improve parameter efficiency and mitigate interference in multi-task and multi-domain settings,
where each expert learns different knowledge. 
Mixture-of-experts (MoE) frameworks dynamically route inputs to expert sub-networks~\cite{Shazeer2017Mixture},
while adapter-based methods introduce lightweight, task-specific modules into pretrained models~\cite{houlsby2019parameterefficienttransferlearningnlp,hu2022lora}.
\citet{li2024selma} studies learning skill-specific expert diffusion models for the text-to-image generation task.
A concurrent work, \citet{liu2025videomind} studies a multi-agent system where each agent is implemented as a LoRA~\cite{hu2022lora} expert.
While \citet{liu2025videomind} relies on predefined expert roles (planner, grounder, verifier, and answerer), specific architectures, and manually curated role-specific annotations, our expert framework flexibly adapts to any video understanding dataset by automatically discovering and leveraging relevant reasoning skills.
