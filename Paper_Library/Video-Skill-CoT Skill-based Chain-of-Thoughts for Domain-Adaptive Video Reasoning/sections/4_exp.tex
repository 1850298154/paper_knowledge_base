\section{Experiments}
\label{sec:experiments}
\subsection{Experiment Setups}
\paragraph{Implementation Details.}
To obtain text embeddings (of skill taxonomy in \cref{sec:data_collection} and of questions in \cref{sec:multi-lora}), we use
\texttt{all-mpnet-base-v2} SentenceTransformers~\cite{reimers-2019-sentence-bert} implementation.
We use LLaVa-Video (7B)~\cite{zhang2024videoinstructiontuningsynthetic} as a main backbone model.
Additional training details including hyperparameters, the specific MLLMs and LLMs used at each stage, as well as results with the Qwen2.5-VL (7B) backbone are provided in Appendix~\cref{sec:detail_cot_generation,sec:detail_skill_description,sec:qwen}.


\paragraph{Datasets and Baselines.}

We experiment with three different video understanding benchmarks with distinct domains:
E.T.Bench~\cite{liu2024etbench} (temporal understanding),
VSI-Bench~\cite{yang2024vsi-bench} (spatial understanding),
and
CinePile~\cite{rawal2024cinepile} (movie narrative understanding).
For multiple-choice questions, we report the average accuracy. For temporal captioning tasks in E.T.Bench, we use the benchmark's official evaluation script. 
Baseline MLLMs include mPLUG-Owl~\cite{ye2024mplug},
Video-ChatGPT~\cite{maaz2023video},
Video-LLaMA2~\cite{zhang2023video}, LLaVa-OneVision~\cite{li2024llava}, and LLaVa-Video~\cite{zhang2024videoinstructiontuningsynthetic},
GPT4o~\cite{gpt4o} and Gemini 1.5 Flash, Pro~\cite{team2024gemini}.
Additional details are provided in the Appendix \cref{sec:detail_training}.


\subsection{Quantitative Evaluation}

\paragraph{Comparison to Baselines.}

\input{tables/single-task-table}

We compare \ours{} to recent MLLM baselines on three video understanding benchmarks (E.T.Bench, VSI-Bench, CinePile) with domains and required skills.
\Cref{tab:domain_specific_video} shows that \ours{} consistently outperforms all baselines, achieving improvements of $+4.10$, $+5.70$, and $+1.59$ over the fine-tuned version of LLaVA-Video on E.T.Bench, VSI-Bench, and CinePile, respectively. These results highlight the effectiveness of our modular, expert-driven framework in enabling domain-adaptive CoT video reasoning by leveraging relevant skills.

\paragraph{Ablation Studies.}


We compare the impact of two key components in our framework: (1) skill-based CoT reasoning and (2) skill-specific expert modules. 
As shown in~\cref{tab:ablation}, our full model, combining both components (Top row), achieves the highest performance. Removing either the skill-specific expert modules (2nd row), the skill-based CoT (3rd row), or both components (last row) consistently leads to performance degradation, highlighting their complementary roles: skill-CoT enables structured reasoning, while expert modules bring modular specialization. This synergy proves essential for improving video understanding.

\input{tables/architecture_ablation}

\input{tables/human}


\begin{figure*}[t]
\vspace{-0.5cm}
    \centering
    {
    \includegraphics[width=\textwidth]{figures/qual_inference.pdf}}
    \vspace{-0.3in}
    \caption{
    \textbf{
    Inference output comparison: (a) LLaVA-Video trained with regular CoT and (b) LLaVA-Video trained with our skill-based CoT.} \ours{} successfully generates temporally grounded and precise rationales that more effectively support accurate answer generation.
    }
    \label{fig:qual_inference}
\end{figure*}


\paragraph{Human Evaluation.}
We conduct a human evaluation with five researchers who are familiar with the relevant field, where 15 randomly selected questions were assessed by comparing regular CoT and the proposed Skill-based CoT. 
Each explanation is rated on a 1–5 Likert scale (5 = best, 1 = worst) across three dimensions: Correctness (factual accuracy), Relevance (task appropriateness), and Coherence (clarity and logical flow). 
As shown in \cref{tab:human}, Skill-based CoT consistently outperforms regular CoT across all criteria, with substantial gains in correctness, relevance, and coherence, confirming that our method produces explanations that are more accurate, aligned, and easier to follow.
These results provide strong evidence that Skill-based CoT produces explanations that are not only more accurate but also more relevant and human-readable.

\subsection{Qualitative Analysis}

\paragraph{Regular CoT vs. Skill-based CoT.}
\cref{fig:data_annotation_examples} compares the different annotated CoTs from the regular CoT and our skill-based CoT. Given a question about which object is closest to the stove, the regular CoT (left) offers a linear, scene-based narration that lacks structure and includes irrelevant details (``Camera first focuses ... it then pans to the right ...''), making it often harder to extract key spatial information.
In contrast, our skill-based CoT starts by identifying relevant skills (e.g., spatial proximity) and breaking the task into focused sub-questions, like comparing the washer and refrigerator.

\paragraph{Inference rationale comparison}
We compare the inference-time rationales generated by LLaVA-Video trained with (a) regular CoT and (b) the proposed skill-based CoT. During inference, we prompt each model with: \textit{“Explain the rationale to answer the question and answer the question.”} As shown in \cref{fig:qual_inference}, the model trained with regular CoT produces an incorrect reasoning process, ultimately leading to a wrong answer. In contrast, \ours{} successfully generates temporally grounded and precise rationales that more effectively support accurate answer generation.
