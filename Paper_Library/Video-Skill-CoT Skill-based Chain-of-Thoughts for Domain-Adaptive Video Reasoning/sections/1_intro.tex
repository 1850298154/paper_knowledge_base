
\begin{figure*}[t]
    \centering
    {
    \includegraphics[width=.99\textwidth]{figures/merged_fig.pdf}}
    \caption{
    \textbf{Left}: Video datasets require different reasoning skills.
    \textbf{Right}: \ours{} that automatically constructs and leverages skill-aware CoT supervisions for domain-adaptive video reasoning.
    }
    \label{fig:main1}\vspace{-0.1in}
\end{figure*}

\section{Introduction}
\label{sec:intro}

Understanding complex video content requires integrating rich spatiotemporal cues and adapting to diverse domain-specific reasoning needs from cinematic narratives, egocentric recordings, to indoor scenes~\cite{fusier2007video,huang2018makes,buch2022revisiting,lin2023univtg,chen2024sharegpt4video,li2024mvbench}.
Models should acquire and integrate a wide range of distinct reasoning skills, such as temporal grounding, spatial relationship recognition, and multi-step planning.

Recent work has extended chain-of-thought (CoT) reasoning~\cite{wei2023chainofthoughtpromptingelicitsreasoning,kojima2022large} to multimodal large language models (MLLMs) for video understanding~\cite{fei2024video,feng2025video,li2025videochat,liu2025videomind,zhi2025videoagent2}.
However, most prior approaches rely on fixed, general-purpose reasoning traces that are insensitive to domain-specific skills.
\cref{fig:main1} (left)
shows a t-SNE~\cite{vanDerMaaten2008tsne} plot of embeddings of questions from different video datasets, where questions from the same datasets are strongly clustered as they require shared skills/domains.
For example, models pretrained on general corpora such as 
LLaVA-Video-178K~\cite{zhang2024videoinstructiontuningsynthetic}
often lack the nuanced narrative understanding needed in CinePile~\cite{rawal2024cinepile}.
This limits their ability to generalize to unseen domains or specialized skills.

To address this, we propose \textsc{Video-Skill-CoT} (aka \textsc{Video-SkoT}), a novel video understanding framework for creating and leveraging skill-aware CoT supervision, helping effective domain adaptation of MLLMs (\cref{sec:method}).
As shown in~\cref{fig:main1} (Right), \ours{} consists of two main components.
First, in skill-based CoT annotation (\cref{sec:data_collection}), we introduce a method to automatically construct high-quality, skill-conditioned CoT rationales for video QA tasks. Given a training question, we first extract high-level reasoning skill descriptions (e.g., ``Determine object location relative to a person's orientation'' and ``Inferring emotional state from expressions and body language''), then cluster them into a shared skill taxonomy (\cref{fig:main1} Right-(a)).
Then, each question is annotated with its top-K relevant skills and used to generate a multi-step CoT annotation conditioned on these skills (\cref{fig:main1} Right-(b)).
This enables diverse and domain-relevant reasoning traces without requiring manual annotation. 

Once we have prepared the skill-based CoT annotations,
in skill-specific expert learning (\cref{sec:multi-lora} and \cref{fig:main1} Right-(c)),
we train skill-specialized expert models with multiple LoRAs~\cite{hu2022lora}.
Each expert specializes in a specific set of reasoning capabilities, determined by a predefined group of related questions. During inference, the model routes each input to the expert aligned with the most relevant question group.

We evaluate \ours{} on three video QA datasets with diverse domains (E.T.-Bench~\cite{liu2024etbench}, VSI-Bench~\cite{yang2024vsi-bench}, and CinePile~\cite{rawal2024cinepile}),
where \ours{} consistently improves over strong baselines, showcasing its strong domain adaptation capabilities.
We also present ablation studies on our design choices and visualize the learned domain-specific skills to validate the effectiveness and interpretability of our skill-guided reasoning framework.
