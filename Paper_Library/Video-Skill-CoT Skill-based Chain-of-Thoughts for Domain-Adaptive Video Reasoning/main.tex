% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[preprint]{acl}
% \usepackage[review]{acl}
\usepackage[final]{acl}

\usepackage{ulem}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}

\usepackage{amsmath}   % advanced math environments (equation, align, etc.)
\usepackage{amssymb}   % extra symbols
\usepackage{amsfonts}  % \mathbb, \mathbb{E}, â€¦
\usepackage{titletoc}
\usepackage{tocloft}
\usepackage{multirow}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{booktabs}
% \input{marcos}

% \renewcommand\@makefnmark{\hbox{\textsuperscript{\normalfont\color{black}\@thefnmark}}}
\usepackage{xcolor}
\makeatletter
\renewcommand\@makefnmark{%
  \hbox{\textsuperscript{\normalfont\color{black}\@thefnmark}}%
}
\makeatother
\newcommand{\ours}{\textsc{Video-SkoT}\xspace}


\title{
\textsc{Video-Skill-CoT}: Skill-based Chain-of-Thoughts\\
for Domain-Adaptive Video Reasoning 
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
Daeun Lee$^{1, }$\thanks{Equal contribution.} \quad
Jaehong Yoon$^{1,2, *}$ \quad
Jaemin Cho$^{1}$ \quad
Mohit Bansal$^{1}$\\
$^{1}$UNC Chapel Hill\quad\quad $^{2}$Nanyang Technological University\\
\texttt{\{daeun,jmincho,mbansal\}@cs.unc.edu}\quad \texttt{jaehong.yoon@ntu.edu.sg}\\
\url{https://video-skill-cot.github.io/}
  % David S.~Hippocampus\thanks{Use footnote for providing further informat^on
  %   about author (webpage, alternative address)---\emph{not} for acknowledging
  %   funding agencies.} \\
  % Department of Computer Science\\
  % Cranberry-Lemon University\\
  % Pittsburgh, PA 15213 \\
  % \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

 
% \setlength{\textfloatsep}{10pt}

\input{macros}
\begin{document}

\maketitle

%%%%%%%%% ABSTRACT
\input{sections/0_abs.tex}

%%%%%%%%% BODY TEXT

\input{sections/1_intro.tex}

\input{sections/2_related.tex}

\input{sections/3_method.tex}

\input{sections/4_exp.tex}

\input{sections/5_conclusion.tex}



\section*{Limitations}
Our proposed framework demonstrates strong video reasoning capabilities, generating fine-grained, domain-adaptive rationales based on required skills.
However, it may still produce occasional inaccuracies or hallucinations~\cite{liu2023mitigating, wang2024mitigating, zhou2024analyzing} in its text outputs.
Additionally, the overall performance is influenced by the underlying pre-trained backbones, namely, the LLM~\cite{achiam2023gpt4} and MLLM~\cite{team2024gemini} used.
Nonetheless, we highlight that \ours{} can benefit further from future advancements in LLM and MLLM backbones.


\section*{Acknowledgments}
This work was supported by DARPA ECOLE Program No. HR00112390060, NSF-AI Engage Institute DRL-2112635, DARPA Machine Commonsense (MCS) Grant N6600119-2-4031, ARO Award W911NF2110220, ONR Grant N00014-23-1-2356, a Bloomberg Data Science PhD Fellowship,
and the Accelerate Foundation Models Research program. The views contained in this article are those of the authors and not of the funding agency.














{
% \bibliographystyle{abbrv}
\bibliography{ref}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\appendix

\section*{Appendix}

\addcontentsline{toc}{section}{Appendix Table of Contents}
\startcontents[appendix]
\printcontents[appendix]{l}{1}{\setcounter{tocdepth}{2}}


\input{sections/appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}