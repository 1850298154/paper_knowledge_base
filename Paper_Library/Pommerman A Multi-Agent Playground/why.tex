\section{Why Pommerman}
In this section, we provide our motivation and goals for both the Pommerman benchmark and the NIPS 2018 competition.
%In this section we describe why we developed this environment, why we are running competitions for it, including at NIPS 2018, and what our overall goals are. 
Currently, there is no consensus benchmark involving either general sum game settings or settings with at least three players. Instead, recent progress has focused on two player zero-sum games such as Go and Chess. We believe that the Pommerman environment can assume this role for multi-agent learning. Additionally, we are organizing competitions for Pommerman because we believe that they are a strong way to push forward the state of the art and can contribute to lasting results for years to come.
%Succinctly, we are addressing a hole in the research community when it comes to multi-agent scenarios. There are  few benchmarks for this important research direction, and almost all have focused on zero-sum two-player games. Pommerman pushes this further and opens up new avenues for which the community can build intelligent agents that exhibit intelligent solutions. When done right, competitions are a strong way to push forward the state of the art and can contribute to lasting results for years to come.

% \begin{itemize}
%     \item There are no widely-used benchmarks for Multi-Agent Learning Algorithms with communication; Competitions have been a strong way to push forward the state of the art.
%     \item Games that include both cooperative and zero-sum components have received little attention in the machine learning community. 
%     \item There is no endogenous aspect of this environment that requires large amounts of compute. In particular, the inputs to agents are a low dimensional symbolic observation.
%     \item People quickly grasp Pommerman; it's intuitive to understand and actually fun to play.
%     \item Empirically, competitions have been a strong way to push forward the state of the art.
% \end{itemize}

\subsection{Multi-Agent Learning}
Historically, a majority of multi-agent research has focused on zero-sum two player games. For example, computer competitions for Poker and Go over the past fifteen years have been vital for developing methods culminating in recent superhuman performance \cite{DBLP:journals/corr/MoravcikSBLMBDW17,ijcai2017-772,DBLP:journals/cacm/BowlingBJT17,SilverHuangEtAl16nature}. These benchmarks have also lead to the discovery of new algorithms and approaches like Monte Carlo Tree Search \cite{Vodopivec:2017:MCT:3207692.3207712,browne2012survey,Kocsis:2006:BBM:2091602.2091633,conf/cg/Coulom06} and Counterfactual Regret Minimization \cite{NIPS2007_3306}.

We believe that an aspect restraining the field from progressing towards general-sum research and scenarios with more than two players is the lack of suitable environments. We propose Pommerman as a solution.

Pommerman is stylistically similar to Bomberman \cite{bomberwiki}, the famous game from Nintendo. At a high level, there are at least four agents all traversing a grid world. Each agent's goal is to have their team be the last remaining. They can plant bombs that, upon expiration, destroy anything (but rigid walls) in their vicinity. It contains both adversarial and cooperative elements.  The Free-For-All (FFA) variant has at most one winner and, because there are four players, encourages research directions that can handle situations where the Nash payoffs are not all equivalent. 
The team variants encourage research with and without explicit communication channels, including scenarios where the agent has to cooperate with previously unseen teammates. The latter is a recently burgeoning subfield of multi-agent learning \cite{foerster2016learning,DBLP:journals/corr/abs-1804-07178,DBLP:journals/corr/EvtimovaDKC17,DBLP:journals/corr/FoersterNFTKW17,lewis2017deal,mordatch2017emergence,lazaridou2018emergence} with established prior work as well \cite{steels:99e,steels03,conf/eelc/LevyK06,fehervari:jr:10}, while the latter has been underexplored. 

We aim for the Pommerman benchmark to provide for multi-agent learning what the Atari Learning Environment \cite{Bellemare:2013:ALE:2566972.2566979} provided for single-agent reinforcement learning and ImageNet \cite{imagenet_cvpr09} for image recognition. Beyond game theory and communication, Pommerman can also serve as a testbed for research into reinforcement learning, planning, and opponent/teammate modeling.

RoboCup Soccer \cite{NardiNoda14} is a similar competition that has been running since 1997. There, eleven agents per side play soccer. Key differences between Pommerman and RoboCup Soccer are:
\begin{enumerate}
\item Pommerman includes an explicit communication channel. This changes the dynamics of the game and adds new research avenues. 
\item Pommerman strips away the sensor input, which means that the game is less apt for robotics but more apt for studying other aspects of AI, games, and strategy.
\item Pommerman uses low dimensional, discrete control and input representations instead of continuous ones. We believe this makes it easier to focus on the high level strategic aspects rather than low level mechanics.
\item In team variants, the default Pommerman setup has only two agents per side, which makes it more amenable to burgeoning fields like emergent communication which encounter training difficulties with larger numbers of agents.
\item Pommerman's FFA variant promotes research that does not reduce to a 1v1 game, which means that a lot of the theory underlying such games (like RoboCup Soccer) is not applicable.
\end{enumerate}

The second, third, and fourth differences above are a positive or negative trade-off depending on one's research goals.

Another, more recent, benchmark is Half-Field Offense \cite{ALA16-hausknecht}, a modification of RoboCup that reduces the complexity and focuses on decision-making in a simplified subtask. However, unlike the FFA scenario in Pommerman, Half-Field Offense is limited to being a zero-sum game between two teams.

In general, the communities that we want to attract to benchmark their algorithms have not gravitated towards RoboCup but instead have relied on a large number of one-off toy tasks. This is especially true for multi-agent Deep RL. We think that the reasons for that could be among the five above. Consequently, Pommerman has the potential to unite these communities, especially when considering that future versions can be expanded to more than four agents.

\begin{table*}[t]
\centering
{
\small
\begin{tabular}{|l|c|c|c|}
\thead{Game} & \thead{Intuitive?} & \thead{Fun?} & \thead{Integration?} \\ \hline
Bridge & 1 & 3 & 5 \\ \hline
Civilization & 2 & 3 & 1 \\ \hline
Counterstrike & 5 & 5 & 2 \\ \hline
Coup & 4 & 5 & 5 \\ \hline
Diplomacy & 1 & 4 & 3 \\ \hline
DoTA & 3 & 5 & 2 \\ \hline
Hanabi & 2 & 3 & 5 \\ \hline
Hearthstone & 1 & 4 & 1 \\ \hline
Mario Maker & 4 & 5 & 3 \\ \hline
\textbf{Pommerman} & \textbf{5} & \textbf{4} & \textbf{5} \\ \hline
PUBG & 5 & 5 & 1 \\ \hline
Rocket League & 5 & 4 & 1 \\ \hline
Secret Hitler & 4 & 4 & 3 \\ \hline
Settlers of Catan & 4 & 3 & 3 \\ \hline
Starcraft 2 & 3 & 5 & 5 \\ \hline
Super Smash & 5 & 5 & 1 \\ \hline
\end{tabular}
}
\caption{Comparing multi-agent games along three important axes for uptake beyond whether the game satisfies the community's intended research direction. Attributes are considered on a 1-5 scale where 5 represents the highest value. Fun takes into account both watching and playing the game. The Intuitive and Fun qualities, while subjective, are noted because they have historically been factors in whether a game is used in research.}
\label{table:game-comparisons}
\end{table*}

\subsection{High Quality Benchmark} 

There are attributes that are common to the best benchmarks beyond satisfying the community's research direction. These include having mechanics and gameplay that are intuitive for humans, being fun to play and watch, being easy to integrate into common research setups, and having a learning problem that is not \textit{too} difficult for the current state of method development. Most games violate at least one of these. For example, the popular game Defense of the Ancients \cite{dota} is intuitive and fun, but extremely difficult to integrate. On the other hand, the card game Bridge is easy to integrate, but it is not intuitive; the gameplay and mechanics are slow to learn and there is a steep learning curve to understanding strategy.

Pommerman satisfies these requirements. People have no trouble understanding basic strategy and mechanics. It is fun to play and to watch, having been developed by Nintendo for two decades. Additionally, we have purposefully made the state input based not on pixel observations but rather on a symbolic interpretation so that it does not require large amounts of compute to build learning agents.

Research game competitions disappear for two reasons - either the administrators stop running it or participants stop submitting entrants. This can be due to the game being `solved', but it could also be because the game just was not enjoyable or accessible enough. We view Pommerman as having a long life ahead of it. Beyond the surface hyperparameters like board size and number of walls, early forays suggest that there are many aspects of the game that can be modified to create a rich and long lasting research challenge and competition venue. These include partial observability of the board, playing with random teammates, communication among the agents, adding power-ups, and learning to play with human players. 

These potential extensions, and the fact that N-player learning by itself has few mathematical guarantees, suggest that Pommerman will be a challenging and fruitful testbed for years to come.

There are, however, limitations to this environment. One difficulty is that a local optimum arises where the agent avoids exploding itself by learning to never use the bomb action. In the long term, this is ineffective because the agent needs to use the bomb to destroy other agents. Players have successfully solved this challenge \cite{2018arXiv180706919R}, but it is an aspect of basic gameplay that has to be handled in order for the multi-agent research benefits to become apparent.