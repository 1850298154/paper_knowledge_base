\section{Description}
In this section, we give details of the Pommerman environment. Note that all of the code to run the game and train agents can be found in our git repository \cite{github}, while our website (pommerman.com) contains further information on how to submit agents.

\subsection{Game Information}
\begin{figure}[h!]
    \centering
    % \captionsetup{width=4cm}
    \includegraphics[width=4cm]{pommerman-start.png}
    \caption{Pommerman start state. Each agent begins in one of four positions. Yellow squares are wood, brown are rigid, and the gray are passages.}
    \label{fig:pommerman-start}
%\end{wrapfigure}
\end{figure}

As previously mentioned, Pommerman is stylistically similar to Bomberman. Every battle starts on a randomly drawn symmetric 11x11 grid (`board') with four agents, one in each corner. Teammates start on opposite corners.

In team variants, the game ends when both players on one team have been destroyed. In FFA, it ends when at most one agent remains alive. The winning team is the one who has remaining members. Ties can happen when the game does not end before the max steps or if the last agents are destroyed on the same turn. If this happens in competitions, we will rerun the game. If it reoccurs, then we will rerun the game with collapsing walls until there is a winner. This is a variant where, after a fixed number of steps, the game board becomes smaller according to a specified cadence. We have a working example in the repository.

Besides the agents, the board consists of wooden and rigid walls. We guarantee that the agents will have an accessible path to each other. Initially, this path is occluded by wooden walls. See Figure \ref{fig:pommerman-start} for a visual reference.

Rigid walls are indestructible and impassable. Wooden walls can be destroyed by bombs. Until they are destroyed, they are impassable. After they are destroyed, they become either a passage or a power-up.

On every turn, agents choose from one of six actions:
\begin{enumerate}
    \item \emph{Stop}: This action is a pass.
    \item \emph{Up}: Move up on the board.
    \item \emph{Left}: Move left on the board.
    \item \emph{Down}: Move down on the board.
    \item \emph{Right}: Move right on the board.
    \item \emph{Bomb}: Lay a bomb.
\end{enumerate}

Additionally, if this is a communicative scenario, then the agent emits a message every turn consisting of two words from a dictionary of size eight. These words are passed to its teammate in the next step as part of the observation. In total, the agent receives the following observation each turn:
\begin{itemize}
	\item \emph{Board}: 121 Ints. The flattened board. In partially observed variants, all squares outside of the 5x5 purview around the agent's position will be covered with the value for fog (5). 
    \item \emph{Position}: 2 Ints, each in [0, 10]. The agent's (x, y) position in the grid.
    \item \emph{Ammo}: 1 Int. The agent's current ammo.
    \item \emph{Blast Strength}: 1 Int. The agent's current blast strength.
    \item \emph{Can Kick}: 1 Int, 0 or 1. Whether the agent can kick or not.
    \item \emph{Teammate}: 1 Int in [-1, 3].  Which agent is this agent's teammate. In non-team variants, this is -1.
    \item \emph{Enemies}: 3 Ints in [-1, 3]. Which agents are this agent's enemies. In team variants, the third int is -1.
    \item \emph{Bomb Blast Strength}: List of Ints. The bomb blast strengths for each of the bombs in the agent's purview. 
    \item \emph{Bomb Life}: List of Ints. The remaining life for each of the bombs in the agent's purview.
    \item \emph{Message}: 2 Ints in [0, 8]. The message being relayed from the teammate. Both Ints are zero only when a teammate is dead or if it is the first step. This field is not included for non-cheap talk variants.
\end{itemize}

The agent starts with one bomb (`ammo'). Every time it lays a bomb, its ammo decreases by one. After that bomb explodes, its ammo will increase by one. The agent also has a blast strength that starts at two. Every bomb it lays is imbued with the current blast strength, which is how far in the vertical and horizontal directions that bomb will effect. A bomb has a life of ten time steps. Upon expiration, the bomb explodes and any wooden walls, agents, power-ups or other bombs within reach of its blast strength are destroyed. Bombs destroyed in this manner chain their explosions.
        
Power-Ups: Half of the wooden walls have hidden power-ups that are revealed when the wall is destroyed. These are:
\begin{itemize}
    \item \emph{Extra Bomb}: Picking this up increases the agent's ammo by one.
    \item \emph{Increase Range}: Picking this up increases the agent's blast strength by one.
    \item \emph{Can Kick}: Picking this up permanently allows an agent to kick bombs by moving into them. The bombs travel in the direction that the agent was moving at one unit per time step until they are impeded either by a player, a bomb, or a wall.
\end{itemize}

\subsection{Early results}
The environment has been public since late February and the competitions were first announced in late March. In that time, we have seen a strong community gather around the game, with more than 500 people in the Discord server (https://discord.gg/mtW7kp) and more than half of the repository commits from open source contributors.

There have also been multiple published papers using Pommerman \cite{2018arXiv180706919R,zhou2018pommermanagent}. These demonstrate that the environment is challenging and we do not yet know what are the optimal solutions in any of the variants. In particular, the agents in \cite{2018arXiv180706919R} discover a novel way of playing where they treat the bombs as projectiles by laying, then kicking them at opponents. This is a strategy that not even novice humans attempt, yet the agents use it to achieve a high success rate.

Preliminary analysis suggests that the game can be very challenging for reinforcement learning algorithms out of the box. Without a very large batch size and a shaped reward \cite{ng1999policy}, neither of Deep Q-Learning \cite{mnih2013playing} nor Proximal Policy Optimization \cite{ppo} learned to successfully play the game against the default learning agent (`SimpleAgent'). One reason for this is because the game has a (previously mentioned) unique feature in that the bomb action is highly correlated with losing but must be wielded effectively to win.

We also tested the effectiveness of DAgger \cite{daume2009search} in bootstrapping agents to match the SimpleAgent. We found that, while somewhat sensitive to hyperparameter choices, it was nonetheless effective at yielding agents that could play at or above the FFA win rate of a single SimpleAgent ($\sim20\%$). This is less than chance because four simple agents will draw a large percentage of the time.

