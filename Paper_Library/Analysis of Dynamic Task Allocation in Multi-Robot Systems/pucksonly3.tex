\section{Analysis of Dynamic Task Allocation}
\label{sec:analysis}

As proposed in the previous section,  a robot may be able to adapt
to a changing environment in the absence of complete global
knowledge if it is  able to make and remember local observations
of the environment. In the treatment below we assume that there
are two types of tasks
---  arbitrarily referred to as $Red$ and $Green$. This simplification is for pedagogical reason only; the model
can be extended to a greater number of task types.


During a sufficiently short time interval, each robot can be
considered to belong to the $Green$ or $Red$ task state. This is a
very high level, coarse-grained description.  In reality, each
state is composed of several robot actions and behaviors, for
example, searching for new tasks, detecting and executing them,
avoiding obstacles, {\em etc}. However, since we want the model to
capture how the fraction of robots in each task state evolves in
time, it is a sufficient level of abstraction to consider only
these two states. If we find that additional levels of detail are
required to explain system behavior, we can elaborate the model by
breaking each of the high level states into its underlying
components.

\subsection{Observations of Tasks Only}
\label{sec:pucksonly}

In this section we study dynamic task allocation mechanism in
which robots make decisions to switch task states based solely on
observations of available tasks. Let $m_r$ and $m_g$ be  the
numbers of the observed $Red$ and $Green$ tasks, respectively, in
a robot's memory or history window.  The robot chooses to change
its state, or the type of task it is assigned to execute, with
probabilities given by transition functions $f_{g \rightarrow
r}(m_r,m_g)$ (probability of switching to $Red$ from $Green$) and
$f_{r \rightarrow g}(m_r,m_g)$ (probability of switching to
$Green$ from $Red$). We would like to define transition rules so
that the fraction of time the robot spends in the $Red$ ($Green$)
state be equal to the fraction of $Red$ ($Green$) tasks. This will
assure that on average the number of $Red$  and $Green$ robots
reflect the desired task distribution. Clearly, if the robots have
global knowledge about the numbers of $Red$ and $Green$ tasks
$M_r$ and $M_g$, then each robot could choose each state with
probability equal to the fraction of the tasks of corresponding
type. Such global knowledge is not available; hence, we want to
investigate how incomplete knowledge of the environment (through
local observations), as well as the dynamically changing
environment (e.g., changing ratio of $Red$ and $Green$ tasks),
affects task allocation.

\subsubsection{Modelling Robot Observations}

As explained above, the transition rate between task execution
states depends on robot's observations stored in its history. In
our model we assume that a robot makes an observation of a task
with a time period $\tau$. For simplicity, by an observation we
mean here detecting a task, such as a target to be monitored, mine
to be cleared or an object to be gathered. Therefore, observation
history of length $h$ comprises of the number of $Red$ and $Green$
tasks a robot has observed during a time interval ${h}\tau$. We
assume that $\tau$ has unit length and drop it. The process of
observing a task is given by a Poisson distribution with rate
$\lambda = \alpha M^0$, where $\alpha$ is a constant
characterizing the physical parameters of the robot such as its
speed, view angles, etc., and $M^0$ is the number of tasks in the
environment. This simplification is based on the idea that robot's
interactions with other robots and the environment are independent
of the robot's actual trajectory and are  governed by
probabilities determined by simple geometric considerations. This
simplification has been shown to produce remarkably good
agreements with experiments~\cite{MarIjsGam99,IMB2001}.

Let $M_r(t)$ and $M_g(t)$ be the number of $Red$ and $Green$ tasks
respectively (can be time dependent), and let
$M(t)=M_r(t) + M_g(t)$ be the total number of tasks. The
probability that in the time interval $[t-{h}, t]$ the robot has
observed exactly $m_r$ and $m_g$ tasks is the product of two
Poisson distributions:
\begin{equation}
\label{eq:poisson} P(m_r,m_g) = \frac{\lambda_r^{m_r}
\lambda_g^{m^g}}{m_r! m_g!}e^{- \lambda_r-\lambda_g}
\end{equation}
where $\lambda_i$~, $i=r,g$, are the means of the respective
distributions. If the task distribution does not change in time,
$\lambda_i = \alpha M_i {h}$. For time dependent task
distributions, $\lambda_i = \alpha \int_{t-{h}}^tdt'M_i(t')$.


\subsubsection{Individual Dynamics: The Stochastic Master Equation}

Let us consider a single robot that has to decide between
executing  $Red$ and $Green$ tasks in a closed arena and makes a
transition to $Red$ and $Green$ states according to its
observations. Let $p_r(t)$ be the probability that a robot is in
the $Red$ state at time $t$. The equation governing its evolution
is
\begin{equation}
\label{eq:inddyn} \frac{dp_r}{dt} = \varepsilon (1-p_r) f_{g
 \rightarrow r} - \varepsilon p_r f_{r \rightarrow
g}
\end{equation}
where $\varepsilon$ is the rate at which the robot makes decisions
to switch its state, and $f_{g  \rightarrow r}$ and $f_{r
\rightarrow g}$ are the corresponding transitions probabilities
between the states. As explained above, these probabilities depend
on the robot's history --- the number of tasks of either type it
has observed during the time interval ${h}$ preceding the
transition. If the robots have global knowledge about the numbers
of $Red$ and $Green$ tasks $M_r$ and $M_g$, one could choose the
transition probabilities as the fraction of tasks of corresponding
type, $f_{g \rightarrow r}\propto M_r/(M_r+M_g)$ and $f_{r
\rightarrow g}\propto M_g/(M_r+M_g)$. In the case when the global
information is not available, it is natural to use similar
transition probabilities using robots' local estimates:
\begin{eqnarray}
\label{eq:rates} f_{g \rightarrow r}(m_r,m_g) =
\frac{m_r}{m_r+m_g}\equiv
\gamma_r(m_r,m_g) \\
\label{eq:ratesg} f_{r \rightarrow g}(m_r,m_g) =
\frac{m_g}{m_r+m_g}\equiv
 \gamma_g(m_r,m_g)
\end{eqnarray}
Note that $\gamma_r(m_r,m_g) + \gamma_g(m_r,m_g) =1$ whenever
$m_r+m_g>0$, \eg, whenever there is at least one observation in
the history window. In the case when there are no observations in
history, $m_r=m_g=0$, robots will choose either state with
probability $1/2$ as it follows from taking the appropriate limits
in Equations \ref{eq:rates} and \ref{eq:ratesg}. Hence, we
supplement \eqref{eq:rates} with $f_{g \rightarrow r}(0,0)=f_{r
\rightarrow g}(0,0)=0$ (and similarly for \eqref{eq:ratesg}) to
assure that robots do not change their state when the history
window does not contain any observations.

\eqref{eq:inddyn}, together with the transition rates shown in
Equations \ref{eq:rates}--\ref{eq:ratesg}, determines the
evolution of the probability density of a robot's state. It is a
stochastic equation since the coefficients (transition rates)
depend on random variables $m_r$ and $m_g$. Moreover, since the
robot's history changes gradually, the values of the coefficients
at different times are correlated, hence making the exact
treatment very difficult. We propose, instead, to study the
problem within the $annealed$ approximation: we neglect
time--correlation between robot's histories at different times,
assuming instead that at any time the real history $\{m_r,m_g\}$
can be replaced by a random one drawn from the Poisson
distribution \eqref{eq:poisson}. Next, we average
\eqref{eq:inddyn} over all histories to obtain
\begin{equation}
\label{eq:inddyn2} \frac{dp_r}{dt} = \varepsilon
\overline{\gamma}_r (1-n_r)  - \varepsilon \overline{\gamma}_g n_r
\end{equation}
Here $\overline{\gamma}_r$ and $\overline{\gamma}_g$ are given by
\begin{equation}
\label{eq:avg_gamma} \overline{\gamma}_r =
\sum_{r,g}P(r,g)\frac{r}{r+g},  \overline{\gamma}_g =
\sum_{r,g}P(r,g)\frac{g}{r+g}
\end{equation}
where $P(m_r,m_g)$ is the Poisson distribution \eqref{eq:poisson}
and the summation excludes the term $r=g=0$. Note that if the
distribution of tasks changes in time, then
$\overline{\gamma}_{r,g}$ are time-dependent,
$\overline{\gamma}=\overline{\gamma}_{r,g}(t)$.

To proceed further, we need to evaluate the summations in
\eqref{eq:avg_gamma}. Let us  define an auxiliary function
\begin{eqnarray}
\label{eq:aux} F(x) =
\sum_{m_r=0}^{\infty}\sum_{m_g=0}^{\infty}{x^{m_r+m_g} \frac
{\lambda_r^{m_r} \lambda_g^{m_g}} {m_r! m_g!}
e^{-\lambda_r}e^{-\lambda_g}\frac {m_r} {m_r+m_g}}
\end{eqnarray}
It is easy to check that $\overline{\gamma}_{r,g}$ are given by
\begin{eqnarray}
\label{eq:aux1} \overline{\gamma}_r &=& F(1) - \frac{1}{2}P(0,0)
= F(1) - \frac{1}{2} e^{\alpha {h} M_0} \nonumber \\
\overline{\gamma}_g &=& 1-F(1) -\frac{1}{2}e^{\alpha {h} M_0}
\end{eqnarray}
Differentiating \eqref{eq:aux} with respect to $x$ yields
\begin{equation}
\label{eq:aux2} \frac{dF}{dx} =
\sum_{m_r=1}^{\infty}\sum_{m_g=0}^{\infty}x^{m_r+m_g-1}\frac{\lambda_r^{m_r}
\lambda_g^{m_g}}{m_r! m_g!}e^{- \lambda_r}e^{-\lambda_g}m_r
\end{equation}
Note that the summation over $m_r$ starts from $m_r=1$. Clearly,
 the sums over $m_r$ and $m_g$ are de--coupled thanks to the
cancellation of the denominator $(m_r+m_g)$:
\begin{equation}
\label{eq:aux3} \frac{dF}{dx} = \biggl( e^{-
\lambda_r}\sum_{m_r=1}^{\infty}x^{m_r-1}\frac{\lambda_r^{m_r}
}{m_r!}m_r \biggr )\biggl( e^{-
\lambda_g}\sum_{m_g=0}^{\infty}\frac{(x \lambda_g)^{m_g} }{m_g!}
\biggr )
\end{equation}
The resulting sums are evaluated easily (as the Taylor expansion
of corresponding exponential functions), and the results is
\begin{equation}
\label{eq:diff} \frac{dF}{dx} = \lambda_r e^{- \lambda_0(1-x)}
\end{equation}
where $\lambda_0 = \lambda_r+\lambda_g$. After elementary
integration of \eqref{eq:diff} (subject to the condition $F(0) =
1/2$),  we obtain using \eqref{eq:aux2} and the expressions for
$\lambda_r$, $\lambda_0$:
\begin{equation}
\label{eq:gamma} \overline{\gamma}_{r,g}(t) = \frac{1-e^{\alpha
{h} M_0}}{{h} }\int_{t-{h}}^t dt^{\prime}\mu_{r,g}(t^{\prime})
\end{equation}
Here $\mu_{r,g}(t)=M_{r,g}(t)/M_0$ are the fraction of $Red$ and
$Green$ tasks respectively.

Let us first consider the case when the task distribution does not
change with time, \ie, $\mu_r(t)=\mu_0$. Then we have
\begin{equation}
\label{eq:gamma-constant} \overline{\gamma}_{r,g}(t)
=(1-e^{-\alpha {h} M_0})\mu_{r,g}^0
\end{equation}
The solution of \eqref{eq:inddyn2} subject to the initial
condition $p_r(t=0)=p_0$ is readily obtained:
\begin{equation}
\label{eq:solution} p_r(t) = \mu_r^0 + \biggl ( p_0 -
\frac{\overline{\gamma}_r}{\overline{\gamma}_r +
\overline{\gamma}_g}\biggr )e^{ - \varepsilon (\overline{\gamma}_r
+ \overline{\gamma}_g)t}
\end{equation}
One can see that the probability distribution approaches  the
desired steady state value $p_r^s = \mu_r^0$ exponentially. Also,
the coefficient of the exponent depends on the density of tasks
and the length of the history window. Indeed, it is easy to check
that $\overline{\gamma}_r + \overline{\gamma}_g = 1-e^{-\alpha
{h} M_0}$. Hence,   for large enough $M_0$  and ${h}$, $\alpha
{h} M_0 \gg 1$, the convergence rate is determined solely by
$\varepsilon$. For a small task density or short history
length, on the other hand, the convergence rate is proportional to
the number of tasks, $ \varepsilon ( 1-e^{-\alpha {h} M_0})\sim
\varepsilon \alpha {h} M_0$. Note that this is a direct
consequence of the rule that robots do not change their state
whenever there are no observation in the history window.


Now let us consider the case where the task distribution changes
suddenly at time $t_0$, $\mu_r(t)= \mu_r^0 + \Delta \mu
\theta(t-t_0)$, where $\theta(t-t_0)$ is the step function. For
simplicity, let us assume that $\alpha {h} M_0 \gg 1$ so that the
exponential term in \eqref{eq:gamma} can be neglected,
\begin{equation}
\label{eq:gamma-timedep} \overline{\gamma}_{r,g}(t) =
\frac{1}{{h} }\int_{t-{h}}^t dt^{\prime}\mu_{r,g}(t^{\prime}),
\overline{\gamma}_{r}(t) + \overline{\gamma}_{g} =1
\end{equation}


Replacing \eqref{eq:gamma-timedep} into \eqref{eq:inddyn2}, and
solving the resulting  differential equation yields
\begin{eqnarray}
\label{eq:jump} p_r(t)& =& \mu_r^0 + \frac{\Delta\mu}{{h}}t
-\frac{\Delta\mu}{\varepsilon {h}}(1-e^{-\varepsilon
t}),~~~~~~~~~~t\leq {h} \nonumber \\
p_r(t)& =& \mu_r^0 + \Delta\mu -\frac{\Delta\mu}{\varepsilon
{h}}(e^{-\varepsilon (t-{h})} -e^{-\varepsilon t} ),~~~t>{h}
\,.
\end{eqnarray}
\eqref{eq:jump} describes how the robot distribution converges to the
new steady state value after the change in task distribution.
Clearly, the convergence properties of the solutions depend on
${h}$ and $\varepsilon$. It is easy to see that in the limiting
case $\varepsilon {h} \gg 1$ the new steady state is attained
after time ${h}$, $|p_r({h})-(\mu_0 + \Delta \mu)| \sim \Delta
\mu/(\varepsilon {h})\ll 1$, so the convergence time is
$t_{conv}\sim {h}$. In the other limiting case $\varepsilon {h}
\ll 1$, on the other hand, the situation is different. A simple
analysis of \eqref{eq:jump} for $t>{h}$ yields $|p_r(t)-(\mu_0
+ \Delta \mu)| \sim \Delta \mu e^{-\varepsilon t}$ so the
convergence is exponential with characteristic time $t_{conv} \sim
1/\varepsilon$.



\subsubsection{Collective Behavior}
In order to make predictions about the behavior of an MRS using a
dynamic task allocation mechanism, we need to develop a
mathematical model of the collective behavior of the system. In
the previous section we derived a model of how an individual
robot's behavior changes in time. In this section we extend it to
model the behavior of a MRS. In particular, we study the
collective behavior of a homogenous system consisting of $N$
robots with identical controllers. Mathematically, the MRS is
described by a probability density function that includes the
states of all $N$ robots. However, in most cases we are interested
in studying the evolution of global, or average, quantities, such
as the average number of robots in the $Red$ state, rather than
the exact probability density function. This applies when
comparing theoretical predictions with results of experiments,
which are usually quoted as an average over many experiments.
Since the robots in either state are independent of each other,
$p_r(t)$, is now the fraction of robots in the $Red$ state, and
consequently $Np_r(t)$ is the average number of robots in that
state. The results of the previous section, namely solutions for
$p_r(t)$ for constant task distribution (\eqref{eq:solution}) and
for changing task distribution (\eqref{eq:jump}), can be used to
study the average collective behavior. \secref{sec:results1}
presents results of analysis of the mathematical model.


\subsubsection{Stochastic Effects}
\label{sec:stochastic1}
In some cases it is useful to know the probability distribution of
robot task states over the entire MRS. This probability function describes
the exact collective behavior from which one could derive the
average behavior as well as the fluctuations around the average.
Knowing the strength of fluctuations is necessary for assessing how
the probabilistic nature of robot's observations and actions affects the global
properties of the system. Below we consider the problem of finding
the probability distribution of the collective state of the
system.

Let $P_n(t)$ be the probability that there are exactly $n$ robots
in the $Red$ state at time $t$. For a sufficiently short time
interval $\Delta t$ we can write~\cite{Lerman03iros}
\begin{equation}
\label{eq:master1} P_n(t+\Delta t) = \sum_{n^{\prime}}
W_{n^{\prime} n}(t;\Delta t)P_{n^{\prime}}(t) -\sum_{n^{\prime}}
W_{n n^{\prime}}(t;\Delta t)P_{n}(t)
\end{equation}
where $W_{i j}(t;\Delta t)$ is the transition probability between
the states $i$ and $j$ during the time interval $(t, t+\Delta t)$.
In our MRS, this transitions correspond to robots changing their
state from $Red$ to $Green$ or vice versa. Since the probability
that more than one robot will have a transition during a time
interval $\Delta t$ is $O(\Delta t)$, then, in the limit $\Delta t
\rightarrow 0$ only transitions between neighboring states are
allowed in \eqref{eq:master1}, $n \rightarrow n \pm 1$. Hence, we
obtain

\begin{equation}
\label{eq:master2} \frac{dP_n}{dt} = r_{n+1}P_{n+1}(t) +
g_{n-1}P_{n-1}(t) - (r_n+g_n)P_n(t) \,.
\end{equation}
Here $r_k$ is the probability density of having one of the $k$
$Red$ robots change its state to $Green$, and $g_k$ is the
probability density of having one of the $N-k$ $Green$ robots
change its state to $Red$. Let us assume again that $\alpha {h}
M_0 \gg 1$ so that $\overline{\gamma}_g = 1-\overline{\gamma}_r$.
Then one has
\begin{equation}
r_k = k  (1 - \overline{\gamma}_r ) \ , \ g_k = (N-k)
\overline{\gamma}_r
\end{equation}
with $r_0=g_{-1}=0$, $r_{N+1}=g_N = 0$. $\overline{\gamma}_r$ is
history-averaged transition rate to $Red$ states.

The steady state solution of \eqref{eq:master2} is given by
\cite{VanKampen}
\begin{equation}
P_n^s = \frac{g_{n-1}g_{n-2}...g_1g_0}{r_nr_{n-1}...r_2r_1}P_0^s
\end{equation}
where $P_0^s$ is determined by the normalization:
\begin{equation}
P_0^s = \left[ 1+ \sum_{n=1}^{N}
\frac{g_{n-1}g_{n-2}...g_1g_0}{r_nr_{n-1}...r_2r_1} \right]^{-1}
\end{equation}
Using the expression for $\overline{\gamma}$,  after
some algebra we obtain
\begin{equation}
\label{eq:Pn} P_n^s = \frac{N!}{(N-n)!n!}
\overline{\gamma}_r^n(1-\overline{\gamma}_r)^{N-n}
\end{equation}
e.g., the steady state is a binomial distribution with parameter
$\overline{\gamma}$. Note again that this is a direct consequence
of the independence of the robots' dynamics. Indeed, since the
robots act independently, in the steady state each robot has the
same probability of being in either state. Moreover, using this
argument it becomes clear that the time-dependent probability
distribution $P_n(t)$ is given by \eqref{eq:Pn} with
$\overline{\gamma}$ replaced by  $p_r(t)$, \eqref{eq:solution}.
