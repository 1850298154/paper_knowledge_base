\subsection{Observations of Tasks and Robots}
\label{sec:phenomenological} In this section we study the more
complex dynamic task allocation mechanism in which robots make
decisions to change their state based on the observations of not
only available tasks but also on the observed task states of other
robots. Specifically, each robot now records the numbers and types
of task as well as the numbers and task types of robots it has
encountered. Again, we let $m_r$ and $m_g$ be the number of tasks
of $Red$ and $Green$ type, and $n_r$ and $n_g$ be the number of
robots in $Red$ and $Green$ task state in a robot's history
window. The probabilities for changing a robot's state are again
given by transition functions that now depend on the fractions of
observed tasks and robots of each type: $\hat{m}_r=m_r/(m_r+m_g)$,
$\hat{m}_g=m_g/(m_r+m_g)$, $\hat{n}_r=n_r/(n_r+n_g)$, and
$\hat{n}_g=n_g/(n_r+n_g)$. In our previous
work~\cite{Lerman03iros} we showed that in order to achieve the
desired long term behavior for task allocation (\ie, in the steady
state the average fraction of $Red$ and $Green$ robots is equal to
the fraction of $Red$ and $Green$ tasks respectively), the
transition rates must have the following functional form:
\begin{eqnarray}
\label{eq:fR}
f_{g \rightarrow r} (\hat{m}_r,\hat{n}_r) &=&\hat{m}_rg(\hat{m}_r-\hat{n}_r),\\
\label{eq:fG} f_{r \rightarrow g} (\hat{m}_r,\hat{n}_r)
&=&\hat{m}_g g(\hat{m}_g-\hat{n}_g) \equiv (1-\hat{m}_r)
g(-\hat{m}_r+\hat{n}_r) .
\end{eqnarray}
Here $g(z)$ is a continuous, monotonically increasing function of
its argument defined on an interval $[-1,1]$. In this paper we
consider the following forms for $g(z)$:
\begin{itemize}
\item {\em Power:}
$g(z)=100^{z}/100$
\item  {\em Stepwise linear:} $g(z)=z \Theta(z)$.\footnote{The step function $\Theta$ is defined as
$\Theta(z)=1$ if $z \geq 0$; otherwise, it is $0$. The step
function guarantees that no transitions to $Red$ state occur when
$ {m}_r < {n}_r$.}
\end{itemize}

To analyze this task allocation model, let us again consider a
single robot that searches for tasks to perform and makes a
transition to $Red$ and $Green$ states according to transition
functions defined above. Let $p_r(t)$ be the probability that the
robot is in the $Red$ state at time $t$, with \eqref{eq:inddyn}
governing its time evolution. Note that $p_r(t)$ is also the
average fraction of $Red$ robots, $p_r(t) = N_r(t)/N$.
%
\comment{
The equation governing the evolution of $p_r(t)$ reads
%\begin{equation}
%\label{eq:inddyn3} \frac{dp_r}{dt} = \varepsilon (1-p_r) f_{g
%\rightarrow r} - \varepsilon p_r f_{r \rightarrow g}
%\end{equation}
Here, again, $\varepsilon$ is the rate at which the robot makes
decisions to switch its state}

As in the previous case, the next step of the analysis is
averaging over the the robot's histories, \ie, $\hat{m}_r$ and
$\hat{n}_r$. Note that a robot's observations of available tasks
can still be modeled by a Poisson distribution similar to
\eqref{eq:poisson}. However, since the number of robots of each
task state changes stochastically in time, the statistics of $n_r$
and $n_g$
 should be modeled as a doubly stochastic
Poisson process (also called Cox process) with stochastic rates.
This would complicate the calculation of the average over $\hat{n}_r =
n_r/(n_r+n_g)$ and require mathematical details that go well
beyond the scope of this paper. Fortunately, as we demonstrated in
the previous section, if a robot's observation window contains
many readings, then the estimated fraction of task types is
exponentially close to the average of the Poisson distribution.
This suggests that for sufficiently high densities of tasks and
robots we can neglect the stochastic effects of modeling
observations for the purpose of our analysis, and replace the
robot's observation by their average (expected) values. In other
words, we use the following approximation:
\begin{eqnarray}
\label{eq:nr2}
\hat{n}_r &\approx& \frac{1} {h} \int_{t-h}^{t}{p_r(t')}dt'\\
\label{eq:mr2} \hat{m}_r &\approx& \frac{1} {h}
\int_{t-h}^{t}{\mu_r(t')}dt' .
\end{eqnarray}

The  Equations~\ref{eq:inddyn}, ~\ref{eq:nr2}, and~\ref{eq:mr2}
are a system of integro--differential equations that uniquely
determine the dynamics of $p_r(t)$. In the most general case it is
not possible to obtain solutions by analytical means, hence one
has to solve the system numerically. However, if the task density
does not change in time, we can still perform steady state
analysis. Steady state analysis looks for long-term solutions that
do not change in time, \ie, $d p_r/dt=0$. Let $\mu_r^0$ be the
density of $Red$ tasks, and $p_0=p_r(t \rightarrow \infty)$ be the
steady state value, so that $\hat{m}_r = \mu_r^0$, $\hat{n}_r =
p_r^0$. Then, by setting left hand side of \eqref{eq:inddyn} to
zero, we get

\begin{equation}
\label{eq:steady} (1-p_0) \mu_r^0 g(\mu_r^0-p_0) = p_0 (1-\mu_r^0)
g(-\mu_r^0+p_0)
\end{equation}

Note that $p_0=\mu_r^0$ is a solution to \eqref{eq:steady} so that
in the steady state the fraction of $Red$ robots equals the fraction
of red tasks as desired. To show that this is the only solution,
we note that for a fixed $\mu_r^0$ the right- and left-hand sides
of the equation are monotonically increasing and decreasing
functions of $p_0$ respectively, due to the monotonicity of $g(z)$.
Consequently, the two curves can meet only once and that proves the
uniqueness of the solution.




\subsubsection{Phenomenological Model}
\label{sec:stochastic}

Exact stochastic models of task allocation can quickly become
analytically intractable, as we saw above. Instead of exact
models, it is often more convenient to work with the so-called
Rate Equations model. These equations can be derived from the
exact stochastic model by appropriately averaging
it~\cite{Lerman03iros}; however, they are often (see, for example,
population dynamics~\cite{Haberman}) phenomenological, or \emph{ad
hoc}, in nature --- constructed by taking into account the
system's salient processes. This approach makes a number of
simplifying assumptions: namely, that the system is uniform and
dilute (not too dense), that actions of individual entities are
independent of one another, that parameters can be represented by
their mean values and that system behavior can be described by its
average value. Despite these simplifications, resulting models
have been shown to correctly describe dynamics of collective
behavior of robotic systems~\cite{Lerman04sab}. Phenomenological
models are useful for answering many important questions about the
performance of a MRS, such as, does the steady state exist, how
long does it take to reach it, and so on. Below we present a
phenomenological model of dynamic task allocation.


Individual robots are making their decisions to change task state
probabilistically and independently of one another. A robot will
change state from $Green$ to $Red$ with probability $f_{g
\rightarrow r}$ and with probability $1-f_{g \rightarrow r}$ it
will remain in the $Green$ state. We can succinctly write $\Delta
N_{g \rightarrow r}$  and $\Delta N_{r \rightarrow g}$, the number
of robots that switch from $Green$ to $Red$ and \emph{vice versa}
during a sufficiently small time interval $\Delta t$, as
\begin{eqnarray}
\Delta N_{g \rightarrow r}& =&\sum_{i=1}^{N_g}{x_i \big(f_{g
\rightarrow r} \delta(x_i-1)+(1- f_{g \rightarrow r})
\delta(x_i)\big)}
\nonumber\\
\Delta N_{r \rightarrow g}&=&\sum_{i=1}^{N_r}{(1-x_i) \big(f_{r
\rightarrow g} \delta(x_i)+(1-f_{r \rightarrow g})
\delta(x_i-1)\big)}\,. \nonumber
\end{eqnarray}
Here we introduced a state variable $x_i$, such that $x_i=1$ when a robot is in the $Green$ state,
and $x_i=0$ when a robot is in the $Red$ state. $\delta(x)$ is Kronecker delta, defined as $\delta(x)=1$ when $x=0$ and $\delta(x)=0$ otherwise.  Therefore, $\Delta N_{g \rightarrow r}$
is a random variable from a binomial distribution specified by a mean $\mu=f_{g \rightarrow r} N_g$ and variance
$\sigma^2=f_{g \rightarrow r} (1-f_{g \rightarrow r}) N_g$.
Similarly, the distribution of the random variable $\Delta N_{r
\rightarrow g}$ is specified by mean $\mu=f_{r \rightarrow g} N_r$
and variance $\sigma^2=f_{r \rightarrow g} (1-f_{r \rightarrow g})
N_r$.


During a time interval $\Delta t$ the total number of robots in
$Red$ and $Green$ task states will change as individual
robots make decisions to change states. The following finite
difference equation specifies how the number of $Red$ will change on average:
\begin{equation}
\label{eq:stochastic} N_r(t+\Delta t)  = N_r(t) + \varepsilon
\Delta N_{g \rightarrow r} \Delta t - \varepsilon  \Delta N_{r
\rightarrow g} \Delta t \nonumber
\end{equation}
Rearranging the equation and taking the continuous time limit
($\Delta t \rightarrow 0$) yields a differential Rate Equation
that describes time evolution of the number of $Red$ robots. By
taking the means of $\Delta N$'s as their values, we recover
\eqref{eq:inddyn}.


Keeping $\Delta N$'s as random variables allows us to study the
effect the probabilistic nature of the robots' decisions have on
the collective behavior.\footnote{Note that we do not model here
the effect of observation noise due to uncertainty in sensor
readings and fluctuations in the distribution of tasks.} We solve
\eqref{eq:stochastic} by iterating it in time and drawing $\Delta
N$'s at random from their respective distributions. The solutions
are subject to the initial condition $N_r(t \leq 0)=N$ and specify
the dynamics of task allocation in robots.



%%%%%%%%%%%%%%%%%%%%%%%

Functions $f_{g \rightarrow r}$ and $f_{r \rightarrow g}$ are
calculated using
estimates of the densities of $Red$ tasks ($m_r$) and robots in $Red$ state ($n_r$)
from the observed counts stored in the robot's history window.

% collective transition rates
Transition rates  $f_{g \rightarrow r}$ and $f_{r \rightarrow g}$ in the model are mean values, averaged over all histories and all robots. In order to compute them, we need to aggregate observations of all robots. Suppose each robot has a history  window of length $h$.
For a particular robot $i$, the values in the most recent observational slot
are $N^0_{i,r}$, $N^0_{i,g}$, $M^0_{i,r}$ and
$M^0_{i,g}$, the observed numbers of $Red$ and $Green$ robots
and tasks respectively at time $t$. In the next latest slot, the
values are $N^1_{i,r}$, $N^1_{i,g}$, $M^1_{i,r}$ and
$M^1_{i,g}$, the observed numbers at time $t-\Delta$, and so
on. Each robot estimates the densities of $Red$ robots and tasks using the following calculation:
\begin{eqnarray}
{n}_{i,r} & = & \frac{1}{h}\sum^{h-1}_{j=0}{\frac{N^j_{i,r}}{N^j_{i,r}+N^j_{i,g}}}= \frac{1}{h}\sum^{h-1}_{j=0}{n^j_{i,r}} \\
{m}_{i,r} & = & \frac {1}{h}
\sum^{h-1}_{j=0}{\frac{M^j_{i,r}}{M^j_{i,r}+M^j_{i,g}}}=
\frac{1}{h}\sum^{h-1}_{j=0}{m^j_{i,r}}.
\end{eqnarray}


When observations of all robots are taken into account, the
mean of the observed densities of $Red$ robots at time $t$ --- $\frac{1} {N} \sum^N_{i=1}{n^0_{i,r}} $  --- will fluctuate due to observation noise, but on average it will be proportional to $N_r(t)/N$, which is the actual density of $Red$
robots at time $t$. The proportionality factor is related to
physical robot parameters, such as speed and observation area (see \secref{sec:results1}).
Likewise, the average of the observed densities at time  $t-j \Delta$ is $\frac{1} {N}
\sum^N_{i=1}{n^j_{i,r}} \propto N_r(t-j\Delta)/N$, the
 density of robots at time $t-j\Delta$.  Thus, the aggregate
estimates of the fractions of $Red$ robots and tasks are:
\begin{eqnarray}
\label{eq:2} \hat{n}_r & = & \frac{1}{N}
\sum^{N}_{i=1}{{n}_{i,r}}=
\frac{1}{Nh}\sum^{h-1}_{j=0}{N_r(t-j\Delta)} \\
\label{eq:2b}\hat{m}_r & = & \frac{1}{N}
\sum^{N}_{i=1}{{m}_{i,r}}=
\frac{1}{Mh}\sum^{h-1}_{j=0}{M_r(t-j\Delta)}
\end{eqnarray}
Robots are making their decisions asynchronously, {\em i.e.}, at
slightly different times. Therefore, the last terms in the above
equations are best expressed in continuous form: {\em e.g.}, $1/Nh
\int^0_{h}N_r(t-\tau)d\tau$ (see \eqref{eq:nr2} and
\eqref{eq:mr2}).

Estimates \eqref{eq:2} and \ref{eq:2b} can be plugged into
\eqref{eq:fR} and \eqref{eq:fG} to compute the values of
transition probabilities for any choice of the transition function
(power or linear). Once we know $f_{r \rightarrow g}$ and $f_{g
\rightarrow r}$, we can solve \eqref{eq:stochastic} to study the
dynamics of task allocation in robots. Note that
\eqref{eq:stochastic} is now a time-delay finite difference
equation, and solutions will show typical oscillations.

We solve the models presented in this section and validate their predictions in context of the multi-foraging task described next.
