%%%%%%%%%%%%%%%%%%pucks only results%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analysis and Simulations Results}
\label{sec:results}
The mathematical models developed in \secref{sec:analysis} can be directly applied to the multi-foraging task if we map $Red$ and $Green$ tasks to $Red$ and $Green$ pucks and task states of robots to their foraging states. Model parameters, such as $\varepsilon$, $\alpha$, etc, depend on physical realizations of the implementation and can be computed from details of the multi-foraging task as described below.

\subsection{Observations of Pucks Only}
\label{sec:results1} First, we study the model of dynamic task
allocation, presented in \secref{sec:pucksonly}, where robots
observe only pucks and make decision to switch foraging state
according to the transition functions given by \eqref{eq:rates}.
We compared theoretical predictions of the robots' collective
behavior with results from simulations. We used
\eqref{eq:solution} and \ref{eq:jump} to compute how the average
number of robots in the $Red$ state changes in time when the puck
distribution is suddenly changed. The parameter values were
obtained from experiments. $p_0 = 1.0$ was the initial density of
$Red$ robots (of $20$ total robots), $\mu_0=0.3$ was the initial
$Red$ puck density (of $50$ total pucks), which remained constant
until it was changed by the experimenter. The first change in puck
density was $\Delta \mu=0.5$, meaning that $80\%$ of the pucks in
the arena are now $Red$. The second change in puck density was
$\Delta \mu=-0.3$, to $50\%$ $Red$ pucks.

$\epsilon$ is the rate at which robots make decisions to switch
states. Robot traveled $2\ m$ between observations at an average
speed of $0.2\ m/s$; therefore, there are $10\ s$ between
observations, and $\varepsilon=0.1$. $h$, the history length, is
the number of pucks in the robot's memory. $\alpha M^0$ is the
rate at which robots encounter pucks. \commentout{
% continuous observations
As a robot travels through the arena, it sweeps out some area
during time $dt$ and will detect objects that fall in that area.
This detection area is $V_R W_R dt$, where $V_R=0.2\ m/s$ is
robot's average speed, and $W_R=5\ m$ is robot's detection width.
The area of the arena is $A=315\ m^2$; therefore, a robot will
detect pucks at a rate $\alpha = V_R W_R /A  = 0.003\ s^{-1}$. } A
robot makes an observation of its local environment at discrete
time intervals. The area visible to the robot is $A_{vis}=(5\ m)^2
\pi/6 =13.09$, with $1/6$ coming from the $60^o$ angle of view.
The arena area is $A=315\ m^2$; therefore,  $\alpha
M^0=A_{vis}M^0/A=2.1$. We studied the dynamics of the system for
different history lengths $h$.



\begin{figure}[tbhp]
\includegraphics[width=1.0\textwidth]{pucksonly2.eps}
\caption{Evolution of the fraction of $Red$ robots for different
history lengths. Robots' decision to change state is based on observations of pucks only.} \label{fig:puckonly}
\end{figure}

\figref{fig:puckonly} shows evolution of the numbers of $Red$
robots for different history lengths. Initially, the distribution
of $Red$ pucks is set to $30\%$ and all the robots are in the
$Red$ foraging state. At $t=500\ s$, the puck distribution changes
abruptly to $80\%$, and at $t=1000\ s$ to $50\%$. The solid line
shows results of simulations --- the fraction of $Red$ robots,
averaged over 10 runs. The dashed line gives theoretical
predictions for the parameters quoted above.
 Since we are in the $\varepsilon h \gg
1$ limit (for $h=50,100$), the time it takes to converge to the
steady state is linear in history length, $t_{conv}\sim h$, as
predicted by \eqref{eq:jump}. The agreement between theoretical
and experimental results is excellent. We stress that there are no
free parameters in the theoretical predictions --- only
experimental values of the parameters were used in producing these
plots.

In addition to being able to predict the average collective
behavior of the multi-robot system, we can also quantitatively
characterize the amount of fluctuations in the system.
Fluctuations are deviations from the steady state (after the
system has converged to the steady state) that arise from the
stochastic nature of robot's observations and decisions.
  These deviations result in fluctuations
from the desired global distribution of $Red$ and $Green$ robots
seen in an individual experiment. One can suppress these
fluctuations by averaging results of many identical experiments.

\begin{figure}[tbhp]
\center{
\includegraphics[width=0.5\textwidth]{fluctuations.eps}
\caption{Histogram of the fraction of $Red$ robots in the steady
state for three different puck distributions (data for $h=10$).
$\mu_0$ specifies fraction of $Red$ pucks. Lines are theoretical
predictions of the distribution of $Red$ robots.} }
\label{fig:fluctuations}
\end{figure}

To measure the strength of the fluctuations, we take data from an
individual experimental run and extract the fraction of $Red$
robots, after the system has converged to the steady state, for
each of the three $Red$ puck distributions: $\mu_0=30\%,\ 50\%,\
80\%$. Because the runs were relatively short, we only have $300\
s$ worth of data (30 data points) in the converged state; however,
since each experiment was repeated ten times, we make the data
sets longer by appending data from all experiments. In the end, we
have 300 measurements of the steady state $Red$ robot density for
three different puck distributions. \figref{fig:fluctuations}
shows the histogram of robot distributions for three different
puck distributions. The solid lines are computed using
\eqref{eq:Pn}, where for $\overline{\gamma}$ we used the actual
means of the steady state distributions ($\overline{\gamma}=0.28$,
$0.47$ and $0.7$ for $\mu_0=30\%$, $50\%$ and $80\%$
respectively). We can see from the plots that the theory correctly
predicts the strength of fluctuations about the steady state. As
is true of binomial distributions, the fluctuations (measured by
the variance) are greatest for cases where the numbers of $Red$
and $Green$ pucks are comparable ($\mu_0=50\%$) and smaller when
their numbers are very different ($\mu_0=80\%$).

\subsection{Observations of Pucks and Robots}
\label{sec:results2} In this section we study the dynamic task
allocation model developed in \secref{sec:phenomenological}, in
which robots use observations of pucks and other robots' foraging
states to make decision to change their own foraging state.


\begin{figure}[tbhp]
\begin{tabular}{c}
\includegraphics[width=0.9\textwidth]{newnoise-lin.eps}
\\
(a) Linear transition function
\\
\includegraphics[width=0.9\textwidth]{newnoise-pow.eps}
\\
(b) Power transition function
\end{tabular}
 \caption{Evolution of the fraction of $Red$ robots for
different history lengths and transition functions, compared to
predictions of the model} \label{fig:noise}
\end{figure}


\figref{fig:noise} shows results of embodied simulations (solid
lines) as well as solutions to the model \eqref{eq:stochastic} (dashed lines) for
different values of robot history length and forms of transition
function (given by \eqref{eq:fR} and \ref{eq:fG}, with $g(z)$ linear or power function). Initially, the $Red$ puck fraction (dotted line) is
$30\%$. It is changed abruptly at $t=500\ s$ to $80\%$ and then
again at $t=2000\ s$ to $50\%$. Each solid line showing $Red$
robot density has been averaged over 10 runs. We rescale the dimensionless time of the model by
parameter $10$, corresponding $\varepsilon=0.1$. The history
length was the only adjustable parameter used in solving the equations.
The values of $h$ used to compute the observed fraction of $Red$
robots $n_r$ in \eqref{eq:2} were $h=2,\ 8,\ 16$, corresponding
to experimental history lengths $10,\ 50,\ 100$ respectively. For
$m_r$, the observed fraction of $Red$ pucks, we used their actual
densities.

In order to explain the difference in history lengths between
theory and experiment, we note that in the simulation experiments, the history
length means the numbers of observed robots and pucks, while in
the model, it means the number of observations, with multiple
objects sighted within a single observation. According to
calculations in \secref{sec:results1}, a robot observes about 2
pucks in a single observation. Moreover, the robot travels $2\ m$
between observations, yet it sees $5\ m$ out during each
observation, meaning that individual observations will be
correlated. Observations will be further correlated because of the
pattern of a robot's motion --- as the robot moves in a straight
line towards a goal, it is likely to observe overlapping regions of
the arena. These considerations could explain the factor of five difference between the history lengths used in the
experiments and the corresponding values used in the model. More
detailed experiments, for example, ones in which robots travel
farther between observations, are necessary to explain these
differences.


Solutions exhibit oscillations, although eventually oscillations
decay and solutions relax to their steady state values. In all
cases, the steady state value is the same as the fraction of red
pucks in the arena. History-induced oscillations are far more
pronounced for the linear transition function
(\figref{fig:noise}(a)) than for the power transition function
(\figref{fig:noise}(b)). For the power transition function, these
oscillations are present but become evident only for longer
history lengths. This behavior is probably caused by the
differences between the values of transition functions near the
steady state: while the value of the power transition function
remains small near the steady state, the value of the linear
transition function grows linearly with the distance from the
steady state, thereby amplifying any deviations from the steady
state solution. The amplitude and period of oscillations and the
convergence rate of solutions to the steady state all depend on
history length, and it generally takes longer to reach the steady
state for longer histories. Another conclusion is that the linear
transition function converges to the desired distribution faster
than the power function, at least for moderate history lengths.
