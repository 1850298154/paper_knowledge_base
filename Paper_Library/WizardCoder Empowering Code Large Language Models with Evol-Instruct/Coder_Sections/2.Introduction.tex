% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{Coder_Figures/Code-Evol-Instruct.pdf}
%     \caption{A brief introduction of our \cname{} to evolve the easy instructions to be harder.}
%     \label{fig:evol_instruct}
% \end{figure}

\section{Introduction}

Recently, Large Language Models (LLMs)~\citep{GPT3,GPT4,PaLM,palm2,Chinchilla,gopher,GLM-130B,opt,llama} have garnered immense attention and demonstrated impressive success. Notably, OpenAI's GPT3.5 (ChatGPT) stands out as a prominent example. These models, through extensive pre-training on vast internet data and fine-tuning with detailed instruction data~\citep{DBLP:conf/nips/Ouyang0JAWMZASR22}, have achieved state-of-the-art (SOTA) zero-shot performance across diverse NLP tasks. This trend also extends to the realm of code understanding and generation, where a multitude of Code LLMs have emerged~\citep{codex,AlphaCode,incoder,codegen,CodeGeeX,codet5,CodeT5+,li2023starcoder,codegen2,codellama}. These models, pre-trained on substantial code data, excel in various code-related tasks, consistently delivering impressive performance.

In contrast to most previous Code LLMs that primarily focus on the pre-training process, there has been limited exploration of fine-grained instruction tuning in the code domain. The introduction of instruction tuning was initially designed to enhance the generalization capabilities of LMs across different tasks via multitask training~\citep{t5,DBLP:conf/iclr/WeiBZGYLDDL22,flan-t5,ExT5,T0,ZeroPrompt,UnifiedQA}. OpenAI's InstructGPT~\citep{DBLP:conf/nips/Ouyang0JAWMZASR22}, for instance, involved soliciting human annotators to provide explicit instructions to ensure alignment with users' intentions. Similarly, recent works such as Alpaca~\citep{alpaca} employed the self-instruct~\citep{wang2022self} method, where GPT3.5 (ChatGPT) generated the instruction data. Vicuna~\citep{vicuna2023} utilized user-shared conversations collected from ShareGPT.com. %Modified%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
WizardLM~\citep{xu2023wizardlm} introduces the \name{} method, which involves evolving existing general instruction data to generate more complex and diverse datasets. Drawing inspiration from these previous works in the general domain, our work, \cname{}, is specifically tailored to the coding domain's distinctive characteristics.

In this study, we aim to enhance the capabilities of the SOTA open-source Code LLMs (i.e., StarCoder and CodeLlama), by introducing our novel \cname{}. The motivation of this fine-grained instruction-tuning method in the code domain is to automatically increase the complexity of code instruction data, so as to make the best of the internal coding ability of the Code LLMs.
%Modified%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Our \cname{} incorporates several novel methods, including heuristics tailored to coding task features, adversarial sample heuristics, time/space complexity requirements, and evolving stop controls.
The whole process includes two steps: initially, we apply our \cname{} to evolve basic code instruction data, specifically Code Alpaca~\citep{codealpaca}. Subsequently, we fine-tune StarCoder and CodeLlama using our newly generated code instruction-following training set, resulting in our \modelname{} models.


% In this work, we aim to enhance the capabilities of the SOTA open-source Code LLM (i.e., StarCoder), by introducing our novel \cname{}, a fine-grained instruction tuning method in the Code domain, which automatically increases the complexity of the code instruction data. Different from the general domain's \name{}, we have made several adaptations to the evolutionary prompt process tailored specifically for code-related tasks. These modifications include refining the evolutionary instructions, simplifying the form of evolutionary prompts, and incorporating code debugging and time-space complexity constraints. Initially, our method is applied to evolve the basic code instruction data, Code Alpaca~\citep{codealpaca}. Subsequently, we conduct fine-tuning of StarCoder using our newly created code instruction-following training set and obtain our \modelname{}.

% We aim to enhance the capabilities of the SOTA open-source Code LLM (i.e., StarCoder~\citep{li2023starcoder}), by generating intricate code instruction data. 

% Motivated by the \name{} method, this study aims to enhance the capabilities of the SOTA open-source Code LLM (i.e., StarCoder~\citep{li2023starcoder}), by generating intricate code instruction data through \cname{}. To achieve this, we have made several adaptations to the evolutionary prompt process tailored specifically for code-related tasks. These modifications include refining the evolutionary instructions, simplifying the form of evolutionary prompts, and incorporating code debugging and time-space complexity constraints. Initially, our method is applied to evolve the basic code instruction data, Code Alpaca~\citep{codealpaca}. Subsequently, we conduct fine-tuning of StarCoder using our newly created code instruction-following training set and obtain our \modelname{}.

Figure~\ref{fig:compare_with_sota} and the experimental results obtained from five code generation benchmarks, namely HumanEval~\citep{humeval}, HumanEval+~\citep{humanevalp}, MBPP~\citep{MBPP}, DS-100~\citep{DS1000}, and MultiPL-E~\citep{multipl_e}, demonstrate that our \modelname{} models outperform all other open-source Code LLMs (before August 24, 2023), achieving state-of-the-art (SOTA) performance. Remarkably, our \modelname{} \textit{15B} even surpasses well-known Anthropic's Claude and Google's Bard in terms of pass rates on HumanEval and HumanEval+. Furthermore, \modelname{} \textit{34B} not only achieves a HumanEval score comparable to GPT3.5 (ChatGPT) but also surpasses it on the HumanEval+ benchmark. Beyond this, our preliminary studies indicate that the complexity of instructions is the key to achieving exceptional coding performance.

The contributions of this work can be summarized as follows:
\begin{itemize}
    \item We introduce \cname{}, a novel instruction fine-tuning approach for code, which enhances the performance of the open-source Code LLMs by a large margin.
    \item We develop \modelname{} models, which surpass all other open-source Code LLMs by a substantial margin in coding tasks. Notably, the 15B version even outperforms the well-known closed-source LLMs, such as Claude, and Bard. The 34B version achieves a HumanEval score comparable to GPT3.5 (ChatGPT) and surpasses it on the HumanEval+ benchmark.
    \item We conduct a preliminary study highlighting the pivotal role of instruction complexity in achieving exceptional coding performance.
\end{itemize}
