\section{Related Work}

\paragraph{Large Language Models.} Recently, LLMs have demonstrated remarkable achievements across a broad spectrum of tasks. Prominent tech companies have made significant strides in developing highly proficient LLMs. These include OpenAI's GPT3\&4~\citep{GPT3,GPT4}, Google's PaLM~\citep{PaLM,palm2}, and Bard\footnote{\url{https://bard.google.com/}}, DeepMind's Chinchilla~\citep{Chinchilla}, and Gopher~\citep{gopher}, as well as Anthropic's Claude\footnote{\url{https://www.anthropic.com/index/introducing-claude}}. However, it is important to note that these models are closed-source and can only be accessed through specific APIs or may not be accessible at all.

The AI community has witnessed the release of several open-source LLMs, where the model weights are made publicly available. EleutherAI has contributed GPT-NeoX-20B~\citep{GPT-NeoX-20B} and GPT-J-6B~\citep{gpt-j}. Google has released UL2-20B~\citep{UL2}. Tsinghua University has introduced GLM-130B~\citep{GLM-130B}. Meta has released OPT~\citep{opt} and LLaMA1\&2~\citep{llama,llama2}. It is worth noting that while these open-source models have made valuable contributions, they generally do not exhibit the same level of performance as their closed-source counterparts.

\paragraph{Large Language Models for Code.} Recent studies have introduced a significant number of LLMs for code-related tasks to address the challenges of code understanding and generation. OpenAI has unveiled Codex~\citep{codex} and Code-Davinci~\citep{Azure}. Google has proposed PaLM-Coder~\citep{PaLM}. They perform outstandingly on the popular code completion benchmarks, like HumanEval~\citep{humeval} and MBPP~\citep{MBPP}. However, these models are closed-source. 

On the other hand, there are several open-source Code LLMs available. Salesforce has introduced CodeGen1\&2~\citep{codegen,codegen2}, CodeT5~\citep{codet5}, and CodeT5+~\citep{CodeT5+}. Tsinghua University has contributed CodeGeeX~\citep{CodeGeeX}, and the BigCode Project has developed StarCoder~\citep{li2023starcoder}. Meta has released the CodeLlama-Series~\citep{codellama}, which achieves open-source SOTA performance on several benchmarks.
%Modified%%%%%%%%%%%%%%%%%%%%%%%%
The closely related model, CodeLlama-Instruct, refines its performance through the self-instruct method. These models have demonstrated notable advancements in code-related tasks. However, when compared to the SOTA closed-source models, they still lag behind significantly. In contrast to the aforementioned models, our work demonstrates that further training Code LLMs with our \cname{} can substantially enhance performance.

\paragraph{Instruction Fine-Tuning.} 

The primary objective of instruction fine-tuning in its early stages was to enhance the cross-task generalization capabilities of LMs. This was achieved by fine-tuning LMs with a substantial corpus of public NLP tasks. T5~\citep{t5} was among the first models to explore this approach, training on a multitude of supervised text-to-text tasks. Subsequent works such as FLAN~\citep{DBLP:conf/iclr/WeiBZGYLDDL22}, ExT5~\citep{ExT5}, T0~\citep{T0}, and UnifiedQA~\citep{UnifiedQA} further expanded the range of tasks to bolster the overall generalization ability of LMs. Notably, ZeroPrompt~\citep{ZeroPrompt} and FLAN-T5~\citep{flan-t5} pushed the envelope by incorporating thousands of tasks in their training pipelines. Across these studies, a consistent finding emerges: fine-tuning LMs with diverse NLP task instructions yields significant performance improvements when applied to new tasks.

While fine-tuning LMs with diverse NLP tasks has shown promising results, it often falls short in aligning with the intentions of real-world users. OpenAI has pursued a different approach by soliciting human annotators to provide a large corpus of human instructions, encompassing diverse forms and a wide range of task types. Building upon this dataset, OpenAI trained its GPT3~\citep{GPT3} model to create InstructGPT~\citep{DBLP:conf/nips/Ouyang0JAWMZASR22}, which better aligns with users' inputs. This line of development has even led to the impressive work known as GPT3.5 (ChatGPT). However, it is important to note that the dataset and model weights associated with these advancements are not publicly available. Alpaca~\citep{alpaca} takes a different route by adopting the self-instruct method~\citep{wang2022self}, leveraging GPT3.5 (ChatGPT) to generate data for training. Vicuna~\citep{vicuna2023} utilizes user-shared conversations collected from ShareGPT.com to train its models.
%Modified%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
WizardLM~\citep{xu2023wizardlm} introduces the \name{} method, which involves evolving existing general instruction data to generate more complex and diverse datasets.  Drawing inspiration from this idea, our work, \cname{}, aligning with the distinctive characteristics of coding domains, is the first instruction fine-tuning method explicitly designed to enhance Code LLMs.