\newpage
\section{Prompt Formats}\label{app:prompt}

In this section, we include the prompt for evaluation on different tasks.

\newenvironment{myblock4}{%
  \begin{tcolorbox}[colback=beaublue!8!white,colframe=beaublue!10!black,title=Zero-Shot Prompt for Evaluation on HumanEval and HumanEval+]
}{%
  \end{tcolorbox}
}
\begin{myblock4}
Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. \\\\\#\#\# Instruction:\\
Create a Python script for this problem:\\\{Question\}\\ \\\#\#\# Response:
\end{myblock4}

\newenvironment{myblock5}{%
  \begin{tcolorbox}[colback=beaublue!8!white,colframe=beaublue!10!black,title=Three-Shot Prompt for Evaluation on MBPP]
}{%
  \end{tcolorbox}
}
\begin{myblock5}
Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. \\\\\#\#\# Instruction:\\
Create a Python script for this problem:\\\{Question\}\\\{Test Example 1\}\\\{Test Example 2\}\\\{Test Example 3\}\\\\\#\#\# Response:
\end{myblock5}

\newenvironment{myblock7}{%
  \begin{tcolorbox}[colback=beaublue!8!white,colframe=beaublue!10!black,title=Zero-Shot Prompt for Evaluation on DS-1000 (Completion)]
}{%
  \end{tcolorbox}
}
\begin{myblock7}
Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. \\\\\#\#\# Instruction:\\
\{Question\}\\Complete the Python code in "...".\\\\\#\#\# Response:
\end{myblock7}

In the case of DS-1000 (Insertion), adherence to the benchmark's specifications necessitates the utilization of StarCoder's specialized insertion symbol. Consequently, we have found it imperative to align with the same prompt format employed by StarCoder for this particular benchmark.

For the MultiPL-E benchmark, we recognized the need to align with the evaluation codes provided by bigcode-evaluation-harness.\footnote{\url{https://github.com/bigcode-project/bigcode-evaluation-harness}} Consequently, we opted to adopt the same prompt format utilized by StarCoder. 
% However, our empirical observations revealed that this particular prompt format did not yield optimal results for our \modelname{} on this benchmark. Thus, we conducted supplementary experiments with the instruction format: 
% \newenvironment{myblock6}{%
%   \begin{tcolorbox}[colback=beaublue!8!white,colframe=beaublue!10!black,title=Zero-Shot Prompt for Evaluation on MultiPL-E]
% }{%
%   \end{tcolorbox}
% }
% \begin{myblock6}
% Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. \\\\\#\#\# Instruction:\\
% Complete the following \{lang\} code:\\
% \{Question\}\\ \\\#\#\# Response:
% \end{myblock6}

% Table~\ref{tab:extra_multipl_e} demonstrates a noteworthy enhancement in performance when transitioning to our instruction-based prompt for four out of the five evaluated programming languages. Consequently, it becomes evident that the performance figures presented in Table~\ref{tab:multipl_e} merely represent the lower bound of our \modelname{}'s capabilities. This underscores the significant impact that prompt design can have on the overall performance, confirming the potential for further optimization through prompt engineering.

% \input{Coder_Tables/Table_ExtraMultiPLE}

\section{Baselines Details}\label{app:baselines}

We include a large amount of models as our baselines. For GPT3.5 (ChatGPT)\&GPT4. their results are obtained from GPT4's report and EvalPlus. The results of Code-Davinci-002, Code-Cushman-001, Codex, PaLM, PaLM 2, LaMDA, AlpahaCode, Incoder, StarCoder, LLaMa, CodeGen, CodeGeeX, CodeT5+, and InstructCodeT5+ are from StarCoder or CodeT5+'s paper. The results of Bard are evaluated with Google's API. The results of Claude are evaluated with Anthropic's API. The results of Instruct-Codegen-16B, Guanaco-65B, Falcon-40B-Instruct, and Vicuna-13B are evaluated with the open-sourced checkpoints. The results of CodeLlama-Series are from CodeLlama's paper. The results of OctoCoder are from its paper. The results of PanguCoder2 are also from its paper.

The MBPP score of StarCoder differs from that in its original paper. Through a personal contact, we were informed that StarCoder was evaluated using a cleaned and smaller version of MBPP, comprising only 397 problems, significantly fewer than the original MBPP benchmarks (500). Consequently, we conducted a re-evaluation of StarCoder using the original MBPP.

\section{Similarity Checking and Data Filtering}\label{app:sim}

The prompt formats to compute the similarity score are as follow:
\newenvironment{myblock8}{%
  \begin{tcolorbox}[colback=beaublue!8!white,colframe=beaublue!10!black,title=System Prompt for Similarity Checking]
}{%
  \end{tcolorbox}
}
\begin{myblock8}
Your task is to evaluate the similarity of the two given coding tasks. Please review the two coding tasks carefully, paying close attention to the overlap in function names, code structures, topics, and contents. Once you have carefully reviewed both coding tasks, provide a similarity score between these two coding tasks. The score should range from 1 to 10 (1: completely different coding tasks; 10: identical coding tasks). You only need to provide your score. The response format is:\\
Score: '...'
\end{myblock8}

\newenvironment{myblock9}{%
  \begin{tcolorbox}[colback=beaublue!8!white,colframe=beaublue!10!black,title=User Input for Similarity Checking]
}{%
  \end{tcolorbox}
}
\begin{myblock9}
\# Task1\\
\{task1\}
\\\\
\# Task2\\
\{task2\}
\end{myblock9}

%Modified%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To thoroughly prevent data leakage from the test datasets to the training dataset, we implemented an additional data filtering step. Utilizing the SOTA embeddings model, gte-large, we treated all test samples as queries to extract the top 5 samples from the training data. Following this, GPT-4 was employed to evaluate the similarity between the retrieved samples and the test sample. The task for GPT-4 is simplified to a binary decisionâ€”either a ``yes" or ``no" indicating a match. In case of a positive match, the sample is excluded from the training data.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Evol Examples}\label{app:evol_example}

In this section, we present some evolved examples to elucidate the influence exerted by our \cname{}.

Example 1:
\begin{itemize}
    \item Round 0: Write a MongoDB query to select all documents in a collection where the field 'category' is 'clothes'.
    \item Round 1: Write a MongoDB query to select all documents in a collection where the field 'category' is 'clothes' and the 'brand' field is not equal to 'Nike'.
    \item Round 2: Write a MongoDB query to select all documents in a collection where the field 'category' is 'clothes' and the 'brand' field is not equal to 'Nike', and the 'price' field is greater than or equal to 100 and less than or equal to 500.
    \item Round 3: Write a MongoDB query to select all documents in a collection where the field 'category' is 'clothes' and the 'brand' field is not equal to 'Nike', and the 'price' field is greater than or equal to 100 and less than or equal to 500, and the 'color' field is either 'red' or 'blue'. Additionally, sort the documents in descending order by the 'date\_added' field and limit the result to the first 10 documents.
\end{itemize}

Example 2:
\begin{itemize}
    \item Round 0: Using Tensorflow, set up a model to predict the classification of a handwritten digit.
    \item Round 1: Using Tensorflow, set up a model to predict the classification of a handwritten digit with the constraint that the model must be trained using only 1000 samples from the MNIST dataset.
    \item Round 2: Using TensorFlow, set up a model to predict the classification of a handwritten digit with the constraint that the model must be trained using only 500 samples from the MNIST dataset. Additionally, implement transfer learning to incorporate the features learned from a pre-trained model.
    \item Round 3: Using TensorFlow and the MNIST dataset, develop a convolutional neural network (CNN) to accurately classify handwritten digits. However, in this new version of the task, you must limit your training to a maximum of 500 images. Furthermore, incorporate transfer learning by using a pre-trained model to extract features from the data and improve your model's accuracy.
\end{itemize}

Example 3:
\begin{itemize}
    \item Round 0: Create a class in Java that can store a list of students and the courses they are enrolled in.
    \item Round 1: Create a Java program that manages a student database with courses, including options to add, remove, and search for students, as well as courses for each student. The program should also be able to display statistics on the number of students and courses.
    \item Round 2: Develop a Java program that manages a student database containing courses and grades. Your program should allow users to add, remove, and search for students, as well as add or drop courses for each student. Furthermore, it should display statistics on the number of students and courses, and calculate the average grade for each student. Ensure that your program implements appropriate error-handling mechanisms.
    \item Round 3: Develop a Java program to manage a library system where users can check-in, check-out and search for books in the library. The system should allow users to add new books and delete existing ones from the library. The program should also provide statistics on the number of books checked-in and checked-out, and calculate the average reading time for each book. Ensure that your program implements appropriate error-handling mechanisms and enforces data integrity constraints such as not allowing users to check-out books that have already been checked-out.
\end{itemize}

\section{Using Different Evolution Execution Models}\label{app:diff_evol}

We explored using the open-source models (OSS) CodeLlama-Instruct-34B for generating evolved instructions. However, it demonstrated relatively low coding performance in response generation. To address this, we fine-tuned it using our code-alpaca dataset and utilized this model for response generation.

% \section{Interaction Examples}

% Table~\ref{tab:example1}~\ref{tab:example2}~\ref{tab:example3} showcases examples of interactions with our \modelname{}. The examples demonstrate that our model consistently generates accurate responses accompanied by clear explanations.

% \input{Coder_Tables/Table_examples}