\begin{abstract}

Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated remarkable performance in various code-related tasks. However, different from their counterparts in the general language modeling field, the technique of instruction fine-tuning remains relatively under-researched in this domain. In this paper, we present \cname{}, a novel approach that adapts the \name{} method to the realm of code, enhancing Code LLMs to create novel models \modelname{}.
Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS-1000, and MultiPL-E, our models showcase outstanding performance. They consistently outperform all other open-source Code LLMs by a significant margin. Remarkably, \modelname{} \textit{15B} even surpasses the well-known closed-source LLMs, including Anthropic's Claude and Google's Bard, on the HumanEval and HumanEval+ benchmarks. Additionally, \modelname{} \textit{34B} not only achieves a HumanEval score comparable to GPT3.5 (ChatGPT) but also surpasses it on the HumanEval+ benchmark. Furthermore, our preliminary exploration highlights the pivotal role of instruction complexity in achieving exceptional coding performance.

\end{abstract}

\begin{figure}[h!]
     \centering
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.95\textwidth]{Coder_Figures/Code-Evol-Instruct.pdf}
     \end{subfigure}
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.72\textwidth]{Coder_Figures/compare_with_sota2.pdf}
     \end{subfigure}
        \caption{An illustration of our novel \cname{} and the superior pass@1 performance of our \modelname{} \textit{34B}, outperforming the open-source SOTA (CodeLlama-34B-Series as of the date before August 24, 2023) by a large margin in 9 different programming languages. The Python score is the mean between HumanEval and MBPP.}
        \label{fig:compare_with_sota}
\end{figure}