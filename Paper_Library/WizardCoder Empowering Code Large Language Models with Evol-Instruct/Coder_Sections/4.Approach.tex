\section{WizardCoder: SOTA Open-Source Code LLM}
In this section, we elaborate on the methodological details of \modelname{}. As illustrated in Figure~\ref{fig:compare_with_sota}, we first adopt our \cname{} to iteratively evolve the Code Alpaca dataset. Subsequently, we fine-tune the pre-trained Code LLMs with the evolved data.


\subsection{Code Evol-Instruct}
%Modified%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Inspired by the \name{} method proposed by WizardLM~\cite{xu2023wizardlm}, this work attempts to automatically enhance the complexity of code instructions, thereby improving the fine-tuning effectiveness of Code LLMs. Diverging from the general domain, our methods are meticulously designed to align with the specific characteristics of coding domains. The evolutionary process introduces the following features:
\begin{enumerate}
    \item Heuristics aligned with coding task features on platforms like LeetCode, strategically increasing the complexity of coding tasks to enhance the model's capabilities.
    \item Introduction of erroneous code as an adversarial sample, inspired by prior research on attacking pre-trained code models~\cite{attackcode,attackcode2}, adds a novel and effective method to escalate task complexity.
    \item Introduction of a heuristic emphasizing time and space complexity leverages insights from previous studies~\cite{constraints}, providing a valuable avenue for improving task complexity.
\end{enumerate}

% To adapt \name{} to the realm of code, we introduce the following modifications to the evolutionary process:
% \begin{enumerate}
% \item Streamlined evolutionary instructions: We optimized the evolutionary instructions by eliminating deepening and complicating inputs, as well as In-Breadth Evolution. This refinement allowed us to concentrate on enhancing the complexity of the instructions.
% \item Simplified evolutionary prompts: We simplified the structure of the evolutionary prompts by standardizing the template. This simplification enabled us to direct our focus more effectively toward the coding tasks.
% \item Addressing the specific characteristics of the code domain: We introduced two new evolutionary instructions: code debugging and considerations for time-space complexity constraints.
% \end{enumerate}
So, the code evolutionary prompt template is as follows:

\definecolor{beaublue}{rgb}{1.0, 0.46, 0.44}
\newenvironment{myblock}{%
  \begin{tcolorbox}[colback=beaublue!8!white,colframe=beaublue!10!black,title=Prompt for Code Evol-Instruct]
}{%
  \end{tcolorbox}
}

\begin{myblock}
Please increase the difficulty of the given programming test question a bit. \\\\You can increase the difficulty using, but not limited to, the following methods:\\ \{method\}\\\\ \{question\}
\end{myblock}
Here, $\{$question$\}$ represents the current code instruction awaiting evolution, and $\{$method$\}$ is the type of evolution. The five types we used are listed as follows:

\newenvironment{myblock2}{%
  \begin{tcolorbox}[colback=beaublue!8!white,colframe=beaublue!10!black,title=Code Evolution Heuristic Methods]
}{%
  \end{tcolorbox}
}
\begin{myblock2}
Add new constraints and requirements to the original problem, adding approximately 10 additional words.\\\\Replace a commonly used requirement in the programming task with a less common and more specific one.\\\\If the original problem can be solved with only a few logical steps, please add more reasoning steps.\\\\Provide a piece of erroneous code as a reference to increase misdirection.\\\\Propose higher time or space complexity requirements, but please refrain from doing so frequently.
\end{myblock2}

\subsection{Training \modelname{}}
We employ the following procedure to train \modelname{}. Initially, we utilize StarCoder 15B~\citep{li2023starcoder} and CodeLlama-34B-Python~\citep{codellama} as the foundations and proceed to fine-tune them using the code instruction-following training set, which was evolved through \cname{}. The prompt format for fine-tuning is outlined as follows:

\newenvironment{myblock3}{%
  \begin{tcolorbox}[colback=beaublue!8!white,colframe=beaublue!10!black,title=Prompt for Fine-Tuning Format]
}{%
  \end{tcolorbox}
}

\begin{myblock3}
Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. \\\\\#\#\# Instruction:\\ \{instruction\}\\ \\\#\#\# Response:
\end{myblock3}
To construct the training dataset, we initialized it with the instruction-following dataset called Code Alpaca\footnote{\url{https://github.com/sahil280114/codealpaca}}. We iteratively employ the \cname{} technique on this dataset consisting of around 20k samples to produce evolved data. After each round of data evolution, we merge the evolved data from all previous rounds with the original dataset to finetune Code LLMs. An external dev set serves as the controlled Evol Stop. If the performance drops, we halt the evolution. In Appendix~\ref{app:sim}, we outline the approach employed to prevent data leakage. Additionally, Appendix~\ref{app:evol_example} showcases some evolved examples for reference.