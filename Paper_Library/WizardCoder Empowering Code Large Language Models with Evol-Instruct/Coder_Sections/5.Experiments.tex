\section{Experiment}

This section begins by providing a comprehensive overview of the baseline models in our experiments. Subsequently, we present the performance of our models on five code generation benchmarks: HumanEval~\citep{humeval}, HumanEval+~\citep{humanevalp}, MBPP~\citep{MBPP}, DS-1000~\citep{DS1000} and MultiPL-E~\citep{multipl_e}.

\subsection{Baselines}

\paragraph{Closed-Source Models.} Multiple technology companies have successfully developed highly proficient LLMs while choosing not to publicly release them. These models are referred to as closed-source models. For our research, we incorporate a substantial number of these models as our baselines. Specifically, our baselines encompass the following: (i) OpenAI's GPT3.5(ChatGPT)\&GPT4~\citep{GPT4}, Code-Davinci-002~\citep{Azure}, Code-Cushman-001~\citep{Azure}, and Codex~\citep{codex}; (ii) Google's Bard, PaLM 2~\citep{palm2}, PaLM~\citep{PaLM}, and LaMDA~\citep{LaMDA}; (iii) Google DeepMind's AlphaCode~\citep{AlphaCode};(iv) Anthropic's Claude; (v) Huawei's PanguCoder2~\citep{pangucoder2}; and (vi) Meta's Unnatural-CodeLlama-34B~\citep{codellama}.

\paragraph{Open-Source Models.} Several open-source LLMs (OSS) have been made available to the AI community, although their performance generally lags behind the closed-source models a lot. As part of our research, we incorporate a significant number of these open-source models as our baselines. Our baselines encompass the following models: InCoder\cite{incoder}, StarCoder and StarCoder-Plus~\citep{li2023starcoder}, LLaMa1\&2~\citep{llama,llama2}, CodeGen~\citep{codegen}, CodeGeeX~\citep{CodeGeeX}, CodeT5+\citep{CodeT5+}, and CodeLlama~\citep{codellama}. In addition, we also include several models with instructions fine-tuning, including CodeLlama-Instruct~\citep{codellama}, OctoCoder~\citep{octocoder}, InstructCodeT5+~\citep{CodeT5+}, Instruct-Codegen-16B,\footnote{\url{https://huggingface.co/sahil2801/instruct-codegen-16B}} Guanaco-65B~\citep{guanaco}, Falcon-40B-Instruct~\citep{falcon} and Vicuna-13B~\citep{vicuna2023}. More details can be found in the Appendix~\ref{app:baselines}.

\subsection{Implementation Details}

The StarCoder and CodeLlama-34B-Python serve as our basic foundation models. OpenAI's gpt3.5-turbo is used to evolve the dataset and generate responses. The evolved dataset consists of approximately 78k samples. To fine-tune the basic models, we employ specific configurations, including a batch size of 512, a sequence length of 2048, 200 fine-tuning steps, 30 warmup steps, a learning rate of 2e-5, a Cosine learning rate scheduler, and fp16 mixed precision.

\begin{figure}
\centering
     \includegraphics[width=0.78\textwidth]{Coder_Figures/pass1.pdf}
     \caption{The percentage of pass rates on the HumanEval and HumanEval+ with a single attempt (greedy decoding), following the EvalPlus leaderboard~\citep{humanevalp}.}
     \label{fig:pass1}
\end{figure}

\input{Coder_Tables/Table_HumanEval_MBPP}

\subsection{Evaluation on HumanEval, HumanEval+, and MBPP}

HumanEval~\citep{humeval}, HumanEval+~\citep{humanevalp}, and MBPP~\citep{MBPP} are key benchmarks in the Code LLM field, featuring diverse Python programming problems validated using test cases. HumanEval comprises 164 problems with an average of 9.6 test cases per problem. HumanEval+ expands the test cases significantly to an average of 774.8 per problem. In contrast, MBPP provides 500 test programming problems with three automated test cases each.\footnote{For a fair comparison, we present results for GPT3.5(ChatGPT)\&GPT4 using Eval-Plus with the latest OpenAI's APIs~\citep{humanevalp} (Figure~\ref{fig:pass1}) and OpenAI's report~\citep{GPT4} (Table~\ref{tab:humaneval_mbpp}). Prompt format details are in Appendix~\ref{app:prompt}.}

\paragraph{Comparing with the Closed-Source Models.} Following the same setting of the EvalPlus leaderboard~\citep{humanevalp}. In Figure~\ref{fig:pass1}, we compare our \modelname{} models with the closed-source models, such as GPT4, Claude, and Bard on this leaderboard. Notably, all models generate code solutions for each problem utilizing a single attempt, and the resulting pass rate percentage is reported. To maintain consistency, we employ the same experimental setup by generating answers using greedy decoding and evaluate our \modelname{} models using the provided evaluation codes.

As depicted in Figure~\ref{fig:pass1}, our \modelname{} \textit{34B} attains the second position in this benchmark, surpassing GPT3.5 (ChatGPT, 64.6 vs. 63.4) on HumanEval+. Our 15B version outperforms Claude-Plus (59.8 vs. 53.0) and Bard (59.8 vs. 44.5). Furthermore, our \modelname{} models demonstrate a remarkable superiority over other open-source LLMs that undergo instruction fine-tuning.

\paragraph{Comparing with the Open-Source Models.} In Table~\ref{tab:humaneval_mbpp}, we conduct a comprehensive comparison of our \modelname{} with other open-source models on the HumanEval and MBPP benchmarks. In contrast to the results presented in Figure~\ref{fig:pass1}, we adhere to the approach outlined in previous studies~\cite{humeval} by generating n samples for each problem to estimate the pass@1 score. The findings presented in Table~\ref{tab:humaneval_mbpp} clearly demonstrate that our \modelname{} exhibits a substantial performance advantage over all the open-source models.

% From the experimental results in Figure~\ref{fig:pass1} and Table~\ref{tab:humaneval_mbpp}, we have the following conclusions:
% \begin{enumerate}
%     \item \modelname{} outperforms the largest closed-source LLMs, including Claude, Bard, PaLM, PaLM-2, and LaMDA, despite being significantly smaller.
%     \item \modelname{} outperforms all the open-source Code LLMs by a large margin (+22.3 on HumanEval), including StarCoder, CodeGen, CodeGee, and CodeT5+.
%     \item \modelname{} significantly outperforms all the open-source Code LLMs with instructions fine-tuning, including InstructCodeT5+, StarCoder-GPTeacher, and Instruct-Codegen-16B.
% \end{enumerate}

\subsection{Evaluation on Multi-Language Coding}

We included comprehensive assessment results across 8 distinct programming languages on the MultiPL-E benchmarks. These languages encompass Java, JavaScript, C++, PHP, R, Julia, Swift, and Rust. The empirical results, as presented in Table~\ref{tab:multipl_e}, distinctly demonstrate the superior performance of our \modelname{} models across all evaluated programming languages, surpassing the SOTA open-source Code LLMs. This underscores the efficacy of our \cname{} method.

\subsection{Evaluation on DS-1000}

The DS-1000 benchmark~\cite{DS1000} comprises 1k distinct data science workflows spanning 7 libraries. It assesses the performance of code generations against test cases and supports two evaluation modes: completion and insertion. In our experiments, we only report insertion scores for models that support. In Table~\ref{tab:ds}, we present pass@1 (n=40) results for each library, along with an overall score.\footnote{Given that this benchmark and its evaluation codes are not designed for the instruction fine-tuned models, we encounter significant challenges in aligning our 34B model with this framework. Moreover, the Codellama-34B base model does not support code insertion. Thus, we only include our 15B model results.} Based on these results, our conclusion is that \modelname{} demonstrates a significant superiority over all other models when tackling data science problems on the DS-1000 benchmark.

\input{Coder_Tables/Table_MultiPLE}
\input{Coder_Tables/Table_DS1000}

\section{Analysis}
\input{Coder_Tables/Table_Different_Evol}

\paragraph{Evolution Models and Rounds.} In Table~\ref{tab:execution}, GPT-4 replaces GPT-3.5 for evolved rounds, boosting HumanEval Pass@1 scores to 73.8 (34B) and 62.2 (15B). Using OSS CodeLlama-Instruct-34B also proves effective, yielding scores of 70.1 (34B) and 55.5 (15B). Despite GPT-4's superior coding performance (88.4 vs. 73.2), the gain in evolved rounds is not proportional (73.8 vs. 73.2). Conversely, CodeLlama's weaker performance narrows when using \cname{} (73.2 vs. 70.1), highlighting its crucial role. More experiments details are listed in Appendix~\ref{app:diff_evol}. Additionally, Figure~\ref{fig:ablation1} presents results for different data evolution rounds. All models are fine-tuned with 200 steps. Due to the limited size of the dev set of MBPP, we merged the training set and dev set, forming the MBPP-400 dev set. The experiments reveal that the highest pass@1 scores on both the MBPP-400 dev set and the HumanEval are achieved subsequent to three rounds of evolution.
\input{Coder_Tables/Table_Ablation_Evol}

\paragraph{Complexity and Quantity.}

While the enhanced performance attributed to our \cname{} method has been evident in prior experiments, it remains an open question whether this performance gain is a result of an increase in the number of samples or tokens. During the evolution, each round includes more samples, and the introduction of more complex instructions inevitably leads to an increase in tokens within the training data. To address this question, we fine-tune the models using only the specific round data separately from scratch with a similar number of samples (upper part) or tokens (lower part) in Table~\ref{tab:num}.
% we present preliminary findings in Table~\ref{tab:num}.
%Modified%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% we fine-tune the models with only the specific round data separately from scratch.

\input{Coder_Tables/Table_Ablation_Complexity}

When each round contains the same number of samples or tokens, the models trained with the seed data still lag behind the evolved rounds. Furthermore, combining data from different rounds leads to the best performance. These results suggest that the primary source of the gain is indeed attributable to our \cname{} method, rather than merely an increase in samples or tokens.

\input{Coder_Tables/Table_Cos}


\paragraph{Complexity and Similarity.} Apart from the quantity analysis, we also investigate whether evolution leads to the inclusion of data more similar to the test set. To address this, we perform an analysis of the HumanEval test set. We employ test samples as queries to retrieve the top-1 sample from each evolved round's training data, utilizing the SOTA embeddings model, gte-large~\citep{gte}. Additionally, we employ GPT4, to provide average similarity scores between the test set and the retrieved top-1 samples. The details are shown in Appendix~\ref{app:sim}.


Figure~\ref{fig:sim} illustrates that the evolution process does not yield higher similarity scores. Furthermore, similarity scores across all rounds remain relatively low. These findings indicate that the primary source of performance gain is the introduction of more complex data.