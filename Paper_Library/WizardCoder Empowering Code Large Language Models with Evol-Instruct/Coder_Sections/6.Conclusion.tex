\section{Conclusion and Future Work}

This paper introduces \modelname{} models, the \cname{} fine-tuned Code LLMs. The experimental results demonstrate that \modelname{} models achieve SOTA performance surpassing all existing open-source Code LLMs on five widely recognized code generation benchmarks: HumanEval, HumanEval+, MBPP, DS-1000 and MultiPL-E. Notably, \modelname{} \textit{15B} model surpasses some of the well-known closed LLMs, such as Claude and Bard. Additionally, \modelname{} \textit{34B} achieves a HumanEval score comparable to GPT3.5 (ChatGPT) and surpasses it on the HumanEval+ benchmark. Furthermore, our analysis underscores the pivotal role of instruction complexity in enhancing performance. For future work, as depicted in Figure~\ref{fig:pass1}, our model still falls significantly behind the SOTA LLM, GPT4. Therefore, future work will further augment the performance of our model.

% \paragraph{Broader Impact.} Similar to the other LLMs, our \modelname{} could also generate unethical, harmful, or misleading information. Therefore, future research to address the ethical and societal implications is needed.

\section*{Acknowledgments}
This work is partially supported by National Natural Science Foundation of China Young Scientists Fund(No. 62206233) and Hong Kong RGC ECS (No. 22200722).
