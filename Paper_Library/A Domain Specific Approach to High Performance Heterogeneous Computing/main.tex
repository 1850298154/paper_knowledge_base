


%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_conf_compsoc.tex,
%%                    bare_jrnl_compsoc.tex, bare_jrnl_transmag.tex
%%*************************************************************************



\documentclass[10pt,journal,compsoc]{IEEEtran}
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}


% For Computer Society journals, IEEEtran defaults to the use of 
% Palatino/Palladio as is done in IEEE Computer Society journals.
% To go back to Times Roman, you can use this code:
%\renewcommand{\rmdefault}{ptm}\selectfont

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)

% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage[sort&compress]{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
%~\cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
%~\cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/tex-archive/info/epslatex/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex

% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}
\usepackage{dsfont}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%\usepackage{acronym}
% acronym.sty was written by Tobias Oetiker. This package provides tools for
% managing documents with large numbers of acronyms. (You don't *have* to
% use this package - unless you have a lot of acronyms, you may feel that
% such package management of them is bit of an overkill.)
% Do note that the acronym environment (which lists acronyms) will have a
% problem when used under IEEEtran.cls because acronym.sty relies on the
% description list environment - which IEEEtran.cls has customized for
% producing IEEE style lists. A workaround is to declared the longest
% label width via the IEEEtran.cls \IEEEiedlistdecl global control:
%
% \renewcommand{\IEEEiedlistdecl}{\IEEEsetlabelwidth{SONET}}
% \begin{acronym}
%
% \end{acronym}
% \renewcommand{\IEEEiedlistdecl}{\relax}% remember to reset \IEEEiedlistdecl
%
% instead of using the acronym environment's optional argument.
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/acronym/


%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/


% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


\usepackage{mdwmath}
\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/


%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.


% NOTE: PDF thumbnail features are not required in IEEE papers
%       and their use requires extra complexity and work.
%\ifCLASSINFOpdf
%  \usepackage[pdftex]{thumbpdf}
%\else
%  \usepackage[dvips]{thumbpdf}
%\fi
% thumbpdf.sty and its companion Perl utility were written by Heiko Oberdiek.
% It allows the user a way to produce PDF documents that contain fancy
% thumbnail images of each of the pages (which tools like acrobat reader can
% utilize). This is possible even when using dvi->ps->pdf workflow if the
% correct thumbpdf driver options are used. thumbpdf.sty incorporates the
% file containing the PDF thumbnail information (filename.tpm is used with
% dvips, filename.tpt is used with pdftex, where filename is the base name of
% your tex document) into the final ps or pdf output document. An external
% utility, the thumbpdf *Perl script* is needed to make these .tpm or .tpt
% thumbnail files from a .ps or .pdf version of the document (which obviously
% does not yet contain pdf thumbnails). Thus, one does a:
% 
% thumbpdf filename.pdf 
%
% to make a filename.tpt, and:
%
% thumbpdf --mode dvips filename.ps
%
% to make a filename.tpm which will then be loaded into the document by
% thumbpdf.sty the NEXT time the document is compiled (by pdflatex or
% latex->dvips->ps2pdf). Users must be careful to regenerate the .tpt and/or
% .tpm files if the main document changes and then to recompile the
% document to incorporate the revised thumbnails to ensure that thumbnails
% match the actual pages. It is easy to forget to do this!
% 
% Unix systems come with a Perl interpreter. However, MS Windows users
% will usually have to install a Perl interpreter so that the thumbpdf
% script can be run. The Ghostscript PS/PDF interpreter is also required.
% See the thumbpdf docs for details. The latest version and documentation
% can be obtained at.
% http://www.ctan.org/tex-archive/support/thumbpdf/


% NOTE: PDF hyperlink and bookmark features are not required in IEEE
%       papers and their use requires extra complexity and work.
% *** IF USING HYPERREF BE SURE AND CHANGE THE EXAMPLE PDF ***
% *** TITLE/SUBJECT/AUTHOR/KEYWORDS INFO BELOW!!           ***
\newcommand\MYhyperrefoptions{bookmarks=true,bookmarksnumbered=true,
pdfpagemode={UseOutlines},plainpages=false,pdfpagelabels=true,
colorlinks=true,linkcolor={black},citecolor={black},urlcolor={black},
pdftitle={An Efficient, Automatic Approach to High Performance Heterogeneous Computing},%<!CHANGE!
pdfsubject={Typesetting},%<!CHANGE!
pdfauthor={Gordon Inggs},%<!CHANGE!
pdfkeywords={Heterogeneous Computing,Partitioning,Multicore-CPUs,GPUs,FPGAs,run-time Modelling,MILP}}%<^!CHANGE!
\ifCLASSINFOpdf
\usepackage[\MYhyperrefoptions,pdftex]{hyperref}
\else
\usepackage[\MYhyperrefoptions,breaklinks=true,dvips]{hyperref}
\usepackage{breakurl}
\fi
% One significant drawback of using hyperref under DVI output is that the
% LaTeX compiler cannot break URLs across lines or pages as can be done
% under pdfLaTeX's PDF output via the hyperref pdftex driver. This is
% probably the single most important capability distinction between the
% DVI and PDF output. Perhaps surprisingly, all the other PDF features
% (PDF bookmarks, thumbnails, etc.) can be preserved in
% .tex->.dvi->.ps->.pdf workflow if the respective packages/scripts are
% loaded/invoked with the correct driver options (dvips, etc.). 
% As most IEEE papers use URLs sparingly (mainly in the references), this
% may not be as big an issue as with other publications.
%
% That said, Vilar Camara Neto created his breakurl.sty package which
% permits hyperref to easily break URLs even in dvi mode.
% Note that breakurl, unlike most other packages, must be loaded
% AFTER hyperref. The latest version of breakurl and its documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/breakurl/
% breakurl.sty is not for use under pdflatex pdf mode.
%
% The advanced features offer by hyperref.sty are not required for IEEE
% submission, so users should weigh these features against the added
% complexity of use.
% The package options above demonstrate how to enable PDF bookmarks
% (a type of table of contents viewable in Acrobat Reader) as well as
% PDF document information (title, subject, author and keywords) that is
% viewable in Acrobat reader's Document_Properties menu. PDF document
% information is also used extensively to automate the cataloging of PDF
% documents. The above set of options ensures that hyperlinks will not be
% colored in the text and thus will not be visible in the printed page,
% but will be active on "mouse over". USING COLORS OR OTHER HIGHLIGHTING
% OF HYPERLINKS CAN RESULT IN DOCUMENT REJECTION BY THE IEEE, especially if
% these appear on the "printed" page. IF IN DOUBT, ASK THE RELEVANT
% SUBMISSION EDITOR. You may need to add the option hypertexnames=false if
% you used duplicate equation numbers, etc., but this should not be needed
% in normal IEEE work.
% The latest version of hyperref and its documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/hyperref/


% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )

%Extra Packages
\usepackage{todonotes}
\usepackage{multirow}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor designation manufacturer heterogeneity consistency barrier simulation partitioning}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{A Domain Specific Approach to\\ High Performance Heterogeneous Computing}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Gordon~Inggs,~\IEEEmembership{Student~Member,~IEEE,}
        David~B.~Thomas,~\IEEEmembership{Member,~IEEE,}
        and~Wayne~Luk,~\IEEEmembership{Fellow,~IEEE}% <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem G.E. Inggs and D.B. Thomas are with the Circuits and Systems Group in the Department of Electrical and Electronic Engineering at Imperial College London\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: \href{mailto:gordon.e.inggs@ieee.org}{gordon.e.inggs@ieee.org}
\IEEEcompsocthanksitem W. Luk is with the Custom Computing Group in Department of Computing at Imperial College London}% <-this % stops a space
}
%\thanks{Manuscript received April 19, 2005; revised September 17, 2014.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%\markboth{Transactions~on~Parallel~and~Distributed~Systems, March~2015}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Advanced Demo of IEEEtran.cls for Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.

% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2014 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2014 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society journal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
Users of heterogeneous computing systems face two problems: firstly, in understanding the trade-off relationships between the observable characteristics of their applications, such as latency and quality of the result, and secondly, how to exploit knowledge of these characteristics to allocate work to distributed computing platforms efficiently. A domain specific approach addresses both of these problems. By considering a subset of operations or functions, models of the observable characteristics or domain metrics may be formulated in advance, and populated at run-time for task instances. These metric models can then be used to express the allocation of work as a constrained integer program, which can be solved using heuristics, machine learning or Mixed Integer Linear Programming (MILP) frameworks.

These claims are illustrated using the example domain of derivatives pricing in computational finance, with the domain metrics of workload latency or \emph{makespan} and pricing accuracy. For a large, varied workload of 128 Black-Scholes and Heston model-based option pricing tasks, running upon a diverse array of 16 Multicore CPUs, GPUs and FPGAs platforms, predictions made by models of both the makespan and accuracy are generally within 10\% of the run-time performance. When these models are used as inputs to machine learning and MILP-based workload allocation approaches, a latency improvement of up to 24 and 270 times over the heuristic approach is seen.
\end{abstract}}

% Note that keywords are not normally used for peerreview papers.
%\begin{IEEEkeywords}
%Heterogeneous Computing, Distributed Computing, Task, Partitioning, Multicore CPUs, GPUs, FPGAs, run-time Modelling, MILP, Domain Specific.
%\end{IEEEkeywords}}


% make the title area
\maketitle

% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when compsoc mode
% is not selected <OR> if conference mode is selected - because compsoc
% conference papers position the abstract like regular (non-compsoc)
% papers do!
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc under a non-conference mode.


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\ifCLASSOPTIONcompsoc
\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
\else
\section{Introduction}
\label{sec:introduction}
\fi
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.

% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by IEEE):
% \IEEEPARstart{A}{}demo file is ....

%\listoftodos



\IEEEPARstart{T}{he} following vignette illustrates the research problem that we address in this paper:

\textit{
Julia is a financial analyst at the Bank of England that monitors counterparty risk between investment banks. She is highly qualified in statistics and financial economics, and relies heavily on computational finance techniques to evaluate the derivative contracts that exist between investment banks. However, beyond the specialised programming environment that she uses, she knows little about computing and often runs her calculations for days on her laptop.
}

\textit{
She learns that a cluster of heterogeneous computing systems could massively accelerate her computations. She manages to cobble one together using the Bank's spare servers and cloud-based resources. Through the use of an open source application framework, she is soon able to execute her problems upon all of the heterogeneous computing platforms. However, she has no idea about how long a problem is going to take on a given platform. Furthermore, she is also mystified as to the relationship between the statistical accuracy she requires and the time it takes to evaluate her problems. Unable to understand the relationships between the metrics she cares about, she finds that some workloads take even longer on the cluster than on her laptop's CPU!
}

\textit{
Julia clearly needs a tool to help her not only understand the resources at her disposal, but also how to use them efficiently.
}

\break

\subsection{Problem Statement}

Julia might be a fiction, but the problems she faces are a reality for the increasing number of high performance computing application programmers. They have two problems:
\begin{enumerate}
\item Understanding the relationships between the run-time characteristics of their application tasks on heterogeneous computing platforms.
\item Allocating tasks to the available platforms so as optimise these run-time characteristics.
\end{enumerate}

In this paper, we both describe and demonstrate practically an approach to high performance, heterogeneous computing that addresses these problems. Our approach is premised on only supporting a subset of operations across all heterogeneous platforms. Computational application domains provide a natural means to limit the operations supported without overly inhibiting programmers, and hence our approach is a domain specific one.

We use the empirical definition of application domains as used in programming research~\cite{SmallMatterProgramming,DSL_bib,Fowler_DSL}, i.e. an identifiable category of computational activities where a small number of computational operations account for all or a disproportionately high proportion of the computations performed. For example, within the domain of Linear Algebra, vector arithmetic is used disproportionately more often than other operations. Hence, by focusing on supporting these frequently-used operations, these application domains can be practically supported across heterogeneous platforms.

%Empirical software engineering research~\cite{DSL_bib} has shown that within studied application domains, a relatively small number of high level operations (10-15) are used disproportionately more than others. 

\subsection{Contributions}
In this paper, we make the following contributions:
%\begin{itemize}

(1) We introduce a domain specific approach for modelling the run-time characteristics or metrics of heterogeneous computing platforms.

(2) We demonstrate metric modelling in the application domain of computational finance derivatives pricing. Our practical evaluation encompasses a large, diverse workload of 128 computational finance tasks across a heterogeneous computing cluster of 16 CPU, GPU and FPGA platforms across three continents.

(3) We show how the allocation of tasks to platforms can be formulated as a constrained integer programming problem. We demonstrate how the allocation problem can be solved using three distinct approaches: heuristics, machine learning and Mixed Integer Linear Programming (MILP).

(4) We apply the three allocation approaches to both synthetic and real world heterogeneous task and platform data. We show that while heuristics provide acceptable results, machine learning and MILP can provide orders of magnitude more efficient task allocations.

%\end{itemize}

\subsection{Proposed Methodology}
%Characterisation claim
We demonstrate that domain specific abstractions provide a means for characterising computing platforms in a manner that is \emph{meaningful} in the context of that domain, and hence to the domain programmer.

%\emph{While it is good to improve general efficiency, it is better to enable programmers to balance objectives for themselves.}

%Partitioning claim
Furthermore, we show how this domain specific characterisation allows for heterogeneous platforms to be evaluated in a coherent manner, allowing for an efficient allocation of work across these resources.

%\emph{While it is good to use all available computational resources, it is better to use the complimentary strengths of these resources.}

We seek to provide domain programmers such as Julia with the following programming flow, as illustrated in Figure \ref{fig:F3InterfaceFlowchart}:

(1) She specifies her tasks in a domain specific form. 

(2) Her tasks are then characterised using domain metrics with respect to the available platforms. 

(3) The optimal task allocations that make up the domain metric trade-off space are found automatically.

(4) Julia then selects the desired trade-off from the metric design space.

(5) Her workload is then evaluated, using the platforms in accordance with her objectives.

\begin{figure}
\centering
\includegraphics[width=0.85\columnwidth]{figs/F3_interface_flowchart_extended.pdf}
\caption{Our proposed domain specific, high-level programming flow for high performance heterogeneous computing.}
\label{fig:F3InterfaceFlowchart}
\end{figure}

%Figure \ref{fig:ParetoCurvesModelsVerification} illustrates the validity of our claims using computational finance as an example application domain. The figure demonstrates our modelling and partitioning approach for a practical workload upon a cluster of highly heterogeneous computing platforms with varying degrees of task, data and pipeline parallelism. As can be seen in the figure, the implementation of our domain specific approach on a real workload of financial option pricing tasks on heterogeneous platforms has generated a design trade-off that would be immediately understandable to the financial domain programmer and outperforms heuristic approaches.

\subsection{The Rest of the Paper}
In Section \ref{sec:Background}, we elaborate on the background to the benefits of domain specific abstractions for heterogeneous computing, as well the state-of-the-art with respect to heterogeneous computing characterisation and workload allocation. We then expanded upon our two claims in Section \ref{sec:DomainCharacterisationPartitioning}: firstly, that domain specific abstractions enable the useful characterisation of heterogeneous platforms, and secondly, that these domain specific metric models can be used in partitioning work across heterogeneous platforms.

%In Section \ref{sec:ComputationalFinanceDomain}, we describe the computational finance domain that we use an example throughout this paper, as well as the financial domain application framework that we use to demonstrate our contributions.

Then, in Section \ref{sec:CaseStudy} we demonstrate the domain specific methodology in practice by applying it to Julia's domain, financial derivatives pricing. We provide a brief overview of the domain and its heterogeneous implementation, after which we describe the latency and accuracy metric models, as well as heuristic, ML and MILP allocation approaches applied.  In Sections \ref{sec:DomainMetricModelEvaluation} and \ref{sec:DomainPartitionerEvaluation} we then evaluate our claims in the context of this case study.
 
 Finally we conclude the paper, summarising our major conclusions and lay out suggestions for further work.

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

\section{Background}
\label{sec:Background}

\subsection{Domain Specific Heterogeneous Computing}

%Previous work has described how domain-specific approaches can make heterogeneous computing systems more accessible through abstraction~\cite{Delite,DSL_MonteCarlo}. Domain specific abstractions provide a useful ``conceptual interface" which the domain programmer may specify their intentions without have to understand the underlying execution model of the system. Such abstraction comes at a cost however, as these domain specific means of expression are difficult to use beyond their intended domain.

An important finding in recent years is that domain specific abstractions can enable improved performance in a heterogeneous computing context~\cite{Delite,DSL_MonteCarlo,F3_P2S2}. As alluded to in the introduction, empirical studies of software engineering~\cite{SmallMatterProgramming} have found that a small set of algorithmic operations or design patterns within an application domain are executed disproportionately more frequently than others, often following a power law distribution. Indeed, application domains are often identified by grouping these operations together~\cite{DSL_bib}. By supporting the efficient, heterogeneous acceleration of these disproportionately influentially operations, significant gains can be realised automatically for programs restricted to a particular domain. %We call this property portable performance.

Previous works have shown domain specific-enabled heterogeneous performance in practice, such as our own use of software application frameworks~\cite{F3_P2S2}, or domain specific languages, as shown by Chafi et al\cite{Delite} and Thomas and Luk\cite{DSL_MonteCarlo}. The key information yielded by the domain specific abstractions is the implicit dependency relationships between computations, allowing for heterogeneous parallelism to be exploited without programmer intervention.

However putting this approach into practice remains a challenge, requiring system developers with domain expertise to create domain specific abstractions~\cite{DSL_MonteCarlo,F3_P2S2} that support heterogeneous execution. Chafi et al's~\cite{Delite} approach advocates the use of language virtualisation, providing both a framework for creating implicitly parallel domain specific languages as well as a dynamic run-time for running applications created using such languages.

%The strength of the work is borne out by the experimental work undertaken in multiple domains such as Machine Learning~\cite{Delite,Green_Marl}. In the case of Machine Learning the framework was shown to deliver consistently better results than contemporary approaches such as Matlab~\cite{Green_Marl}. However, the work thus far has been confined to one multi-core system with the use of only one accelerator, a GPU. A further consideration is that support for architectures beyond standard multicore CPUs have to be provided by the DSL-developer.

%Green-Marl is a standalone Domain-Specific Language for graph analysis algorithms. The language attempts to expose as much as possible the data-level parallelism in the algorithms under consideration, through language constructs for parallel operations as well as speculatively processing operations in parallel until conflicts are detected. Additional architecture independent and dependent optimisations are used to create as an efficient implementation of the algorithm under consideration in C++. This work is of particular interest as graph theory is itself a common programming abstraction, upon which a diverse range of applications such as data analytics, social network analysis and bioinformatics may be mapped.

\subsection{Task Characterisation and Allocation \\on Heterogeneous Platforms}

The problem of characterising and allocating computational tasks to heterogeneous computing platforms has been widely studied for almost 40 years~\cite{HC_goals_methods_Open_Problems,HCChallengesOpportunities,Braun11Heuristics,TaskPartitioningMemoryConstrainedCPU,TaskAllocationDDP,AppLeS,SmartNet,StaticOpenCLScheduling,TaskAssignmentIteratedGreedy,HeuristicAlgorithmsSchedulingIndependentTasks, MILPHeterogeneous, FSP2015}. 

%Starting with computing grids located in specialist facilities to ad-hoc clusters created for the duration of hours using cloud computing infrastructure, the question of how to relate applications and resources efficiently has proved remarkably resilient to definitive solution.

\subsubsection{Task Characterisation}

As identified by Braun et al~\cite{ HC_goals_methods_Open_Problems}, characterising the execution of tasks upon heterogeneous computing platform is comprised of three interrelated activities:

%\begin{enumerate}
\emph{Task Profiling}: identifies the atomic (i.e. indivisible) tasks that comprises the current application. These tasks can then be further qualified by performing analysis or profiling of the task code. A key insight from Khokhar et al~\cite{HCChallengesOpportunities} is that profiling should determine the parallel execution modes possible for the given task. An increasingly popular approach is to require the programmer to identify the parallel execution modes, either through a specially designed API~\cite{Qilin} or by embedding this within the language itself~\cite{Delite}.

\emph{Analytic Platform Benchmarking}: identifies the capabilities of the heterogeneous computational platforms available. Another insight from Khockar et al~\cite{HCChallengesOpportunities} is that this process details how well the platform supports different parallel execution modes. A heterogeneous benchmark such as Rodinia~\cite{Rodinia} could be used for this purpose, or a representative subset of the current tasks.

\emph{Task-Platform Characterisation}: synthesises the data from the two previous activities, which results in models of how the specified tasks will execute upon the available resources. Grewe's work~\cite{StaticOpenCLScheduling} illustrates how a sophisticated machine learning-based approach can be used to do so.
%\end{enumerate}

As described in the next subsection, the last activity is usually not distinguished from allocating of tasks to platforms~\cite{StaticOpenCLScheduling,Qilin}. We argue that maintaining this separation is useful, as it allows for the quality of the characterisation activities to be evaluated independently from the allocation approach that is being used.

\subsubsection{The Allocation Problem}
\label{sec:PartitioningProblem}

When considering the allocation of tasks to heterogeneous computing resources, the general scenario considered in the literature, i.e.~\cite{Braun11Heuristics,TaskAllocationDDP,TaskAssignmentIteratedGreedy,HeuristicAlgorithmsSchedulingIndependentTasks,StaticOpenCLScheduling,TaskPartitioningMemoryConstrainedCPU, MILPHeterogeneous, FSP2015} is a set of independent or atomic tasks being partioned across or allocated to multiple heterogeneous platforms. It is assumed that a task will block a platform for its duration, i.e. occupy the computing resource completely. It is also commonly assumed that the allocation is being performed statically, in advance of the execution of any of the tasks.

In this general problem formulation, the general objective is to minimise the makespan. The makespan is the latency between when the first task is initiated until the last result returned for the task set. 

%As the tasks are being evaluated on multiple platforms, the makespan is equivalent to the longest time it takes for any of the platforms to return the results of the tasks allocated to it. 

As described above, minimising the makespan with \emph{a priori} knowledge of the execution time of atomic tasks is a well studied problem. As we shall show in Section \ref{sec:DomainCharacterisationPartitioning}, as others have~\cite{MILPHeterogeneous}, this problem can be expressed formally as a 0-1 integer linear programming problem which has famously been shown to be NP-complete by Karp~\cite{Karp21Problems}.

\subsubsection{Allocation Approaches}
\label{sec:PartitionApproaches}
Surveying the literature, there are three categories of suggested approaches to the allocation problem described above:
%\begin{itemize}

\emph{Heuristic}~\cite{Braun11Heuristics,TaskAllocationDDP,HeuristicAlgorithmsSchedulingIndependentTasks}: a simple algorithmic rule is applied to allocate tasks to the available resources. Under specified circumstances such a rule might achieve a provably optimal allocation of tasks, and there is usually a lower bound on the quality of the solution relative to the optimal solution.

\emph{Machine Learning}~\cite{StaticOpenCLScheduling,TaskAssignmentIteratedGreedy}: a feasible task-platform allocation is improved using machine learning techniques such as Danzig's Simplex algorithm, simulated annealing or genetic algorithms. At worst these techniques can confirm the quality of the starting solution.

\emph{Integer Linear Programming}~\cite{TaskPartitioningMemoryConstrainedCPU,ILPPartitioningPipelinedScheduling,MILPHeterogeneous,FSP2015}: the problem can be formulated as a constrained integer program that can be solved using linear optimisation techniques. In addition to applying standard optimisation heuristics, a dual formulation of the problem can be used to prove the optimality of the solution.
%\end{itemize}

\subsubsection{Analysis}

Generally heuristic approaches have been the most studied in the context of heterogeneous computing. Braun's comprehensive study~\cite{Braun11Heuristics} found that simpler heuristics achieve better results than more complex ones for the general case. This suggests that the truly optimal approach is case-specific, dependent upon the dynamics between the task and platforms concerned, and so the more complex an allocation approach, the more likely it is to map better to certain configurations than others.

ILP appears to be an understudied approach, usually applied only in environments of pressing resource constraint~\cite{TaskPartitioningMemoryConstrainedCPU}. This lack of attention is likely due to the NP-hard complexity of mixed integer linear programs and the NP-complete complexity of binary valued programs. 

However considerable progress has been made in ILP in the last three decades~\cite{WhyMILPWorks}, and hence we believe that this approach is now practical for run-time allocation~\cite{FSP2015}, as do others~\cite{MILPHeterogeneous}. A key insight is that an external measurement of solution quality is desirable so that a high quality solution that is not necessarily provably optimal can be identified.

\section{Domain Characterisation \& Allocation}
\label{sec:DomainCharacterisationPartitioning}

In this section we elaborate on our claims that a domain specific approach to heterogeneous computing allows for both the useful characterisation of task upon heterogeneous platforms, and in turn, an efficient allocation of those tasks to platforms. To illustrate our explanation, in this section we use examples from the domains of image filtering and linear algebra arithmetic. 

In Section \ref{sec:CaseStudy}, we apply our domain specific approach to the financial derivatives pricing domain.

\subsection{Characterising Tasks upon Platforms}

By useful characterisation, we mean actionable, i.e. the domain specific approach enables predictive modelling of the run-time characteristics of domain tasks upon a wide range of heterogeneous platforms. Characterisation would be useful to domain programmers such as Julia as it allows for the static comparison of different platforms which, as we will show in the next subsection, is critical in the efficient allocation of task workloads.

However, this domain specific characterisation is a contribution in its own right because it relates tasks and platforms using the fundamental concepts of the application domain. By modelling the task-platform relationship using domain metrics, the computational design space is made accessible to anyone working within that domain.

These models allow domain programmer to balance their objectives in terms they understand.

%We believe that these relationships are best represented as a Pareto surface that captures the inherent design space trade-offs that exist for a particular task upon a platform. Providing such a representation allows programmers to balance their objectives for themselves, in terms they understand.

\subsubsection{Domain Metrics}

%Similarly we assert that an application domain also has defined subset of measures of quality or domain specific metrics.  Domain specific metrics reflect a measure of the achievement of a goal commonly held within the domain. For example, the upper bound on the wall time of a computation has special value within a domain such as safety-critical aeronautic systems, whilst a measure such as the computational throughput would only have meaning as it would affect other goals indirectly.

To find the computational design space for a task or group of tasks within an application domain, we first need to know what the dimensions of that design space should be, i.e. the quantitative measurements used within the domain. We define these quantitative characteristics of the domain \emph{metrics}.

While the actual metric used will vary from domain to domain, all fall into one of four categories:
\begin{itemize}
\item \emph{Latency}: the time between initiation and completion.
\item \emph{Throughput}: the rate at which the task is completed.
\item \emph{Quality}: the degree to a quantifiable goal is achieved.
\item \emph{Resource Use}: the resources used to complete the task.
\end{itemize}

For example, within the domain of image filtering, latency could be measured in the seconds required to filter an image, while throughput could be the number of images processed per second.

In the linear algebra arithmetic domain, quality might be measured as the unit of least precision in the calculations performed, while the resource use might be expressed using the average monetary cost per matrix arithmetic operation.

\subsubsection{Metric Models}

To predict metrics, we require models for how the task inputs map to the domain metrics on the target platform. We formalise these models in (\ref{eq:MetricMapping}): we seek model functions that map $p$ real-valued inputs to domain functions to $m$ real-valued metric values, i.e. $ \vec{F}: \vec{P} \rightarrow \vec{M} $, where $\vec{F}$ is the domain function model, $\vec{P}$ the inputs and $\vec{M}$ the metric values.

\begin{equation}
\label{eq:MetricMapping}
\begin{aligned}
 \vec{F}=(f_1,f_1,\cdots,f_{m}):  \vec{P} \rightarrow \vec{M} &  & \vec{P} \in \mathds{R}^p, \vec{M} \in \mathds{R}^m,\\
f_k(\vec{P}) = M_k &  & k = {1,2,\ldots,m}. \\
\end{aligned}
\end{equation}

As the application domain identifies in advance those operations which are disproportionately used, a function for mapping $\vec{P}$ to $\vec{M}$ of key domain functions for heterogeneous platforms can be found in advance.

For example, in the domain of image filtering, a candidate function for modelling would be the convolution operation used in all image filtering operations. In linear algebra arithmetic, the arithmetic operations would be modelled.

%Hence, the general structure of $\vec{F}$ for the most important functions in a domain can be known \emph{a priori}.

\subsubsection{Domain Variables and Parameters}

We refine the metric models further in (\ref{eq:ParametersDecomp}), $\vec{P}$ defines all possible input vectors to the domain specific operation. This space can be divided into two disjoint subsets, valid ($\vec{P_v}$) and invalid ($\vec{P_i}$) inputs. $\vec{P_i}$ are all of the inputs that will return a result that violates the correctness of the function as defined within the domain.

\begin{equation}
\label{eq:ParametersDecomp}
\begin{aligned}
& \vec{P} = \vec{P_i} \cup \vec{P_v} & \vec{P_i} \cap \vec{P_v} = \emptyset.
\end{aligned}
\end{equation}

For example, in the image filtering domain, when applying a uniform blur to an image, the set of inputs that define a non-uniformly weighted filter would be within $\vec{P_i}$ for that function. $\vec{P_v}$ is thus all of those inputs which return a valid result, representing the design space for that function.

By supplying the definition of ``correctness", the application domain makes explicit what input elements may be varied without affecting the correctness of the result. For example, in the linear algebra arithmetic domain, an input which specifies the maximum number of elements computed in parallel can be varied without affecting the correctness of the result.

%For example, varying the number of nodes used in a finite element-based airflow modelling function would not invalidate a result whilst varying the starting temperatures would.

We define those input elements which can be varied as \emph{domain variables} and those that cannot as \emph{domain parameters}. In our formalism, the domain definition identifies the subset of $\vec{P}$ upon which membership of $\vec{P_v}$ is defined.

%\subsubsection{The Pareto Optimal Design Space}

%As given in (\ref{eq:ValidParametersDecomp}), $\vec{P_v}$ can itself be partitioned into two disjoint subsets, Pareto optimal ($\vec{P_p}$) and non-Pareto optimal ($\vec{P_{np}}$) input values. $\vec{P_p}$ as given in (\ref{eq:ParetoDef}), defines those inputs which are not only valid, but also maximises or minimises at least a single element of $M$, as defined in the domain. $\vec{P_{np}}$ defines all of those inputs which do not. While $\vec{P_v}$ constitutes the acceptable design space, $\vec{P_p}$ is the Pareto optimal design surface that we believe domain programmers should consider.

%\begin{equation}
%\label{eq:ValidParametersDecomp}
%\begin{aligned}
%& \vec{P_v} = \vec{P_{np}} \cup \vec{P_p} & \vec{P_{np}} \cap \vec{P_p} = \emptyset.\\
%\end{aligned}
%\end{equation}

%\begin{equation}
%\label{eq:ParetoDef}
%\begin{aligned}
%& \{ f_k(\vec{p_p}) > f_k(\vec{p'_p})\} = \emptyset & k = 1,2,\ldots,m, \\
%& & \vec{p_p} \in \vec{P_p},\vec{p'_p} \in \vec{P_v},\vec{p_p} \neq \vec{p'_p}.

%\forall \vec{x} \in \mathcal{P}_p \quad \forall \vec{y} \in \mathcal{V} - \vec{x} \quad \exists k \in 1,2,\ldots,m \qquad f_k(\vec{x}) \prec f_k(\vec{y})
%\end{aligned}
%\end{equation}

%To return to the uniform blur filter example, the domain variable value that limited the filter to the fastest available device memory would likely be a member of $\vec{P_p}$, as it would maximise the throughput.

%We are interested in exposing $P_p$ to the domain programmer.

%Furthermore, hence by exploiting information made explicit in the domain, the Pareto optimal space $P_p$ for a given platform can be found relatively predictably.

%We believe that the information of what metrics matter is of crucial importance in exploiting heterogeneous computing resources, as these metrics provide a practical means for comparing heterogeneous platforms. Furthermore, by creating models of the relationships between domain metrics as achieved through different configurations, the achievable design space as defined by the configurations is represented in a form which the domain user both understands and can reason about in light of their own objectives.

\subsubsection{Identifying and Populating Metric Models}
The formalism above provides the criteria for potential metric model functions, however for each domain function there are an infinite number of possible metric model functions. When choosing one, we found that the simplest models to be the most broadly applicable.

For example, in the linear algebra arithmetic domain, a hypothetical metric model for latency of matrix-matrix addition operation might be expressed as the product of the size in the two matrices concerned ($N$) and the time per element-wise operation on that platform ($\alpha$), i.e. $$ f_L(N) = \alpha (N).$$

Similarly, in image filtering, the cost metric for applying a certain filter might be the cost per second of the platform ($\beta$) multiplied by the latency of the image processing ($f_L(S)$), i.e. $$ f_C(S) = \beta f_L(S).$$

As the structure of $\vec{F}$ is deterministic, an online benchmarking approach can be used to find the task and platform-specific metric model coefficients. We suggest a benchmarking procedure to generate a set of domain variable and metric values i.e. $\mathds{R}^{b \times p}$ and $\mathds{R}^{b \times m}$, where $b$ is the number of benchmarking iterations. The benchmarking data can then be used to to solve for the metric model function's coefficients.

We found weighted least squares regression to be effective in solving for the metric model coefficients. By using the variable benchmark values as weights, we reduced the impact of ``noise" present in metric measurements.

\subsection{Allocating Task to Platforms}
While the characterisation described in the previous subsection is useful when considering a heterogeneous platform in isolation, it is less helpful when faced with a cluster of heterogeneous platforms that can be used cooperatively. In this subsection we address how multiple computational domain metric model functions can be combined so as to create a unified, efficient design space.

\subsubsection{The General Allocation Problem}
We begin by expressing the makespan minimisation problem, as described in Section \ref{sec:PartitioningProblem}, as a binary valued integer linear program in (\ref{eq:MakespanMinimisation}).

Each non-zero element of the binary \emph{allocation} matrix ($\boldsymbol{A}$) represents an allocation of one of the $\tau$ tasks in a workload to one of the $\mu$ platforms, i.e. if $A_{i,j}=1$, then task $j$ has been allocated to platform $i$. The relative latency matrix ($\boldsymbol{L}$) gives the latency of each task upon each platform. Hence, similar to $A_{i,j}$, $L_{i,j}$ is the estimated relative latency of task $j$ upon platform $i$.

%The makespan of a given allocation is found by taking the maximum of the platform running times ($\boldsymbol{H}(\boldsymbol{A},\boldsymbol{L})$). The platform running times can be found by taking the Hadamard product, or element-wise multiplication of the $\boldsymbol{A}$ and the estimated relative latency ($\boldsymbol{X}$) matrices, and dot multiplying it by a single vector.

\begin{equation}
\label{eq:MakespanMinimisation}
\begin{aligned}
& \underset{\boldsymbol{A} \in \{0,1\}^{\mu \times \tau}}{\text{minimise}} & & G(\boldsymbol{A},\boldsymbol{L}) \quad \boldsymbol{L} \in \mathds R_+^{\mu \times \tau},\\
& \text{subject to} & & \sum\limits_{i=1}^\mu A_{i,j} = 1 \quad j =1,2,\ldots,\tau.\\
\end{aligned}
\end{equation}

where:
\begin{equation*}
\label{eq:MakespanMinimisationDefines}
\begin{aligned}
 & G(\boldsymbol{A},\boldsymbol{L}) = \max(\vec{H}(\boldsymbol{A},\boldsymbol{L})),\\
 & \vec{H}(\boldsymbol{A},\boldsymbol{L}) = (\boldsymbol{A} \circ \boldsymbol{L}) \cdot \boldsymbol{1}.\\
\end{aligned}
\end{equation*}

Reflecting the makespan minimisation problem's objective, we seek to minimise $G(\boldsymbol{A},\boldsymbol{L})$ while ensure that each task is completed, hence the constraint that the sum of each task entry, i.e. a column of $\boldsymbol{A}$, is 1.

This representation contains contains two reduction functions: firstly, the \emph{task latency reduction} ($\vec{H}(\boldsymbol{A},\boldsymbol{L})$), that is given by the element-wise multiplication or Hadamard product ($\boldsymbol{A} \circ \boldsymbol{L}$), dot multiplied by a vector of ones ($\boldsymbol{1}$); secondly, the \emph{platform latency reduction} ($G(\boldsymbol{A},\boldsymbol{L})$), that finds the maximum latency amongst the platforms for that allocation. 

These reduction functions map the allocation and task latency matrices ($\boldsymbol{A},\boldsymbol{L}$) to a vector of platform latencies, with an entry for each platform, and by which the vector of platform latencies are mapped to a scalar makespan value.

We now generalise this program, making use of the notion of domain metric models given in (\ref{eq:MetricMapping}). We assume that the valid variables $\vec{P}_v$ for each of the $\mu$ platforms are already known or can be easily approximated for each of the $\tau$ tasks. In (\ref{eq:AllocationGeneralised}) we seek an allocation ($\boldsymbol{A}$) so that we optimise the metric ($M_k$) for all tasks, as mapped by the task and platform reduction functions ($\boldsymbol{F}_k(\boldsymbol{A},\boldsymbol{P_v})$, $\vec{H}_k(\boldsymbol{A},\boldsymbol{P_v})$ and $G_k(\boldsymbol{A},\boldsymbol{P_v})$) into a scalar value.

\begin{equation}
\label{eq:AllocationGeneralised}
\begin{aligned}
& \underset{\boldsymbol{A}\in \{0,1\}^{\mu \times \tau}}{\text{optimise}} & & G_k(\boldsymbol{A},\boldsymbol{P_v}) \quad \boldsymbol{P_v} \in \mathds{R}^{\mu \times \tau \times p},\\
%& \text{where} & & \boldsymbol{E}(\boldsymbol{A},\boldsymbol{P_v}) = \max(\boldsymbol{G}(\boldsymbol{A},\boldsymbol{ETC})) \\
& \text{subject to} & & \sum\limits_{i=1}^\mu A_{i,j} = 1 \quad j =1,2,\ldots,\tau.\\
\end{aligned}
\end{equation}

where:
\begin{equation*}
\label{eq:AllocationGeneralisedDefines}
\begin{aligned}
 & G_k(\boldsymbol{A},\boldsymbol{P_v}): \vec{M_k} \to M_k \quad \vec{M_k} \in \mathds{R}^\mu, M_k \in \mathds{R},\\
 & \vec{H}_k(\boldsymbol{A},\boldsymbol{P_v}): \boldsymbol{M_k} \to \vec{M_k} \quad \boldsymbol{M_k} \in \mathds{R}^{\mu \times \tau},\\
 & \boldsymbol{F_k}(\boldsymbol{A},\boldsymbol{P_v}): (\boldsymbol{A},\boldsymbol{P_v}) \to \boldsymbol{M}_k.
\end{aligned}
\end{equation*}

\subsubsection{Splitting the Atomicity of Tasks}
Similar to the problem of heterogeneous characterisation, knowledge from the application domain can help find an efficient solution of the allocation problem. As structure of tasks is known in advance, the degree of parallelism within a task is known. As a result, allocation approaches can incorporate this information so as to allow for a task to be divided into subtasks while still providing a correct result. Making parallelism explicit enables a greater degree of work sharing between distributed computing resources, as discussed in Section \ref{sec:Background}. 

In this formulation, if the degree of parallelism is sufficiently large, this allows the elements of the allocation matrix, $\boldsymbol{A}$, to be ``relaxed", i.e. to be real-valued, and hence, the problem becomes linear and more tractable, as expressed in (\ref{eq:AllocationRelaxed}).

\begin{equation}
\label{eq:AllocationRelaxed}
\begin{aligned}
& \underset{\boldsymbol{A}\in \mathds R_+^{\mu \times \tau}}{\text{optimise}} & & G_k(\boldsymbol{A},\boldsymbol{P_v}) \quad \boldsymbol{P_v} \in \mathds{R}^{\mu \times \tau \times p},\\
& \text{subject to} & & \sum\limits_{i=1}^\mu A_{i,j} = 1 \quad j =1,2,\ldots,\tau.\\
\end{aligned}
\end{equation}

\subsubsection{Multimetric Pareto Surfaces}
As the metrics under consideration are also known ahead of execution, additional constraints may be added to the optimisation program for every other metric being considered ($M_x$), as described in (\ref{eq:AllocationConstrained}). This program requires that the allocation also satisfies all of the metric values specified in addition to optimising $M_k$.

\begin{equation}
\label{eq:AllocationConstrained}
\begin{aligned}
& \underset{\boldsymbol{A}\in \mathds R_+^{\mu \times \tau}}{\text{optimise}} & & G_k(\boldsymbol{A},\boldsymbol{P_v}) \quad \boldsymbol{P_v} \in \mathds{R}^{\mu \times \tau \times v},\\
& \text{subject to} & & \sum\limits_{i=1}^\mu A_{i,j} = 1 \quad j =1,2,\ldots,\tau,\\
& & & G_x(\boldsymbol{A},\boldsymbol{P_v}) = G_x \quad x \neq k,x = 1,2,\ldots,m.
\end{aligned}
\end{equation}

The multimetric optimisation program can be used to generate a Pareto surface, representing the heterogeneous computing platforms in terms of domain metric trade-offs. These trade-offs are achieved by changing the allocation of tasks to platforms.

For the metric Pareto surface to be populated, a range of values are required for all metrics that satisfy the program. This ranges of metrics can be found using the $\epsilon$-constraint method, as described by Kirlik and Say{\i}n~\cite{epsilonconstraint}.

This multimetric Pareto surface represents the culmination of our application of domain knowledge to heterogeneous computing. Our domain specific approach abstracts the allocation of task to platforms as the balancing of domain metrics. Hence, programmers such as Julia would be seamlessly able to use the capabilities of their heterogeneous platform by merely balancing their objectives.

\section{Case Study: Derivatives Pricing}
\label{sec:CaseStudy}
In this case study, we show how the domain specific modelling and allocation approach that we developed in the previous section can be applied to a new domain. We use the derivatives pricing domain, as we have done in other work~\cite{FSP2015}, in the broader area of computational finance, given its importance in global commerce.

We first describe the derivatives pricing domain, we then introduce the metric models of latency and accuracy that we use in our evaluation, and finally how the associated allocation problem can be solved using three different methods.

\subsection{Derivative Pricing Application Domain}
\label{sec:ComputationalFinanceDomain}

In this subsection we introduce the computational finance application domain, derivatives pricing, that we use as an example in the explanation of our approach and experiments to justify our claims. First, we describe derivatives pricing in general, and then define it as a computational domain. Finally, we describe an implementation of the domain. 

%domain specific application software framework, the Forward Financial Framework ($F^3$), that implements the domain.

\subsubsection{Derivatives Pricing Background}

Computational finance is an important activity in modern commerce. The problems in the area are concerned with the quantitative measurement of uncertainty or risk. Derivatives pricing is one of the largest activities in this area, with \( \approx \) \$100 trillion of derivatives products currently active. Derivative pricing is also computationally intensive, and as a result is a major consumer of high performance computing, including multicore CPUs and GPUs.

%\subsubsection{Forward Looking Option Pricing}
An example of a derivative is an option contract. An option is contract where a holder pays a premium to the writer in order to obtain rights with regards to an underlying, an asset such as a stock or commodity. This right either allows the holder to buy or sell the underlying at a defined strike price at a defined exercise time. The holder has bought the \emph{right} to exercise the transaction if they so choose, and is in no way obligated to so. In derivatives pricing, the intrinsic value of the option is the payoff, the difference between the strike price and spot price of the underlying at the exercise time, or zero, whichever is higher~\cite{Hull}.

%\begin{figure}
%\centering
%\includegraphics[width=.5\textwidth]{figs/option_simple_overview.pdf}
%\caption{Overview of option pricing}
%\label{fig:PlatformCurves}
%\end{figure}

%\subsubsection{Monte Carlo-based Option Pricing}
The popular Monte Carlo technique for option pricing uses random numbers to create scenarios or simulation paths for the underlying based upon a model of its spot price evolution. The average outcome of these paths is then used to approximate the payoff~\cite{Hull}, as illustrated in Figure \ref{fig:OptionMCOverview}. Although computational expensive, this technique is robust, capable of tolerating underlying models with many more stochastic variables than competing methods~\cite{DSL_MonteCarlo,Hull}. Another advantage is that it is amenable to parallel execution. In fact, Monte Carlo is the canonical ``Embarrassingly Parallel'' algorithm~\cite{view_from_berkeley_2006}.

\begin{figure}
\centering
\includegraphics[width=.38\textwidth]{figs/option_mc_overview.pdf}
\caption{Overview of Monte Carlo derivatives pricing.}
\label{fig:OptionMCOverview}
\end{figure}

%i.e. $${V_{t}=e^{-r(T-t)}\int_{w}V(w)d\mathbf{P}(w)\approx e^{-r(T-t)}\frac{1}{N}\sum_{i=0}^{x_{N_{pp}-1}}V(S_{i})}$$
%Where $V_{t}$ is the current value of the option, $e^{-r(T-t)}$ the discount factor, $P(w)$ the probability space defined by the underlying asset and $S_{i}$ the price of the asset. 
%Each underlying asset's price evolution and its interaction with the derivative product comprises an indepedent \emph{map} operation, while the \emph{reduction} is the average of the payoffs. We have described this pattern in C code in listing 1.

\subsubsection{Application Domain}
We now describe derivative pricing as an application domain in terms of types and functions.

%This domain is relatively simple, with two fundamental types, underlyings and derivatives, and one function, pricing.

%\subsubsection{The Underlying and Derivative Types}
\emph{The Underlying and Derivative Types}: the data in an option pricing task may be subdivided into two components, the derivative contract which is being priced and the underlying asset from which that derivative derives its value. The underlying encapsulates the probabilistic model, such as the Black-Scholes or Heston, being used to model the behaviour of the asset. The derivative embodies the details of the option contract both during the lifetime of the option as well at its expiration. 

The communication within a task can be formulated as a directed, acyclic graph, in which underlyings feed their prices to the derivatives which depend upon them.

%\subsubsection{Pricing Function}
\emph{Pricing Function}: The option pricing domain's sole function is finding the value of a type. Hence, the pricing function is typically only applied to the derivative type, as by definition an underlying type can provide its price at any point in time. Different techniques such as Monte Carlo or Tree-based methods could be used to implement the pricing function, provided the end result is the price of the derivative under consideration.

%An advantage of using the Monte Carlo approach is that it is amenable to task parallel execution. In fact, it is  a ``Embarrassingly Parallel'' algorithm\cite{view_from_berkeley_2006}. Each underlying asset's price evolution and its interaction with the derivative product comprises an indepedent \emph{map} operation, while the \emph{reduction} is the average of the payoffs. We have described this pattern in C code in listing 1.

%\begin{lstlisting}[language=C,caption={Monte Carlo Option Pricing},basicstyle=\ttfamily]
%//Map behaviour
%for(i=0;i<PATHS;++i){
%    state = path_init(seed++);
%    for(j=0;j<PATH_POINTS;++j)
%        state = path(state);
%    value[i] = payoff(state);
%} 

%//Reduce Behaviour
%for(i=0;i<PATHS;++i)  
%    result += value[i]/PATHS;
%\end{lstlisting}

%\todo[inline]{Add path code for Black Scholes underlying}

\subsubsection{Domain Implementation}
We now describe the Forward Financial Framework\footnotemark ($F^{3}$), an open source, financial domain application framework that we have developed, that implements the derivatives pricing domain.

\footnotetext{\url{https://github.com/Gordonei/ForwardFinancialFramework}}

%We first describe the high level abstractions that the domain programmer interacts with, and then on the lower level, heterogeneous computing implementations automatically generated by the framework.

\emph{Task description}: $F^3$ is implemented at the high level as a Python library~\cite{F3_P2S2}. A domain user, a financial engineer such as Julia, may use $F^{3}$'s classes to describe their derivatives pricing computations. There are three fundamental base classes that mirror the key concepts in the domain: derivatives, underlyings and solvers.

The underlying and derivative objects capture the attributes and behaviours of the underlying and derivative types as described above. The solver class is a collection for the derivatives that the programmer wishes to price as well as the platforms they wish to use.

%To support the pricing of the derivatives, the solver must be capable of three functions: generation, compilation and execution of a pricing algorithm for the specified derivatives upon at least one of the target platforms.

%To use the framework, the domain programmer imports the framework library within their code, instantiates the underling and derivative objects they wish to price, and allocate the derivatives to a solver object.

\emph{Heterogeneous Implementations}: The solver class supports three behaviours upon heterogeneous platforms: code generation, compilation and execution. $F^3$ uses a wide array of back-end technologies: multicore CPUs using POSIX C; GPUs, Xeon Phi coprocessors and Altera FPGAs using the OpenCL standard~\cite{opencl}; Maxeler FPGAs using Maxeler's MaxJ.

All of the platform back-ends use a host-accelerator configuration, where a high performance coprocessor or subsystem is managed by a commodity CPU host. Communication between $F^3$ and platforms use the Secure Shell (SSH) protocol, allowing for tasks to be executed on remote platforms via TCP/IP networks.

\subsection{Financial Latency and Accuracy Metric Models}
As described in the previous subsection, for our example domain, pricing is the only function required. In this subsection, we develop the metric models, as per (\ref{eq:MetricMapping}), for the metrics of latency, (\ref{eq:LatencyModel}), and price accuracy, (\ref{eq:AccuracyModel}), for the pricing function as implemented using the Monte Carlo algorithm in $F^3$. This is in contrast to our other work, where we developed a financial cost metric model~\cite{FSP2015}.

\subsubsection{Latency Model} 

The latency between when a pricing operation is initiated and when it returns a price is fundamentally important within the financial domain~\cite{Hull}. This is because the time at which prices are received affects how traders use those prices. Minimising the latency of the pricing operation is desirable, as this confers first-mover advantage. 

We have used a simple, linear latency model in (\ref{eq:LatencyModel}), a function of a single domain variable, the number of simulation paths ($n$), i.e $n \in \mathds{Z},n \in \vec{P_v}$. The linear nature of the model reflects the $O(N)$ complexity of the Monte Carlo Algorithm. The model's coefficient ($\beta$) translates to the time spent per Monte Carlo path. Similarly, $\gamma$, the constant component of the latency metric model, captures the fixed time spent initialising the computation, as well as any time spent communicating the task to, and returning the result from, the target platform.

\begin{equation}
\label{eq:LatencyModel}
f_L(n) = \beta n + \gamma.
\end{equation}

\subsubsection{Accuracy Model}
In the financial domain, the accuracy of a computed price is expressed in probabilistic terms. When using the Monte Carlo technique, the size of 95\% confidence interval, as measured in currency of pricing (i.e. \$) is used. The accuracy measure is the size of the finite interval around the computed price for which there is a 95\% confidence that the true value lies within that interval. As small a confidence interval as possible is desired, as this means less variance in the price has to be accounted for.

The accuracy model that we used is based upon the convergence of the Monte Carlo algorithm, which is given by the inverse square root of the paths, scaled by a coefficient ($\alpha$). The model is given in (\ref{eq:AccuracyModel}).
\begin{equation}
\label{eq:AccuracyModel}
f_C(n) = \alpha n^{-\frac{1}{2}}.
\end{equation}

\subsubsection{Combined Model}

To relate the two domain metrics of latency and accuracy, we can solve for $n$ and use it to relate (\ref{eq:LatencyModel}) and (\ref{eq:AccuracyModel}) into a trade-off between the latency and accuracy ($c$), as given in (\ref{eq:CombinedModel}). %This lets us find $n$, $P_v$ in this implementation, for a given accuracy ($c$).

\begin{equation}
\label{eq:CombinedModel}
\begin{aligned}
& & f_L(c) = \delta c^{-2} + \gamma.\\
%& \text{where} & \delta = \beta\alpha^2 \\
\end{aligned}
\end{equation}

where:
\begin{equation*}
\label{eq:CombinedModelDefine}
\begin{aligned}
& \delta = \beta\alpha^2.\\
\end{aligned}
\end{equation*}

\subsection{Derivative Pricing Task Allocation}
%\todo[inline]{Revisit option pricing task partitioning subsection}
We can now formulate the allocation problem using the derivative pricing metric models from the previous subsection, as well as outline three approaches for solving the problem.

\subsubsection{Reformulating the Allocation Problem}
%We seek to minimise: \todo{rewrite using succinct, optimise over formulation}
%$$\max(\boldsymbol{F}(\boldsymbol{A}) \cdot \boldsymbol{1})$$
%where:
%%$$ \boldsymbol{ETC} = \boldsymbol{F}(\boldsymbol{A}',\boldsymbol{A})$$
%$$ \boldsymbol{F}(\boldsymbol{A}) = \boldsymbol{\beta} \circ \boldsymbol{A} +\boldsymbol{\gamma} \circ \lceil \boldsymbol{A} \rceil $$
%$$ \boldsymbol{A} :=  \{A \in \mathds{R}  \mid 0 \leq A \leq 1\}^{\mu \times \tau}$$
%subject to:
%$$ \sum\limits_{i=1}^\mu A_{i,j} = 1 \quad j =1,2,..,\tau $$

In (\ref{eq:AppliedAllocation}) the unified domain metric model described in (\ref{eq:CombinedModel}) has been applied to the general, constrained allocation problem formulated in (\ref{eq:AllocationConstrained}). The vector $\vec{c}$ gives the required accuracies for the tasks, while $\boldsymbol{\gamma}$ is the task-platform constant matrix. Similarly, $\boldsymbol{\delta}:\vec{c}^2$ is the element-wise division of the delta coefficients by the required accuracies of the tasks. In this case, we have not to had to add an additional accuracy constraint, as the unified metric model has already captured this constraint.

\begin{equation}
\label{eq:AppliedAllocation}
\begin{aligned}
& \underset{\boldsymbol{A}\in \mathds R_+^{\mu \times \tau}}{\text{minimise}} & & G_L(\boldsymbol{A},\vec{c}) \quad \vec{c} \in \mathds{R}_+^\tau \\
%& \text{where} & & \boldsymbol{F_L}(\boldsymbol{A},\boldsymbol{C}) = \max(\boldsymbol{G_L}(\boldsymbol{A},\boldsymbol{C})) \\
%& & & \boldsymbol{G_L}(\boldsymbol{A},\boldsymbol{C}) = (\boldsymbol{\delta}:\boldsymbol{C}^2 \circ \boldsymbol{A} +\boldsymbol{\gamma} \circ \lceil \boldsymbol{A} \rceil)\cdot \boldsymbol{1}\\
%& & & \boldsymbol{\delta} \in \mathds R_{> 0}^{\mu \times \tau},\boldsymbol{\gamma} \in \mathds R_{> 0}^{\mu \times \tau} \\
& \text{subject to} & & \sum\limits_{i=1}^\mu A_{i,j} = 1 \quad j =1,2,\ldots,\tau.\\
\end{aligned}
\end{equation}

where:
\begin{equation*}
\label{eq:AppliedAllocationDefined}
\begin{aligned}
G_L(\boldsymbol{A},\vec{c}) &= \max(\vec{H_L}(\boldsymbol{A},\vec{c})),\\
\vec{H_L}(\boldsymbol{A},\vec{c}) &= (\boldsymbol{\delta}:\vec{c}^2 \circ \boldsymbol{A} +\boldsymbol{\gamma} \circ \lceil \boldsymbol{A} \rceil)\cdot \boldsymbol{1},\\
& \qquad \boldsymbol{\delta} \in \mathds R_+^{\mu \times \tau},\boldsymbol{\gamma} \in \mathds R_+^{\mu \times \tau} \\
\end{aligned}
\end{equation*}

An important feature of the formulation given in (\ref{eq:AppliedAllocation}) is its non-linearity as a result of the ceiling function in $\vec{H_L}(\boldsymbol{A},\vec{c}) $. This reflects (\ref{eq:LatencyModel}) and (\ref{eq:CombinedModel}), as there is a constant value for each platform-task entry, regardless of the scale of the allocation.

%the problem has been changed from being a binary allocation problem to a Mixed Integer Programming problem, where $\boldsymbol{A}$ is a continuous version of the allocation matrix.

%We have investigated three approaches to solving this program from the categories identified in Section~\ref{sec:PartitionApproaches}.

\subsubsection{Proportional Allocation Heuristic}
%The first best platform heuristic we propose in (\ref{eq:BestPlatform}) is intuitive: all of the tasks are allocated the single platform that completes all the tasks with the shortest makespan.

%\begin{equation}
%\label{eq:BestPlatform}
%\begin{aligned}
%& \vec{L}_i < \vec{L}_x & (i,x)=1,2,\ldots,\mu,i \neq x,\\
%& A_{i,j} = 1 & j =1,2,\ldots,\tau. 
%& \text{where} & \delta = \beta\alpha^2 \\
%\end{aligned}
%\end{equation}

%where:
%\begin{equation*}
%\label{eq:BestPlatformDefines}
%\begin{aligned}
%& \vec{L} = \vec{H_L}(\boldsymbol{1},\vec{c}).\\
%\end{aligned}
%\end{equation*}

The first allocation approach, the proportional allocation heuristic, is given in (\ref{eq:ProportionalAllocation}). The heuristic allocates tasks inversely proportionally to the individual makespans of all of the platforms, attempting to balance tasks according to the relative capabilities of the different platforms.

%It is a minor refinement of the first heuristic, 

%as given in Equation~(\ref{eq:ProportionalAllocation}) , $\vec{F_{TL}}$

\begin{equation}
\label{eq:ProportionalAllocation}
\vec{A}_{i,j} = \left (\vec{L}_i \sum^\mu_{o=1}{\frac{1}{\vec{L}_o}} \right ) ^{-1} \quad i = 1,2,\ldots,\mu,j = 1,2,\ldots,\tau.
\end{equation}

where:
\begin{equation*}
\label{eq:ProportionalAllocationDefines}
\begin{aligned}
& \vec{L} = \vec{H_L}(\boldsymbol{1},\vec{c}).\\
\end{aligned}
\end{equation*}

The heuristic only require an estimate of the relative latency of all tasks upon each platform. The proportional allocation heuristic works well provided the elements of $\boldsymbol{\gamma}$ are significantly smaller than the elements of $\boldsymbol{\delta}:\vec{c}^2$ for all platforms. If not, the tasks' cumulative constants dominate each platform's makespan, regardless of allocation. Theoretically, if there were no constant components, i.e. no setup time, then this heuristic would return the optimal allocation.

%The best platform heuristic performs well when there is a single platform significantly faster than the others. 

\subsubsection{Machine Learning Allocation}
The second approach uses the heuristic as a starting allocation of tasks. The platform reduction function $G_L(\boldsymbol{A},\vec{c})$ is then specified as the objective function for a time-constrained, global optimisation algorithm, the simulated annealing algorithm provided in SciPy~\cite{SCIPY}, combined with a ``polishing", convex optmisation algorithm, Danzig's Simplex algorithm, also available in SciPy. 

%By using the task-platform information, this approach should improve upon the intuitive heuristics described above.
%it also uses the metric model-predicted setup latency ($\boldsymbol{\gamma}$) and proportional latency ($\boldsymbol{\delta}:\boldsymbol{C}^2$) information of each task upon each platform.

As this approach incorporates domain specific platform and task information as well as the heuristic, it should at worst confirm the heuristic, and at best find the most optimal allocation. As we will show in Section \ref{sec:DomainPartitionerEvaluation}, the objective function's linearity is a key determinate of the allocation optimality. Furthermore, another significant factor is problem size, as this problem suffers from the curse of dimensionality with respect to both $\mu$ and $\tau$.

\subsubsection{Mixed Integer Linear Programming Allocation}
The MILP approach uses the formulation of the domain allocation problem as the input to an open source, constrained integer programming framework, SCIP~\cite{SCIP}. SCIP applies global optimisation techniques as well as a variety of mathematical transformations and heuristics to solve the constrained problem.

SCIP accepts problems in a form very similar to (\ref{eq:AppliedAllocation}), expressed in Zuse Institut Mathematical Programming Language (ZIMPL)~\cite{ZIMPL}, which $F^3$ is capable of generating. However ZIMPL/SCIP does not allow for non-linear objective and constraint functions. This requires the problem to be reformulated as given in (\ref{eq:AppliedAllocationReformulated}), adding additional variables ($G_L$ and $\boldsymbol{B}$) and constraints to capture the non-linearities in the problem.

\begin{equation}
\label{eq:AppliedAllocationReformulated}
\begin{aligned}
& \underset{G_L,\boldsymbol{A}, \boldsymbol{B}} {\text{minimise}} & & G_L \quad G_L \in \mathds{R}_+,\boldsymbol{A}\in \mathds{R}_+^{\mu \times \tau}, \boldsymbol{B} \in \{0,1\}^{\mu \times \tau}, \\
& \text{subject to} & & \sum\limits_{i=1}^\mu A_{i,j} = 1 \quad j =1,2,\ldots,\tau, \\
& & & H_{L,i}(\boldsymbol{A},\vec{c}) \leq G_L \quad \vec{c} \in \mathds{R}_+^\tau, i = 1,2,\ldots,\mu, \\
& & & A_{i,j} \leq B_{i,j} \quad i = 1,2,\ldots,\mu,j = 1,2,\ldots,\tau.
\end{aligned}
\end{equation}
Where
\begin{equation*}
\label{eq:AppliedAllocationReformulatedDefined}
\begin{aligned}
\vec{H_L}(\boldsymbol{A},\vec{c}) &= (\boldsymbol{\delta}:\vec{c}^2 \circ \boldsymbol{A} +\boldsymbol{\gamma} \circ \boldsymbol{B} )\cdot \boldsymbol{1}.\\
\end{aligned}
\end{equation*}

Although binary integer linear programs are known to be NP-complete~\cite{Karp21Problems}, there has been progress in solving these problems efficiently~\cite{WhyMILPWorks}. 

%Furthermore, a useful feature of this approach is that the dual formulation of the problem can be used to prove a solution provably optimal.

%\section{Evaluation}
\section{Evaluating Derivative Pricing Metrics}
\label{sec:DomainMetricModelEvaluation}
In this section, we evaluate our claim that the derivatives pricing metric models are able to characterise tasks on platform. 

To do so we need to evaluate the following two properties for the latency and accuracy models using a large, diverse set of platforms and tasks:
\emph{Incorporation}: When provided with additional information, the domain metric model predict the run-time value of that domain metric more accurately.
\emph{Extrapolation}: For a given amount of benchmarking, the domain metric values predicted by the models remains reasonably close to those seen at run-time for an increasing problem size.

To assess the degree to the properties were achieved, we measured the relative error ($E_k$) as given in (\ref{eq:RelativeError}), where the absolute difference between the predicted metric value ($f_k(n)$) and the run-time value ($\hat{f}_{k,n} $) is divided by the run-time value. The run-time metric value is measured when the task is run with the specified number of paths ($n$). 

%We measure this for both the latency and accuracy models described above across sets of heterogeneous computing platforms and tasks.

\begin{equation}
\label{eq:RelativeError}
%\begin{aligned}
E_k = {{\left| f_k(n) - \hat{f}_{k,n} \right|}\over{\tilde{f}_{k,n}}}
%\end{aligned}
\end{equation}

\subsection{Experimental Setup}
%In this subsection we describe the broad sets of heterogeneous platforms and tasks that we use to test the incorporation and extrapolation properties of the latency and accuracy metric models for the derivatives pricing domain.

%\emph{Derivatives pricing tasks}:
\subsubsection{Derivatives Pricing Tasks}
Table \ref{table:TaskDetails} provides an overview of the 128 option pricing tasks that were used to evaluate the financial domain metric models. In addition to the types of underlying and derivatives used, the total amount of computational work for each task is specified. 

The domain parameters for the pricing task operations, such as the proprieties of underlying model, were generated using uniform random numbers within the values of the Kaiserslautern option pricing benchmark~\cite{KS_Option_Benchmark}. We used a rejection procedure to keep the relative complexity of the pricing tasks within an order of magnitude.

\begin{table}
%\setlength{\fboxsep}{0pt}%
%\setlength{\fboxrule}{0pt}%
%\begin{boxedverbatim}

%\begin{center}
\caption{Evaluation workload of 128 derivative pricing tasks. Underlying types are Black-Scholes (\emph{BS}) and Heston (\emph{H}) model-based. Derivative types are Asian (\emph{A}), Barrier (\emph{B}), Double Barrier (\emph{DB}), Digital Double Barrier (\emph{DBB}) and European Options (\emph{E}).}
\label{table:TaskDetails}
%\centering
%\tabsize
\begin{tabular}{c|cccc}
    \parbox[c]{1.5cm}{Task Designation} & \parbox[c]{1cm}{Number} & \parbox[c]{1.2cm}{Underlying} & \parbox[c]{1cm}{Option} & \parbox[c]{1.5cm}{Computational Operations (kFLOP~/~path)}\\\hline 
    BS-A & 10 & BS & A & 139.267 \\ %[2ex]
    BS-B & 10 & BS & B & 139.266 \\ %[2ex]
    BS-DB & 10 & BS & DB & 143.360 \\ %[2ex]
    BS-DDB & 5 & BS & DDB & 143.361 \\ %[3ex]
    H-A & 25 & H & A & 319.492 \\ %[2ex]
    H-B & 29 & H & B & 319.491 \\ %[2ex]
    H-DB & 29 & H & DB & 323.585 \\ %[2ex]
    H-DDB & 5 & H & DDB & 323.586 \\ %[3ex]
    H-E & 5 & H & E & 315.395 \\ %[2ex]
\end{tabular}
%\end{table}

%\end{boxedverbatim}
%\end{center}
%\vspace{-6pt}
%\caption{Example table layout}
%\vspace{-6pt}
\end{table}

%\emph{Heterogeneous Platforms}: 
\subsubsection{Heterogeneous Platforms}
An overview of the heterogeneous platforms that we used are described in Table \ref{table:PlatformOverview}. The first class of platform heterogeneity is device type and manufacturer - we used a wide array of multicore CPUs, GPU and FPGA-based computational platforms from a variety of vendors.  The other is the diversity of interconnections used between the computational platforms, achieved with varied geographic locations.

%The next dimension is the manufacturer within each device category, which we have varied as much as is possible.
%Related to this, the geographic location was varied across all of the platforms used so as to have task communication times that are orders of magnitude greater in latency.

The computational characteristics of the platforms are also described in Table \ref{table:PlatformOverview}. We describe the compute capabilities of the experimental platforms using the Kaiserslautern option pricing benchmark~\cite{KS_Option_Benchmark} and the Network Round-trip Time (RTT) as measured by the \emph{ping} network utility.

As the Monte Carlo algorithm being used is amenable to parallel execution, it is unsurprising that GPUs provide the best application performance, although an important caveat is that these performance figures are of implementations produced by $F^3$. A prominent data-point in terms of network latency is the Remote Server and Phi, which have orders of magnitude longer communication times than the other platforms due to being located in Cape Town, South Africa.

We expect the compute capabilities of platforms to determine the coefficient of the latency models ($\beta$) while the network latency will determine the constant coefficient, ($\gamma$).

%\footnotetext[1]{\url{http://www.uni-kl.de/en/benchmarking/option-pricing/}}

%In addition to providing the clock speed, we describe the degree of task and Single Instruction, Multiple Data (SIMD) parallelism available in each device. The product of the clock rate and parallelism factors are good predictors of Multicore CPU and GPU performance for compute-bound Monte Carlo pricing operation such as the one we consider here. The clock rate-parallelism product is less useful for FPGA-based devices, as it does not capture the fine-grained, pipeline parallelism that these devices exploit. The difficulty of comparison between heterogeneous computing platforms is indeed one of the motivations for this approach as the application domain provides a means of comparison abstracted from the platform architectures.

\begin{table*}
%\setlength{\fboxsep}{0pt}%
%\setlength{\fboxrule}{0pt}%
\begin{center}
%\begin{boxedverbatim}

%\begin{table}
\caption{Overview of Experimental Heterogeneous Computing Platforms}
\label{table:PlatformOverview}
\centering
%\tabsize
\begin{tabular}{c|cccccrr}
\parbox[l]{1.2cm}{Device Category} & \parbox[c]{1.5cm}{\center Device Designation} & \parbox[c]{1.2cm}{Device Vendor} & \parbox[c]{2.3cm}{Device Name} & \parbox[c]{1.5cm}{Network Location} & \parbox[c]{3cm}{Geographic Location} & \parbox[c]{1.8cm}{\center Application Performance (GFLOPS)} & \parbox[c]{1.5cm}{\center Network Round-trip Time (ms)} \\ \hline
 \multirow{9}{*}{\parbox[l]{1.2cm}{CPUs}}
    & Desktop &  Intel \textsuperscript{\textregistered} & \parbox[c]{2.3cm}{Core \textsuperscript{\textregistered} i7-2600} & Localhost & \parbox[c]{3.3cm}{ICL, London, UK} & 5.916 & 0.024 \\ %[2ex]
    & Local Server & AMD \textsuperscript{\textregistered} & \parbox[c]{2.3cm}{Opteron \textsuperscript{\textregistered} 6272} & LAN & \parbox[c]{3.3cm}{ICL, London, UK} & 27.002 & 0.380 \\ %[2ex]
    & Local Pi & ARM \textsuperscript{\textregistered} & \parbox[c]{2.3cm}{11 76JZF-S} & LAN & \parbox[c]{3.3cm}{ICL, London, UK} & 0.049 & 2.463\\ %[2ex]
    & Remote Server & Intel \textsuperscript{\textregistered} & \parbox[c]{2.3cm}{Xeon \textsuperscript{\textregistered} E5-2680} & WAN & \parbox[c]{3.3cm}{UCT, Cape Town, ZA} & 11.523 & 3300.000 \\ %[2ex]
    & AWS Server EC1 & Intel \textsuperscript{\textregistered} & \parbox[c]{2.3cm}{Xeon \textsuperscript{\textregistered} E5-2680} & WAN & \parbox[c]{3.3cm}{AWS, USA East Region} & 12.269 & 88.859 \\ %[2ex]
    & AWS Server EC2 & Intel \textsuperscript{\textregistered} & \parbox[c]{2.3cm}{Xeon \textsuperscript{\textregistered} E5-2670} & WAN & \parbox[c]{3.3cm}{AWS, USA East Region} & 4.913 & 88.216 \\ %[2ex]
    & AWS Server WC1 & Intel \textsuperscript{\textregistered} & \parbox[c]{2.3cm}{Xeon \textsuperscript{\textregistered} E5-2680} & WAN & \parbox[c]{3.3cm}{AWS, USA West Region} & 12.200 & 157.100\\ %[2ex]
    & AWS Server WC2 & Intel \textsuperscript{\textregistered} & \parbox[c]{2.3cm}{Xeon \textsuperscript{\textregistered} E5-2670} & WAN & \parbox[c]{3.3cm}{AWS, USA West Region} & 4.926 & 159.578\\ %[2ex]
    & GCE Server & Intel \textsuperscript{\textregistered} & \parbox[c]{2.3cm}{Xeon \textsuperscript{\textregistered}} & WAN & \parbox[c]{3.3cm}{GCE, USA Central Region} & 6.022 & 111.232\\    \hline
 \multirow{5}{*}{\parbox[l]{1.2cm}{GPUs}}
    & Local GPU 1 & AMD\textsuperscript{\textregistered} & \parbox[c]{2.3cm}{FirePro \textsuperscript{\textregistered} W5000} & LAN & \parbox[c]{3.3cm}{ICL, London, UK} & 212.798 & 0.269 \\ %[2ex]
    & Local GPU 2 & Nvidia\textsuperscript{\textregistered} & \parbox[c]{2.3cm}{Quardo \textsuperscript{\textregistered} K4000} & LAN & \parbox[c]{3.3cm}{ICL, London, UK} & 250.027 & 0.278 \\ %[2ex]
    & Remote Phi & Intel\textsuperscript{\textregistered} & \parbox[c]{2.3cm}{Xeon Phi \textsuperscript{\textregistered} 3120P} & WAN & \parbox[c]{3.3cm}{UCT, Cape Town, ZA} & 70.850 & 3300.000 \\ %[2ex]
    & AWS GPU EC & Nvidia\textsuperscript{\textregistered} & \parbox[c]{2.3cm}{Grid \textsuperscript{\textregistered} GK104}  & WAN & \parbox[c]{3.3cm}{AWS, USA East Region} & 441.274 & 88.216 \\ %[2ex]
    & AWS GPU WC & Nvidia\textsuperscript{\textregistered} & \parbox[c]{2.3cm}{Grid \textsuperscript{\textregistered} GK104} & WAN & \parbox[c]{3.3cm}{AWS, USA West Region} & 406.230 & 159.578 \\    \hline
 \multirow{2}{*}{\parbox[l]{1.2cm}{FPGAs}}
    & Local FPGA 1 & Xilinx\textsuperscript{\textregistered} & \parbox[c]{2.3cm}{Virtex \textsuperscript{\textregistered} 6 475T} & LAN & \parbox[c]{3.3cm}{ICL, London, UK} & 114.590 & 0.217 \\ %[2ex]
    & Local FPGA 2 & Altera\textsuperscript{\textregistered} & \parbox[c]{2.3cm}{Stratix \textsuperscript{\textregistered} V D5} & LAN & \parbox[c]{3.3cm}{ICL, London, UK} & 161.074 & 0.299 \\ %[2ex]
\end{tabular}
%\end{table}

%\end{boxedverbatim}
\end{center}
%\vspace{-6pt}
%\caption{Example table layout}
%\vspace{-6pt}
\end{table*}

\subsection{Model Error Results}
%\emph{Latency Model}:
\subsubsection{Latency Model}
The latency model results are given in Figures \ref{fig:LatencyModelSanity} and \ref{fig:LatencyModelScaling}. The latency models were evaluated on a per platform basis, as well as the geometric mean of the three platform categories. 

The mean error figures for all of the tasks upon the platform reported, with the error bars being too small to plot. The independent variable is the ratio of the Monte Carlo benchmark vs run-time paths, so as to report on all tasks across each platform.

\begin{figure*}%
\centering

\begin{subfigure}{.75\columnwidth}
        \includegraphics[width=\columnwidth]{figs/latency_model_sanity_plot_categories.pdf}%
        \caption{Device Categories}
        \label{fig:LatencyModelSanityCatogories}
\end{subfigure}\quad%
\begin{subfigure}{.75\columnwidth}
        \includegraphics[width=\columnwidth]{figs/latency_model_sanity_plot_cpus.pdf}%
        \caption{CPUs}
        \label{fig:LatencyModelSanityCPUs}
\end{subfigure}\vskip\baselineskip%
\begin{subfigure}{.75\columnwidth}
        \includegraphics[width=\columnwidth]{figs/latency_model_sanity_plot_gpus.pdf}%
        \caption{GPUs}
        \label{fig:LatencyModelSanityGPUs}
\end{subfigure}\quad%
\begin{subfigure}{.75\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figs/latency_model_sanity_plot_fpgas.pdf}%
        \caption{FPGAs}
        \label{fig:LatencyModelSanityFPGAs}
\end{subfigure}%

\caption{Error of latency models for a total run-time target of 10 minutes ($\frac{4.69s}{\text{task}}$) and varying benchmark time, evaluating model incorporation.}
\label{fig:LatencyModelSanity}
\end{figure*}

Figure~\ref{fig:LatencyModelSanity} illustrates that as a longer benchmarking procedure is performed relative to the total fixed run-time of 10 minutes (or 4.69 seconds per task) being predicted by the model, the models became more accurate. Figure~\ref{fig:LatencyModelScaling} shows how the models scale as the run-time prediction target is increased for a fixed benchmarking time of 4.69 seconds per task or 10 minutes in total, and an increasing run-time target. The remote Phi and server models' poor performance are notable data points.

%Hence, these results demonstrate the incorporation property.

\begin{figure*}%
\centering
%\includegraphics[width=\textwidth]{figs/latency_model_plots_sanity.png}

\begin{subfigure}{.75\columnwidth}
        \includegraphics[width=\columnwidth]{figs/latency_model_scaling_plot_categories.pdf}%
        \caption{Device Categories}
        \label{fig:LatencyModelScalingCatogories}
\end{subfigure}\quad%
\begin{subfigure}{.75\columnwidth}
        \includegraphics[width=\columnwidth]{figs/latency_model_scaling_plot_cpus.pdf}%
        \caption{CPUs}
        \label{fig:LatencyModelScalingCPUs}
\end{subfigure}\vskip\baselineskip%
\begin{subfigure}{.75\columnwidth}
        \includegraphics[width=\columnwidth]{figs/latency_model_scaling_plot_gpus.pdf}%
        \caption{GPUs}
        \label{fig:LatencyModelScalingGPUs}
\end{subfigure}\quad%
\begin{subfigure}{.75\columnwidth}
        \includegraphics[width=\columnwidth]{figs/latency_model_scaling_plot_fpgas.pdf}%
        \caption{FPGAs}
        \label{fig:LatencyModelScalingFPGAs}
\end{subfigure}%

\caption{Error of latency models for a fixed benchmark total time of 10 minutes ($\frac{4.69s}{\text{task}}$) and varying run-time targets, evaluating model extrapolation.}
\label{fig:LatencyModelScaling}
\end{figure*}

%The results demonstrate that for a run-time target of more than an order of magnitude greater than the benchmarking procedure, the latency models is capable of extrapolating. 

%On the device category level, the FPGA platforms scale well for an order of magnitude, and in fact improve with accuracy, suggesting that the error largely resides in the setup component of the task model, and as the task becomes longer it is proportionately shrinking faster than any other source of error is growing. The CPU and GPU device categories appear to perform poorly over an order of magnitude, however upon closer inspection it is the error scaling characteristics of the Remote Server and Remote Xeon Phi platform that are the main culprits of the error, while the other platforms scaling well (indeed, several improve in accuracy similar to the FPGAs). The poor performance of the remote platforms is explained by the relatively large, yet highly variable network round trip time (see \ref{table:PlatformComputationalDetails}) resulting in poor prediction models.

%\emph{Accuracy Model}:
\subsubsection{Accuracy Model}
The accuracy model results are given in Figures \ref{fig:AccuracyModelSanity} and \ref{fig:AccuracyModelScaling}. The accuracy model results are presented as  minimum, geometric mean and maximum of the model results within the pricing task categories. Similar to the latency model, the ratio of benchmark to run-time paths is the independent variable.

%Figure \ref{fig:AccuracyModelSanity} illustrates that as information is added to the benchmarking procedure, the relative error in the accuracy prediction model decreases across the different tasks categories. This is explained by the convergence of the Monte Carlo algorithm being a proven property with a very limited degree of variance, hence a low number of data points are required to solve for the apparent convergence coefficient ($\alpha$). The tasks with Heston underlyings present a relatively high maximum error, however, as can be seen by the task category geometric mean these error averages out close to 10\%.

\begin{figure*}%
\centering
%\includegraphics[width=\textwidth]{figs/latency_model_plots_sanity.png}

\begin{subfigure}{.45\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figs/accuracy_model_sanity_0125.pdf}%
        \caption{1:8}
        \label{fig:AccuracyModelSanity0125}
\end{subfigure}\hfill%
\begin{subfigure}{.45\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figs/accuracy_model_sanity_025.pdf}%
        \caption{1:4}
        \label{fig:AccuracyModelSanity025}
\end{subfigure}\hfill%\vskip\baselineskip%
\begin{subfigure}{.45\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figs/accuracy_model_sanity_05.pdf}%
        \caption{1:2}
        \label{fig:AccuracyModelSanity05}
\end{subfigure}\hfill%\hfill%
\begin{subfigure}{.45\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figs/accuracy_model_sanity_10.pdf}%
        \caption{1:1}
        \label{fig:AccuracyModelSanity10}
\end{subfigure}%

\caption{Percent error of accuracy models for a fixed run-time target and varying benchmark time, evaluating model incorporation. Ratio is expressed as \emph{Benchmark Paths}~:~\emph{Run-time Paths}. The innermost region represents the minimum error, the middle region the geometric mean relative error and the outermost the maximum.}

\label{fig:AccuracyModelSanity}
\end{figure*}

\begin{figure*}%
\centering
%\includegraphics[width=\textwidth]{figs/latency_model_plots_sanity.png}

\begin{subfigure}{.45\columnwidth}
        \includegraphics[width=\columnwidth]{figs/accuracy_model_scaling_20.pdf}%
        \caption{1:2}
        \label{fig:AccuracyModelSanity20}
\end{subfigure}\hfill%
\begin{subfigure}{.45\columnwidth}
        \includegraphics[width=\columnwidth]{figs/accuracy_model_scaling_40.pdf}%
        \caption{1:4}
        \label{fig:AccuracyModelSanity40}
\end{subfigure}\hfill%\vskip\baselineskip%
\begin{subfigure}{.45\columnwidth}
        \includegraphics[width=\columnwidth]{figs/accuracy_model_scaling_80.pdf}%
        \caption{1:8}
        \label{fig:AccuracyModelSanity50}
\end{subfigure}\hfill%
\begin{subfigure}{.55\columnwidth}
        \includegraphics[width=.8\columnwidth]{figs/accuracy_model_scaling_160.pdf}%
        \caption{1:16}
        \label{fig:AccuracyModelSanity160}
\end{subfigure}%

\caption{Percent error of accuracy models for a fixed benchmark time and varying run-time, evaluating model extrapolation. Ratios and the ordering of the regions are the same as in Figure \ref{fig:AccuracyModelSanity}.}

\label{fig:AccuracyModelScaling}
\end{figure*}

Figure \ref{fig:AccuracyModelSanity} shows how the accuracy models become increasingly predictive with additional benchmarking. For all of the task categories,  the minimum, mean and maximum errors all decrease as more benchmarking is performed. Figure \ref{fig:AccuracyModelScaling} shows how the models remain stable as the run-time target is increased. Hence, similar to the latency model results, the models scale well for more than an order of magnitude.

\subsection{Discussion}

The incorporation property of the models is demonstrated in Figures \ref{fig:LatencyModelSanity} and \ref{fig:AccuracyModelSanity}. This means that the predictive capability of the models improves as additional information is provided to the models. 

As Figures \ref{fig:LatencyModelScaling} and \ref{fig:AccuracyModelScaling} illustrate, the models also have the extrapolation property. There is a relatively minor increase in latency and accuracy error for run-times considerably longer than the benchmarking time.

The relatively poor latency model performance of the Remote Phi and Server platforms is explained by the benchmarking time being too short to accurately solve for the true coefficient and constant values. This is due to long network round-trip time that both platforms experienced, where the almost all of the benchmarking time is spent on communication.

The tasks with Heston underlyings present a relatively high maximum accuracy error, between 10\% and 100\%. However, as can be seen by the task category geometric mean these error average out to a considerably lower figure, allowing for these models to still be useful for modelling groups of tasks.

%Figure \ref{fig:AccuracyModelSanity} illustrates that as information is added to the benchmarking procedure, the relative error in the accuracy prediction model decreases across the different tasks categories. This is explained by the convergence of the Monte Carlo algorithm being a proven property with a very limited degree of variance, hence a low number of data points are required to solve for the apparent convergence coefficient ($\alpha$). The tasks with Heston underlyings present a relatively high maximum error, however, as can be seen by the task category geometric mean these error averages out close to 10\%.

%Figure~\ref{fig:PlatformCurves} synthesises these results, illustrating that a domain specific approach enables the characterisation of platforms in terms of the application domain. The trade-off curves are a representation of the domain design space for Table~\ref{table:TaskDetails}'s pricing tasks on Table~\ref{table:PlatformOverview}'s platforms. As is to be expected, with the lesser accuracy requirement, the latency ordering of platforms is determined by the constant setup time, of which the network latency is the most prominent component. However, as the accuracy requirement increases, the ordering is determined by the relative computational capabilities.

%\section{Using Heterogeneous Computers Efficiently}
%\vspace{-2pt}
%Should take up about 9 pages.
\section{Domain Allocation Approach Evaluation}
\label{sec:DomainPartitionerEvaluation}
In this section we describe our evaluation of the allocation approaches that make use of domain knowledge provided through the metric models, machine learning and MILP. We first characterise the performance of the domain allocation approaches with respect to problem size and problem non-linearity using synthetic data. We then verify this characterisation by applying the different allocation approaches to the tasks and platforms described in Tables \ref{table:TaskDetails} and \ref{table:PlatformOverview}.

We report on the time required by the domain allocation approach algorithms as well as the quality of the solution returned with respect to the proportional allocation heuristic. For an allocation approach to be acceptable, it needs to cope with a wide variety of allocation problems in we what we heuristically define as a reasonable amount of time, 10 minutes, while providing a significant improvement over the allocation returned by the heuristic approach.

%Use the synthetic data generation procedure described and a variety of scenarios, we have evaluated the two domain allocation approaches in terms of the size of the allocation problem, as well as the relative size of the coefficient and constant latency matrices. 

%\subsection{Partitioner Characterisation}
%\todo[inline]{Describe purpose and method for abstract evaluation}
%We first describe the procedure used to generate the synthetic data used in our characterisation and then the results of the characterisation using this data.

\subsection{Experimental Setup}
%We used the procedure outlined in Braun et al's~\cite{Braun11Heuristics} work to generate synthetic $\boldsymbol{\delta}$ and $\boldsymbol{\gamma}$ matrices for our partitioner characterisation experiment. The parameters used in conjunction with Braun et al's procedure are provided in Table \ref{table:SyntheticDetails}. We additionally multiplied the $\boldsymbol{\gamma}$ matrix by a scalar factor, which represents the ratio between the constant setup to coefficient component ratio. 

%\emph{Synthetic Data Generation Procedure}: 
\subsubsection{Synthetic Data Generation Procedure}
Drawing upon Braun et al's work~\cite{Braun11Heuristics}, we have used the following procedure ($s(\tau,\mu,\theta_\tau,\theta_\mu,\omega_\tau,\omega_\mu,\psi)$) to generate the synthetic co-efficient ($\boldsymbol{\delta}$) and constant ($\boldsymbol{\gamma}$) matrices so to evaluate the different approaches to allocation:
%\begin{enumerate}

%\item 
(1) Construct the baseline vector ($\vec{x}$) and initial matrix ($\boldsymbol{Y}$). $\vec{x}$ is $\tau$ uniformly distributed integer elements, bounded by the task heterogeneity factor ($\theta_\tau$). $\boldsymbol{Y}$, is $\mu \times \tau$ uniformly distributed integer elements, bounded by the platform heterogeneity factor ($\theta_\mu$):
$$ x_j \in [1,\theta_\tau] \quad j = \{1,2,\ldots,\tau\}, $$
$$Y_{i,j} \in [1,\theta_\mu] \quad i = \{1,2,\ldots,\mu\},j = \{1,2,\ldots,\tau\}.$$

%\item 
(2) Construct the $\boldsymbol{\delta}$ matrix by multiplying the elements of each row of $\boldsymbol{Y}$ and of $\vec{x}$. i.e. $$\delta_{i,j} = x_j Y_{i,j}  \quad i = \{1,2,\ldots,\mu\},j = \{1,2,\ldots,\tau\}.$$

%\item 
(3) Sort the first $\tau\omega_\tau$ columns of the $\boldsymbol{\delta}$ matrix, and the first $\mu\omega_\mu$ rows, where $\omega_\tau$ and $\omega_\mu$ are the degree of task and platform consistency.
%\item 

(4) Construct the $\boldsymbol{\gamma}$ matrix by repeating steps 1-3, however then multiply the resulting matrix by the task constant to coefficient ratio ($\psi$), i.e. the $\gamma$ to $\beta$ ratio in the latency metric model.
%\end{enumerate}

%\emph{Procedure Parameter Values}: 
\subsubsection{Procedure Parameter Values}
The parameters varied in our experiments, in conjunction with the procedure above are provided in Table \ref{table:SyntheticDetails}. The four cases consider a range of different scenarios, from completely homogeneous, consistent to extremely heterogeneous and inconsistent platforms and tasks, using the values from Braun et al's study~\cite{Braun11Heuristics}.

\begin{table}
\begin{center}
\caption{Synthetic task-platform generation parameters. Columns are platform and task heterogeneity ($\theta_\mu,\theta_\tau$), and platform and task consistency ($\omega_\mu,\omega_\tau$).}
\label{table:SyntheticDetails}
\centering
\begin{tabular}{ccccc}
\parbox[c]{1.5cm}{\center Case Designation} & $\theta_\mu$ & $\omega_\mu$ & $\theta_\tau$ & $\omega_\tau$\\ \hline\\
    Hom-Con & 10 & 1.0 & 100 & 1.0 \\
    Het-Con & 100 & 1.0 & 3000 & 1.0 \\
    Het-Mix & 100 & 0.5 & 3000 & 0.5 \\
    Het-Inc & 100 & 0.0 & 3000 & 0.0
\end{tabular}
\end{center}
\end{table}

\subsection{Allocation Characterisation Results}
%\emph{Synthetic Data Characterisation}
\subsubsection{Synthetic Data Characterisation}
The results of the allocation characterisation can be seen in Figure \ref{fig:SyntheticCharacterisation}. For the allocation times (Figures \ref{fig:SyntheticPartitionerVariablesLatency} and \ref{fig:SyntheticPartitionerRatioLatency}), a timeout of 600 seconds (or 10 minutes) was set, the same time given to the benchmarking described in the previous section.

Similarly for the quality of the solution relative to the proportional heuristic (Figures \ref{fig:SyntheticPartitionerVariablesImrpovement} and \ref{fig:SyntheticPartitionerRatiosImprovement}), we found that both the MILP and machine learning task allocations' improvements over the heuristic are a function of problem variables and constant to coefficient ratio. 

\begin{figure*}
\centering
%\includegraphics[width=\textwidth]{figs/latency_model_plots_sanity.png}
\caption{Characterisation of allocation approaches using synthetic data.}

%The Mixed Integer Linear Programming approach is the solid line, and the Machine Learning the dotted line.

\begin{subfigure}{\columnwidth}
        \includegraphics[width=.9\columnwidth]{figs/partitioner_synthetic_variables_latency_keys.pdf}%
        \caption{Impact of allocation problem size on allocation time. The constant to coefficient ratio is 1}
        \label{fig:SyntheticPartitionerVariablesLatency}
\end{subfigure}\hfill%
\begin{subfigure}{\columnwidth}
        \includegraphics[width=.9\columnwidth]{figs/partitioner_synthetic_setup2prop_latency_key.pdf}%
        \caption{Impact of problem characteristic on allocation time. The number of variables is 1024}
        \label{fig:SyntheticPartitionerRatioLatency}
\end{subfigure}\vskip\baselineskip%
\begin{subfigure}{\columnwidth}
        \includegraphics[width=.9\columnwidth]{figs/partitioner_synthetic_variables_improvment_keys.pdf}%
        \caption{Improvement for varied problem sizes. The constant to coefficient ratio is 1.}
        \label{fig:SyntheticPartitionerVariablesImrpovement}
\end{subfigure}\hfill%
\begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=.9\columnwidth]{figs/partitioner_synthetic_setup2prop_improvment_keys.pdf}%
        \caption{Improvement for varied problem characteristics. The number of variables is 1024}
        \label{fig:SyntheticPartitionerRatiosImprovement}
\end{subfigure}%

\label{fig:SyntheticCharacterisation}
\end{figure*}

\begin{figure*}
\centering
\caption{Allocation approaches using heterogeneous platforms from Table \ref{table:PlatformOverview} and derivatives pricing tasks from Table \ref{table:TaskDetails}. Smaller latency and accuracy values are better.}
\includegraphics[width=0.9\textwidth]{figs/Partitioner_Pareto_Verification.pdf}
\label{figure:ModelCurves}
\end{figure*}

%\emph{Practical Verification}:
\subsubsection{Practical Verification}
While the characterisation of the allocation approaches using synthetic data provides insight, we have verified these these results with real platform and task data in Figure \ref{figure:ModelCurves}. We put the portfolio of pricing tasks in Table \ref{table:TaskDetails} through the allocation approaches for the platforms in Table \ref{table:PlatformOverview} over a range of accuracies. We then ran the generated task allocations, and measured the domain metrics of latency and accuracy.

%\begin{figure}
%\centering
%\includegraphics[width=0.45\textwidth]{figs/improvement_accuracy_keys.pdf}
%\caption{Improvement of Numerical Optimiser and MILP Partitioners over proportional heuristic for domain models results and verification. The model is the dashed line, while the verification data is the solid line}
%\label{figure:SyntheticModelVerificationCharacterisation}
%\end{figure}

\subsection{Discussion}

%\emph{Allocation Characterisation}: 
Broadly, the machine learning-based task allocation approach was soon limited by the timeout, as evidenced by Figure \ref{fig:SyntheticPartitionerVariablesLatency}, while the MILP task allocation's time grows exponentially as a function of the number of variables. As the ratio between the coefficient ($\beta$) and constant ($\gamma$) component is varied in Figure \ref{fig:SyntheticPartitionerRatioLatency}, there is a peak latency centred around 1, reflecting the considerable linear and non-linear allocation problems that both have to be solved and balanced. The machine learning approaches perform well at this inflection point, while the MILP approach is at its relative worst.

In terms of improvement over the heuristic allocation, Figures \ref{fig:SyntheticPartitionerVariablesImrpovement} and \ref{fig:SyntheticPartitionerRatiosImprovement}, the trends can be explained by the potential for improvement. The linear trend with respect to problem size is due to the increased potential for improvement that larger problems allow. Similarly, for the constant to coefficient ratio, as the constant becomes more dominant, there is increased scope for improvement as the heuristic is further from its optimal condition.

%Notably, as the problems become more binary (i.e. more non-linear), the MILP approach show an order of magnitude improvement in all cases, as does the machine learning in the Het-Inc case, reflecting that both approaches handle non-linearities in the allocation problem well.

%\emph{Practical Verification}:
Figure \ref{figure:ModelCurves} illustrate that the allocation approaches using the metric models are close to what is measured in reality. The differences between the predictions and run-time measurements are well within the error of the metric models. Furthermore, both the domain knowledge-based machine learning and MILP-based allocation approaches are orders of magnitude more efficient than that suggested by the proportional heuristic for problems with strong non-linear characteristics, i.e. derivatives pricing tasks with 95\% confidence intervals greater than \$0.005.

\section{Conclusion}
\label{sec:Conclustion}

In this paper we have described and demonstrated in practice that a domain specific approach to heterogeneous computing offers two features beyond portable execution.

Firstly, using image filtering, linear algebra arithmetic and derivatives pricing as example domains, we described how domain metric models derived from the application domain provide a natural means to characterise a task on a heterogeneous platform. 

We evaluated the metric models of latency and accuracy for the derivative pricing domain practically. We found that when using an online benchmarking procedure, these domain metric models incorporate additional information to improve predictions, and extrapolate well as tasks increase in scale. 

These metric models are an accessible way to visualise the design space of the available heterogeneous platforms for domain programmers, such as Julia, as illustrated in Figure \ref{fig:PlatformCurves}. As is to be expected, when the accuracy requirement is low, constant communication time dominate and the platform makespans are geographically ordered, but when high accuracy is required, the compute dominates and the makespans order according to the measured computational capability of the platforms. 

\begin{figure}
\centering
\includegraphics[width=.55\textwidth]{figs/latency_accuracy_pareto_curves_keys.pdf}
\caption{Heterogeneous task-platform metric curves. Smaller accuracy and latency values are better.}
\label{fig:PlatformCurves}
\end{figure}

Secondly, we described how the metric models for multiple platforms can be combined into a constrained optimisation program. We also showed domain specific knowledge can allow this allocation to be relaxed, transforming the binary problem to a more tractable, mixed integer form.

We described and evaluated three approaches for solving this allocation problem, heuristic, machine learning and MILP. Our evaluation, making use of both synthetic data as well as the derivatives pricing examples, demonstrated that both MILP and machine learning can produce viable task allocation in a practical amount of time whilst outperforming the heuristic approach by up to two orders of magnitude, as illustrated in Figure \ref{fig:ParetoCurvesModelsVerification}.

\begin{figure}
\centering
\includegraphics[width=.55\textwidth]{figs/headline_curves.pdf}
\caption{Differing domain specific task allocation approaches. Smaller accuracy and latency values are better.}
\label{fig:ParetoCurvesModelsVerification}
\end{figure}

Beyond the practical benefits, our domain specific methodology makes distributed, heterogeneous computing platforms \emph{accessible} to domain users, such as Julia in the Introduction. Our approach shows how to abstract away details of implementation into choices about the nature of computational results. 
We only require that Julia balances her objectives to use heterogeneous computing effectively.

%\emph{Future Work}: 
\subsubsection*{Future Work}
Future directions for this work include increasing both the allocation problem sizes as well as the number of platforms utilised. A further direction is in increasing the degree of heterogeneity, both in terms of the problems considered as well as more varied computing resources.

Another area of ongoing work is optimisation of the MILP software used. Improvements being considered are seeding the optimiser with the proportional heuristics proposed in this paper, as well as ordering the heuristics applied within the optimiser more carefully.

% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi
Conversations with Prof George Constantinides, Dr Victor Magron, Dr Eric Kerrigan, Dr Andrea Suardi, Mr Shane Fleming and Mr Andrea Picciau have been invaluable.

We are grateful for the funding support from the South African National Research Foundation and Oppenheimer Memorial Trust. We have also benefited from the support of the Nallatech, Altera, Xilinx, Intel and Maxeler university programs.

% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv,TOPDS2015.bib}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}

%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to {\LaTeX}}, 3rd~ed.\hskip 1em plus0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

%\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:


\vspace{-15 mm}

\begin{IEEEbiographynophoto}{Gordon Inggs}
received the B.Sc.(Eng.) \emph{Hons}. and M.Sc.(Eng.) degrees in electrical engineering from the University of Cape Town, and recently completed his Ph.D. degree in Computer Engineering at Imperial College London. His research interests include distributed, heterogeneous computing, and domain specific programming tools.
\end{IEEEbiographynophoto}

\vspace{-15 mm}

\begin{IEEEbiographynophoto}{David B. Thomas} 
received the M.Eng. and Ph.D. degrees in computer science from Imperial College London. He is a Lecturer with the Electrical and Electronic Engineering Department, Imperial College London. His research interests include hardware-accelerated cluster computing, Monte Carlo simulation, random number generation, and financial computing.
\end{IEEEbiographynophoto}

\vspace{-15 mm}

\begin{IEEEbiographynophoto}{Wayne Luk} 
received the M.A., M.Sc., and D.Phil. degrees in engineering and computing science from the University of Oxford. He is a Professor of computer engineering with Imperial College London, and a Visiting Professor with Stanford University and Queens University Belfast. His research interests include theory and practice of customising computing for application domains, such as multimedia, networking, and finance.
\end{IEEEbiographynophoto}

%\newpage

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\vfill
%\enlargethispage{-30cm}
%\newpage

% that's all folks
\end{document}


