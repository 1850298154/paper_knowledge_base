\documentclass[nonatbib]{article}


%
%
%


%
\usepackage[final]{neurips_2023}
\usepackage{soul}

%
%
%


%
%


%
%
\usepackage{cite}

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %

%
%
%
\usepackage[title]{appendix}

\usepackage[ruled]{algorithm2e}
\usepackage[noend]{algpseudocode}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{nccmath}
\newtheorem{theorem}{Theorem}
\newtheorem*{theorem2}{Theorem 2'}
\newtheorem*{theorem3}{Theorem 3}
\newtheorem{lemma}{Lemma}
\newtheorem*{lemma6}{Lemma 6'}
\newtheorem{proposition}{Proposition}
\newtheorem*{proposition4}{Proposition 4'}
\newtheorem*{proposition5}{Proposition 5'}
\newtheorem{corollary}{Corollary}
\DeclareMathOperator{\argmin}{argmin}

\newtheorem{assumption}{Assumption}
\newtheorem*{assumption1}{Assumption 1}
\newtheorem*{assumption2}{Assumption 2}
\newtheorem*{assumption3}{Assumption 3}
\newtheorem*{assumption4}{Assumption 4}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

\usepackage{mathtools}
\usepackage[pdftex]{graphicx}
\graphicspath{ {./images/} }
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{xcolor}
\usepackage{float}
\usepackage{balance}
\usepackage[noabbrev]{cleveref}
\crefformat{equation}{(#1)}
\usepackage{bbm}
\usepackage{bm}
\usepackage{comment}
\usepackage{arcs}

\usepackage{resizegather}

\newcommand\prob[1]{\mathbb{P}\left[#1\right]}

\usepackage{acronym}
\newacro{lqr}[LQR]{Linear Quadratic Regulator}
\newacro{slqr}[SLQR]{Structured Linear Quadratic Regulators}
\newacro{olqr}[OLQR]{Output-feedback Linear Quadratic Regulators}
\newacro{lqg}[LQG]{Linear Quadratic Gaussian}
\newacro{dare}[DARE]{Discrete-time Algebraic Riccati Equation}
\newacro{ouralgo}[RNPO]{Riemannian Newton-type Policy Optimization}
\newacro{po}[PO]{Policy Optimization}
\newacro{pg}[PG]{Projected Gradient}
\newacro{gf}[GF]{Gradient Flow}
\newacro{gd}[GD]{Gradient Descent}
\newacro{sgd}[SGD]{Stochastic Gradient Descent}
\newacro{sde}[SDE]{Stochastic Differential Equation}
\newacro{PBH}[PBH]{Popov-Belevitch-Hautus}


\newcommand{\ie}{{i}.{e}.}
\newcommand{\eg}{{e}.{g}.}

\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage[inline]{enumitem}
\newcommand{\RomanNumeralCaps}[1]
    {\MakeUppercase{\romannumeral #1}}

\newcommand{\amir}[1]{{\color{blue}#1}}
\newcommand{\shahriar}[1]{{\color{violet}#1}}
%
\def\x{{\bm x}}
\def\u{{\bm u}}
\def\z{{\bm z}}
\def\v{{\bm v}}
\def\y{{\bm y}}
\def\e{{\bm e}}
\def\oomega{{\bm \omega}}
\def\ogamma{{\bm \gamma}}
\def\bR{{\mathbb{R}}}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\hess}{Hess}
\DeclareMathOperator{\euchess}{\text{$\overline\hess$}}
\DeclareMathOperator{\tproj}{\pi^\top}
\DeclareMathOperator{\nproj}{\pi^\perp}
\DeclareMathOperator{\eucproj}{Proj}
\DeclareMathOperator{\diff}{d}
\def\metric{g}
\def\submetric{\widetilde{\metric}}
\def\lyap{{\,\mathbb{L}}}
\def\lyapij{dY}
\def\Exp{\mathrm{Exp}}
\def\nghbd{\mathcal{U}}
\def\wein{\mathbb{W}}

\def\vz{{\Vec{z}\,}}
\def\vomega{{\Vec{\omega}\,}}
\def\vxi{{\Vec{\xi}\,}}
\def\veta{{\Vec{\eta}_L}}
\def\vu{{\Vec{u}\,}}
\def\calA{\mathcal{A}}
\def\calX{\mathcal{X}}
\def\calM{\mathcal{M}}
\def\calN{\mathcal{N}}
\def\calR{\mathcal{R}}
\def\calQ{\mathcal{Q}}
\def\calZ{\mathcal{Z}}


\def\ptrans{\mathfrak{P}}

\def\fX{\mathfrak{X}}

\def\stabcer{s}
\def\descer{d}

\def\lyaptrace{{Lyapunov-trace~}}

\def\Amatrices{{\text{M}(n\times n,\mathbb{R})}}
\def\Astable{{\mathcal{M}}}
\def\Bmatrices{{\text{M}(n\times m,\mathbb{R})}}
\def\Cmatrices{{\text{M}(d\times n,\mathbb{R})}}
\def\Lmatrices{{\text{M}(m\times d,\mathbb{R})}}
\def\Kmatrices{{\text{M}(m\times n,\mathbb{R})}}
\def\constraint{{\mathcal{K}}}
\def\stableK{\mathcal{S}}
\def\substableK{\widetilde{\stableK}}
\def\sublevelf{\mathcal{L}}
\def\sublevelh{\widetilde{\sublevelf}}
\def\ray{L}

\def\dist{\mathrm{dist}}

\newcommand{\tensor}[3]{\ensuremath{\left\langle #1, #2 \right \rangle}_{#3}}

\newcommand{\rnorm}[1]{\ensuremath{\left\| #1 \right\|_g}}

\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

\newcommand{\tr}[1]{\ensuremath{\mathrm{tr}\left[ #1 \right]}}

\newcommand{\E}[2]{\ensuremath{\mathbb{E}_{#1}\left[ #2 \right]}}


\DeclareMathOperator{\lambdamax}{\text{$\overline{\lambda}$}}
\DeclareMathOperator{\lambdamin}{\text{$\underline{\lambda}$}}

%
\makeatletter
\newcommand{\algorithmfootnote}[2][\footnotesize]{%
  \let\old@algocf@finish\@algocf@finish%
  \def\@algocf@finish{\old@algocf@finish%
    \leavevmode\rlap{\begin{minipage}{\linewidth}
    #1#2
    \end{minipage}}%
  }%
}
\makeatother




%
\usepackage{nomencl}
\makenomenclature
\renewcommand{\nomname}{}%

%
%
\usepackage{etoolbox}
\renewcommand\nomgroup[1]{%
  \item[\bfseries
  \ifstrequal{#1}{P}{Problem Parameters}{%
  \ifstrequal{#1}{S}{System Quantities}{%
  \ifstrequal{#1}{C}{Constant Values}{%
  \ifstrequal{#1}{O}{Other symbols}{}}}}%
]}
%

\title{
%
%
Data-driven Optimal Filtering for \\Linear Systems with Unknown Noise Covariances
}
\author{%
  Shahriar Talebi$^{1,2}$ $\quad$ Amirhossein Taghvaei$^{1}$ $\quad$ Mehran Mesbahi$^{1}$\\
  $^{1}$University of Washington,
  Seattle, WA, 98105\\
  $^{2}$Harvard University, Cambridge, MA, 02138 \\
    \texttt{talebi@seas.harvard.edu} $\quad$ \texttt{amirtag@uw.edu} $\quad$ \texttt{mesbahi@uw.edu}\\
}


\begin{document}


\maketitle


\begin{abstract}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
This paper examines learning the optimal filtering policy, known as the Kalman gain, for a linear system with unknown noise covariance matrices using noisy output data.  The learning problem is formulated as a stochastic policy optimization problem, aiming to minimize the output prediction error. This formulation provides a direct bridge between data-driven optimal control and, its dual, optimal filtering. 
Our contributions are twofold. Firstly, we conduct a thorough convergence analysis of the stochastic gradient descent algorithm, adopted for the filtering problem, accounting for biased gradients and stability constraints. 
Secondly, we carefully leverage a combination of tools from linear system theory and high-dimensional statistics to derive 
bias-variance error bounds that scale logarithmically with problem dimension, and, in contrast to subspace methods,
the length of output trajectories only affects the bias term.
%

%
%
\end{abstract}




\section{Introduction}
The duality of control and estimation plays a crucial role in system theory, linking two distinct synthesis problems~\cite{kalman1960general,kalman1960new,pearson1966duality,mortensen1968maximum,bensoussan2018estimation,fleming1982optimal}. This duality is an effective bridge between two distinct disciplines, facilitating development of theoretical and computational techniques in one domain and then adopting them for use in the other. For example, the stability proof of the Kalman filter relies on the stabilizing characteristic of the optimal feedback gain in the dual \ac{lqr} optimal control problem~\cite[Ch. 9]{xiong2008introduction}. In this paper, we leverage this duality to learn optimal filtering policies using recent advances in data-driven algorithms for optimal control.


We consider the estimation problem for a system with a known linear dynamic and observation model, but unknown process and measurement noise covariances. Our objective is to learn the optimal steady-state Kalman gain using a training dataset comprising independent realizations of the observation signal.
This problem has a rich history in system theory, often explored within the context of adaptive Kalman filtering~\cite{mehra1970identification,mehra1972approaches,carew1973identification,belanger1974estimation,myers1976adaptive,tajima1978estimation}. A comprehensive summary of four solution approaches to this problem can be found in the classical reference~\cite{mehra1972approaches}. These approaches include Bayesian inference \cite{magill1965optimal,hilborn1969optimal,matisko2010noise}, Maximum likelihood~\cite{kashyap1970maximum,shumway1982approach}, covariance matching~\cite{myers1976adaptive}, and innovation correlation methods~\cite{mehra1970identification,carew1973identification}. While Bayesian and maximum likelihood approaches are computationally intensive, and covariance matching introduces biases in practice, the innovation correlation-based approaches have gained popularity and have been the subject of recent research~\cite{odelson2006new,aakesson2008generalized,dunik2009methods}. For an excellent survey on this topic, refer to the article~\cite{zhang2020identification}.
However, it is important to note that these approaches often lack non-asymptotic guarantees and heavily depend on statistical assumptions about the underlying model.

In the realm of optimal control, significant progress has been made in the development of data-driven synthesis methods. Notably, recent advances have focused on the adoption of first-order methods for state-feedback \ac{lqr} problems \cite{bu2019lqr, bu2020policy}. The direct optimization of policies from a gradient-dominant perspective has first proven in \cite{fazel2018global} to be remarkably effective with global convergence despite non-convex optimization landscape. It has been demonstrated that despite the non-convex nature of the cost function, when expressed directly in terms of the policy, first-order methods exhibit global convergence to the optimal policy.
%
Building upon this line of work, the use of first-order methods for policy optimization has been explored in variants of the \ac{lqr} problem. These include \ac{olqr} \cite{fatkhullin2020optimizing}, model-free setup \cite{mohammadi2021linear}, risk-constrained setup \cite{zhao2021global}, \ac{lqg} \cite{Tang2021analysis}, and most recently, Riemannian constrained \ac{lqr} \cite{talebi2022policy}. These investigations have expanded the scope of data-driven optimal control, demonstrating the versatility and applicability of first-order methods for a wide range of synthesis problems.

The objective of this paper is to provide fresh insights into the classical estimation problem by leveraging the duality between control and estimation and incorporating recent advances in data-driven optimal control. Specifically, building on the fundamental connection between the optimal mean-squared error estimation problem and the \ac{lqr} problem (Prop.~\ref{prop:duality}), we reformulate determining the optimal Kalman gain as a problem of synthesizing an optimal policy for the adjoint system, under conditions that differ from those explored in the existing literature (see~\eqref{eq:opt-time-indepen} and Remark~\ref{remark:duality}).
%
Upon utilizing this relationship, we propose a \ac{sgd} algorithm for learning the optimal Kalman gain, accompanied by novel non-asymptotic error guarantees in presence of biased gradient and stability constraint. Our approach opens up promising avenues for addressing the estimation problem with robust and efficient data-driven techniques. The following is an informal statement of our main results (combination of Thm.~\ref{thm:sgd} and Thm.~\ref{thm:oracle}), and missing proofs appear in the supplementary materials.
%
\begin{theorem3}[Informal]
    Suppose the system is observable and both dynamic and measurement noise are bounded. Then, with high probability, direct policy updates using stochastic gradient descent with small stepsize converges \emph{linearly} and \emph{globally} (from any initial stabilizing policy) to the optimal steady-state Kalman gain.  
\end{theorem3}

More recently, the problem of learning the Kalman gain has been considered from a system identification perspective, for completely \textit{unknown} linear systems~\cite{lale2020logarithmic,tsiamis2019finite,tsiamis2023online,umenberger2022globally}. In~\cite{tsiamis2019finite} and \cite{tsiamis2023online}, subspace system identification methods are used to obtain error bounds for learning the Markov parameters of the model over a time horizon and establish logarithmic regret guarantee for output prediction error. Due to the inherent difficulty of learning a completely unknown stochastic system from partial observations, subspace methods assume marginal \textit{stability of the unknown system}, and lead to sub-optimal sample complexity bounds that grow with the number of Markov parameters, instead of the number of unknowns~\cite[pp. 14]{tsiamis2022statistical}. Alternatively, \cite{umenberger2022globally} considers minimizing the output prediction error and introduces a model-free policy gradient approach, under the same stability assumptions, that achieves \textit{sublinear convergence rate}. 
%
This paper provides a middle ground between completely known and completely unknown systems, for a learning scenario that not only has relevant practical implications, but also utilizes the duality relationship to \ac{lqr} to establish \textit{linear convergence rates} even for \textit{unstable systems} as long as they are \textit{observable}. See the Appendix~\ref{sec:related-work} for the discussion of additional related works~\cite{zheng2021sample,Zhang2023learning,liu2023learning}. 














%
 %


%

%
%
%

%
%
%
%
%
%
%

%
%
%

 
\section{Background and Problem Formulation}\label{sec:problem}
Herein, first we propose the model setup in detail and discuss the Kalman filter as the estimation strategy.
Consider the discrete-time filtering problem given by the stochastic difference equations,
\begin{equation} \label{eqn:sysdyn}
    x(t+1) = A x(t) + \xi(t), \quad \text{and} \quad
    y(t) = H x(t) + \omega(t),
\end{equation}
where $x(t) \in \mathbb R^n$ is the state of the system, $y(t)\in \mathbb R^m$ is the observation signal, and $\{\xi(t)\}_{t\in \mathbb Z}$ and $\{\omega(t)\}_{t\in \mathbb Z}$ are the uncorrelated zero-mean random vectors, that represent the process and measurement noise respectively, with the following covariances,
\[\E{}{\xi(t)\xi(t)^\intercal} = Q \in \bR^{n\times n}, \quad \E{}{\omega(t)\omega(t)^\intercal} = R \in \bR^{m\times m},\]
for some positive (semi-)definite matrices $Q, R \succeq 0$. Let $m_{0}$ and $P_0 \succeq 0$ denote the mean and covariance of the initial condition $x_0$. 

In the filtering setup, the state $x(t)$ is hidden, and the objective is to estimate it given the history of the observation signal $\mathcal{Y}(t)=\{y(0),y(1),\ldots,y(t-1)\}$. The best linear mean-squared error (MSE) estimate of $x(t)$ is defined according to
\begin{align}\label{eq:MSE}
    \hat{x}(t) = \underset{\hat x \in \mathcal L(\mathcal Y(t))}{\argmin}\,\E{}{\|x(t)-\hat{x}\|^2}
\end{align}
where $\mathcal L(\mathcal Y(t))$ denotes the space of all linear functions of the history of the observation signal $\mathcal Y(t)$.  If the model parameters $(A,H,Q,R)$ are known, the optimal MSE estimate $\hat{x}(t)$ can be recursively computed by the Kalman filter algorithm~\cite{kalman1960new}: 
\begin{subequations}
\begin{align}
    \hat{x}(t+1) &= A\hat{x}(t) + L(t)(y(t) - H \hat{x}(t)),\quad \hat{x}(0) = m_{0},\label{eq:KF-mean}\\
          P(t+1) &= AP(t)A^\intercal + Q - AP(t)H^\intercal S(t)^{-1} HP(T)A^\intercal,\quad P(0) = P_0,\label{eq:Kf-P}
\end{align}
\end{subequations}
where $S(t)=HP(t)H^\intercal + R$, $L(t):=AP(t)H^\intercal S(t)^{-1}$ is the Kalman gain, and $P(t):=\mathbb E[(x(t) - \hat{x}(t))(x(t) - \hat{x}(t))^\intercal]$ is the error covariance matrix.

\begin{assumption}\label{assmp:detectable}
    The pair $(A,H)$ is detectable, and the pair $(A,Q^{\frac{1}{2}})$ is stabilizable, where
    $Q^{\frac{1}{2}}$ is the unique positive semidefinite square root of $Q$. 
\end{assumption}

Under this assumption, the error covariance $P(t)$ converges to a steady-state value $P_\infty$, resulting in a unique steady-state Kalman gain $L_\infty=AP_\infty H^\intercal(HP_\infty H^\intercal + R)^{-1}$\cite{kwakernaak1969linear,lewis1986optimal}.  
It is common to evaluate the steady-state Kalman gain $L_\infty$ offline and use it, instead of $L(t)$, to update the estimate in real-time. 
Furthermore, we note that the assumption of uncorrelated random vectors is sufficient to establish that Kalman filter provides the best \textit{linear}
estimate of the states given the observations for minimizing the MSE criterion \cite[Theorem 2]{kalman1960new}. 
 
 \subsection{Learning problem}
 Now, we describe our learning setup:
\begin{enumerate*}
    \item[1)] The system matrices $A$ and $H$ are known, but the process and the measurement noise covariances, $Q$ and $R$, are \emph{not} available.
    \item[2)] We have access to an oracle that generates independent realizations of the observation signal for given length $T$: $\{y(t)\}_{t=0}^T$. However, ground-truth measurements of the state $x(t)$ is \emph{not} available.
\end{enumerate*}


\begin{remark}
    Our proposed  learning setup arises in various important engineering applications where merely approximate or reduced-order linear models are available due to difficulty in analytically capturing the effect of complex dynamics or disturbances, hence represented by noise with unknown covariance matrices.
Additionally, the system identification procedure often occurs through the application of physical  principles and collection of data from experiments in a controlled environment (e.g., in a wind tunnel). However, identifying the noise covariance matrices strongly depends on the operating environment which might be significantly different than the experimental setup. Therefore, it is common engineering practice to use the learned system matrices and tune the Kalman gain to improve the estimation error. We refer to  \cite{hinson2022autocovariance} for the application of this procedure for gust load alleviation in wings and \cite{odelson2006autocovariance} for estimation in chemical reactor models. We also emphasize that this learning setup has a rich history in adaptive filtering with numerous references with a  recent survey on this topic \cite{zhang2020identification}. As part of our future research, we will carry-out a robustness analysis, similar to  its LQR dual counterpart \cite{safonov1992singular,chen2016stability}, to study the effect of the error in system matrices on the learning performance.
\end{remark}

Inspired by the structure of the Kalman filter, our goal is to learn the steady-state Kalman gain $L_\infty$ from the data described in the learning setup:
\begin{align*}
    \text{Given:}&\quad \text{independent random realizations of } \{y(0),\ldots,y(T)\} \text{~with the parameters~} A, H\\
    \text{Learn:}&\quad \text{steady-state Kalman gain $L_\infty$}
\end{align*}
For that, we formulate the learning problem as a stochastic optimization described next. 

\subsection{Stochastic optimization formulation}
%
%
%
%
%
%
%
Define $\hat{x}_L(T)$ to be the estimate  given by the Kalman filter at time $T$ realized by the constant gain $L$. Rolling out the update law~\eqref{eq:KF-mean} for $t=0$ to $t=T-1$, and replacing $L(t)$ with $L$, leads to the following expression for the estimate $\hat{x}_L(T)$ as a function of $L$, 
\begin{align}\label{eq:estimate-x-L}
    \textstyle \hat{x}_L(T) = A_L^Tm_0 + \sum_{t=0}^{T-1}A_L^{T-t-1} L y(t),
\end{align}
where $A_L\coloneqq A-LH$.
Note that evaluating this estimate does not require knowledge of $Q$ or $R$. However, it is not possible to directly aim to learn the gain $L$ by minimizing the MSE~\eqref{eq:MSE} because the ground-truth measurement of the state $x(T)$ is not available. Instead, we propose to minimize the MSE in predicting the observation $y(T)$ as a  surrogate objective function:\footnote{The expectation in \cref{eq:optimization-L-T} is taken over all the random variables; consisting of the initial state $x_0$, dynamic noise $\xi(t)$, and measurement noise $\omega(t)$ for $t= 0, \cdots, T$. A conditional expectation is not necessary as the estimate is constrained to be measurable with respect to the history of observation.}
\begin{equation}\label{eq:optimization-L-T}
  \textstyle \min_L\,J^{\text{est}}_T(L):=\E{}{\|y(T)-\hat{y}_{L}(T)\|^2}
\end{equation}
%
%
%
where $\hat{y}_L(T):=H\hat{x}_L(T)$. Note that while the objective function involves finite time horizon $T$, our goal is to learn the steady-state Kalman gain $L_\infty$. 

The justification for using the surrogate objective function in \eqref{eq:optimization-L-T} instead of MSE error~\eqref{eq:MSE} lies in the detectability assumption~\ref{assmp:detectable}.  Detectability implies that all unobservable states are stable; in other words, their impact on the output signal vanishes quickly---depending on their stability time constant.

%
%
%
%
%


%
%
%
%


%
%
%
%
%


%

%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%



%
%
%

%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%


%



%
%
%
%
%
%
%
%
%

Numerically, the optimization problem~\eqref{eq:optimization-L-T} falls into the category of stochastic optimization and can be solved by algorithms such as \acf{sgd}. Such an algorithm would need access to independent realizations of the observation signal which are available. 
%
%
Theoretically however, it is not yet clear if this optimization problem is well-posed  and admits a unique minimizer. This is the subject of following section where certain properties of the objective function, such as its gradient dominance and smoothness, are established. These theoretical results are then used to analyze first-order optimization algorithms and provide stability guarantees of the estimation policy iterates. The results are based on the duality relationship between estimation and control that is presented next.


\section{Estimation-Control Duality Relationship}\label{sec:duality}
The stochastic optimization problem~\eqref{eq:optimization-L-T} is related to an \ac{lqr} problem through the application of the classical duality relationship between estimation and control~\cite[Ch.7.5]{aastrom2012introduction}. 
%
In order to do so, we introduce the adjoint  system (dual to~\eqref{eqn:sysdyn}) according to:
\begin{equation}\label{eqn:adjdyn}
    z(t) = A^\intercal z(t+1) - H^\intercal u(t+1),\quad z(T)=a
\end{equation}
where $z(t) \in \mathbb R^n$ is the adjoint state and $\mathcal U(T):=\{u(1),\ldots,u(T)\} \in \mathbb R^{mT}$ are the control variables (dual to the observation signal $\mathcal{Y}(T)$). The adjoint state is initialized at $z(T)=a \in \mathbb R^n$ and simulated  \textit{backward in time} starting with $t= T-1$. We introduce an \ac{lqr} cost for the adjoint system:
\begin{equation}\label{eq:lqr}
     \textstyle J_T^{\text{LQR}}(a,\mathcal U_T)  := %
     z^\intercal(0) P_0 z(0) + \sum_{t=1}^{T} \left[ z^\intercal(t) Q z(t) + u^\intercal(t) R u(t) \right],
\end{equation}
and formalize a relationship between linear estimation policies for the  system~\eqref{eqn:sysdyn} and linear control policies for the adjoint system~\eqref{eqn:adjdyn}. A linear estimation policy takes the observation history $\mathcal{Y}_T\in \mathbb R^{mT}$ and outputs an estimate $\hat{x}_{\mathcal L}(T) := \mathcal L(\mathcal{Y}_T)$ where $\mathcal L:\mathbb R^{mT} \to \mathbb R^n$  is a linear map. The adjoint of this linear map, denoted by $\mathcal L^\dagger: \mathbb R^n \to \mathbb R^{mT}$, is used to define a control policy for the adjoint system~\eqref{eqn:adjdyn} which takes the initial condition $a\in \mathbb R^n$ and outputs  the control signal $\mathcal U_{\mathcal L^\dagger}=\mathcal L^\dagger(a)$; i.e.,
%
\begin{equation*}
    \begin{aligned}
    \{y(0),\ldots,y(T-1)\}  \overset{\mathcal L}{\longrightarrow} 
    \hat{x}_{\mathcal L}(T) \qquad \text{and} \qquad
     \{u(1),\ldots,u(T)\}  \overset{\mathcal L^\dagger}  {\longleftarrow}  a.
    %
        %
    \end{aligned}
\end{equation*}
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

The duality relationship between optimal MSE estimation and \ac{lqr} control is summarized in the following proposition. The proof is presented in the supplementary material. 
\begin{proposition} \label{prop:duality}
Consider the estimation problem for the system~\eqref{eqn:sysdyn} and the \ac{lqr} problem~\eqref{eq:lqr} subject to the adjoint dynamics~\eqref{eqn:adjdyn}. 
For any linear estimation policy $\hat{x}_{\mathcal L}(T)= \mathcal L (\mathcal{Y}_T)$, and for any $a \in \mathbb R^n$, we have the identity 
\begin{equation}\label{eq:dual-policy}
    \textstyle \E{}{|a^\intercal x(T)-a^\intercal \hat x_{\mathcal L}(T)|^2} =   J_T^{\text{LQR}}(a,\mathcal U_{\mathcal L^\dagger}(T)),
\end{equation}
where $\mathcal U_{\mathcal L^\dagger}(T)=\mathcal L^\dagger(a)$. 
In particular, for a Kalman filter with constant gain $L$, the output prediction error~\eqref{eq:optimization-L-T}
\begin{equation}\label{eq:dual}
  \textstyle \E{}{\|y(T)-\hat{y}_{L}(T)\|^2}= \sum_{i=1}^m  J_T^{\text{LQR}}(H_i,\mathcal U_{L^\intercal} (T))+ \tr{R},
\end{equation}
where $\mathcal U_{L^\intercal}(T)=\{L^\intercal z(1),L^\intercal z(2),\ldots,L^\intercal z(T)\}$, i.e., 
the feedback control policy with constant gain $L^\intercal$, and $H_i^\intercal \in \mathbb R^n$ is the $i$-th row of the $m\times n$ matrix $H$ for $i=1,\ldots,m$.

%
%
%
%
%
\end{proposition}



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%
%




%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%

\begin{remark}
The duality is also true in the continuous-time setting where the estimation problem is related to a continuous-time \ac{lqr}.    Recent extensions to the nonlinear setting appears in~\cite{kim2019lagrangian} with a comprehensive study in~\cite{kim2022duality}.
%
This duality is distinct from the maximum likelihood approach which involves an optimal control problem over the original dynamics instead of the adjoint system~\cite{bensoussan2018estimation}.
\end{remark}



%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%


%
 
 
 \subsection{Duality in steady-state regime}
 Using the duality relationship~\eqref{eq:dual}, the MSE in  prediction~\eqref{eq:optimization-L-T} is expressed as:  
 \begin{equation*}
    J^\text{est}_T(L)= \tr{X_T(L)H^\intercal H} + \tr{R},
 \end{equation*}
 where $ X_T(L) \coloneqq A_L^T P_0(A_L^\intercal)^T   + \sum_{t=0}^{T-1}  A_L^{t} (Q+LRL^\intercal)(A_L^\intercal)^{t}$.
Define the set of Schur stabilizing gains
\[\mathcal{S} \coloneqq \{L \in \bR^{n\times m}: \rho(A_L) < 1\}.\]
For any $L \in \mathcal S$, in the steady-state, 
the mean-squared prediction error assumes the form,
\[\textstyle\lim_{T \to \infty} J_{T}^\text{est}(L) = \tr{X_{\infty}(L) H^\intercal H} + \tr{R},\]
where $X_{\infty}(L):=\lim_{T\to \infty}X_T(L)$ and 
%
%
coincides with the unique solution $X$ of the  discrete Lyapunov equation
$X = A_L X A_L^\intercal + Q + L R L^\intercal$ (existence of unique solution follows from $\rho(A_L) < 1$).
%
%
%
%
Given the steady-state limit, we formally analyze the following constrained optimization problem:
%
%
%
%
%
%
\begin{align}\label{eq:opt-time-indepen}
    \min_{L \in \mathcal{S}} \; &\leftarrow J(L) \coloneqq \tr{X_{(L)} H^\intercal H}, \quad \text{s.t.} \quad X_{(L)} = A_L X_{(L)} A_L^\intercal + Q + L R L^\intercal .
\end{align}
\begin{remark}\label{remark:duality}
Note that the latter problem is technically the dual of the optimal \ac{lqr} problem as formulated in \cite{bu2019lqr} by relating  $A \leftrightarrow A^\intercal$, $-H \leftrightarrow B^\intercal$, $L \leftrightarrow K^\intercal$, and $H^\intercal H \leftrightarrow \Sigma$. However, the main difference here is that the product $H^\intercal H$ may \emph{not} be positive definite, for example, due to rank deficiency in $H$ specially when $m < n$ (whereas $\lambdamin(\Sigma)>0$ appears in all of the bounds in \cite{fazel2018global,bu2019lqr}). Thus, in general, the cost function $J(L)$ is not necessarily coercive in $L$, which can drastically effect the optimization landscape. For the same reason, in contrast to the \ac{lqr} case \cite{fazel2018global,bu2019lqr},
the gradient dominant property of $J(L)$ is not clear in the filtering setup. In the next section, we show that such issues can be avoided as long as the pair $(A,H)$ is observable. Also, the learning problem posed here is distinct from its \ac{lqr} counterpart (see \Cref{tab:lqr-estimation}).
%
\end{remark}

\subsection{Optimization landscape}
The first result is concerned with the behaviour of the objective function at the boundary of the optimization domain. It is known \cite{bu2019topological} that the set of Schur stabilizing gains $\mathcal{S}$ is regular open, contractible, and unbounded when $m\geq 2$ and the boundary $\partial \mathcal{S}$ coincides with the set $\{L \in \bR^{n\times m}: \rho(A-LH) = 1\}$. For simplicity of presentation, we consider a slightly stronger assumption: 
%

\begin{assumption}\label{assmp:observable}
    The pair $(A,H)$ is observable, and the noise covariances $Q\succ 0$ and $R \succ 0$. 
\end{assumption}
%
%
%

\begin{lemma}\label{lem:coercive}
The function $J(.):\mathcal{S} \to \bR$ is real-analytic and coercive with compact sublevel sets; i.e.,
\[L \to \partial\mathcal{S} \text{ or } \|L\| \to \infty \;\text{ each implies }\; J(L) \to \infty,\]
and $\mathcal{S}_\alpha \coloneqq \{L \in \bR^{n\times m}: J(L) \leq \alpha\}$ is compact and contained in $\mathcal{S}$ for any finite $\alpha >0$.
\end{lemma}

The next result establishes the gradient dominance property of the objective function. While this result is known in the \ac{lqr} setting (\cite{fazel2018global,bu2019lqr}), the extension to the estimation setup is  not trivial as $H^\intercal H$, which takes the role of the covariance matrix of the initial state in \ac{lqr}, may not be positive definite (instead, we only assume $(A,H)$ is observable).
%
%
%
 This, apparently minor issue, hinders establishing the gradient dominated property globally. However, we recover this property on every sublevel sets of $J$ which is sufficient for the subsequent convergence analysis.
 \begin{lemma}\label{prop:graddom} 
 Consider the constrained optimization problem~\eqref{eq:opt-time-indepen}.Then,  
 \begin{itemize}[leftmargin=*]
     \item 
The explicit formula for the gradient of $J$ is:
\(\nabla  J(L) 
    = 2Y_{(L)} \left(-LR + A_L X_{(L)} H^{\intercal} \right),\)
where $Y_{(L)}=Y$ is the unique solution of
\(Y = A_L^\intercal Y A_L + H^\intercal H.\)
\item  The global minimizer $L^* = \arg\min_{L \in \mathcal{S}} J(L)$
%
%
satisfies
\(L^* = A X^* H^\intercal \left(R + H X^* H^\intercal\right)^{-1},\)
with $X^*$ being the unique solution of 
\(X^* = A_{L^*} X^* A_{L^*}^\intercal + Q + L^* R (L^*)^\intercal.\)
\item The function $J(.):\mathcal{S}_\alpha \to \bR$, for any non-empty sublevel set $\mathcal{S}_\alpha$ for some $\alpha>0$, satisfies the following inequalities; for all $ L,L' \in \mathcal{S}_\alpha$:
\begin{subequations}
\begin{align}
 c_1 [J(L) - J(L^*)] + c_2 \|L-L^*\|_F^2 &\leq  \langle \nabla J(L), \nabla J(L) \rangle, \label{eq:gradient-dominance} \\
c_3 \|L - L^*\|_F^2 &\leq J(L) - J(L^*), \\
\|\nabla J(L) - \nabla J(L')\|_F &\leq \ell\; \|L - L'\|_F, \label{lem:lipschitz}
\end{align}
\end{subequations}
for positive constants $c_1, c_2, c_3$ and $\ell$ that are only a function of $\alpha$ and independent of $L$.
 \end{itemize}
 \end{lemma}


%
%
%
%
%
%
%
%
%
%
Note that the expression for the gradient is consistent with Proposition 3.8 in \cite{bu2019lqr} after applying the duality relationship explained in \Cref{remark:duality}.

%
%
%
%
%
%




%
%
%
%
%
%
%
%
%

\begin{remark}\label{rmk:pl-property}
The proposition above implies that $J(.)$ has the Polyak-\L{}ojasiewicz (PL) property (aka gradient dominance) on every $\mathcal{S}_\alpha$; i.e., for any $L \in \mathcal{S}_\alpha$ we have
\(\textstyle J(L) - J(L^*) \leq  \frac{1}{c_1(\alpha)} \langle \nabla J(L), \nabla J(L) \rangle.\)
The  inequality~\eqref{eq:gradient-dominance} is more general as it characterizes the dominance gap in terms of the iterate error from the optimality. This is useful in obtaining the iterate convergence results in the next section. Also, the Lipschitz bound resembles its ``dual'' counterpart in \cite[Lemma 7.9]{bu2019lqr}, however, it is \emph{not}  implied as a simple consequence of duality because $H^\intercal H$ may not be positive definite.
\end{remark}


\section{SGD for Learning the Kalman Gain}\label{sec:theoery}
In order to emphasize on the estimation time horizon $T$ for various measurement sequences, we 
use  $\mathcal{Y}_{T}:=\{y(t)\}_{t=0}^T$ to denote the measurement time-span. 
Note that, any choice of $L \in \mathcal{S}$ corresponds to a filtering strategy that outputs the following prediction,
\begin{equation*}
  \textstyle \hat{y}_L(T) = H A_L^{T} m_0 +\sum_{t=0}^{T-1} H  A_L^{T-t-1} L y(t).
\end{equation*}
We denote the squared-norm of the estimation error for this filtering strategy as,
\[\varepsilon(L,\mathcal{Y}_{T}) \coloneqq \|e_{T}(L)\|^2,\]
where $e_{T}(L) \coloneqq y(T) - \hat{y}_L(T)$.
%
%
We also define the \textit{truncated} objective function as
\begin{equation*}
    J_{T}(L):=\E{}{\varepsilon(L,\mathcal{Y}_{T})},
\end{equation*}
where the expectation is taken over all possible random measurement sequences, and note that, at the steady-state limit, we obtain $\lim_{T\to \infty}J_{T}(L) = J(L)$.
%
%
%
%

The \ac{sgd} algorithm aims to solve this optimization problem by replacing the gradient, in the \ac{gd} update, with an unbiased estimate of the gradient in terms of samples from the measurement sequence. In particular, with access to an oracle that produces independent realization of the measurement sequence, say $M$ random independent measurements sequences $\{\mathcal{Y}_{T}^i\}_{i=1}^M$, the gradient can be approximated as follows: denote the approximated cost value 
\begin{equation*}
    \textstyle \widehat J_T(L) \coloneqq \frac{1}{M}\sum_{i=1}^M \varepsilon(L,\mathcal{Y}^i_{T}),
\end{equation*}
then the approximate gradient with batch-size $M$ is $\nabla\widehat J_{T}(L) = \frac{1}{M}\sum_{i=1}^M \nabla_L \varepsilon(L,\mathcal{Y}^i_{T})$. 
This forms an unbiased estimate of the gradient of the ``truncated objective'', i.e., $\E{}{\nabla\widehat J_T(L)} =  \nabla J_{T}(L).$
%
Next, for implementation purposes, we compute the  gradient estimate  explicitly in terms of the measurement sequence and the filtering policy $L$.
\begin{lemma}\label{lem:grad-approx}
Given $L \in \mathcal{S}$ and a sequence of measurements $\mathcal{Y}= \{y(t)\}_{t=0}^T$, we have, 
%
\begin{multline*}
    \nabla_L \varepsilon(L,\mathcal{Y})= -2 H^\intercal e_{T}(L) y(T-1)^\intercal \\
    \textstyle +2\sum_{t=1}^{T-1}-(A_L^\intercal)^{t} H^\intercal e_{T}(L) y(T-t-1)^\intercal    
    + \sum_{k=1}^{t} (A_L^\intercal)^{t-k} H^\intercal e_{T}(L) y(T-t-1)^\intercal L^\intercal (A_L^\intercal)^{k-1} H^\intercal.
\end{multline*}
\end{lemma}

Finally, using this approximate gradient, the so-called \ac{sgd} update proceeds as, 
\begin{equation*} 
 L_{k+1} = L_k - \eta_k \nabla_L \widehat J_T(L),
\end{equation*}
for $k \in \mathbb Z$, where $\eta_k>0$ is the step-size. Numerical results of the application of this algorithm appears in Appendix~\ref{sec:numerics}. 

\begin{remark}
Computing this approximate gradient only requires the knowledge of the system matrices $A$ and $H$, and does \emph{not} require the noise covariance information $Q$ and $R$. Simulation results for the \ac{sgd} algorithm are provided in the supplementary material. 
\end{remark}

Although the convergence of the \ac{sgd} algorithm is expected to follow similar to the \ac{gd} algorithm under the  gradient dominance condition and Lipschitz property, the analysis becomes complicated due to the possibility of the iterated gain $L_k$ leaving the sub-level sets. It is expected that a convergence guarantee would hold under high-probability due to concentration of the gradient estimate around the true gradient. The complete analysis in this direction is provided in the subsequent sections.

We first provide sample complexity and convergence guarantees for SGD with a biased estimation of gradient for locally Lipschitz objective functions and in presence of stability constraint $\mathcal{S}$. Subsequently, we study the stochastic problem of estimating the gradient for the estimation problem. Distinct features of our approach as compared with similar formulations in the literature are highlighted in \Cref{tab:lqr-estimation}.


We now provide a road map to navigate the technical results that concludes with \Cref{thm:combined}: Section 4.1 is concerned with the convergence analysis of the SGD algorithm under an assumption for the biased gradient oracle, summarized in \Cref{thm:sgd} which concludes the linear convergence of the iterates for sufficiently small stepsize.  
%
Section 4.2 is concerned with the bias-variance error analysis of the gradient estimate, summarized in \Cref{thm:oracle} providing the sufficient values for the batch-size and trajectory length that guarantees the desired bound on the gradient oracle required in \Cref{thm:sgd}. Finally, combining the results from \Cref{thm:sgd} and \Cref{thm:oracle} concludes our main result in \Cref{thm:combined}.













\subsection{SGD with biased gradient and stability constraint}
First, we characterize the ``robustness'' of a policy at which we aim to estimate the gradient. This is formalized in the following lemma which is a consequence of \cite[Lemma IV.1]{talebi2022policy}.

\begin{lemma}\label{lem:stability}
Consider any $L \in \mathcal{S}$ and let $Z$ be the unique solution of
\(Z = A_L Z A_L^\intercal + \Lambda\) for any $ \Lambda \succ 0$. Then, $L+ \Delta \in \mathcal{S}$ for any $\Delta \in \bR^{n \times m}$ satisfying
\(0 \leq \|\Delta\|_F \leq {\lambdamin(\Lambda)}\big/{\left[2 \lambdamax(Z)\|H\|\right]}.\)
\end{lemma}


Second, we provide a uniform lowerbound for the stepsize of gradient descent for an approximated direction ``close'' to the true gradient direction.



\begin{table}
  \caption{Differences between SGD algorithms for optimal LQR and optimal estimation problems}
  \label{tab:lqr-estimation}
  \centering
  \begin{tabular}{lllllll}
    \toprule
    Problem & \multicolumn{3}{c}{Parameters}     & Constraints & \multicolumn{2}{c}{Gradient Oracle}         \\
    \cmidrule(r){2-4} \cmidrule(r){5-5} \cmidrule(r){6-7}
            & cost value & $Q$ and $R$ & $A$ and $H$ & stability $\mathcal{S}$ & model & biased \\
    \midrule
    LQR  \cite{fazel2018global}       & known   & known  & unknown  & yes  & $\E{}{J(L+r\Delta) \Delta}$ & yes\\
    &&&&& $\;\;\Delta\sim U(\mathbb{S}^{mn})$ &\\[3pt]
    Estimation  & unknown & unknown & known  & yes  & $\E{}{\nabla \varepsilon(L,\mathcal{Y})}$ & yes \\
    (this work)&&&&& $\;\;\mathcal{Y}\sim$output data &\\[3pt]
    Vanila SGD  & * & *  & * & no & $\E{}{\nabla \varepsilon(L,\mathcal{Y})}$  & no \\
    &&&&& $\;\;\mathcal{Y}\sim$ data dist. &\\
    \bottomrule
  \end{tabular}
\end{table}



\begin{lemma}[Uniform Lower Bound on Stepsize]\label{lem:lowerbound-eta}
Let $L_0 \in \mathcal{S}_\alpha$ for some $\alpha \geq \alpha^* \coloneqq J(L^*)$, and choose any finite $\beta \geq \alpha$. Consider any direction $E$ such that $\|  E - \nabla J(L_0)\|_F \leq \gamma \|\nabla J(L_0)\|_F$ for some $\gamma \in [0,1]$, then we have $J(L_0 - \eta E) \leq \beta$ for any $\eta$ satisfying:
\[0 \leq \eta \leq \frac{1-\gamma}{(\gamma+1)^2}\cdot\frac{1}{\ell(\beta)} + \frac{ c_3(\alpha) }{\ell(\alpha) [\alpha - \alpha^*]} \sqrt{\frac{\beta- \alpha}{2\ell(\beta)}}.\]
\end{lemma}
\begin{remark}
    Note that for the case of exact gradient direction, i.e. when $E = \nabla J(L_0)$, we have $\gamma = 0$ and choosing $\beta = \alpha$ implies the known uniform bound of $\eta \leq \frac{1}{\ell(\beta)}$ for feasible stepsizes as expected. Also, by this choice of $\beta$, this guarantees that the next iterate remains in sublevel set $\mathcal{S}_\alpha$. This lemma generalizes this uniform bound for general directions and (potentially) larger sublevel set.
\end{remark}




The next proposition provides a decay guarantee for one iteration of gradient descent with an approximate direction which will be used later for convergence of \ac{sgd} with a biased gradient estimate. 

\begin{proposition}[Linear Decay in Cost Value]\label{prop:sgd-decay}
Suppose $L_0 \in \mathcal{S}_\alpha$ for some $\alpha > 0$ and a direction $E\neq 0$ is given such that $\|E - \nabla J(L)\|_F \leq \gamma \|\nabla J(L)\|_F$ for some $\gamma <1$. Let
\(\bar\eta \coloneqq \nicefrac{(1-\gamma)}{(\gamma+1)^2 \ell(\alpha)}.\)
Then, $L_1\coloneqq L_0 - \bar\eta E$ remains in the same sublevel set, i.e., $L_1 \in \mathcal{S}_{\alpha}$. Furthermore, we obtain the following linear decay of the cost value:
\begin{gather*}
\begin{aligned}
    J(L_1) -J(L^*) \leq \left[1-c_1(\alpha)\bar\eta{(1 - \gamma)}/2\right] [J(L_0)- J(L^*)].
\end{aligned}
\end{gather*}
\end{proposition}

Next, we guarantee that SGD algorithm with this biased estimation of gradient obtains a linear convergence rate outside a small set $\mathcal{C}_{\tau}$ around optimality defined as
\[ \mathcal{C}_{\tau} \coloneqq \{L \in\mathcal{S}\;\big|\; \|\nabla J(L)\|_F \leq {s_0}/{\tau}\},\] 
for some $\tau \in (0,1)$ and arbitrarily small $s_0>0$.
First, we assume access to the following oracle that provides a biased estimation of the true gradient.
\begin{assumption}\label{asmp:noisy-grad}
Suppose, for some $\alpha>0$, we have access to a biased estimate of the gradient $\nabla\widehat J(L)$ such that, there exists constants $s, s_0 > 0$ implying $\|\nabla\widehat J(L) - \nabla J(L)\|_F \leq s \|\nabla J(L)\|_F + s_0$ for all $L \in \mathcal{S}_\alpha \setminus \mathcal{C}_{\tau}$.
\end{assumption}

%
%
%
%
%
%
%
%
%
%
%

\begin{theorem}[Convergence]\label{thm:sgd}
    Suppose Assumption \ref{asmp:noisy-grad} holds with small enough $s$ and $s_0$ such that $s \leq \gamma/2$ and $\mathcal{S}_\alpha \setminus \mathcal{C}_{\gamma/2}$ is non-empty for some $\gamma \in (0,1)$.
    Then, SGD algorithm starting from any $L_0 \in \mathcal{S}_\alpha \setminus \mathcal{C}_{\gamma/2}$ with fixed stepsize $\bar\eta \coloneqq \frac{(1-\gamma)}{(\gamma+1)^2 \ell(\alpha)}$ generates a sequence of policies $\{L_k\}$ that are stable (i.e. each $L_k \in \mathcal{S}_\alpha$) and cost values decay linearly before entering $\mathcal{C}_{\gamma/2}$; i.e.,
    \[{J(L_k) -J(L^*)}  \leq \left[1-c_1(\alpha)\bar\eta{(1 - \gamma)}/2\right]^k \left[J(L_0)- J(L^*)\right],\]
    for each $k \geq 0$ unless $L_j \in \mathcal{C}_{\gamma/2}$ for some $j \leq k$.
\end{theorem}


\begin{remark}\label{rmk:convergence}
    By combining \Cref{thm:sgd} and the PL property of the cost in (11a), we immediately obtain a sample complexity for our algorithm; e.g., choose $\gamma =1/2$, then in order to guarantee an $\varepsilon$ error on the cost $J(L_k)-J(L^*)\leq \varepsilon$, it suffices to have a small enough bias term $s_0\leq \frac{\sqrt{c_1(\alpha) \varepsilon}}{4}$ and variance coefficient $s\leq \frac{1}{4}$ for the oracle, and run the SGD algorithm for $k>\ln(\frac{\varepsilon}{\alpha})/\ln(1-\frac{c_1(\alpha)}{18\ell(\alpha)}))$ steps. 
\end{remark}











\subsection{Observation model for the estimation problem}\label{sec:observation-model}
%




%
Herein, first we show that the estimation error and its differential can be characterized as a ``simple norm'' of the concatenated noise (\Cref{prop:error-vector-form}). This norm is induced by a metric that encapsulates the system dynamics which is explained below.
Before proceeding to the results of this section, we assume that both the process and measurement noise are bounded:
\begin{assumption}\label{asmp:noise-bound}
    Assume that (almost surely) $\|x_0\|, \|\xi(t)\| \leq \kappa_\xi$ and $\|\omega(t)\| \leq \kappa_\omega$ for all $t$. Also, for simplicity, suppose the initial state has zero mean, i.e., $m_0=0_n$.
    %
\end{assumption}

For two vectors $\Vec{v},\Vec{w} \in \mathbb{R}^{(T+1)n}$, we define
\[\tensor{\Vec{v}}{\Vec{w}}{\calA_L} \coloneqq \tr{\Vec{v}\Vec{w}^\intercal \calA_L^\intercal H^\intercal H \calA_L},\]
where 
\(\calA_L \coloneqq \begin{pmatrix}A_L^0 & A_L^1 & \dots & A_L^{T} \end{pmatrix}.\)
Also, define
\(\calM_L [E] \coloneqq \begin{pmatrix}
    M_0[E] & M_1[E] &  \cdots & M_{T}[E]
\end{pmatrix}\)
with $M_0[E] = 0$, $M_1[E] = EH$ and $M_{i+1}[E] = \sum_{k=0}^i A_L^{i-k} E H A_L^k$ for $i =1,\cdots,T-1$. 
%
%

\begin{proposition}\label{prop:error-vector-form}
    %
    The estimation error $\varepsilon(L,\mathcal{Y}_T)$ takes the following form 
\begin{align*}
    \varepsilon(L,\mathcal{Y}_T) = \|\veta\|_{\calA_L}^2,
\end{align*}
where $\veta \coloneqq \vxi - (I \otimes L) \vomega$ with
\(
\vxi^\intercal = \begin{pmatrix}\xi(T-1)^\intercal & \dots & \xi(0)^\intercal & x(0)^\intercal \end{pmatrix}\)
and
\(\vomega^\intercal = \begin{pmatrix}\omega(T-1)^\intercal & \dots & \omega(0)^\intercal & 0_m^\intercal \end{pmatrix}.
\)
Furthermore, its differential acts on small enough $E \in \mathbb{R}^{n\times m}$ as,
\begin{equation*}
    %
    \diff \varepsilon(\cdot,\mathcal{Y}_T)\big|_L(E) = -2\tensor{\veta}{(I \otimes E) \vomega}{\calA_L}
    +\tr{\calX_L \calN_L [E]},
\end{equation*}
where $\calX_L \coloneqq \veta \veta^\intercal$ and $\calN_L[E] \coloneqq \calM_L [E]^\intercal H^\intercal H \calA_L+ \calA_L^\intercal H^\intercal H \calM_L [E]$.
%
%
%
%
\end{proposition}


Now, we want to bound the error in the estimated gradient $\nabla\widehat J_T(L)$ by considering the concentration error (on length $T$ trajectories) and truncation error separately as follows:
\begin{equation*}
\|\nabla\widehat J_T(L) - \nabla J(L)\| \leq \|\nabla\widehat J_T(L) -\nabla J_T(L)\| 
+ \|\nabla J_T(L)-\nabla J(L)\|,
\end{equation*}
recalling that $J_T(L) = \E{}{\varepsilon(L,\mathcal{Y}_T)}$ by definition.



Next, we aim to provide the analysis of concentration error on trajectories of length $T$ with probability bounds. However, for any pair of real $(T\times T)$-matrices $M$ and $N$, by Cauchy Schwartz inequality we obtain that
\(|\tr{M N}|\leq \|M\|_F \|N\|_F \leq \sqrt{T} \|M\| \|N\|_F.\)
This bound becomes loose (in terms of dimension $T$) as the condition number of $N$ increases\footnote{The reason is that the first equality is sharp whenever $M$ is in the ``direction'' of $N$, while the second inequality is sharp whenever condition number of $M$ is close to one.}.

Nonetheless, we are able to provide concentration error bounds that ``scale well with respect to the length $T$'' that hinges upon the following idea: from von Neumann Trace Inequality \cite[Theorem 8.7.6]{horn2012matrix} one obtains,
\begin{equation}\label{eq:nuclear-norm}
   \textstyle |\tr{M N}|\leq \sum_{i=1}^T \sigma_i(M) \sigma_i(N) \leq \|M\| \|N\|_*,
\end{equation}
where $\|N\|_* \coloneqq \tr{\sqrt{N^\intercal N}} = \sum_i \sigma_i(N)$ is the \textit{nuclear norm} with $\sigma_i(N)$ denoting the $i$-th largest singular value of $N$. Additionally, the same inequality holds for non-square matrices of appropriate dimension which is tight in terms of dimension.


\begin{proposition}[Concentration Error Bound]\label{prop:concen-n}
Consider $M$ independent length $T$ trajectories $\{\mathcal{Y}_{T}^i\}_{i=1}^M$ and suppose Assumption \ref{asmp:noise-bound} holds. Let $\nabla \widehat J_{T}(L) \coloneqq \frac{1}{M}\sum_{i=1}^M \nabla \varepsilon(L,\mathcal{Y}^i_{T})$, then for any $s>0$,
\begin{gather*}
    \prob{\|\nabla\widehat J_T(L) -\nabla J_T(L)\| \geq s }  \leq 2n \exp\left[\frac{- M s^2/2}{\nu_L^2  + 2\nu_L s/3}\right],
\end{gather*} 
where $\nu_L \coloneqq { 4 \kappa_L^2 C_L^3 \|H\|^2 \, \| H \|_* }\big/{[1-\sqrt{\rho(A_L)}]^3}$ with $\kappa_L = \kappa_\xi + \|L\| \kappa_\omega$.
\end{proposition}






We can also show how the truncation error decays linearly as $T$ grows, with constants that are independent of the system dimension $n$:
\begin{proposition}[Truncation Error Bound]\label{prop:trunc-error}
    Under Assumption \ref{asmp:noise-bound}, the truncation error is bounded:
    \[ \| \nabla J(L) - \nabla J_T(L)\| \leq \bar\gamma_L\; {\sqrt{\rho(A_L)}^{T+1}},\]
    where $\bar\gamma_L \coloneqq 10  \kappa_L^4 C_L^6 \,\|H\|^2 \| H\|_* \big/{[1-\rho(A_L)]^2}  .$
\end{proposition}


\begin{remark}
    Notice how the trajectory length $T$ determines the bias in the estimated gradient. However, the concentration error bound is independent of $T$ and only depends on the noise bounds proportionate to $\kappa_\xi^4$, $\kappa_\omega^4$ and the stability margin of $A_L$ proportionate to $C_L^6\big/(1-\sqrt{\rho(A_L)})^6$.
\end{remark}
Finally, by combining the truncation bound in \Cref{prop:trunc-error} with concentration bounds in \Cref{prop:concen-n} we can provide probabilistic bounds on the ``estimated cost'' $\widehat{J}_T(L)$ and the ``estimated gradient'' $\nabla \widehat{J}_T(L)$. Its precise statement is deferred to the supplementary materials (\Cref{thm:estimated-gradient}).



\subsection{Sample complexity of SGD for Kalman Gain}
Note that the open-loop system may be unstable. Often in learning literature, it is assumed that the closed-loop system can be contractible ( i.e., the spectral norm $\|A_L\|< 1$) which is quite convenient for analysis, however, it is not a reasonable system theoretic assumption.
Herein, we emphasize that we only require the close-loop system to be Schur stable, meaning that $\rho(A_L) < 1$; yet, it is very well possible that the system is not contractible. Handling systems that are merely stable requires more involved system theoretic tools that are established in the following lemma.
\begin{lemma}[Uniform Bounds for Stable Systems] \label{lem:bound-rho-L}
Suppose $L \in \mathcal{S}$, then there exit a constants $C_L>0$ such that 
\[\|A_L^k\| \leq C_L \; \sqrt{\rho(A_L)}^{k+1}, \quad \forall k\geq 0.\]
%
%
%
%
%
Furthermore, consider $\mathcal{S}_\alpha$ for some $\alpha>0$, then there exist constants $D_\alpha >0$, $C_\alpha >0$ and $\rho_\alpha \in(0,1)$ such that
\(\|L\| \leq D_\alpha, \; C_L \leq C_\alpha,\) and \(\rho(A_L) \leq \rho_\alpha\)  for all \(L \in \mathcal{S}_\alpha.\)

%
%
\end{lemma}

The following result provides sample complexity bounds for this stochastic oracle to provide a biased estimation of the gradient that satisfies our oracle model of SGD analysis in Assumption \ref{asmp:noisy-grad}.

\begin{theorem}\label{thm:oracle}
%
    Under the premise of \Cref{prop:concen-n}, consider $\mathcal{S}_\alpha$ for some $\alpha>0$ and choose any $s, s_0>0$ and $\tau \in (0,1)$. Suppose the trajectory length $T  \geq \ln\left( {\bar\gamma_\alpha \sqrt{\min(n,m)}}/{s_0} \right)\Big/\ln\left( {1}/{\sqrt{\rho_\alpha}} \right)$ and the batch size \( M \geq 4 \nu_\alpha^2 \min(n,m) \ln(2n/\delta) \big/{(s\, s_0)^2 },\)
    where $\bar\gamma_\alpha \coloneqq 10  (\kappa_\xi + D_\alpha \kappa_\omega)^4 C_\alpha^6 \,\|H\|^2 \| H\|_*$ and $\nu_\alpha \coloneqq {5C_\alpha^3  \|H\|^2 \| H\|_* (\kappa_\xi + D_\alpha \kappa_\omega)^2}\big/{[1-\sqrt{\rho_\alpha}]^3}$.
    %
    Then, with probability no less than $1-\delta$, Assumption \ref{asmp:noisy-grad} holds.
    %
    %
    %
    %
    
\end{theorem}
\begin{proof}
    First, note that for any $L \in \mathcal{S}_\alpha$ by \Cref{lem:bound-rho-L} we have that $\bar\gamma_L \leq \bar\gamma_\alpha$ and $\nu_L \leq \nu_\alpha$.
    Then, note that the lower bound on $T$ implies that
    \(\bar\gamma_\alpha \sqrt{\min(n,m)} \sqrt{\rho_\alpha}^{T+1} \leq s_0.\)
    The claim then follows by applying \Cref{thm:estimated-gradient} and noting that for any $L \not\in \mathcal{C}_\tau$ the gradient is lowerbound as $\|\nabla J(L)\| > s_0/\tau$.
\end{proof}


\begin{theorem}\label{thm:combined}
    Consider the linear system (1) under Assumptions \ref{assmp:observable} and \ref{asmp:noise-bound}. Suppose the SGD algorithm is implemented  with initial stabilizing gain $L_0$, the step-size $\bar\eta \coloneqq \frac{2}{9 \ell(J(L_0))}$, for $k$ iterations,  a batch-size of $M$, and data-length $T$. Then, $\forall \varepsilon>0$ and with probability larger than $1-\delta$, $J(L_k)  - J(L^*)\leq \varepsilon$ if 
    \begin{equation}
        T \geq  O(\ln(\frac{1}{\varepsilon})),\quad  M \geq  O(\frac{1}{\varepsilon}\ln(\frac{1}{\delta}) \ln(\ln(\frac{1}{\varepsilon}))), \quad \text{and}\quad k \geq O(\ln(\frac{1}{\varepsilon})). 
    \end{equation}
\end{theorem}

\begin{proof}[Proof of \Cref{thm:combined}]
    Recall that according to \Cref{rmk:convergence}, in order to obtain $\varepsilon$ error on the cost value we require $k \geq O(\ln(1/\varepsilon))$ of SGD algorithm using gradient estimates with small enough bias term $s_0 \leq \frac{\sqrt{c_1(\alpha) \varepsilon}}{4}$ and variance coefficient $s< \frac{1}{4}$. 
    %
    Now, we can guarantee Assumption~\ref{asmp:noisy-grad} holds for such $s_0$ and $s$ (with probability at least $1-\delta$) by invoking \Cref{thm:oracle}. In fact, it suffices to ensure that the length of data trajectories $T$ and the batch size $M$ are large enough; specifically,  $T \geq O(\ln(1/s_0)) = O(\ln(1/\varepsilon))$ and by using union bound for bounding the failure probability $M \geq O(\ln(1/\delta)\ln(k)\big/ s_0^2) = O(\ln(1/\delta)\ln(\ln(1/\varepsilon)) \big/ \varepsilon)$.
\end{proof}
%
%
%


%

%

%

%

%

%
%

%

%

%

%






\section{Conclusions, Broader Impact, and Limitations}
In this work, we considered the problem of learning the optimal (steady-state) Kalman gain for linear systems with unknown process and measurement noise covariances. Our approach builds on the duality between optimal control and estimation, resulting in a direct stochastic \ac{po} algorithm for learning the optimal filter gain. We also provided convergence guarantees and finite sample complexity with bias and variance error bounds that scale well with problem parameters. In particular, the variance is independent of the length of data trajectories and scales logarithmically with problem dimension, and bias term decreases exponentially with the length.

This work contributes a generic optimization algorithm and introduces a filtering strategy for estimating dynamical system states. While theoretical, it raises privacy concerns similar to the model-based Kalman filter. Limitations include the need for prior knowledge of system parameters; nonetheless, parameter uncertainties can be treated practically as process and measurement noise. Finally, sample complexities depend on the stability margin $1-\sqrt{\rho(A_L)}$, inherent to the system generating the data.

Finally, a direction for future research is to study how to adapt the proposed algorithm and error analysis for the setting when a single long trajectory is available as opposed to several independent trajectories of finite length. Another interesting direction is to carry out a robustness analysis, similar to its LQR dual counterpart, to study the effect of the error in system parameters on the policy learning accuracy. Of course, the ultimate research goal is to use the recently introduced duality in nonlinear filtering \cite{kim2022duality} as a bridge to bring tools from learning to nonlinear filtering.

\begin{ack}
The research of A. Taghvaei has been supported by NSF grant EPCN-2318977; S. Talebi and M. Mesbahi have been supported by NSF grant ECCS-2149470 and AFOSR grant FA9550-20-1-0053. The authors are also grateful for constructive feedback from the reviewers.
\end{ack}
%
%
%
%


\bibliographystyle{plain}
\bibliography{citations}


\newpage


%

%
%
%
%
%
%
%
%





















%
%
%


\tableofcontents

\begin{appendices}

%
\section{Discussion of additional related works}\label{sec:related-work}

After the initial submission of this manuscript, we encountered the following related works, and we will now briefly discuss their connections and differences in relation to our work: 
%
\cite{zheng2021sample} establishes an end-to-end sample complexity bound on learning a robust \ac{lqg} controller (which is different than our filtering problem). As the system parameters are also unknown, this work only considers the \textit{open-loop stable} systems. Additionally, it establishes a nice trade-off between optimality and robustness characterized by a suboptimality performance bound as a result of the robust \ac{lqg} design. While our filtering design is based on the knowledge of system parameters, we do not consider the open-loop stability assumption; and the robust synthesis is included as one of the main future directions of this work. 
Furthermore, the complexity bounds in \cite{zheng2021sample} depends on the length of trajectory, whereas ours does not. Also, their suboptimality bound scale as $O(1/\sqrt{N})$ in the number of trajectories, whereas ours scales as $O(1/N)$.
%
Next, \cite{Zhang2023learning} similarly considers the problem of learning the steady-state Kalman gain but in a different setup: The model is assumed to be completely unknown. However, the algorithm requires access to a simulator that provides noisy measurement of the MSE~ $\mathbb E[\|X(t)- \hat{X}(t)\|^2]$ which requires generation of ground-truth state trajectories $X(t)$ (see Assumption 3.2 therein). The proposed approaches are different: zeroth-order global optimization (theirs) vs first-order stochastic gradient descent (ours). The analysis reports a $O(\varepsilon^{-2})$ sample complexity on optimal policy error which aligns with our result. However, it is difficult to provide more detailed comparison as explicit dependence of the error terms on problem dimension is not provided.  
%
Finally, \cite{liu2023learning} considers the problem of simultaneously learning the optimal Kalman filter and linear feedback control policy in the \ac{lqg} setup. Their approach involves solving Semidefinite Programmings (SDP) using input-state-output trajectories. Their result, for the the case when trajectories involve noise, relies on the assumption that the magnitude of the noise can be made arbitrary small. This is in contrast to our setup where we only assume a bound on the noise level and do not require access to state trajectories. 
\section{Summary of Assumptions}
Herein, we provide a summary of all assumptions considered in our work and their relation to our results.

Note that if the given pair $(A,H)$ of system parameters is not detectable, then the estimation problem is not well-posed, simply because there may not exist any stabilizing policy $L_0$. Therefore, we consider the minimum required assumption for well-posedness of the problem as follows:
\begin{assumption1}
    The pair $(A,H)$ is detectable, and the pair $(A,Q^{\frac{1}{2}})$ is stabilizable, where
    $Q^{\frac{1}{2}}$ is the unique positive semidefinite square root of $Q$. 
\end{assumption1}

It is indeed possible to work with the weaker conditions compared with Assumption~\ref{assmp:detectable} and obtain similar results presented here---by incorporating more system theoretic tools. However, in order to improve the clarity of the learning problem with less system theoretic technicalities, we work with the following stronger assumption in lieu of Assumption~\ref{assmp:detectable}.
\begin{assumption2}
    The pair $(A,H)$ is observable, and the noise covariances $Q\succ 0$ and $R \succ 0$. 
\end{assumption2}
\noindent It is clear that Assumption~\ref{assmp:observable} implies Assumption~\ref{assmp:detectable}.


Next, in order to provide an independent analysis of the SGD algorithm for locally Lipschitz functions in presence of gradient bias, we consider the following assumption on the gradient oracle.
\begin{assumption3}
Suppose, for some $\alpha>0$, we have access to a biased estimate of the gradient $\nabla\widehat J(L)$ such that, there exists constants $s, s_0 > 0$ implying $\|\nabla\widehat J(L) - \nabla J(L)\|_F \leq s \|\nabla J(L)\|_F + s_0$ for all $L \in \mathcal{S}_\alpha \setminus \mathcal{C}_{\tau}$.
\end{assumption3}


Next, in order to utilize the complexity analysis of SGD algorithm provided in \Cref{thm:sgd}, we need to build an oracle that satisfies Assumption~\ref{asmp:noisy-grad}. This is oracle is built in Section 4.2, building on the following assumptions on the process and observation noise.
\begin{assumption4}
    Assume that (almost surely) $\|x_0\|, \|\xi(t)\| \leq \kappa_\xi$ and $\|\omega(t)\| \leq \kappa_\omega$ for all $t$. Also, for simplicity, suppose the initial state has zero mean, i.e., $m_0=0_n$.
    %
\end{assumption4}
\noindent Then, based on this assumption, \Cref{thm:oracle} provides lowerbounds on the trajectory length and the batch-size in order for the oracle to satisfy Assumption~\ref{asmp:noisy-grad}. Finally, the combined version of our results is presented in \Cref{thm:combined}. 



\section{Proof of the duality relationship: Proposition~\ref{prop:duality}}
1. By pairing the original state dynamics~\eqref{eqn:sysdyn} and its dual~\eqref{eqn:adjdyn}:
\begin{equation*}
    z(t+1)^\intercal x(t+1) - z(t)^\intercal x(t) = z(t+1)^\intercal \xi(t) + u(t+1)^\intercal H x(t). 
\end{equation*}
Summing this relationship from $t=0$ to $t=T-1$ yields,
\begin{gather*}
    z(T)^\intercal x(T) = z(0)^\intercal x(0) +  \sum_{t=0}^{T-1} z(t+1)^\intercal \xi(t) + u(t+1)^\intercal H x(t). 
\end{gather*}
Upon subtracting the estimate $a^\intercal \hat{x}_{\mathcal L}(T)$, using the adjoint relationship
\begin{align}\label{eq:adjoint}
       \sum_{t=0}^{T-1}u(t+1)^\intercal y(t) &= a^\intercal \hat{x}_{\mathcal L}(T).
\end{align}
and $z(T)=a$, lead to  
\begin{align*}
    a^\intercal x(T) -& a^\intercal\hat{x}_{\mathcal L}(T)  = z(0)^\intercal x(0) + \textstyle\sum_{t=0}^{T-1} z(t+1)^\intercal \xi(t) - u(t+1)^\intercal w(t).
\end{align*}
Squaring both sides and taking the expectation concludes the duality result in \cref{eq:dual-policy}.

2. 
Consider the adjoint system~\eqref{eqn:adjdyn} with the linear feedback law $u(t)=L^\intercal z(t)$.
Then, 
\begin{equation}\label{eq:z-sol}
z(t)=(A_L^\intercal)^{T-t}a,\quad \text{for}\quad t=0,1,\ldots,T.
\end{equation}
Therefore, as a function of $a$, $u(t) = L^\intercal(A_L^\intercal)^{T-t}a$. 
%
These relationships are used to identify the control policy 
%
\begin{align*}
  \mathcal L^\dagger (a) &= (u(1),\ldots,u(T))
  =(L^\intercal(A_L^\intercal)^{T-1}a,\ldots,L^\intercal  a).
\end{align*}
This control policy corresponds to an estimation policy by the adjoint relationship~\eqref{eq:adjoint}:
\begin{equation*}
    a^\intercal \hat{x}_{\mathcal L}(T) =\sum_{t=0}^{T-1} a^\intercal A_L^{T-t-1} L y(t),\quad \forall a \in \mathbb R^n.
\end{equation*}
This relationship holds for all $a\in \mathbb R^n$. Therefore, 
\begin{align*}
  \hat{x}_{\mathcal L}(T) = \sum_{t=0}^{T-1} A_L^{T-t-1} L y(t),
\end{align*}
which coincides with the Kalman filter estimate with constant gain $L$ given by the formula~\eqref{eq:estimate-x-L} (with $m_0=0$). Therefore, the adjoint relationship~\eqref{eq:adjoint} relates the control policy with constant gain $L^\intercal$ to the Kalman filter with constant gain $L$. 
The result~\eqref{eq:dual} follows from the identity
\begin{align*}
 &\E{}{\|y(T)\!-\!\hat{y}_{L}(T)\|^2}  \!=  \!\E{}{\|Hx(T)\!-\!H\hat{x}_{ L}(T)\|^2 \!+\!\|w(T)\|^2}\\
 &\quad =\textstyle\sum_{i=1}^m \E{}{|H_i^\intercal x(T) - H_i^\intercal \hat{x}_{ L}(T)|^2} + \tr{R}, 
\end{align*}
and the application of the first result~\eqref{eq:dual} with $a=H_i$. 


\section{Proofs for the results for the analysis of the optimization landscape}
\subsection{Preliminary lemma}
The following  lemmas are a direct consequence of duality and useful for
our subsequent analysis.
\begin{lemma}\label{lem:topology}
The set of Schur stabilizing gains $\mathcal{S}$ is regular open, contractible, and unbounded when $m\geq 2$ and the boundary $\partial \mathcal{S}$ coincides with the set $\{L \in \bR^{n\times m}: \rho(A-LH) = 1\}$. Furthermore, $J(.)$ is real analytic on $\mathcal{S}$.
\end{lemma}
\begin{proof}
Consider the duality described in \Cref{remark:duality}. The proof then follows identical to \cite[Lemmas 3.5 and 3.6]{bu2019lqr} by noting that the spectrum of a matrix is identical to the spectrum of its transpose.
\end{proof}

Next, we present the proof of \Cref{lem:coercive} that provides sufficient conditions to recover the coercive property of $J(.)$ which resembles Lemma 3.7 in \cite{bu2019lqr} (but extended for the time-varying parameters). The proof is removed on the account of space and appears in the extended version of this work.

\begin{remark}
This approach recovers the claimed coercivity also in the control setting with weaker assumptions. In particular, using this result, one can replace the positive definite condition on the covariance of the initial condition in \cite{bu2019lqr}, i.e., $\Sigma \succ 0$, with just the controllability of $(A, \Sigma^{1/2})$. 
\end{remark}

\subsection{Proof of Lemma~\ref{lem:coercive}}
\begin{proof}
Consider any $L \in \mathcal{S}$ and note that the right eigenvectors of $A$ and $A_L$ that are annihilated by $H$ are identical. Thus, by \ac{PBH} test, observability of $(A,H)$ is equivalent to observability of $(A_L,H)$. Therefore, there exists a positive integer $n_0 \leq n$ such that 
\begin{equation*}
    H_{n_0}^\intercal(L):=\begin{pmatrix}
    H^\intercal & A_L^\intercal H^\intercal & \ldots & (A_L^\intercal)^{n_0-1}H^\intercal
    \end{pmatrix}
\end{equation*}
is full-rank, implying that $H_{n_0}^\intercal(L) H_{n_0}(L)$ is positive definite.
%
Now, recall that for any such stabilizing gain $L$, we compute
\begin{gather*}
\begin{aligned}
    J(L) &= \tr{\sum_{t=0}^\infty (A_L)^t(Q+L R L^\intercal)(A_L^\intercal)^t H^\intercal H} \\
    &= \tr{\sum_{t=0}^\infty  \sum_{k=0}^{n_0-1}(A_L)^{n_0t+k}(Q+L R L^\intercal)(A_L^\intercal)^{n_0t+k} H^\intercal H} \\
    & =  \tr{\sum_{t=0}^\infty  (A_L)^{n_0t}(Q+L R L^\intercal)(A_L^\intercal)^{n_0t} H_{n_0}^\intercal(L) H_{n_0}(L)}\\
    & \eqqcolon \tr{X_{n_0}(L) H_{n_0}^\intercal(L) H_{n_0}(L)},
\end{aligned}
\end{gather*}
where we used the cyclic property of trace and the inequality follows because for any PSD matrices $P_1 , P_2 \succeq 0$ we have
\begin{equation}\label{eq:postrace}
    \tr{P_1P_2} = \tr{P_2^{\frac{1}{2}} P_1 P_2^{\frac{1}{2}}} \geq 0.
\end{equation}
Also, $X_{n_0}(L)$ is well defined because $A_L$ is Schur stable if and only if $(A_L)^{n_0}$ is. Moreover, $X_{n_0}(L)$ coincides with the unique solution to the following Lyapunov equation
\[X_{n_0}(L) = (A_L)^{n_0} X_{n_0}(L) (A_L^\intercal)^{n_0} + Q + L R L^\intercal.\]
Next, as $R \succeq 0$,
\begin{align}
    J(L) \geq& \lambdamin(H_{n_0}^\intercal(L) H_{n_0}(L)) \tr{X_{n_0}(L)} \nonumber\\
    \geq& \lambdamin(H_{n_0}^\intercal(L) H_{n_0}(L)) \tr{\sum_{t=0}^\infty  (A_L)^{n_0t} Q (A_L^\intercal)^{n_0t}} \nonumber\\
    \geq& \lambdamin(H_{n_0}^\intercal(L) H_{n_0}(L)) \lambdamin(Q) \sum_{t=0}^\infty  \tr{(A_L^\intercal)^{n_0t}(A_L)^{n_0t}} \nonumber\\
    \geq& \lambdamin(H_{n_0}^\intercal(L) H_{n_0}(L)) \lambdamin(Q) \sum_{t=0}^\infty  \rho(A_L)^{2n_0t} , \label{eqn:Jlower}
\end{align}
where the last inequality follows by the fact that
\begin{multline*}
    \tr{(A_L^\intercal)^{n_0t}(A_L)^{n_0t}} = \|(A_L)^{n_0t}\|_F^2 
    \geq \|(A_L)^{n_0t}\|_{\text{op}}^2 \geq \rho((A_L)^{n_0t})^2 = \rho(A_L)^{2n_0t},
\end{multline*}
with $\|\cdot\|_{\text{op}}$ denoting the operator norm induced by 2-norm.
Now, by \Cref{lem:topology} and continuity of the spectral radius, as $L_k \to \partial \mathcal{S}$ we observe that $\rho(A_{L_k}) \to 1$. But then, the obtained lowerbound implies that $J(L_k) \to \infty$. On the other hand, as $Q\succ 0$, $R \succ 0$ are both time-independent, by using a similar technique we also provide the following lowerbound
\begin{align*}
    J(L) \geq& \tr{(Q+L R L^\intercal)\sum_{t=0}^\infty(A_L^\intercal)^{n_0t}  H_{n_0}^\intercal(L) H_{n_0}(L) (A_L)^{n_0t}}\\
    \geq& \lambdamin(H_{n_0}^\intercal(L) H_{n_0}(L)) \tr{Q + L R L^\intercal} \\
    \geq& \lambdamin(H_{n_0}^\intercal(L) H_{n_0}(L)) \tr{R LL^\intercal}\\
    \geq& \lambdamin(H_{n_0}^\intercal(L) H_{n_0}(L)) \lambdamin(R) \|L\|_F^2,
\end{align*}
where $\|\cdot\|_F$ denotes the Frobenius norm.
Therefore, by equivalency of norms on finite dimensional spaces, $\|L_k\| \to \infty$ implies that $J(L_k) \to \infty$ which concludes that $J(.)$ is coercive on $\mathcal{S}$. 
Finally, note that for any $L \not\in \mathcal{S}$, by \cref{eqn:Jlower} we can argue that $J(L) = \infty$, therefore the sublevel sets $\mathcal{S}_\alpha \subset \mathcal{S}$ whenever $\alpha$ is finite. The compactness of $\mathcal{S}_\alpha$ is then a direct consequence of the coercive property and continuity of $J(.)$ (\Cref{lem:topology}).
\end{proof}

\subsection{Derivation of the gradient formula in Lemma \ref{prop:graddom}}
Next, we aim to compute the gradient of $J$ in a more general format. We do the derivation for time-varying $R$ and $Q$ with specialization to time-invariant setting at the end.  
For any admissible $\Delta$, we have
\begin{align*}
    X_{\infty}(L+\Delta)- X_{\infty}(L) =
    &\sum_{t=1}^{\infty} \left(A_L\right)^{t} \left(  Q_t + L R_t  L^\intercal \right) \left(\star\right)^\intercal + \left(\star\right) \left(  Q_t + L R_t  L^\intercal \right) \left(A_L^\intercal\right)^{t} \\
    &\quad-\sum_{t=0}^{\infty} \left(A_L\right)^{t} \left(  \Delta R_t  L^\intercal + L R_t \Delta^\intercal \right)\left(A_L^\intercal\right)^{t} + o(\|\Delta\|),
\end{align*}
where the $\star$ is hiding the following term
\[\sum_{k=1}^t (A_L)^{t-k} \Delta H (A_L)^{k-1}.\]
Therefore, by linearity and cyclic permutation property of trace, we get that
\begin{gather*}
    \begin{aligned}
        J(L+\Delta)- J(L) =
    &\tr{\Delta H\sum_{t=1}^{\infty}\sum_{k=1}^{t} 2\left(A_L\right)^{k-1}  \left(  Q_t + L R_t  L^\intercal \right)  \left(A_L^\intercal\right)^{t} H^\intercal H \left(A_L\right)^{t-k}}\\
    &\quad -\tr{\Delta \sum_{t=0}^{\infty} 2R_t  L^\intercal \left(A_L^\intercal\right)^{t} H^\intercal H \left(A_L\right)^{t}} + o(\|\Delta\|).
    \end{aligned}
\end{gather*}
Finally, by considering the Euclidean metric on real matrices induced by the inner product $\langle Q, P\rangle = \tr{Q^\intercal P}$, we obtain the gradient of $J$ as follows 
\begin{gather*}
\begin{aligned}
    \nabla & J(L) = -2\sum_{t=0}^{\infty} \left(A_L^\intercal\right)^{t} H^\intercal H  \left(A_L\right)^{t} L R_t \\
    +&2\sum_{t=1}^{\infty}\sum_{k=1}^{t}     \left(A_L^\intercal\right)^{t-k}  H^\intercal H\left(A_L\right)^{t} \left(  Q_t + L R_t  L^\intercal \right)  \left(A_L^\intercal\right)^{k-1} H^\intercal,
\end{aligned}
\end{gather*}
whenever the series are convergent!
And, by switching the order of the sums it simplifies to
\begin{gather*}
    \begin{aligned}
            \nabla & J(L) = -2\sum_{t=0}^{\infty} \left(A_L^\intercal\right)^{t} H^\intercal H  \left(A_L\right)^{t} L R_t\\ &+2\sum_{k=1}^{\infty}\sum_{t=k}^{\infty} \left[\left(A_L^\intercal\right)^{t-k}  H^\intercal H \left(A_L\right)^{t-k}\right]  A_L \left[\left(A_L\right)^{k-1}\left(  Q_t + L R_t  L^\intercal \right)  \left(A_L^\intercal\right)^{k-1}\right] H^\intercal.\\
            =& -2\sum_{t=0}^{\infty} \left(A_L^\intercal\right)^{t} H^\intercal H  \left(A_L\right)^{t} L R_t +2\sum_{t=0}^{\infty} \left[\left(A_L^\intercal\right)^{t}  H^\intercal H \left(A_L\right)^{t}\right] \\
            &\quad\cdot A_L \left[\sum_{k=0}^{\infty}\left(A_L\right)^{k}\left(  Q_{t+k+1} + L R_{t+k+1}  L^\intercal \right)  \left(A_L^\intercal\right)^{k}\right] H^\intercal.
    \end{aligned}
\end{gather*} For the case of time-independent $Q$ and $R$, this reduces to
\begin{gather*}
\begin{aligned}
    \nabla  J(L) 
    =& -2Y_{(L)} L R  + 2Y_{(L)} A_L\left[\sum_{k=0}^{\infty} \left(A_L\right)^{k}\left(  Q + L R  L^\intercal \right)  \left(A_L^\intercal\right)^{k}\right] H^\intercal\\
    =& 2Y_{(L)} \left[-LR + A_L X_{(L)} H^{\intercal} \right].
\end{aligned}
\end{gather*}
where $Y_{(L)}=Y$ is the unique solution of
\[Y = A_L^\intercal Y A_L + H^\intercal H.\]

\subsection{Proof of existence of global minimizer in Lemma~\ref{prop:graddom}}
The domain $\mathcal{S}$ is non-empty whenever $(A,H)$ is observable. 
Thus, by continuity of $L \to J(L)$, there exists some finite $\alpha > 0$ such that the sublevel set $\mathcal{S}_\alpha$ is non-empty and compact. Therefore, the minimizer  is an interior point and thus must satisfy the first-order optimality condition $\nabla J(L^*) = 0$. Therefore, by coercivity, it is stabilizing and unique which satisfies
\[L^* = A X^* H^\intercal \left(R + H X^* H^\intercal\right)^{-1},\]
with $X^*$ being the unique solution of 
\begin{equation}\label{eqn:Xstar}
    X^* = A_{L^*} X^* A_{L^*}^\intercal + Q + L^* R (L^*)^\intercal.
\end{equation}
As expected, the global minimizer $L^*$ is equal to the steady-state Kalman gain, but explicitly dependent on the noise covariances $Q$ and $R$.

\subsection{Proof of gradient dominance property in Lemma~\ref{prop:graddom}}
Note that $X=X_{(L)}$ satisfies 
\begin{equation}\label{eqn:X}
    X = A_{L} X A_{L}^\intercal + Q + L R L^\intercal.
\end{equation}
%
%
%
%
%
Then, by combining \cref{eqn:Xstar} and \cref{eqn:X}, and some algebraic manipulation, we recover part of the gradient information, i.e., $(-LR + A_L X H^\intercal)$, in the gap of cost matrices by arriving at the following identity
\begin{gather}\label{eq:X-X}
\begin{aligned}
    X&-X^* - A_{L^*} (X-X^*) A_{L^*}^\intercal 
    \\
    =&(LR - A_L X H^\intercal)(L-L^*)^\intercal 
    + (L-L^*)(R L^\intercal - H X A_L^\intercal) \\
    &- (L -L^*)R(L-L^*)^\intercal 
    - (L - L^*) H X H^\intercal (L - L^*)^\intercal \\
    \preceq& \frac{1}{a}(LR - A_L X H^\intercal) (R L^\intercal - H X A_L^\intercal)
    + a(L-L^*)(L-L^*)^\intercal\\
    &- (L - L^*) (R + H X H^\intercal) (L - L^*)^\intercal\\
\end{aligned}
\end{gather}
where the upperbound is valid for any choice of $a > 0$. Now, as $R\succ 0$, we choose $a = \lambdamin(R)/2$. As $X \succeq 0$, it further upperbounds
\begin{gather*}
\begin{aligned}
    X-X^* - A_{L^*} &(X-X^*) A_{L^*}^\intercal 
    \\
    \preceq& \frac{2}{\lambdamin(R)}(-LR + A_L X H^\intercal) (-R L^\intercal + H X A_L^\intercal) \\
    &- \frac{\lambdamin(R)}{2}(L - L^*) (L - L^*)^\intercal .
\end{aligned}
\end{gather*}
Now, let $\Tilde{X}$ and $\widehat{X}$ be, respectively, the unique solution of the following Lyapunov equations
\begin{align*}
    \Tilde{X} &= A_{L^*} \Tilde{X} A_{L^*}^\intercal + (-LR + A_L X H^\intercal) (-R L^\intercal + H X A_L^\intercal),\\
    \widehat{X} &= A_{L^*} \widehat{X} A_{L^*}^\intercal + (L - L^*) (L - L^*)^\intercal.
\end{align*}
Then by comparison, we conclude that 
\[X - X^* \preceq \frac{2}{\lambdamin(R)}\Tilde{X} - \frac{\lambdamin(R)}{2}\widehat{X}.\]
Recall that by the fact in \cref{eq:postrace},
\begin{multline}\label{eqn:ineqJ}
    J(L) - J(L^*) = \tr{(X-X^*)H^\intercal H}
    \leq \frac{2}{\lambdamin(R)}\tr{\Tilde{X}H^\intercal H} - \frac{\lambdamin(R)}{2}\tr{\widehat{X} H^\intercal H}.
\end{multline}
Let $Y^* \succ 0$ be the unique solution of
\[Y^* = A_{L^*}^\intercal Y^* A_{L^*} + H^\intercal H,\]
then, by cyclic permutation property
\begin{gather}
    \tr{\Tilde{X}H^\intercal H} = \tr{(-LR + A_L X H^\intercal) (-R L^\intercal + H X A_L^\intercal)Y^*} \nonumber\\
    \leq \frac{\lambdamax(Y^*)}{\lambdamin^2(Y_{(L)})} \tr{Y_{(L)}(-LR + A_L X H^\intercal) (-R L^\intercal + H X A_L^\intercal)Y_{(L)}} \nonumber\\
    = \frac{\lambdamax(Y^*)}{4\lambdamin^2(Y_{(L)})} \langle \nabla J(L), \nabla J(L) \rangle \label{eqn:ineqgrad}
\end{gather}
where the last equality follows by the obtained formula for the gradient $\nabla J(L)$.
%
Similarly, we obtain that
\begin{equation}\label{eq:Xhatbound}
    \tr{\widehat{X}H^\intercal H} = \tr{(L - L^*) (L - L^*)^\intercal Y^*} 
    \geq \lambdamin(Y^*)\|L-L^*\|_F^2.
\end{equation}
Notice that the mapping $L \to Y_{(L)}$ is continuous on $\mathcal{S} \supset \mathcal{S}_\alpha$, and also by observability of $(A,H)$, $Y_{(L)} \succ 0$ for any $L \in \mathcal{S}$. To see this, let $H_{n_0}(L) \succ 0$ be as defined in \Cref{lem:coercive}. Then,
\begin{gather*}
\begin{aligned}
    Y_{(L)} &= \sum_{t=0}^\infty (A_L^\intercal)^t (H^\intercal H) (A_L)^t \\
    &= \sum_{t=0}^\infty  \sum_{k=0}^{n_0-1}(A_L^\intercal)^{n_0t+k} (H^\intercal H) (A_L)^{n_0t+k} \\
    &= \sum_{t=0}^\infty (A_L^\intercal)^{n_0t} H_{n_0}^\intercal(L) H_{n_0}(L) (A_L)^{n_0t}\\
    &\succeq H_{n_0}^\intercal(L) H_{n_0}(L) \succ 0.
\end{aligned}
\end{gather*}
Now, by \Cref{lem:coercive},  $\mathcal{S}_\alpha$ is compact and therefore we claim that the following infimum is attained with some positive value $\kappa_\alpha$:
\begin{equation}\label{eqn:kappa}
    \inf_{L \in \mathcal{S}_\alpha} \lambdamin(Y_{(L)}) \eqqcolon \kappa_\alpha >0.
\end{equation}
Finally, the first claimed inequality follows by combining the inequalities \cref{eqn:ineqJ}, \cref{eqn:ineqgrad} and \cref{eq:Xhatbound}, with the following choice of
\[c_1(\alpha) \coloneqq \frac{2\lambdamin(R)}{\lambdamax(Y^*)}\kappa_\alpha^2, \quad\text{and}\quad c_2(\alpha) \coloneqq \frac{\lambdamin(Y^*)\lambdamin(R)^2 }{\lambdamax(Y^*)}\kappa_\alpha^2.\]
%
For the second claimed inequality, one arrive at the following identity by similar computation to \cref{eq:X-X}:
\begin{gather*}
\begin{aligned}
    X&-X^* - A_{L} (X-X^*) A_{L}^\intercal 
    \\
    =&(L^*R - A_{L^*} X^* H^\intercal)(L-L^*)^\intercal 
    + (L-L^*)(R {L^*}^\intercal - H X^* A_{L^*}^\intercal) \\
    &+ (L -L^*)R(L-L^*)^\intercal 
    + (L - L^*) H X^* H^\intercal (L - L^*)^\intercal \\
    =& (L - L^*) (R + H X^* H^\intercal) (L - L^*)^\intercal
    %
\end{aligned}
\end{gather*}
where the second equality follows because $Y_{(L)}\succ 0$ and thus
\[L^*R - A_{L^*} X^* H^\intercal = -Y_{(L)}^{-1}\nabla J(L^*) =0.\]
Recall that
\[J(L) - J(L^*) = \tr{(X-X^*)H^\intercal H},\]
then by the equality in \cref{eq:X-X} and cyclic property of trace we obtain
\[J(L) - J(L^*) = \tr{Z Y_{(L)}},\]
where
\begin{gather*}
\begin{aligned}
    Z \coloneqq& (L - L^*) (R + H X^* H^\intercal) (L - L^*)^\intercal\\
    \succeq& \lambdamin(R) (L - L^*) (L - L^*)^\intercal.
\end{aligned}
\end{gather*}
Therefore, for any $L \in \mathcal{S}_\alpha$, we have
\[J(L) - J(L^*) \geq \lambdamin(Y_{(L)})\tr{Z} \geq \lambdamin(R) \kappa_\alpha \|L - L^*\|_F^2,\]
and thus, we complete the proof of first part by the following choice of 
\[c_3(\alpha) = \lambdamin(R) \kappa_\alpha.\]
\subsection{Proof of Lipschitz property in Lemma~\ref{prop:graddom}}

Next, we provide the proof of locally Lipschitz property.
Notice that the mappings $L \to X_{(L)}$, $L \to Y_{(L)}$ and $L \to A_{L}$ are all real-analytic on the open set $\mathcal{S} \supset \mathcal{S}_\alpha$, and thus so is the mapping $L \to \nabla J(L) = 2Y_{(L)} \left[-LR + A_L X_{(L)} H^{\intercal} \right]$. Also, by \Cref{lem:coercive},  $\mathcal{S}_\alpha$ is compact and therefore the mapping $L \to \nabla J(L)$ is $\ell$-Lipschitz continuous on $\mathcal{S}_\alpha$ for some $\ell =\ell(\alpha)>0$. In the rest of the proof, we attempt to characterize $\ell(\alpha)$ in terms of the problem parameters.
%
By direct computation we obtain
\begin{gather*}
\begin{aligned}
    &\nabla J(L_1) - \nabla J(L_2) =(2Y_{(L_1)} - 2Y_{(L_2)}) \left[-L_1R + A_{L_1} X_{(L_1)} H^{\intercal} \right] \\
    &+ 2Y_{(L_2)} \left(\left[-L_1R + A_{L_1} X_{(L_1)} H^{\intercal} \right] -\left[-L_2R + A_{L_2} X_{(L_2)} H^{\intercal} \right]\right)\\
    &= 2(Y_{(L_1)} - Y_{(L_2)}) \left[-L_1(R + H X_{(L_1)} H^{\intercal}) + A X_{(L_1)} H^{\intercal} \right]\\
    &+ 2Y_{(L_2)} \big[(L_2 - L_1)(R + H X_{(L_1)} H^{\intercal}) + A_{L_2} ( X_{(L_1)}  - X_{(L_2)}) H^{\intercal} \big].
\end{aligned}
\end{gather*}
Therefore,
\begin{multline}\label{eqn:grad-lip1}
    \|\nabla J(L_1) - \nabla J(L_2)\|_F^2 \leq
    \ell_1^2\|Y_{(L_1)} - Y_{(L_2)}\|_F^2 + \ell_2^2\|L_1 - L_2\|_F^2 + \ell_3^2\|X_{(L_1)}  - X_{(L_2)}\|_F^2
\end{multline}
where 
\begin{align*}
    \ell_1 &= \ell_1(L_1) \coloneqq 2\|-L_1(R + H X_{(L_1)} H^{\intercal}) + A X_{(L_1)} H^{\intercal}\|_{\text{op}},\\
    \ell_2 &= \ell_2(L_1,L_2) \coloneqq 2 \|Y_{(L_2)}\|_{\text{op}}\, \|R + H X_{(L_1)} H^{\intercal}\|_{\text{op}},\\
    \ell_3 &= \ell_3(L_2) \coloneqq 2 \|Y_{(L_2)}\|_{\text{op}}\, \|A_{L_2}\|_{\text{op}}\, \|H^{\intercal}\|_{\text{op}}.
\end{align*}
%
On the other hand, by direct computation we obtain
\begin{gather}\label{eqn:gapY}
\begin{aligned}
    Y_{(L_1)}& - Y_{(L_2)} - A_{L_1}^\intercal (Y_{(L_1)} - Y_{(L_2)}) A_{L_1} \\
     =&(L_2-L_1)^\intercal H^\intercal  Y_{(L_2)} A_{L_2} + A_{L_2}^\intercal Y_{(L_2)} H (L_2-L_1) \\
    &+ (L_1-L_2)^\intercal H^\intercal  Y_{(L_2)} H (L_1-L_2) \\
    \preceq&  \|L_1-L_2\|_F \, \ell_4 \, I
\end{aligned}
\end{gather}
where
\begin{equation*}
    \ell_4 = \ell_4(L_1,L_2) \coloneqq  2\|
     H^\intercal  Y_{(L_2)} A_{L_2}\|_{\text{op}} 
     + \|H^\intercal  Y_{(L_2)} H (L_1-L_2)\|_{\text{op}}.
\end{equation*}
Now, consider the mapping $L \to Z_{(L)}$ where $Z_{(L)} = Z$ is the unique solution of the following Lyapunov equation:
\[Z = A_L^\intercal Z A_L + I,\]
which is well-defined and continuous on $\mathcal{S}\supset \mathcal{S}_\alpha$. Therefore, by comparison, we claim that
\[\|Y_{(L_1)} - Y_{(L_2)}\|_F \preceq \|L_1-L_2\|_F \; \ell_4 \; \|Z_{(L_1)}\|_F.\]
%
By a similar computation to that of \cref{eq:X-X}, we obtain that
\begin{gather}\label{eqn:gapX}
\begin{aligned}
    X_{(L_1)}-X_{(L_2)} - &A_{L_2} (X_{(L_1)}-X_{(L_2)}) A_{L_2}^\intercal 
    \\
    =&(L_1R - A_{L_1} X_{(L_1)} H^\intercal)(L_1-L_2)^\intercal \\
    &+ (L_1-L_2)(R L_1^\intercal - H X_{(L_1)} A_{L_1}^\intercal) \\
    &- (L_1 -L_2)R(L_1-L_2)^\intercal \\
    &- (L_1 - L_2) H X_{(L_1)} H^\intercal (L_1 - L_2)^\intercal \\
    \preceq& \|L_1-L_2\|_F \, \ell_5 \, (Q + L_2 R L_2^\intercal)
\end{aligned}
\end{gather}
where
\begin{equation*}
    \ell_5 = \ell_5(L_1) \coloneqq  2\|-L_1R + A_{L_1} X_{(L_1)} H^\intercal\|_{\text{op}}/ \lambdamin(Q).
\end{equation*}
Therefore, by comparison, we claim that
\[\|X_{(L_1)} - X_{(L_2)}\|_F \preceq \|L_1-L_2\|_F \; \ell_5 \; \|X_{(L_2)}\|_F.\]
%
%
%
%
Finally, by compactness of $\mathcal{S}_\alpha$, we claim that the following supremums are attained and thus, are achieved with some \emph{finite} positive values:
\begin{align*}
    & \bar\ell_1(\alpha) \coloneqq \sup_{L_1,L_2\in\mathcal{S}_\alpha} \ell_1(L_1) \ell_4(L_1,L_2) \; \|Z_{(L_1)}\|_F,\\
    & \bar\ell_2(\alpha) \coloneqq \sup_{L_1,L_2\in\mathcal{S}_\alpha} \ell_2(L_1,L_2),\\
    & \bar\ell_3(\alpha) \coloneqq \sup_{L_1,L_2\in\mathcal{S}_\alpha} \ell_3(L_2) \ell_5(L_1) \|X_{(L_2)}\|_F.
\end{align*}
Then, the claim follows by combining the bound in \cref{eqn:grad-lip1} with \cref{eqn:gapY} and \cref{eqn:gapX},  and the following choice of 
\begin{equation*}
    \ell(\alpha) \coloneqq \sqrt{\bar{\ell}_1^2(\alpha) + \bar{\ell}_2^2(\alpha) + \bar{\ell}_3^2(\alpha)}.
\end{equation*}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%






%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\subsection{\acf{gf}}
For completeness, in the next two sections, we anlayze first-order methods in order to solve the minimization problem~\eqref{eq:opt-time-indepen}, although they are not part of the main paper.   
In this section, we consider a policy update according to the  the \ac{gf} dynamics:
\begin{flalign*} 
\text{[\ac{gf}]} \qquad\qquad\qquad\quad \dot{L}_s = - \nabla J(L_s). &&
\end{flalign*}
We summarize the convergence result in the following proposition which is a direct consequence of \Cref{prop:graddom}.

\begin{proposition}\label{prop:gradflow}
Consider any sublevel set $\mathcal{S}_\alpha$ for some $\alpha >0$. Then, for any initial policy $L_0 \in \mathcal{S}_\alpha$, the \ac{gf} updates converges to optimality at a linear rate of $\exp(-c_1(\alpha))$ (in both the function value and the policy iterate). In particular, we have
\[J(L_s) -J(L^*) \leq (\alpha - J(L^*)) \exp(-c_1(\alpha)s),\]
and 
\[\|L_s - L^*\|_F^2  \leq \frac{\alpha - J(L^*)}{c_3(\alpha)} \exp(-c_1(\alpha) s).\]
\end{proposition}
\begin{proof}
Consider a Lyapunov candidate function $V(L) \coloneqq J(L) -J(L^*)$. Under the \ac{gf} dynamics
\[\dot{V}(L_s) = - \langle \nabla J(L_s), \nabla J(L_s) \rangle \leq 0.\]
Therefore, $L_s \in \mathcal{S}_\alpha$ for all $s>0$. But then, by \Cref{prop:graddom}, we can also show that
\[\dot{V}(L_s) \leq - c_1(\alpha) V(L_s) - c_2(\alpha) \|L_s -L^*\|_F^2, \quad \text{ for } s>0.\]
By recalling that $c_1(\alpha)> 0$ is a positive constant independent of $L$, we conclude the following exponential stability of the \ac{gf}:
\[V(L_s) \leq V(L_0)\exp(-c_1(\alpha) s) ,\]
for any $L_0 \in \mathcal{S}_\alpha$ which, in turn, guarantees convergence of $J(L_s) \to J(L^*)$ at the linear rate of $\exp(-c_1(\alpha))$.
Finally, the linear convergence of the policy iterates follows directly from the second bound in \Cref{prop:graddom}: 
\[\textstyle \|L_s - L^*\|_F^2 \leq \frac{1}{c_3(\alpha)} V(L_s) \leq \frac{V(L_0)}{c_3(\alpha)} \exp(-c_1(\alpha) s).\]
The proof concludes by noting that $V(L_0) \leq \alpha - J(L^*)$ for any such initial value $L_0 \in \mathcal{S}_\alpha$.
\end{proof}

\subsection{\acf{gd}}
Here, we consider the \ac{gd} policy update:
\begin{flalign*} 
\text{[\ac{gd}]} \qquad\qquad\quad L_{k+1} = L_k - \eta_k \nabla J(L_k), &&
\end{flalign*}
for $k \in \mathbb Z$ and a positive  stepsize $\eta_k$.
%
Given the convergence result for the \ac{gf}, establishing convergence for \ac{gd} relies on carefully choosing the stepsize $\eta_k$, and bounding the rate of change of $\nabla J(L)$---at least on each sublevel set. This is achieved by the Lipschitz bound for $\nabla J(L)$ on any sublevel set. 

In what follows, we establish linear convergence of the \ac{gd} update. Our convergence result only depends on the value of $\alpha$ for the initial sublevel set $\mathcal{S}_\alpha$ that contains $L_0$. Note that our proof technique is distinct from those in \cite{bu2019lqr} and \cite{mohammadi2021convergence}; nonetheless, it involves a similar argument using the gradient dominance property of $J$.
\begin{theorem}\label{thm:graddescent }
Consider any sublevel set $\mathcal{S}_\alpha$ for some $\alpha >0$. Then, for any initial policy $L_0 \in \mathcal{S}_\alpha$, the \ac{gd} updates with any fixed stepsize $\eta_k = \eta \in (0, 1/\ell(\alpha)]$ converges to optimality at a linear rate of $1- \eta c_1(\alpha)/2$ (in both the function value and the policy iterate). In particular, we have
\[J(L_k) - J(L^*) \leq [\alpha - J(L^*)] (1- \eta c_1(\alpha)/2)^k,\]
and 
\[\|L_k - L^*\|_F^2 \leq \left[\frac{\alpha - J(L^*)}{c_3(\alpha)}\right] (1- \eta c_1(\alpha)/2)^k,\]
with $c_1(\alpha)$ and $c_3(\alpha)$ as defined in \Cref{prop:graddom}.
\end{theorem}
\begin{proof}
First, we argue that the \ac{gd} update with such a step size does not leave the initial sublevel set $\mathcal{S}_\alpha$ for any initial $L_0 \in \mathcal{S}_\alpha$.
In this direction, consider $L(\eta) = L_0 - \eta \nabla J(L_0)$ for $\eta \geq 0$ where $L_0 \neq L^*$. 
Then, by compactness of $\mathcal{S}_\alpha$ and continuity of the mapping $\eta \to J(L(\eta))$ on $\mathcal{S} \supset \mathcal{S}_\alpha$, the following supremum is attained with a positive value $\bar\eta_0$:
\[\bar\eta_0 \coloneqq \sup \{\eta: J(L(\zeta)) \leq \alpha, \forall \zeta \in [0,\eta]\}, \]
where positivity of $\bar\eta_0$ is a direct consequence of the strict decay of $J(L(\eta))$ for sufficiently small $\eta$ as $\nabla J(L_0) \neq 0$.
This implies that $L(\eta) \in \mathcal{S}_\alpha \subset \mathcal{S}$ for all $\eta \in [0,\bar\eta_0]$ and $J(L(\bar\eta_0)) = \alpha$. Next, by the Fundamental Theorem of Calculus and smoothness of $J(\cdot)$ (\Cref{lem:topology}), for any $\eta \in [0,\bar\eta_0]$ we have that,
\begin{align*}
    J(L(\eta)) -J(L_0) - \langle \nabla J(L_0), L(\eta) - L_0\rangle
    &= \int_0^1 \langle \nabla J(L(\eta s)) -\nabla J(L_0), L(\eta) - L_0\rangle d s\\
    &\leq \|L(\eta) - L_0\|_F \int_0^1 \|\nabla J(L(\eta s)) -\nabla J(L_0)\|_F  d s\\
    &\leq \ell(\alpha) \|L(\eta) - L_0\|_F \int_0^1 \|L(\eta s) -L_0\|_F  d s\\
    &= \frac{1}{2}\ell(\alpha)\eta \|L(\eta) - L_0\|_F \|\nabla J(L_0)\|_F,
\end{align*}
where $\|\cdot\|_F$ denotes the Frobenius norm, the first inequality is a consequence of Cauchy-Schwartz, and the second one is due to \Cref{lem:lipschitz} and the fact that $L(\eta s)$ remains in $\mathcal{S}_\alpha$ for all $s \in [0,1]$.\footnote{Note that a direct application of Descent Lemma \cite[Lemma 5.7]{beck2017frist} may not be justified as one has to argue about the uniform bound for the Hessian of $J$ over the non-convex set $\mathcal{S}_\alpha$ where $J$ is $\ell(\alpha)$-Lipschitz only on $\mathcal{S}_\alpha$. Also see the proof of \cite[Theorem 2]{mohammadi2021convergence}.} By the definition of $L(\eta)$, it now follows that,
\begin{align}\label{eq:decayineq}
    J(L(\eta)) -J(L_0) \leq \eta \|\nabla J(L_0)\|_F^2 \left( \frac{\ell(\alpha) \eta}{2} - 1 \right).
\end{align}
This implies $J(L(\eta)) \leq J(L_0)$ for all $\eta \leq 2/\ell(\alpha)$, and thus concluding that $\bar\eta_0 \geq 2/\ell(\alpha)$. This justifies that $L(\eta) \in \mathcal{S}_\alpha$ for all $\eta \in [0, 2/\ell(\alpha)]$.
Next, if we consider the \ac{gd} update with any fixed stepsize $\eta \in (0,1/\ell(\alpha)]$ and apply the bound in \cref{eq:decayineq} and the gradient dominance property in \Cref{prop:graddom}, we obtain
\[\textstyle J(L_1) - J(L_0) \leq  \eta c_1 (\frac{\ell(\alpha) \eta}{2}-1)[J(L_0) - J(L^*)],\]
which by subtracting $J(L^*)$ results in
\[\textstyle J(L_1) - J(L^*) \leq  \left(1- \frac{\eta c_1}{2} \right)[J(L_0) - J(L^*)],\]
as $\eta c_1 ({\ell(\alpha) \eta}/{2}-1) \leq -{\eta c_1}/{2}$ for all $\eta \in (0,1/\ell(\alpha)]$.
By induction, and the fact that both $c_1(\alpha)$ and the choice of $\eta$ only depends on the value of $\alpha$, we conclude the convergence in the function value at a linear rate of $1- (\eta c_1/2)$ and the constant coefficient of $\alpha - J(L^*) \geq J(L_0) - J(L^*)$.
To complete the proof, the linear convergence of the policy iterates  follows directly from the second bound in \Cref{prop:graddom}.
\end{proof}



%
%
%
%
%


%
%
%
%

%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
 

%
%
%
%



\section{Proofs for the analysis of the constrained SGD algorithm}
\subsection{Proof of \cref{lem:grad-approx}: Derivation of stochastic gradient formula}

\begin{proof}
For small enough $\Delta \in \mathbb R^{n\times m}$,
\begin{align*}
   \varepsilon(L+\Delta,\mathcal{Y}) - \varepsilon(L,\mathcal{Y}) &=  \|e_{T}(L + \Delta) \|^2 - \|e_{T}(L) \|^2 \\
   &= 2 \tr{(e_{T}(L + \Delta) - e_{T}(L) )e_{T}^\intercal(L)} + o(\|\Delta\|)).
    %
    %
\end{align*}
The difference 
\begin{gather*}
    e_{T}(L+\Delta) - e_{T}(L) = E_1(\Delta) + E_2(\Delta) + o(\|\Delta\|),
\end{gather*}
with the following terms that are linear in $\Delta$:
\begin{align*}
    E_1(\Delta) &\coloneqq \textstyle -\sum_{t=0}^{T-1} H  (A_L)^{t} \Delta y(T-t-1),\\
    E_2(\Delta) &\coloneqq \textstyle \sum_{t=1}^{T-1}\sum_{k=1}^{t} H  (A_L)^{t-k} \Delta H (A_L)^{k-1} L y(T-t-1) .
\end{align*}
Therefore, combining the two identities, the definition of gradient under the inner product $\langle A,B\rangle :=\tr{AB^\intercal}$, and ignoring the higher order terms in $\Delta$ yields,
\begin{gather*}
    \langle \nabla_L \varepsilon(L,y), \Delta \rangle =  2 \tr{(E_1(\Delta)+ E_2(\Delta) )e_{T}^\intercal(L)},
\end{gather*}
which by linearity and cyclic permutation property of trace reduces to:
\begin{gather*}
    \langle \nabla_L \varepsilon(L,y), \Delta \rangle = - 2 \tr{\Delta \left(\sum_{t=0}^{T-1} y(T-t-1) e_{T}^\intercal(L) H (A_L)^{t} \right)}\\
    + 2 \tr{\Delta \left(\sum_{t=1}^{T-1}\sum_{k=1}^{t} H (A_L)^{k-1} L y(T-t-1) e_{T}^\intercal(L) H (A_L)^{t-k} \right)}.
\end{gather*}
This holds for all admissible $\Delta$, concluding the formula for the gradient.
\end{proof}



%
%
%

\subsection{Proof of \Cref{lem:stability}: Robustness of the policy with respect to perturbation}

\begin{proof}
Recall the stability certificate $s_K$ as proposed in \cite[Lemma IV.1]{talebi2022policy} for a choice of constant mapping $\mathcal{Q}:K \to \Lambda\succ 0$ and dual problem parameters as discussed in \Cref{remark:duality}. Then, we arrive at 
\(s_K = {\lambdamin(\Lambda)}/{(2 \lambdamax(Z)\|H^\intercal \Delta^\intercal\|)},\)
for which $\rho \left(A^\intercal + H^\intercal (L^\intercal + \eta \Delta^\intercal) \right) < 1$ for any $\eta \in [0, s_K]$.
But, the spectrum of a square matrix and its transpose are identical, thus $L + \eta \Delta \in \mathcal{S}$ for any such $\eta$.
The claim then follows by noting that the operator norm of a matrix and its transpose are identical, and the resulting lowerbound as follows
\(s_K \geq {\lambdamin(\Lambda)}/{(2 \lambdamax(Z)\|H\| \|\Delta\|_F)}.\)
\end{proof}
\subsection{Proof of \Cref{lem:lowerbound-eta}: Uniform lower-bound on stepsize}
\begin{proof}
Without loss of generality, suppose $L_0 \neq L^*$ and $E \neq 0$, and let $L(\eta) \coloneqq L_0 - \eta E$. 
By compactness of $\mathcal{S}_{\beta}$ and continuity of the mapping $\eta \to J(L(\eta))$ on $\mathcal{S} \supset \mathcal{S}_{\beta}$, the following supremum is attained by $\eta_\beta$:
\begin{equation}\label{eqn:eta0}
    \eta_\beta \coloneqq \sup \{\eta: J(L(\zeta)) \leq \beta, \forall \zeta \in [0,\eta]\}.
\end{equation}
Note that $\eta_\beta$ is strictly positive for $\beta > \alpha$ because $L_0 \in \mathcal{S}_\alpha \subset \mathcal{S}_{\beta}$ and $J(\cdot)$ is coercive (\Cref{lem:coercive}) and its domain is open (\Cref{lem:stability}). This implies that $L(\eta) \in \mathcal{S}_{\beta} \subset \mathcal{S}$ for all $\eta \in [0,\eta_\beta]$ and $J(L(\eta_\beta)) = \beta$.

Next, we want to show that $\eta_\beta$ is uniformly lower bounded with high probability. By the Fundamental Theorem of Calculus, for any $\eta \in [0,\eta_\beta]$ we have 
\begin{align}
    J(L(\eta)) -J(L_0) - \langle \nabla J(L_0), L(\eta) - L_0\rangle &= \int_0^1 \langle \nabla J(L(\eta s)) -\nabla J(L_0), L(\eta) - L_0\rangle d s \nonumber\\
    &\leq \|L(\eta) - L_0\|_F \int_0^1 \|\nabla J(L(\eta s)) -\nabla J(L_0)\|_F  d s \nonumber\\
    &\leq \ell(\beta) \|L(\eta) - L_0\|_F \int_0^1 \|L(\eta s) -L_0\|_F  d s \nonumber\\
    &= \frac{1}{2}\ell(\beta)\eta^2 \|E\|_F^2, \label{eqn:J-upperbound-E}
\end{align}
where the first inequality is a consequence of Cauchy-Schwartz, and the second one is due to \Cref{lem:lipschitz} and the fact that $L(\eta s)$ remains in $\mathcal{S}_{\beta}$ for all $s \in [0,1]$ by definition of $\eta_\beta$. 
Note that the assumption implies $\|E\|_F \leq (\gamma+1) \|\nabla J(L_0)\|_F$. Thus, \cref{eqn:J-upperbound-E} implies that
\begin{gather}\label{eqn:J-bound-m}
\begin{aligned}
    J(L(\eta)) \leq& J(L_0) - \eta \langle \nabla J(L_0), E \rangle + \frac{1}{2}\ell(\beta)\eta^2 \|E\|_F^2 \\
    \leq& J(L_0) - \eta \langle \nabla J(L_0), E-\nabla J(L_0)\rangle - \eta \|\nabla J(L_0)\|_F^2+ \frac{1}{2}\ell(\beta)\eta^2 \|E\|_F^2 \\
    \leq& J(L_0) + \|\nabla J(L_0)\|_F^2 \left[ \frac{1}{2}(\gamma+1)^2 \ell(\beta) \eta^2 + (\gamma-1)\eta\right].
\end{aligned}
\end{gather}
Therefore, for $\eta$ to be a feasible point in the supremum in \cref{eqn:eta0}, it suffices to satisfy:
\begin{gather*}
    \frac{1}{2}\|\nabla J(L_0)\|_F^2\left[(\gamma+1)^2 \ell(\beta) \eta^2 +2 (\gamma-1)\eta\right]\leq \beta- \alpha,
\end{gather*}
or equivalently,
\begin{gather*}
    \left[(\gamma+1) \ell(\beta) \eta + \frac{\gamma-1}{\gamma+1}\right]^2 \leq \left(\frac{\gamma-1}{\gamma+1}\right)^2 + \frac{2\ell(\beta) [\beta- \alpha]}{\|\nabla J(L_0)\|_F^2}.
\end{gather*}
But then it suffices to have
\begin{gather*}
    (\gamma+1) \ell(\beta) \eta + \frac{\gamma-1}{\gamma+1} \leq \frac{\sqrt{2\ell(\beta) [\beta- \alpha]}}{\|\nabla J(L_0)\|_F}.
\end{gather*}
Finally, note that by \Cref{prop:graddom} and \Cref{lem:lipschitz} we have
\begin{gather}\label{eqn:grad-uniform-bound}
\|\nabla J(L_0)\|_F \leq \frac{\ell(\alpha)}{c_3(\alpha)}[J(L_0) - J(L^*)] \leq \frac{\ell(\alpha)}{c_3(\alpha)}[\alpha - \alpha^*].
\end{gather}
Using this uniform bound of gradient on sublevel set $\mathcal{S}_\alpha$ and noting that $\gamma \in [0,1]$, we can obtain the sufficient condition for $\eta$ to be feasible. This completes the proof.
%
%
%
%
\end{proof}

\subsection{Proof of \Cref{prop:sgd-decay}: Linear decay in cost value}
\begin{proof}
Suppose $L_0 \neq L^*$ and let $L(\eta) \coloneqq L_0 - \eta E$. 
Note that $E$ may not be necessarily in the direction of decay in $J(L)$, however, we can argue the following: 

Choose $\beta = \alpha$ in \Cref{lem:lowerbound-eta} and note that $\eta_\beta$ as defined in \cref{eqn:eta0} will be lower bounded as \(\eta_\beta \geq \bar\eta_0\), and thus $\bar\eta_0$ is feasible.
%
%
%
%
%
%
%
%
%
%
Recall that $L(\eta) \in \mathcal{S}_{\beta} \subset \mathcal{S}$ for all $\eta \in [0,\eta_\beta]$. 
%
Also, for any $\eta \in [0,\bar\eta_0]$ and $\gamma \in [0,1)$, from \cref{eqn:J-bound-m} we obtain that:%
\begin{gather*}
\begin{aligned}
    J(L(\eta)) -J(L_0)  
    &\leq \|\nabla J(L_0)\|_F^2 \left[ \frac{1}{2}(\gamma+1)^2 \ell(\alpha) \eta^2 + (\gamma-1)\eta\right]\\
    &\leq c_1(\alpha)[J(L_0)- J(L^*)] \left[ \frac{1}{2}(\gamma+1)^2 \ell(\alpha) \eta^2 + (\gamma-1)\eta\right]\\
\end{aligned}
\end{gather*}
where, as $\gamma<1$, the last inequality follows by \Cref{eq:gradient-dominance} for any $\eta \leq \min\{2\bar\eta_0, \eta_\beta\}$.
By the choice of $\bar\eta_0$, then we obtain that
\begin{gather*}
\begin{aligned}
    &J(L(\bar\eta_0)) -J(L_0)  \leq -c_1(\alpha)\left[ \frac{(\gamma-1)^2}{2(\gamma+1)^2 \ell(\alpha)}\right] [J(L_0)- J(L^*)]
\end{aligned}
\end{gather*}
This implies that
\begin{gather*}
\begin{aligned}
    &J(L(\bar\eta_0)) -J(L^*)  \leq \left(1-c_1(\alpha)\left[ \frac{(\gamma-1)^2}{2(\gamma+1)^2 \ell(\alpha)}\right]\right) [J(L_0)- J(L^*)].
\end{aligned}%
\end{gather*}%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\end{proof}

\subsection{Proof of \Cref{thm:sgd}: Convergence of SGD algorithm}
Before proving this result, we first discuss that the claim of \Cref{thm:sgd} is sufficient for establishing the complexity result of \Cref{thm:combined}; i.e., it suffices to guarantee convergence from every initial (stabilizing) policy to a neighborhood of optimality where norm of the gradient is controlled. 

As shown in \Cref{prop:graddom} (and discussed in Remark~\ref{rmk:pl-property}), the cost maintains the PL property on each sublevel set. In particular (11a) implies that on each $\mathcal S_\alpha$, $\|\nabla J (L)\|$ characterizes the optimality gap $J(L) - J(L^*)$ by:
\[c_1(\alpha) [J(L) - J(L^*)] \leq  \|\nabla J(L)\|^2\]
for some constant $c_1(\alpha)$. This implies that if we have arrived at a candidate policy $L_k$ for which the gradient is small, then the optimality gap should be small (involving the constant $c_1(\alpha)$ that is independent of $L_k$). 
%
This is the reason that in \Cref{thm:sgd}, it suffices to argue about the generated sequence $L_k$ to have a linear decay unless entering a neighborhood of $L^*$ containing policies with small enough gradients (denoted by $\mathcal{C}_\tau$). In particular, if for some $j<k$, we arrive at some policy $L_j \in \mathcal{C}_\tau$, then by (11a) we can conclude that:
\[J(L_j)-J(L^*) \leq \frac{1}{c_1(\alpha)} \|\nabla J(L_j)\|_F^2  \leq \frac{s_0^2}{c_1(\alpha) \tau^2}, \]
which is directly controlled by the bias term $s_0$. This is the bound used also in the proof of \Cref{thm:combined}.

Finally, recall that every (stabilizing) initial policy $L_0$ amounts to a \textit{finite} value of $J(L_0)$ and thus lies in some sublevel set $S_\alpha$. So, starting from such $L_0$, \Cref{thm:sgd} guarantees linear decay of the optimality gap till the trajectory enters that small neighborhood.
Finally, note that the radius of this neighborhood $\mathcal{C}_\tau$ is characterized by the bias term $s_0$ which itself is exponentially decaying to zero in the trajectory length $T$.

Next, we provide the proof of this result that is essentially an induction argument using \Cref{prop:sgd-decay}.



\begin{proof}[Proof of \Cref{thm:sgd}]
The first step of the proof is to show that the assumption of \Cref{prop:sgd-decay} is satisfied for $E=\nabla \widehat J(L)$ for all $L \in \mathcal{S}_\alpha \setminus \mathcal{C}_{\gamma/2}$. This is true because
\begin{align*}
    \|\nabla \widehat J(L) - \nabla J(L)\| &\leq s\|\nabla J(L)\| + s_0 \\
    &\leq s\|\nabla J(L)\| + \frac{\gamma}{2} \|\nabla J(L)\|\\
    &\leq \gamma \|\nabla J(L)\|
\end{align*}
where the first inequality follows from Assumption~\ref{asmp:noisy-grad}, the second inequality follows from $L \not \in C_{\gamma/2}$ (i.e. $s_0\|\nabla J(L)\|\geq \gamma/2$), and the last step follows from the assumption $s\leq \gamma/2$. \\
The rest of the proof relies on repeated application of \Cref{prop:sgd-decay}. In particular, starting from $L_0 \in \mathcal{S}_\alpha \setminus \mathcal{C}_{\gamma/2}$, the application of \Cref{prop:sgd-decay} implies that $L_1\coloneqq L_0 - \bar\eta \nabla\widehat J(L_0)$ remains in the same sublevel set, i.e., $L_1 \in \mathcal{S}_{\alpha}$, and we obtain the following linear decay of the cost value:
    \[J(L_1) -J(L^*) \leq \left[1-c_1(\alpha)\bar\eta{(1 - \gamma)}/2\right] [J(L_0)- J(L^*)].\]
    Now, if $L_1 \in \mathcal{C}_{\gamma/2}$ then we stop; otherwise $L_1 \in \mathcal{S}_\alpha \setminus \mathcal{C}_{\gamma/2}$ and we can repeat the above process to arrive at  $L_2 \coloneqq L_1 - \bar\eta \nabla\widehat J(L_1) \in \mathcal{S}_\alpha$, with a guaranteed linear decay
    \[J(L_2) -J(L^*) \leq \left[1-c_1(\alpha)\bar\eta{(1 - \gamma)}/2\right] [J(L_1)- J(L^*)].\]
    Combining the last two linear decays yields
    \[J(L_2) -J(L^*) \leq \left[1-c_1(\alpha)\bar\eta{(1 - \gamma)}/2\right]^2 [J(L_0)- J(L^*)].\]
    Repeating the  process generates a sequence of policies $L_0, L_1, L_2 ...$ with a combined linear decay of 
    \[J(L_k) -J(L^*) \leq \left[1-c_1(\alpha)\bar\eta{(1 - \gamma)}/2\right]^k [J(L_0)- J(L^*)],\]
    unless at some iteration $j$, we arrive at a policy $L_j$ such that $L_j \in \mathcal{C}_{\gamma/2}$. This completes the proof.
\end{proof}








\section{Proofs of the result for observation model and sample complexity}
\subsection{Preliminary lemmas and their proofs}
First, we provide the proof for the  complete version of \Cref{lem:bound-rho-L}:\footnote{See \cite[Definition 3.1]{cohen2018online} for an alternative notation; however, we prefer the explicit and simple form of expressing spectral radius in the bounds established in our work, which also facilitates the comparison to literature on first order methods for stabilizing policies.}

\begin{lemma6}[Uniform Bounds for Stable Systems]
Suppose $L \in \mathcal{S}$, then there exit a constants $C_L>0$ such that 
\[\|A_L^k\| \leq C_L \; \left(\sqrt{\rho(A_L)}\right)^{k+1}, \quad \forall k\geq 0,\]
whenever $\rho(A_L) >0$, and otherwise $\sqrt{\rho(A_L)}$ is replaced with any arbitrarily small $r \in (0,1)$.
Additionally,
\begin{align*}
    \sum_{i=0}^\infty \|A_L^i\| &\leq  \frac{C_L}{1-\sqrt{\rho(A_L)}}  \\
    \sum_{i=0}^\infty \|M_i[E]\| &\leq \frac{1 + 2 C_L^2 \rho(A_L)^{3/2}}{[1-\sqrt{\rho(A_L)}]^2} \; \|EH\|
\end{align*}
Furthermore, consider $\mathcal{S}_\alpha$ for some $\alpha>0$, then there exist constants $D_\alpha >0$, $C_\alpha >0$ and $\rho_\alpha \in(0,1)$ such that
\(\|L\| \leq D_\alpha, \;\; C_L \leq C_\alpha\;\;\textit{and}\;\; \rho(A_L) \leq \rho_\alpha,  \;\forall L \in \mathcal{S}_\alpha.\)
\end{lemma6}



\begin{proof}[Proof of \Cref{lem:bound-rho-L}]
    Recall the Cauchy Integral formula for matrix functions \cite[Theorem 1.12]{higham2008functions}: for any matrix $M \in \mathbb{C}^{n \times n}$,
    \[f(M) = \frac{1}{2\pi i} \oint_\Gamma f(z) (zI - M)^{-1} d z,\]
    whenever $f$ is real analytic on and inside a closed contour $\Gamma$ that encloses spectrum of $M$. Note that $L \in \mathcal{S}$ implying that $\rho(A_L) <1$. Now, fix some $r \in (\rho(A_L), 1)$ and define $\Gamma(\theta) = re^{i\theta}$ with $\theta$ ranging on $[0, 2\pi]$. Therefore, Cauchy Integral formula applies to $f(z) = z^k$ for any positive integer $k$ and the contour $\Gamma$ defined above. So, for matrix $A_L$, we obtain
    \[A_L^k = \frac{1}{2\pi i} \oint_\Gamma z^k (zI - A_L)^{-1} d z = \frac{1}{2\pi i} \int_0^{2\pi} r^k 
    e^{ik\theta}(re^{i\theta}I - A)^{-1} \; r \,d e^{i\theta},\]
    implying that
    \[\|A_L^k\| \leq \frac{r^{k+1}}{2\pi} \int_0^{2\pi} \|(re^{i\theta}I - A_L)^{-1}\| d \theta \leq r^{k+1} \; \max_{\theta \in [0,2\pi]} \|(re^{i\theta}I - A_L)^{-1}\|. \]
    Finally, the first claim follows by choosing $r = \sqrt{\rho(A_L)}$ (whenever $\rho(A_L) > 0$, otherwise $r\in(0,1)$ can be chosen arbitrarily small) and defining 
    \[C_L \coloneqq \max_{\theta \in [0,2\pi]} \|(\sqrt{\rho(A_L)}e^{i\theta}I - A_L)^{-1}\|\]
    which is attained and bounded.

    Next, by applying the first claim, we have
    \[\sum_{i=0}^T \|A_L^i\| \leq C_L \frac{1-\sqrt{\rho(A_L)}^T}{1-\sqrt{\rho(A_L)}},\]
    implying the second bound. 
    For the third claim, note that for $i =1,2,\cdots$ we obtain
    \begin{multline} \label{eq:bound_M}
        \|M_{i+1}[E]\| \leq \sum_{k=0}^i \|A_L^{i-k}\|  \|A_L^k\| \|E H\|
        \leq \|E H\| C_L^2 \sum_{k=0}^i \left[\sqrt{\rho(A_L)}\right]^{(i-k + 1) + (k+1)} \\
        \leq \|E H\| \, C_L^2 \sqrt{\rho(A_L)} \left[(i+1) \cdot \rho(A_L)^{(i+1)/2} \right].
    \end{multline}
    But, then by recalling that $M_0[E] = 0$ and $\|M_1[E]\| = \|E H\|$ we have
    \begin{equation*}
        \sum_{i=0}^\infty \|M_{i}[E]\| \leq \|E H\| + \|E H\|\left[\frac{2 C_L^2 \rho(A_L)^{3/2}}{[1-\sqrt{\rho(A_L)}]^2}\right]
    \end{equation*}
    where we used the following convergent sum for any $\rho \in (0,1)$:
    \[\sum_{i=1}^\infty (i+1) \cdot \rho^{i+1} = \frac{(2-\rho)\rho^2}{(1-\rho)^2} \leq \frac{2\rho^2}{(1-\rho)^2}.\]
    This implies the third bound. 
    
    The final claim follows directly from compactness of sublevel set $\mathcal{S}_\alpha$ (\Cref{lem:coercive}) and continuity of the mappings $(L,\theta) \mapsto (\rho(A_L),\theta) \mapsto \|(\sqrt{\rho(A_L)}e^{i\theta}I - A_L)^{-1}\|$ on $\mathcal{S}_\alpha \times [0,2\pi]$ whenever $\rho(A_L)>0$ (and otherwise considering the mapping $(L,\theta) \mapsto \|(r e^{i\theta}I - A_L)^{-1}\|$ for arbitrarily small and fixed $r \in(0,1)$).
\end{proof}



As mentioned in \Cref{sec:observation-model}, a key idea behind these error bounds that scale well with respect to the length T is the following consequence of von Neumann Trace Inequality \cite[Theorem 8.7.6]{horn2012matrix}:
\begin{equation*}
   \textstyle |\tr{M N}|\leq \sum_{i=1}^T \sigma_i(M) \sigma_i(N) \leq \|M\| \|N\|_*,
\end{equation*}
with $\|\cdot\|_*$ denoting the nuclear norm. Additionally, as a direct consequence of Courant-Fischer Theorem, one can also show that nuclear norm is sub-multiplicative. More precisely,
\[\|AB\|_* \leq \|A\|\;\|B\|_*\leq \|A\|_*\|B\|_*.\] 
Next, we require the following lemma to bound these errors.
    \begin{lemma}\label{lem:matrix-bounds}
    %
    For any $L \in \mathcal{S}_\alpha$, we have
    \begin{align*}
        \|\calA_L^\intercal H^\intercal H \calA_L\|_* &\leq \frac{ C_L^2 \|H^\intercal H\|_*}{1-\rho(A_L)},\\
        \|\calN_L[E]\|_* &\leq \frac{\left[2C_L+4C_L^3\rho(A_L)^{3/2}\right]\|H\|\, \|H^\intercal H\|_*}{[1-\rho(A_L)]^{2}} \|E\|.
    \end{align*}
    \end{lemma}
    \begin{proof}[Proof of \Cref{lem:matrix-bounds}]
    For the first claim, note that $\calA_L^\intercal H^\intercal H \calA_L$ is positive semi-definite, so
    \begin{align*}
        \|\calA_L^\intercal H^\intercal H \calA_L\|_* 
        &= \tr{ \calA_L^\intercal H^\intercal H \calA_L}\\
        &\leq \tr{H^\intercal H}\, \|\calA_L \calA_L^\intercal\|\, \\
        &\leq \|H^\intercal H\|_* \, \left\|\sum_{i=0}^T A_L^i (A_L^\intercal)^i \right\| \\
        &\leq \|H^\intercal H\|_* \sum_{i=0}^T \|A_L^i\|^{2}  \\
        &\leq \|H^\intercal H\|_* \frac{C_L^2}{1-\rho(A_L)}
    \end{align*}
    where the last inequality follows by \Cref{lem:bound-rho-L}.
    Next, we have
    \begin{align*}
        \|\calM_L[E]\| &= \left\|\calM_L[E] \calM_L[E]^\intercal \right\|^{1/2} \\
        &\leq \left[\sum_{i=0}^T \|M_i[E]\|^2 \right]^{1/2}\\
        &\leq \|E H\| + \|E H\| \, C_L^2 \sqrt{\rho(A_L)} \left[\sum_{i=0}^T(i+1)^2 \cdot \rho(A_L)^{(i+1)} \right]^{1/2}\\
        &\leq \|E H\| + \|E H\| \, C_L^2 \sqrt{\rho(A_L)} \frac{2\rho(A_L)}{[1-\rho(A_L)]^{3/2}}\\
        &\leq \|E H\|\left[\frac{1+2C_L^2\rho(A_L)^{3/2}}{[1-\rho(A_L)]^{3/2}}\right]
    \end{align*}
    where the second inequality follows by \cref{eq:bound_M} and the third one by the following convergent sum for any $\rho \in (0,1)$:
    \[\sum_{i=1}^\infty (i+1)^2 \cdot \rho^{i+1} = \frac{\rho^2(\rho^2 -3\rho +4)}{(1-\rho)^3} \leq \frac{4\rho^2}{(1-\rho)^3}.\]
    Also, by the properties of nuclear norm 
    \begin{align*}
        \|H^\intercal H \calA_L\|_* &= \tr{\sqrt{H^\intercal H \calA_L  \calA_L^\intercal H^\intercal H}}\\
        &\leq \|\calA_L \calA_L^\intercal\|^{1/2} \|H^\intercal H\|_*\\
        &\leq \left[\sum_{i=0}^\infty \|A_L^i\|^2 \right]^{1/2} \|H^\intercal H\|_*\\
        &\leq \left[\frac{C_L^2}{1-\rho(A_L)} \right]^{1/2} \|H^\intercal H\|_*,
    \end{align*}
    where the last inequality follows by \Cref{lem:bound-rho-L}. Finally, notice that 
    \begin{align*}
        \|\calN_L[E]\|_* 
        &\leq 2 \|\calA_L^\intercal H^\intercal H \calM_L[E]\|_* \\
        &\leq 2 \|\calM_L[E]\| \|H^\intercal H \calA_L\|_* \\
        %
    \end{align*}
    and thus combining the last three bounds implies the second claim. This completes the proof.
    \end{proof}


The next tool we will be using is the following famous bound on random matrices which is a variant of Bernstein inequality:
%
%
%
%
%

\begin{lemma}[Matrix Bernstein Inequality {\cite[Corollary 6.2.1]{tropp2015introduction}}]\label{lem:matrix-Bernstein}
Let $Z$ be a $d_1\times d_2$ random matrices such that $\E{}{Z} = \bar{Z}$ and $\|Z\| \leq K$ almost surely. Consider $M$ independent copy of $Z$ as $Z_1,\cdots,Z_M$, then for every $t \geq 0$, we have
    \[\prob{\left\|\frac{1}{M}\textstyle\sum_i Z_i - \bar{Z}\right\|\geq t} \leq (d_1+d_2) \exp\left\{\frac{-Mt^2/2}{\sigma^2 + 2Kt/3}\right\}\]
where $\sigma^2 = \max\{\|\E{}{Z Z^\intercal}\|,\;\|\E{}{Z^\intercal Z}\|\}$ is the per-sample second moment. 
This bound can be expressed as the mixture of sub-gaussian and sub-exponential tail as $(d_1 + d_2)\exp\left\{-c\min\{\frac{t^2}{\sigma^2}, \frac{t}{2 K}\}\right\}$ for some $c$.
\end{lemma}

We are now well-equipped to provide the main proofs.

\subsection{Proof of \Cref{prop:error-vector-form}: The Observation model}
\begin{proof}
Recall that 
\[\varepsilon(L,\mathcal{Y}_T) = \|H x(T) - H \hat{x}(T)\|^2 = \sum_{i=1}^m |H_i^\intercal x(T) - H_i^\intercal \hat{x}(T)|^2\]
where $H_i^\intercal$ is the $i$-th row of $H$. Also, by duality, if $z(t) = (A_L^\intercal)^{T-t}H_i$ is the adjoint dynamics' closed-loop trajectory with control signal $u(t) = L^\intercal z(t)$ then 
\[H_i^\intercal x(T) - H_i^\intercal \hat{x}(T) = \vz_i^\intercal \vxi - \vu^\intercal \vomega\]
where 
\begin{align*}
 \vz_i^\intercal =& \begin{pmatrix}z(T)^\intercal & z(T-1)^\intercal & \dots & z(1)^\intercal & z(0)^\intercal \end{pmatrix},  \\
\vu^\intercal =& \begin{pmatrix}u(T)^\intercal & u(T-1)^\intercal & \dots & u(1)^\intercal & 0_m^\intercal\end{pmatrix}.
\end{align*}
But $\vu = (I \otimes L^\intercal) \vz_i$ and then $\vz_i = \calA_L^\intercal H_i$. Therefore,
\begin{align*}
    \varepsilon(L,\mathcal{Y}_T) =& \sum_{i=1}^m |H_i^\intercal x(T) - H_i^\intercal \hat{x}(T)|^2 \\
    = & \sum_{i=1}^m \tr{\vxi \vxi^\intercal \vz_i \vz_i^\intercal} 
    -\tr{\left(\vxi \vomega^\intercal (I \otimes L^\intercal)+ (I \otimes L) \vomega \vxi^\intercal \right) \vz_i \vz_i^\intercal}\\
    &+ \tr{\vomega \vomega^\intercal (I \otimes L^\intercal) \vz_i \vz_i^\intercal (I \otimes L)}
\end{align*}
Then, by using the fact that $\sum_{i=1}^m H_i H_i^\intercal = H^\intercal H$, we obtain that
\[ \varepsilon(L,\mathcal{Y}_T) = \tr{\calX_L \calA_L^\intercal H^\intercal H \calA_L}.\]
Thus, we can rewrite the estimation error as
\begin{align*}
    \varepsilon(L,\mathcal{Y}_T) =& \tensor{\vxi}{\vxi}{\calA_L} + \tensor{(I \otimes L) \vomega}{(I \otimes L) \vomega}{\calA_L} 
    - 2  \tensor{\vxi}{(I \otimes L) \vomega}{\calA_L}= \|\veta\|_{\calA_L}^2
\end{align*}

Next, we can compute that for small enough $E$
\[\calA_{L+E} - \calA_L = \calM_L [E] + o(\|E\|).\]
This implies that
\begin{equation*}
    \diff (\calA_L^\intercal H^\intercal H \calA_L)\big|_L [E] = \\
    \calM_L [E]^\intercal H^\intercal H \calA_L+ \calA_L^\intercal H^\intercal H \calM_L [E]
\end{equation*}
On the other hand, 
\begin{multline*}
    \calX_{L+E} - \calX_L = (I \otimes E) \vomega \vomega^\intercal (I \otimes L^\intercal) \\+ (I \otimes L) \vomega \vomega^\intercal (I \otimes E^\intercal) \\
    - \vxi \vomega^\intercal (I \otimes E^\intercal)+ (I \otimes E) \vomega \vxi^\intercal + o(\|E\|).
\end{multline*}
Therefore, the second claim follows by the chain rule.
\end{proof}




\subsection{Proof of \Cref{prop:concen-n}: Concentration bounds}
We provide the proof for a detailed version of \Cref{prop:concen-n}:
\begin{proposition4}[Concentration independent of length $T$]
Consider length $T$ trajectories $\{\mathcal{Y}_{[t_0,t_0 + T]}^i\}_{i=1}^M$ and let $\widehat J_{T}(L) \coloneqq \frac{1}{M}\sum_{i=1}^M \varepsilon(L,\mathcal{Y}^i_{T})$. Then, under Assumption \ref{asmp:noise-bound}, for any $s>0$
\begin{equation*}
        \prob{|\widehat J_T(L) - J_T(L)| \leq s}  \geq 1- 2n \exp\left[\frac{- M s^2/2}{\mu_L^2 + 2\mu_L s/3}\right],
\end{equation*}
\begin{gather*}
    \prob{\|\nabla\widehat J_T(L) -\nabla J_T(L)\| \leq s }  \geq 1- 2n \exp\left[\frac{- M s^2/2}{\nu_L^2  + 2\nu_L s/3}\right]
\end{gather*} 
where $\kappa_L = \kappa_\xi + \|L\| \kappa_\omega$ and
\begin{align*}
    \mu_L &\coloneqq \frac{\kappa_L^2 C_L^2}{[1-\sqrt{\rho(A_L)}]^2} \| H^\intercal H \|_*\\
    %
    \nu_L &\coloneqq \frac{2\kappa_L \kappa_\omega C_L^2 + \left[C_L + 2C_L^3 \rho(A_L)^{3/2}\right] \|H\| \kappa_L^2}{[1-\sqrt{\rho(A_L)}]^3}\, \| H^\intercal H \|_*.
\end{align*}
\end{proposition4}


\begin{proof}[Proof of \Cref{prop:concen-n}]
Note that $\E{}{\vxi \vxi^\intercal} = \begin{pmatrix} I \otimes Q & 0 \\ 0 & P_0 \end{pmatrix} \eqqcolon \calQ$, $\E{}{\vomega \vomega^\intercal} = \begin{pmatrix} I \otimes R & 0 \\ 0 & 0_m \end{pmatrix} \eqqcolon \calR$, and $\E{}{\vxi \vomega^\intercal} = 0$.
    Assume $m_0 = 0$ and recall that $\langle \nabla \varepsilon(L,\mathcal{Y}_T), E \rangle = \diff \varepsilon(\cdot,\mathcal{Y}_T)\big|_L(E)$ thus, using \Cref{prop:error-vector-form}, we can rewrite the $J_T(L)$ and its gradient as
    \begin{align*}
        J_T(L) = \E{}{\varepsilon(L,\mathcal{Y}_T)} 
        = \E{}{\|\veta\|_{\calA_L}^2}
        = \tr{\E{}{\calX_L} \calA_L^\intercal H^\intercal H \calA_L}
        %
    \end{align*}
    where $\E{}{\calX_L} = \calQ + (I \otimes L) \calR (I \otimes L^\intercal)$.
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    Therefore, by definition of $\widehat J_T(L)$ we obtain 
    \begin{align*}
        \widehat J_T(L)
        = \tr{\calZ_L \calA_L^\intercal H^\intercal H \calA_L}
    \end{align*}
    with $\calZ_L = \frac{1}{M}\sum_{i=1}^M \calX_L(\mathcal{Y}^i)$
    which can be expanded as
    \[\calZ_L = \calZ_1 + (I \otimes L) \calZ_2 (I \otimes L^\intercal) - \calZ_3 (I \otimes L^\intercal) - (I \otimes L) \calZ_3^\intercal,\]
    where
    \begin{align*}
        \calZ_1 = \frac{1}{M} \sum_{i=1}^M \vxi_i \vxi_i^\intercal,\,\quad
        \calZ_2 = \frac{1}{M} \sum_{i=1}^M \vomega_i \vomega_i^\intercal,\,\quad
        \calZ_3 = \frac{1}{M} \sum_{i=1}^M \xi_i \vomega_i^\intercal,
    \end{align*}
    and $\E{}{\calZ_L} = \calQ + (I \otimes L) \calR (I \otimes L^\intercal)$.
    Therefore, 
    \begin{align*}
        \widehat J_T(L)& - J_T(L) 
        = \tr{\left(\calZ_L -\E{}{\calZ_L}\right) \calA_L^\intercal H^\intercal H \calA_L}.
    \end{align*}
    Thus, by cyclic permutation property of trace and \cref{eq:nuclear-norm} we obtain
    \begin{align}\label{eq:Jbound-nuclear}
        |\widehat J_T(L) - J_T(L)|
        \leq& \|\calA_L \left(\calZ_L -\E{}{\calZ_L}\right) \calA_L^\intercal\|\, \| H^\intercal H \|_*.
    \end{align}
    Next, we consider the symmetric random matrix $\calA_L \left(\calZ_L -\E{}{\calZ_L}\right) \calA_L^\intercal$. Note that $\|\xi(t) - L \omega(t) \| \leq \kappa_L$ almost surely and thus
    \begin{equation*}
        \|\calA_L \calX_L \calA_L^\intercal\| = \left\|\calA_L \veta\right\|^2 \leq \kappa_L^2 \left[\sum_{i=0}^\infty \|A_L^i\|\right]^2 \leq \mu_L/ \| H^\intercal H \|_*.
    \end{equation*}
    It then follows that
    \begin{equation*}
        \left\|\E{}{(\calA_L \calX_L \calA_L^\intercal)^2}\right\| \leq \E{}{\|\calA_L \calX_L \calA_L^\intercal\|^2} \leq \mu_L^2/ \| H^\intercal H \|_*^2.
    \end{equation*}
    Therefore, by \Cref{lem:matrix-Bernstein} we obtain that
    \begin{equation*}
        \prob{\|\calA_L \left(\calZ_L -\E{}{\calZ_L}\right) \calA_L^\intercal\|\geq t} \\ \leq 2n \exp\left[\frac{- M \| H^\intercal H \|_*^2 t^2/2}{\mu_L^2 + 2\mu_L \| H^\intercal H \|_* t/3}\right].
    \end{equation*}
    Substituting $t$ with $t/\|H^\intercal H \|_*$ together with \cref{eq:Jbound-nuclear} implies the first claim. 
    
    Similarly, we can compute that
    \begin{align*}
        \langle \nabla J_T(L), E \rangle =& \langle \E{}{\nabla \varepsilon(L,\mathcal{Y}_T)}, E \rangle\\
        =& 2\tr{(I \otimes L) \calR (I \otimes E^\intercal) \calA_L^\intercal H^\intercal H \calA_L}
    +\tr{(\calQ + (I \otimes L) \calR (I \otimes L^\intercal)) \calN_L [E]},
    \end{align*}
    and thus
    \begin{align*}
        \langle \nabla \widehat J_T(L)& - \nabla J_T(L), E \rangle 
        = -2 \tr{\calZ_3 (I \otimes E^\intercal) \calA_L^\intercal H^\intercal H \calA_L} \nonumber\\ 
        &+2 \tr{(I \otimes L) (\calZ_2 - \calR) (I \otimes E^\intercal) \calA_L^\intercal H^\intercal H \calA_L} \nonumber\\
    &+\tr{(\calZ_L - \E{}{\calZ_L}) \,\calN_L [E]}.
    \end{align*}
    Thus, by cyclic permutation property of trace and \cref{eq:nuclear-norm} we obtain that
    \begin{equation}\label{eq:nabla-Jhat-E}
        |\langle \nabla \widehat J_T(L) - \nabla J_T(L), E \rangle | \leq
        \|\frac{1}{M}\sum_{i = 1}^M S_L(E,\mathcal{Y}_T^i) - \E{}{S_L(E,\mathcal{Y}_T^i)}\| \|H^\intercal H\|_*
    \end{equation}
    where $S_L(E,\mathcal{Y})$ is the symmetric part of the following random matrix
    \begin{equation*}
        -2 \calA_L \vxi \vomega^\intercal (I \otimes E^\intercal) \calA_L^\intercal \\
        + 2 \calA_L (I \otimes L) \vomega \vomega^\intercal (I \otimes E^\intercal) \calA_L^\intercal
        + 2\calA_L \calX_L \calM_L[E]^\intercal.
    \end{equation*}
    Next, we provide the following almost sure bounds for each term: first,
    \begin{align*}
        \|\calA_L \calZ_3 (I \otimes E^\intercal) &\calA_L^\intercal\| 
        \leq \|\calA_L \vxi\|\, \|\calA_L (I \otimes E) \vomega\| \\
        &\leq \kappa_\xi \left[\sum_{i=0}^T \|A_L^i\|\right] \; \kappa_\omega \|E\| \,\left[\sum_{i=0}^T \|A_L^i\|\right] \\
        &\leq \kappa_\xi \kappa_\omega \frac{C_L^2}{[1-\sqrt{\rho(A_L)}]^2} \|E\|    
    \end{align*}
    where the last equality follows by \Cref{lem:bound-rho-L};
    second, similarly
    \begin{align*}
        \|\calA_L (I \otimes L) \vomega& \vomega^\intercal (I \otimes E^\intercal) \calA_L^\intercal\| \\
        &\leq\|\calA_L (I \otimes L) \vomega\|\,\|\calA_L (I \otimes E) \vomega\|\\
        &\leq \kappa_\omega^2 \|L\|\, \|E\| \,\left[\sum_{i=0}^T \|A_L^i\|\right]^2 \\
        &\leq \kappa_\omega^2 \frac{C_L^2}{[1-\sqrt{\rho(A_L)}]^2}\|L\|\; \|E\|;
    \end{align*} 
    and finally
    \begin{align*}
        \|\calA_L \calX_L \calM_L[E]^\intercal\| 
        \leq& \|\calA_L \veta\|\, \|\calM_L[E] \veta\|\\
        \leq& \kappa_L^2 \left[\sum_{i=0}^T \|A_L^i\|\right] \, \left[\sum_{i=0}^T \|M_i[E]\|\right]\\
        \leq& \kappa_L^2 \left[\frac{C_L + 2 C_L^3 \rho(A_L)^{3/2}}{[1-\sqrt{\rho(A_L)}]^3}\right] \|E H\|\,
    \end{align*}
    where the last inequality follows by \Cref{lem:bound-rho-L}.
    Now, by combining the last three bounds we can claim that almost surely
    \begin{equation*}
        \|S_L(E,\mathcal{Y})\|
        %
        \leq \frac{\nu_L}{\| H^\intercal H \|_*} \|E\|.
    \end{equation*}
    This also implies that
    \begin{equation*}
        \|\E{}{S_L(E,\mathcal{Y})^2}\| \leq \E{}{\|S_L(E,\mathcal{Y})\|^2} \leq \frac{\nu_L^2}{\| H^\intercal H \|_*^2} \|E\|^2.
    \end{equation*}
    Therefore, by \Cref{lem:matrix-Bernstein} we obtain that
    \begin{multline*}
        \prob{\|\frac{1}{M}\sum_{i = 1}^M S_L(E,\mathcal{Y}_T^i) - \E{}{S_L(E,\mathcal{Y}_T^i)}\| \geq t} \\
        \leq 2n \exp\left[\frac{- M \| H^\intercal H \|_*^2 t^2/2}{\nu_L^2 \|E\|^2 + 2\nu_L \| H^\intercal H \|_* \|E\| t/3}\right]
    \end{multline*}
    Thus, by substituting $t$ with $t \|E\|\big/ \|H^\intercal H\|_*$ and applying this bound to \cref{eq:nabla-Jhat-E} we obtain that 
    \begin{equation*}
    \prob{|\langle \nabla \widehat J_T(L) - \nabla J_T(L), E \rangle | \leq t\|E\| } \\ \geq 1- 2n \exp\left[\frac{- M t^2/2}{\nu_L^2 + 2\nu_L t/3}\right]
\end{equation*}
    Finally, choosing $E = \nabla\widehat J_T(L) -\nabla J_T(L)$ proves the second claim.
    %
    %
    %
\end{proof}

\subsection{Proof of the  \Cref{prop:trunc-error}: Truncation error bound}
We provide the proof for a detailed version of \Cref{prop:trunc-error}:
\begin{proposition5}[Truncation Error Bound]
    Suppose $m_0 = 0$, then under Assumption \ref{asmp:noise-bound} we have
    \[ |J(L) - J_T(L)| \leq \bar\xi_L \; \frac{\rho(A_L)^{T+1}}{1-\rho(A_L)} ,\]
    and
    \[ \| \nabla J(L) - \nabla J_T(L)\| \leq \bar\gamma_L\; \frac{\sqrt{\rho(A_L)}^{T+1}}{[1-\rho(A_L)]^2}\]
    where 
    \begin{gather*}
    \begin{aligned}
        \bar\xi_L \coloneqq& \left[    \kappa_\xi^2 + (\kappa_\xi^2 + \kappa_\omega^2 \, \|L\|^2) C_L^2  \right] \|H^\intercal H\|_* C_L^2, \\
        \bar\gamma_L \coloneqq& 2\left[  {\kappa_\xi^2  + C_L^2 (\kappa_\xi^2+\kappa_\omega^2 \|L\|^2) } \right] C_L^2 \|H\|\, \|H^\intercal H\|_* \\
        &+ 2  \kappa_\omega^2 (\kappa_\xi^2+\kappa_\omega^2 \|L\|^2) \|L\| \,\|H\| \|H^\intercal H\|_*  \left(C_L+2C_L^3\rho(A_L)^{3/2}\right)   { C_L^3  \sqrt{\rho(A_L)}^{T+1} }.
    \end{aligned}
    \end{gather*} 
\end{proposition5}

\begin{proof}[Proof of \Cref{prop:trunc-error}]
    For the purpose of this proof, we denote the same matrices by $\calA_{L,T}$ and $\calM_{L,T}[E]$ in order to emphasize on length $T$. Recall that
    \[J_T(L) = \E{}{\varepsilon(L,\mathcal{Y})} = \tr{\E{}{\calX_L}\calA_{L,T}^\intercal H^\intercal H \calA_{L,T}},\]
    where $\E{}{\calX_L} = \calQ + (I \otimes L)\calR (I \otimes L^\intercal)$, which implies
    \begin{equation*}
        J_T(L) = \tr{[I \otimes (Q + L R L^\intercal)]\calA_{L,T-1}^\intercal H^\intercal H \calA_{L,T-1}}
        + \tr{P_0 (A_L^\intercal)^T H^\intercal H A_L^T},
    \end{equation*}
    On the other hand,
    \[J(L) = \lim_{t\to \infty} \tr{[I \otimes (Q + L R L^\intercal)]\calA_{L,t}^\intercal H^\intercal H \calA_{L,t}},\]
    and thus
    \begin{align*}
        J(L) - J_T(L) =&- \tr{P_0 (A_L^\intercal)^T H^\intercal H A_L^T}  \\
        &+ \lim_{t\to \infty} \tr{[I \otimes (Q + L R L^\intercal)][I \otimes (A_L^\intercal)^T]\calA_{L,t}^\intercal H^\intercal H  \calA_{L,t} [I \otimes A_L^T]}\\
        =&- \tr{A_L^T P_0 (A_L^\intercal)^T H^\intercal H } \\
        &+ \lim_{t\to \infty} \tr{  [I \otimes A_L^T(Q + L R L^\intercal)(A_L^\intercal)^T] \calA_{L,t}^\intercal H^\intercal H \calA_{L,t} }.
    \end{align*}
    Therefore, by the properties of trace and \Cref{lem:bound-rho-L} we obtain that
    \begin{align*}
        |J(L) - J_T(L)| \leq& \|P_0\|\, \|A_L^T\|^2 \;\tr{H^\intercal H} \\
        &+ \|Q + L R L^\intercal\|\, \|(A_L)^T\|^2\, \lim_{t\to \infty} \tr{\calA_{L,t}^\intercal H^\intercal H \calA_{L,t}}\\
        \leq&\; \|P_0\| C_L^2 \rho(A_L)^{T+1} \|H^\intercal H\|_*\\
        &+ (\|Q\| + \|R\| \, \|L\|^2) C_L^2 \rho(A_L)^{T+1} \frac{ C_L^2 \|H^\intercal H\|_*}{1-\rho(A_L)},
    \end{align*}
    where the last line follows by \Cref{lem:matrix-bounds}. So,
    \begin{gather*}
        |J(L) - J_T(L)| \leq 
        \left[ \|P_0\| +  \frac{ (\|Q\| + \|R\| \, \|L\|^2) C_L^2 }{1-\rho(A_L)} \right] \|H^\intercal H\|_* C_L^2  \rho(A_L)^{T+1} 
    \end{gather*}
    This, together with Assumption \ref{asmp:noise-bound} imply the first claim.


    Next, for simplicity we adopt the notation $\calA_{L,\infty}$ to interpret the limit as $t \to \infty$, then similar to the proof of \Cref{prop:concen-n} we can compute that
    \begin{align*}
        \langle \nabla J(L) - \nabla J_T(L), E \rangle =&- 2\tr{M_T[E] P_0 (A_L^\intercal)^T H^\intercal H } \\
        &+ \tr{ [I \otimes A_L^T(Q + L R L^\intercal)(A_L^\intercal)^T] \calN_{L,\infty}[E] }\\
        &+ 2 \tr{ [I \otimes M_T[E] (Q + L R L^\intercal)(A_L^\intercal)^T]  \calA_{L,\infty}^\intercal H^\intercal H \calA_{L,\infty} }\\
        &+ 2\tr{ [I \otimes A_L^T E R L^\intercal (A_L^\intercal)^T] \calA_{L,\infty}^\intercal H^\intercal H \calA_{L,\infty} }.
    \end{align*}
    Therefore, using \cref{eq:nuclear-norm} and \Cref{lem:bound-rho-L} we have the following bound
    \begin{gather*}
    \begin{aligned}
        |\langle \nabla J(L) - \nabla J_T(L), E \rangle| \leq&  2 \|EH\| C_L^2 (T+1) \rho(A_L)^{T+1} \|P_0\|  \|H^\intercal H\|_* \\
        &+ \|Q + L R L^\intercal\| C_L^2 \rho(A_L)^{T+1} \|\calN_{L,\infty}[E]\|_*\\
        &+ 2 \|Q + L R L^\intercal\| \|EH\| C_L^2 (T+1) \rho(A_L)^{T+1} \|\calA_{L,\infty}^\intercal H^\intercal H \calA_{L,\infty} \|_* \\
        &+ 2 \| E R L^\intercal\| C_L^2 \rho(A_L)^{T+1}  \|\calA_{L,\infty}^\intercal H^\intercal H \calA_{L,\infty} \|_*
    \end{aligned}
    \end{gather*}
    which by \Cref{lem:matrix-bounds} is bounded as follows
    \begin{gather*}
    \begin{aligned}
        |\langle \nabla J(L) - \nabla J_T(L), E/\|E\| \rangle| \leq & 2   \|P_0\| \|H\| \|H^\intercal H\|_* C_L^2 (T+1) \rho(A_L)^{T+1}\\
        &+ \|Q + L R L^\intercal\| \|H\|\,  \|H^\intercal H\|_* \frac{\left[2C_L^3+4C_L^5\rho(A_L)^{3/2}\right] \rho(A_L)^{T+1}}{[1-\rho(A_L)]^{2}} \\
        &+ 2 \|Q + L R L^\intercal\| \|H\| \|H^\intercal H\|_*  \frac{ C_L^4  (T+1) \rho(A_L)^{T+1} }{1-\rho(A_L)}  \\
        &+ 2 \|R\| \|L\| \|H^\intercal H\|_*  \frac{ C_L^4  \rho(A_L)^{T+1} }{1-\rho(A_L)}.
    \end{aligned}
    \end{gather*} 
    Finally, choosing $E = \nabla J(L) - \nabla J_T(L)$ together with Assumption \ref{asmp:noise-bound} implies 
    \begin{gather*}
    \begin{aligned}
        \| \nabla J(L) - \nabla J_T(L)\| \leq & 2\left[     \frac{\kappa_\xi^2  + C_L^2 (\kappa_\xi^2+\kappa_\omega^2 \|L\|^2) }{1-\rho(A_L)} \right] C_L^2 \|H\|\, \|H^\intercal H\|_* (T+1) \rho(A_L)^{T+1}\\
        &+ 2 \left[    \frac{\kappa_\omega^2 (\kappa_\xi^2+\kappa_\omega^2 \|L\|^2) \|L\| \,\|H\| \left(C_L+2C_L^3\rho(A_L)^{3/2}\right) }{1-\rho(A_L)} \right] \|H^\intercal H\|_*  \frac{ C_L^3  \rho(A_L)^{T+1} }{1-\rho(A_L)}.
    \end{aligned}
    \end{gather*} 
    Finally, the second claim follows by the following simple facts:
    \[(T+1)\rho(A_L)^{T+1} \leq \frac{\sqrt{\rho(A_L)}^{T+1}}{1-\rho(A_L)},\quad \forall T>0,\]
    as $\max_{t\geq 0} t \rho^t = \frac{2}{e \ln{1/\rho}} \leq \frac{1}{ \ln{1/\rho}} \leq \frac{1}{1-\rho}$ for any $\rho \in(0,1)$.
    This completes the proof.
\end{proof}

\subsection{Complete version of \Cref{thm:oracle}: Sample complexity bounds for the stochastic oracle}
The following is a detailed version of \Cref{thm:oracle}:
\begin{theorem2}
Suppose $m_0 = 0_n$ and Assumption \ref{asmp:noise-bound} holds for a data-set $\{\mathcal{Y}_{T}^i\}_{i=1}^M$. Define
\(\nabla \widehat J_{T}(L) \coloneqq \frac{1}{M}\sum_{i=1}^M \nabla \varepsilon(L,\mathcal{Y}_{T}^i),\)
where $\nabla_L \varepsilon(L,\mathcal{Y})$ is obtained in \Cref{lem:grad-approx}.
%
%
%
%
Consider $\mathcal{S}_\alpha$ for some $\alpha>0$ and any $s, s_0>0$ and $\tau \in (0,1)$. Suppose the trajectory length
    \begin{equation*}
        T  \geq \ln\left( \frac{\bar\gamma_\alpha \sqrt{\min(n,m)}}{s_0} \right)\big/\ln\left( \frac{1}{\sqrt{\rho_\alpha}} \right)
    \end{equation*}
    and the batch size
    \begin{gather*}
        M \geq \left[2\left(\frac{\nu_\alpha \sqrt{\min(n,m)}}{s\, s_0 \, / \tau}\right)^2+ \frac{4}{3}\left(\frac{\nu_\alpha\sqrt{\min(n,m)}}{s\, s_0 \, / \tau}\right)\right]\ln(2n/\delta),
    \end{gather*}
    where
    \begin{gather*}
    \begin{aligned}
        \bar\gamma_\alpha \coloneqq& \quad 2\left[  {\kappa_\xi^2  + C_\alpha^2 (\kappa_\xi^2+\kappa_\omega^2 D_\alpha^2) } \right] C_\alpha^2 \|H\|\, \|H^\intercal H\|_* \\
        &+ 2  \kappa_\omega^2 (\kappa_\xi^2+\kappa_\omega^2 D_\alpha^2) D_\alpha \,\|H\| \|H^\intercal H\|_* \left(C_\alpha+2C_\alpha^3\rho_\alpha^{3/2}\right)   { C_\alpha^3  \sqrt{\rho_\alpha}^{T+1} },\\
        \nu_\alpha \coloneqq& \frac{2(\kappa_\xi + D_\alpha \kappa_\omega) \kappa_\omega C_\alpha^2 + \left[C_\alpha + 2C_\alpha^3 \rho_\alpha^{3/2}\right] \|H\| (\kappa_\xi + D_\alpha \kappa_\omega)^2}{[1-\sqrt{\rho_\alpha}]^3/ \| H^\intercal H \|_*}\, ,
    \end{aligned}
    \end{gather*} 
    with $\rho_\alpha$, $C_\alpha$ and $D_\alpha$ defined in \Cref{lem:bound-rho-L}.
    Then, with probability no less than $1-\delta$, Assumption \ref{asmp:noisy-grad} holds.
    %
    %
    %
    %
    
\end{theorem2}

\subsection{Additional concentration bound results}
Combining the truncation bound in \Cref{prop:trunc-error} with concentration bounds in \Cref{prop:concen-n} we can provide probabilistic bounds on the ``estimated cost'' $\widehat{J}_T(L)$ and the ``estimated gradient'' $\nabla \widehat{J}_T(L)$. The result involves the bound for the Frobenius norm of the error with probabilities independent of $T$. 

\Cref{thm:oracle}, can be viewed as a simplified application of \Cref{thm:estimated-gradient} to characterize the required minimum trajectory length and minimum batch so that the approximate gradient satisfies Assumption \ref{asmp:noisy-grad}, with a specific $s$ and $s_0$.

\begin{theorem}\label{thm:estimated-gradient}
    Suppose Assumption \ref{asmp:noise-bound} holds. For any $s>0$ and $L \in \mathcal{S_\alpha}$, if
    \begin{gather*}
        M \geq \left[2\left[\frac{\nu_L\sqrt{\min(n,m)}}{s\, \|\nabla J(L)\|_F}\right]^2+ \frac{4}{3}\left[\frac{\nu_L\sqrt{\min(n,m)}}{s\, \|\nabla J(L)\|_F }\right]\right]\ln(2n/\delta),
    \end{gather*}
    then with probability no less than $1-\delta$,
    \begin{equation*}
        \|\nabla\widehat J_T(L) - \nabla J(L)\|_F \leq s \|\nabla J(L)\|_F 
        +  \bar\gamma_L \sqrt{\min(n,m)} \sqrt{\rho(A_L)}^{T+1}, 
    \end{equation*}
    with $\nu_L$ and $\bar\gamma_L$ defined in \Cref{prop:concen-n} and \Cref{prop:trunc-error}, respectively.
\end{theorem}

\begin{proof}[Proof of \Cref{thm:estimated-gradient}]
    Recall that for any $L \in \mathcal{S}_\alpha$ for some $\alpha>0$ we have
    \begin{equation*}
    \|\nabla\widehat J_T(L) - \nabla J(L)\| \leq \|\nabla\widehat J_T(L) -\nabla J_T(L)\| 
    + \|\nabla J_T(L)-\nabla J(L)\|.
    \end{equation*}
    Thus, by \Cref{prop:concen-n} with $s$ replaced by $s \|\nabla J(L)\|_F / \sqrt{\min(n,m)}$ and applying \Cref{prop:trunc-error} to the second term, we obtain that with probability at least $1-\delta$:
    \[\|\nabla\widehat J_T(L) - \nabla J(L)\| \leq \frac{s \|\nabla J(L)\|_F}{\sqrt{\min(n,m)}} + \bar\gamma_L\; \frac{\sqrt{\rho(A_L)}^{T+1}}{[1-\rho(A_L)]^2},\]
    where 
    \[\delta \geq 2n \exp\left[\frac{- M s^2/2}{\left[\frac{\nu_L\sqrt{\min(n,m)}}{\|\nabla J(L)\|_F}\right]^2  + 2\left[\frac{\nu_L\sqrt{\min(n,m)}}{\|\nabla J(L)\|_F}\right] s/3}\right].\]
    Noticing $\|\nabla\widehat J_T(L) - \nabla J(L)\|_F \leq \sqrt{\min(n,m)} \|\nabla\widehat J_T(L) - \nabla J(L)\|$  and rearranging terms will complete the proof.
\end{proof}


One can also provide the analogous concentration error bounds where the probabilities are independent of the system dimension $n$.
\begin{proposition}[Concentration independent of system dimension $n$]\label{prop:concen-T}
    Under the same hypothesis, we have
    \begin{equation*}
        \prob{ |\widehat J_T(L) - J_T(L)| \leq s} 
        \geq 1 - 2T \exp\left[\frac{- M  s^2/2}{\bar\mu_L^2 T^2 + 2\bar\mu_L T s/3}\right],
    \end{equation*}
    and
    \begin{multline*}
        \prob{\| \nabla \widehat J_T(L) - \nabla J_T(L)\| \leq s}  
        \geq 1-2T \exp\left[\frac{- M s^2/2}{\bar\nu_L^2 T^2 + 2 \bar\nu_L T s/3}\right] \\
        -2T \exp\left[\frac{- M s^2/2}{\kappa_\omega^2 \bar\mu_L^2 T^2 + 2 \kappa_\omega \bar\mu_L T s/3}\right]
    \end{multline*}
    where 
    \begin{align*}
        \bar\mu_L &\coloneqq  \frac{ C_L^2 \|H^\intercal H\|_*}{1-\rho(A_L)} \kappa_L^2 \\
        \bar\nu_L &\coloneqq  \frac{\left[2C_L+4C_L^3\rho(A_L)^{3/2}\right]\|H\|\, \|H^\intercal H\|_*}{[1-\rho(A_L)]^{2}} \kappa_L^2 .
    \end{align*}
\end{proposition}
\begin{proof}[Proof of \Cref{prop:concen-T}]
Similar to the previous proof, we have \begin{align*}
        \widehat J_T(L)& - J_T(L) 
        = \tr{\left(\calZ_L -\E{}{\calZ_L}\right) \calA_L^\intercal H^\intercal H \calA_L},
    \end{align*}
    and thus, by \cref{eq:nuclear-norm} we obtain
    \begin{align}\label{eq:Jbound-nuclear-T}
        |\widehat J_T(L) - J_T(L)|
        \leq& \| \left(\calZ_L -\E{}{\calZ_L}\right) \|\, \|\calA_L^\intercal H^\intercal H \calA_L\|_*.
    \end{align}    
    Next, we consider the symmetric random matrix $\left(\calZ_L -\E{}{\calZ_L}\right)$ and recall that $\|\xi(t) - L \omega(t) \| \leq \kappa_L$ almost surely; thus
    \begin{equation*}
        \|\calX_L\| = \left\| \veta\right\|^2 \leq \kappa_L^2 T^2.
    \end{equation*}
    It then follows that
    \begin{equation*}
        \left\|\E{}{\calX_L^2}\right\| \leq \E{}{\|\calX_L\|^2} \leq \kappa_L^4 T^4 .
    \end{equation*}
    Therefore, by \Cref{lem:matrix-Bernstein} we obtain that
    \begin{equation}\label{eq:concentration-Z}
        \prob{\| \left(\calZ_L -\E{}{\calZ_L}\right) \|\geq t}
        \leq 2T \exp\left[\frac{- M t^2/2}{\kappa_L^4 T^4 + 2\kappa_L^2 T^2 t/3}\right]
    \end{equation}
    Substituting $t$ with $t/\|\calA_L^\intercal H^\intercal H \calA_L \|_*$ together with \cref{eq:Jbound-nuclear-T} implies the first claim because by \Cref{lem:matrix-bounds} $\kappa_L^2 T^2 \| \calA_L^\intercal H^\intercal H \calA_L \|_* \leq \bar\mu_L T$. 


    Again, similar to \cref{eq:nabla-Jhat-E} in the previous proof, by \cref{eq:nuclear-norm} we obtain that
    \begin{multline}\label{eq:nabla-Jhat-E2}
        |\langle \nabla \widehat J_T(L) - \nabla J_T(L), E \rangle | \leq
        \|\frac{1}{M}\sum_{i = 1}^M S_L(E,\mathcal{Y}_T^i) - \E{}{S_L(E,\mathcal{Y}_T^i)}\| \|\calA_L^\intercal H^\intercal H \calA \|_* \\
        + \|\frac{1}{M}\sum_{i=1}^M \calX_L(\mathcal{Y}^i) - \E{}{\calX_L(\mathcal{Y}^i)}\| \|\calN_L[E]\|_*
    \end{multline}
    where $S_L(E,\mathcal{Y})$ is the symmetric part of the following random matrix
    \begin{equation*}
        -2  \vxi \vomega^\intercal (I \otimes E^\intercal)
        + 2 (I \otimes L) \vomega \vomega^\intercal (I \otimes E^\intercal) .
    \end{equation*}
    So, we claim that almost surely
    \begin{align*}
        \|S_L(E,\mathcal{Y})\| 
        &\leq 2 \|(I \otimes E) \vomega\| (\|\vxi\| + \|(I \otimes L) \vomega\|)\\
        &\leq 2 \|E\|\, \kappa_\omega T (\kappa_\xi T + \|L\| \kappa_\omega T)\\
        &= \kappa_L \kappa_\omega T^2 \|E\|,
    \end{align*}
    %
    and thus
    \begin{align*}
        \|\E{}{S_L(E,\mathcal{Y})^2}\| &\leq  \E{}{\|S_L(E,\mathcal{Y})\|^2} \leq  \kappa_L^2 \kappa_\omega^2 T^4 \|E\|^2.
        %
    \end{align*}
    Therefore, by \Cref{lem:matrix-Bernstein} we obtain that
    \begin{equation*}
        \prob{\|\frac{1}{M}\sum_{i = 1}^M S_L(E,\mathcal{Y}_T^i) - \E{}{S_L(E,\mathcal{Y}_T^i)}\| \geq t}
        \leq 2T \exp\left[\frac{- M t^2/2}{\kappa_\omega^2 \kappa_L^2 T^4 \|E\|^2 + 2 \kappa_L \kappa_\omega T^2 \|E\| t/3}\right]
    \end{equation*}
    Substituting $t$ with $t/\|\calA_L^\intercal H^\intercal H \calA_L \|_*$ implies that 
    \begin{gather*}
        \prob{\|\frac{1}{M}\sum_{i = 1}^M S_L(E,\mathcal{Y}_T^i) - \E{}{S_L(E,\mathcal{Y}_T^i)}\| \, \|\calA_L^\intercal H^\intercal H \calA_L \|_*\geq t} \\
        \leq 2T \exp\left[\frac{- M t^2/2}{\kappa_\omega^2 \bar\mu_L^2 T^2 \|E\|^2 + 2 \kappa_\omega \bar\mu_L T \|E\| t/3}\right]
    \end{gather*}
    because by \Cref{lem:matrix-bounds} we have $\kappa_L^2 T^2 \| \calA_L^\intercal H^\intercal H \calA_L \|_* \leq \bar\mu_L T$. 
    
    Next, by substituting $t$ with $t/\|\calN_L[E]\|_*$ in \cref{eq:concentration-Z} we have
    \begin{equation*}
        \prob{\|\calZ_L -\E{}{\calZ_L} \|\, \|\calN_L[E]\|_*\geq t} 
        \leq 2T \exp\left[\frac{- M t^2/2}{\bar\nu_L^2 T^2 \|E\|^2 + 2 \bar\nu_L T \|E\| t/3}\right]
    \end{equation*}
    because \Cref{lem:matrix-bounds} implies that $\kappa_L^2 T^2 \|\calN_L[E]\|_* \leq \bar\nu_L T \|E\|$.
    %
    Thus, by combining the last two inequalities and using the union bound for \cref{eq:nabla-Jhat-E2}  we obtain that    
    \begin{multline*}
        \prob{|\langle \nabla \widehat J_T(L) - \nabla J_T(L), E \rangle | \geq t}
        \leq 2T \exp\left[\frac{- M t^2/2}{\bar\nu_L^2 T^2 \|E\|^2 + 2 \bar\nu_L T \|E\| t/3}\right] \\
        +2T \exp\left[\frac{- M t^2/2}{\kappa_\omega^2 \bar\mu_L^2 T^2 \|E\|^2 + 2 \kappa_\omega \bar\mu_L T \|E\| t/3}\right]
    \end{multline*}
    Finally, substituting $t$ with $t \|E\| $ and choosing $E = \nabla\widehat J_T(L) -\nabla J_T(L)$ proves the second claim.
\end{proof}


\begin{remark}
    We obtained a better bound for truncation of the gradient as
     \begin{equation*}
         \| \nabla J(L) - \nabla J_T(L)\| \leq \bar\gamma_1(L)\; \frac{\sqrt{\rho(A_L)}^{T+1}}{[1-\rho(A_L)]\ln(1/\rho(A_L))} 
         + \bar\gamma_2(L) \frac{\rho(A_L)^{T+1}}{[1-\rho(A_L)]^2},
     \end{equation*}
     which has been simplified for clarity of the presentation.
\end{remark}














%

%


%


%


%
%
%
%
%


%


%
%
%
%


%


%
%
%
%
%
%
%


%
%
%


%
%
%
%


%
%
%
%
%
%
%


%
%
%
%


%
%
%


%
%


%
%


%
%
%
%
%
%


%
%
%
%
%


%
%
%
%
%


%
%


%
%


%
%


%


%


%


%


%


%


%
%
%


%
%


%


%


%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%


%
%
%
%
%


%
%
%
%
%


%


%
%
%
%


%
%


%


%
%
%
%
%


%
%
%
%
%


%
%
%




%
%

%

%
%
%
%
%


%


%
%


%
%
%


%


%


%
%
%
%


%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%


%


%
%


%


%
%
%
%
%
%
%
%
%
%


%
%
%


%
%
%
%
%


%
%



%

%


%


%
%
%
%
%
%


%
%


%
%
%
%


%
%
%


%
%
%
%

%


%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%


\section{Numerical Results}\label{sec:numerics}
Herein, we showcase the application of the developed theory for improving the estimation policy for an LTI system. Specifically, we consider an undamped mass-spring system  with known parameters $(A,H)$ with $n=2$ and $m=1$. In the hindsight, we consider a variance of $0.1$ for each state dynamic noise, a state covariance of $0.05$ and a variance of $0.1$ for the observation noise. Assuming a trajectory of length $T$ at every iteration, the approximate gradient is obtained as in \Cref{lem:grad-approx}, only requiring an output data sequence collected from the system in \cref{eqn:sysdyn}. Then, the progress of policy updates using the \ac{sgd} algorithm for different values of trajectory length $T$ and batch size $M$ are depicted in \Cref{fig:sim} where each figure shows \textit{average progress} over 50 rounds of simulation. The figure demonstrates a linear convergence outside of a neighborhood of global optimum that depends on the bias term in the approximate gradient (due to truncated data trajectories). The rate then drops when the policy iterates enter into this neighborhood which is expected as every update only relies on a \textit{biased gradient}---in contrast to the linear convergence established for deterministic \ac{gd} (to the exact optimum) using the true gradient. 

Specifically, recall that our convergence guarantee to a small neighborhood around the optimal value is due to the finite-length of the data trajectories. The region can be made arbitrary small by choosing larger trajectory length. In particular, to achieve $\varepsilon$ error, we only require the length $T\geq O(\ln(1/\varepsilon))$---see \Cref{thm:combined}. Also, Fig 1(d) is illustrating that optimality gap at the final iteration (i.e. the radius of the small neighborhood around optimality) which is decaying linearly as a function of trajectory length $T \leq 50$---until the variance error dominates beyond $T=50$. It is clear that, increasing the batch size $M$ will allow further decrease of this optimality gap beyond $T=50$.

The code for regenerating these results is available online at this GitHub repository \cite{Talebi_SGD_for_Filtering_2023}.

\begin{figure}[ht]
\centering 
\begin{subfigure}{0.48\hsize}
\centering
\includegraphics[width=\hsize]{error_J_k_M_rebuttal-50-runs.pdf}
\caption{}

\end{subfigure}
\begin{subfigure}{0.48\hsize}
\centering
\includegraphics[width=\hsize]{error_J_M_rebuttal-50-runs.pdf}
\caption{}

\end{subfigure}\\
\begin{subfigure}{0.48\hsize}
\centering
\includegraphics[width=\hsize]{error_J_k_T_rebuttal-50-runs.pdf}
\caption{}

\end{subfigure}
\begin{subfigure}{0.48\hsize}
\centering
\includegraphics[width=\hsize]{error_J_T_rebuttal-50-runs.pdf}
\caption{}

\end{subfigure}
\caption{Simulation result of the SGD algorithm to learn the steady-state Kalman gain for the mass-spring example. (a) The optimality gap as a function of iterations $k$ for different value of batch-size $M$ averaged over $50$ simulations; (b) The optimality gap at final iteration as a function of batch-size $M$ for all $50$ simulations; (c)  The optimality gap as a function of iterations $k$ for different value trajectory length  $T$. The figure depicts the linear decay of the optimality gap, with respect to the iteration $k$, before the iterate enters the small neighborhood of optimality where the direction of the oracle gradient is not informative anymore. The neighborhood shrinks as $T$ increases; (d) The optimality gap at final iteration as a function of trajectory length $T$. The gap is decaying linearly as a function of trajectory length $T$ until the variance error dominates beyond $T=50$. }
\label{fig:sim}
\end{figure}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



%
%
\nomenclature[C]{\(n\)}{Dimension of state vector}
\nomenclature[C]{\(m\)}{Dimension of observation vector}
\nomenclature[C]{\(M\)}{Batch-size of SGD algorithm}
\nomenclature[C]{\(\eta\)}{Step-size of SGD algorithm}
\nomenclature[C]{\(k\)}{Iteration step of SGD algorithm}
\nomenclature[C]{\(t\)}{Time step of the system}
\nomenclature[C]{\(T\)}{Length of output data trajectory}
\nomenclature[C]{\(\alpha\)}{Scalar value of the cost}
\nomenclature[C]{\(\kappa_\xi\)}{Scalar upperbound on 2-norm of dynamic noise}
\nomenclature[C]{\(\kappa_\omega\)}{Scalar upperbound on 2-norm of measurement noise}
\nomenclature[C]{\(\kappa_L\)}{Combined Scalar upperbound on 2-norm of noise}
\nomenclature[C]{\(\nu_L\)}{Combined matrix variance statistics of gradient estimate---see Proposition 4}
\nomenclature[C]{\(\nu_\alpha\)}{Uniform constant for gradient estimate---see \Cref{thm:oracle}}
\nomenclature[C]{\(\bar\gamma_\alpha\)}{Uniform constant for gradient estimate---see \Cref{thm:oracle}}
\nomenclature[C]{\(\bar\gamma_L\)}{Constant for bounding bias of gradient estimate---see Proposition 5}



\nomenclature[C]{\(s_0\)}{Bias term of oracle model---see Assumption~\ref{asmp:noisy-grad}}
\nomenclature[C]{\(s\)}{Variance coefficient term of oracle model---see Assumption~\ref{asmp:noisy-grad}}
\nomenclature[C]{\(\gamma\)}{Positive constant smaller than one---see Proposition 2}

\nomenclature[C]{\(\delta\)}{Failure probability}




\nomenclature[O]{\(c_1,c_2,c_3\)}{Uniform positive constants related to the cost function on $\mathcal{S}_\alpha$--see Lemma 2}
\nomenclature[O]{\(\rho_\alpha\)}{Uniform upperbound of spectral radius of closed-loop system over $\mathcal{S}_\alpha$--see Lemma 6}
\nomenclature[O]{\(\rho(\cdot)\)}{Spectral radius of a matrix}
\nomenclature[O]{\(\rho_\alpha\)}{Uniform upperbound of spectral radius of closed-loop system over $\mathcal{S}_\alpha$--see Lemma 6}
\nomenclature[O]{\(C_L\)}{Constant coefficient of upperbound in Lemma 6}
\nomenclature[O]{\(C_\alpha\)}{Uniform upperbound of constant coefficient over $\mathcal{S}_\alpha$--see Lemma 6}
\nomenclature[O]{\(D_\alpha\)}{Uniform upperbound of 2-norm of policy $L$ over $\mathcal{S}_\alpha$--see Lemma 6}

\nomenclature[O]{\(\lVert\,{\cdot}\,\rVert\)}{2-norm of a matrix}
\nomenclature[O]{\(\lVert\,{\cdot}\,\rVert_F\)}{Frobenius-norm of a matrix}

\nomenclature[O]{\(\mathcal{L}\)}{Linear operator describing the filtering policy that maps measurements to state estimation}
\nomenclature[O]{\(\mathcal{L}^\dagger\)}{Adjoint of the operator $\mathcal{L}$}



\nomenclature[P]{\(A\)}{Dynamics matrix}
\nomenclature[P]{\(H\)}{Observation matrix}
\nomenclature[P]{\(Q\)}{State noise covariance matrix}
\nomenclature[P]{\(R\)}{Output (measurement) noise covariance matrix}
\nomenclature[P]{\(P\)}{Error covariance matrix}
\nomenclature[P]{\(P_0\)}{Initial state covariance matrix}
\nomenclature[P]{\(m_0\)}{Initial state mean vector}
\nomenclature[P]{\(\mathcal{A}_L\)}{Concatenation of closed-loop dynamics matrices---see Proposition 3}




\nomenclature[S]{\(x\)}{State vector}
\nomenclature[S]{\(\xi\)}{State (dynamics) noise vector}
\nomenclature[S]{\(\Vec\xi\)}{Concatenated state noise vector}
\nomenclature[S]{\(\Vec\omega\)}{Concatenated measurement noise vector}
\nomenclature[S]{\(y\)}{Output (measurement) vector}
\nomenclature[S]{\(\omega\)}{Output (measurement) noise vector}
\nomenclature[S]{\(\eta\)}{Combined noise vector}
\nomenclature[S]{\(\Vec\eta\)}{Concatenated combined noise vector}
\nomenclature[S]{\(z\)}{Adjoint state vector}
\nomenclature[S]{\(u\)}{Adjoint input vector}
\nomenclature[S]{\(\hat x\)}{Estimation of state vector}
\nomenclature[S]{\(\hat y\)}{Estimation of output vector}
\nomenclature[S]{\(\mathcal{Y}\)}{Trajectory of output vectors}
\nomenclature[S]{\(\mathcal{U}\)}{Trajectory of adjoint input vector}
\nomenclature[S]{\(L\)}{Filtering policy matrix}
\nomenclature[S]{\(J\)}{Cost function}
\nomenclature[S]{\(\nabla J\)}{Gradient of cost function}
\nomenclature[S]{\(\nabla \widehat J\)}{Estimation of gradient of cost function}
\nomenclature[S]{\(J_T\)}{Cost function on length $T$ trajectory}
\nomenclature[S]{\(\widehat J_T\)}{Estimation of the cost function on length $T$ trajectory}
\nomenclature[S]{\(\mathcal{S}\)}{Set of Schur stabilizing policies}
\nomenclature[S]{\(\partial\mathcal{S}\)}{Boundary of the set of Schur stabilizing policies}
\nomenclature[S]{\(\mathcal{S}_\alpha\)}{$\alpha$-sublevel set of $J$ contained in $\mathcal{S}$}
\nomenclature[S]{\(\mathcal{C}_\tau\)}{$s_0/\tau$-neighborhood of the globally optimal policy $L^*$}
\nomenclature[S]{\(X\)}{Cost matrix}
\nomenclature[S]{\(Y\)}{Auxiliary cost matrix}
\nomenclature[S]{\(X_T\)}{Cost matrix on length $T$ trajectory}
\nomenclature[S]{\(\varepsilon(L,\mathcal{Y}_T)\)}{Squared-norm of the estimation error vector of policy $L$ for trajectory $\mathcal{Y}_T$}
\nomenclature[S]{\(e_T(L)\)}{Estimation error vector of policy $L$ for trajectory $\mathcal{Y}_T$}
\nomenclature[S]{\(\mathcal{M}_L\)}{Concatenated matrix regarding the differential of \(\varepsilon(L,\mathcal{Y}_T)\)---see Proposition 3}
\nomenclature[S]{\(\mathcal{N}_L\)}{Concatenated matrix regarding the differential of \(\varepsilon(L,\mathcal{Y}_T)\)---see Proposition 3}
\nomenclature[S]{\(\mathcal{X}_L\)}{Concatenated matrix regarding the differential of \(\varepsilon(L,\mathcal{Y}_T)\)---see Proposition 3}



\section{Nomenclature}
\printnomenclature


\end{appendices}

\end{document}