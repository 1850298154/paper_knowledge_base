\documentclass[twoside,11pt]{article}
\pdfoutput=1
\newif\iffrozen

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

%\PassOptionsToPackage{unicode}{hyperref}
%\PassOptionsToPackage{naturalnames}{hyperref}

%\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
%\usepackage{amsfonts}       % blackboard math 
%\usepackage{nicefrac}       % compact symbols for 1/2, etc.
%\usepackage{microtype}      % microtypography

\usepackage{comment}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{environ} 
\usepackage{tabularx} 
\usepackage{blindtext}


\usepackage{color}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{enumitem}
%
% comment to hyphenate again
%
%\usepackage[none]{hyphenat}
%\usepackage[first,dark]{draftcopy}
%\usepackage[firstpageonly=true,scale=5.0]{draftwatermark}
%\usepackage{setspace}
% If-else statement for todo notes
% \newif\ifshowtodos

% % Show existing ToDo notes in this project?
% % 'Show' - Uncomment line below; 'Hide' - Comment out
% \showtodosfalse

% \ifshowtodos
%    \usepackage[colorinlistoftodos,prependcaption]{todonotes}
% \else
%    \usepackage[disable, colorinlistoftodos,prependcaption]{todonotes}
% \fi
%\usepackage{minted}
\frozenfalse

% set to true for JMLR version
\newif\ifjmlr
\jmlrfalse

\usepackage{placeins}

\usepackage[
font=small
]{subfig}
\usepackage{graphbox} % adds align to includegraphics

\usepackage{graphicx}
\usepackage{array,calc}
%\usepackage[figure, boxed]{algorithm2e}
%\usepackage[boxed]{algorithm2e}
\usepackage[ruled,algonl,vlined]{algorithm2e}
%boxed
\usepackage{makecell}
\usepackage{relsize}
%\usepackage{fontawesome5}

\renewcommand\theadalign{bc}
%\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}



\usepackage{jmlr2e}
%\usepackage{jmlr2earxiv}
\usepackage{cleveref}
%
% macros
%
\newcommand{\mimi}{\mkern-4.0mu}
\newcommand{\todo}[1]{\colorbox{yellow}{TODO: #1}}
\newcommand{\km}{\texttt{\texorpdfstring{$k$}{k}-means}} 
\newcommand{\Km}{\texttt{\texorpdfstring{$K$}{K}-means}} 
\newcommand{\KM}{\texttt{\texorpdfstring{$K$}{K}-Means}} 

\newcommand{\bkm}{\texttt{breathing \texorpdfstring{$\mimi k$}{k}-means}} 
\newcommand{\Bkm}{\texttt{Breathing \texorpdfstring{$\mimi k$}{k}-means}} 
\newcommand{\BkM}{\texttt{Breathing \texorpdfstring{$\mimi k$}{k}-Means}} 
\newcommand{\BKM}{\texttt{Breathing \texorpdfstring{$\mimi K$}{K}-Means}} 

\newcommand{\bkmr}{breathing k-means} 
\newcommand{\Bkmr}{Breathing k-means} 
\newcommand{\BkMr}{Breathing k-Means} 
\newcommand{\BKMr}{Breathing K-Means} 

\newcommand{\scali}{\gamma} 
\newcommand{\mc}{\mathcal{C}}
\newcommand{\mx}{\mathcal{X}}
\newcommand{\mr}{\mathcal{R}}
\newcommand{\phicicx}{$\phi$(c_i,\mc,\mx)}
\newcommand{\optimal}{presumably optimal~}
\newcommand{\optimall}{presumably optimal}
\newcommand{\poptimal}{p-optimal~}
\newcommand{\poptimall}{p-optimal}


\newcommand{\lloyd}{\texttt{Lloyd}}
\newcommand{\kmp}{\texttt{\texorpdfstring{$k$}{k}-means++}}
\newcommand{\kMp}{\texttt{\texorpdfstring{$k$}{k}-Means++}}
\newcommand{\KMp}{\texttt{\texorpdfstring{$K$}{K}-Means++}}
\newcommand{\Kmp}{\texttt{\texorpdfstring{$K$}{K}-means++}}

\newcommand{\vkmp}{\texttt{vanilla \!\texorpdfstring{$k$}{k}-means++}}
\newcommand{\Vkmp}{\texttt{Vanilla\hspace{0.25em}\texorpdfstring{$k$}{k}-means++}}
\newcommand{\ahawo}{\texttt{algorithm of Hartigan-Wong}}
\newcommand{\hawo}{\texttt{Hartigan-Wong}}

\newcommand{\gkmp}{\texttt{greedy \texorpdfstring{$\mimi k$}{k}-means++}}
\newcommand{\gkMp}{\texttt{greedy\,\texorpdfstring{$\mimi k$}{k}-Means++}}
\newcommand{\Gkmp}{\texttt{Greedy \texorpdfstring{$\mimi k$}{k}-means++}}
% used in a header
\newcommand{\GKMp}{\texttt{Greedy \texorpdfstring{\!\!$K$}{K}-Means++}}

\newcommand{\onerun}{\hspace{0.2em}\texttt{(1 run)}}

\newcommand{\gkmpr}{greedy \!k-means++}
\newcommand{\gkMpr}{greedy\,k-Means++}
\newcommand{\GKMpr}{Greedy\,K-Means++}
\newcommand{\Gkmpr}{Greedy\,k-means++}


\newcommand{\bkmp}{\texttt{better \texorpdfstring{$\mimi k$}{k}-means++}}
\newcommand{\bkMp}{\texttt{better\,\texorpdfstring{$\mimi k$}{k}-Means++}}
\newcommand{\BKMp}{\texttt{Better\,\texorpdfstring{$\mimi K$}{K}-Means++}}
\newcommand{\Bkmp}{\texttt{Better \texorpdfstring{$\mimi k$}{k}-means++}}

\newcommand{\rs}{\texttt{random swap}}
\newcommand{\Rs}{\texttt{Random swap}}
\newcommand{\rsc}{random swap clustering}
\newcommand{\Rsc}{Random swap clustering}

\newcommand{\ga}{\texttt{genetic algorithm}}
\newcommand{\Ga}{\texttt{Genetic Algorithm}}

% Roman km++
\newcommand{\kmpr}{k-means++}
\newcommand{\kMpr}{k-Means++}
\newcommand{\KMpr}{K-Means++}

\newcommand{\noopt}{generic}
\newcommand{\Tkmp}{T_{\text{kmp}}}
\newcommand{\tkmp}{t_{\text{km++}}}
\newcommand{\Tbkm}{T_{\text{bkm}}}
\newcommand{\tbkm}{t_{\text{bkm}}}
\newcommand{\La}{Lloyd´s algorithm}
\newcommand{\LA}{Lloyd\texorpdfstring{´}{'}s Algorithm}
\newcommand{\gla}{\texttt{GLA}}
\newcommand{\GLAS}{\mbox{GLA}}
\newcommand{\mdefault}{5}

\newcommand{\GLa}{\texttt{Generalized Lloyd algorithm}}
\newcommand{\GLA}{\texttt{Generalized Lloyd Algorithm}}
\newcommand{\gLa}{\texttt{generalized Lloyd algorithm}}
\newcommand{\gLA}{\texttt{generalized Lloyd Algorithm}}

%\newcommand{\ssekmp}{\mbox{SSE}_{\text{km++}}}
%\newcommand{\ssekmp}{\mbox{SSE}(\text{km++})}
%\newcommand{\ssekmp}{\phi(\text{km++})}
\newcommand{\ssekmp}{\phi_{\text{km++}}}
%\newcommand{\ssekmpmean}{\overline{\mbox{SSE}_{\text{km++}}}}
\newcommand{\ssekmpmean}{\overline{\ssekmp}}

%\newcommand{\ssebkm}{\mbox{SSE}_{\text{bkm}}}
%\newcommand{\ssebkm}{\mbox{SSE}(\text{bkm})}
%\newcommand{\ssebkm}{\phi(\text{bkm})}
\newcommand{\ssebkm}{\phi_{\text{bkm}}}
\newcommand{\ssebkmmean}{\overline{\ssebkm}}


%\newcommand{\cpukmp}{\mbox{CPU}_{\text{km++}}}
%\newcommand{\cpukmp}{\mbox{CPU}(\text{km++})}
%\newcommand{\cpukmp}{\tkmp}
\newcommand{\cpukmp}{\tkmp}
\newcommand{\cpukmpmean}{\overline{\cpukmp}}


%\newcommand{\cpubkm}{\mbox{CPU}_{\text{bkm}}}
%\newcommand{\cpubkm}{\mbox{CPU}(\text{bkm})}
\newcommand{\cpubkm}{\tbkm}
\newcommand{\cpubkmmean}{\overline{\cpubkm}}

%\newcommand{\sseopt}{\mbox{SSE}_{\text{opt}}}
\newcommand{\sseopt}{\phi_{\text{opt}}}

%\newcommand{\delsse}{\Delta \mbox{SSE}}
\newcommand{\delsse}{\Delta \phi}
\newcommand{\delssemean}{\overline{\delsse}}
%\newcommand{\delcpu}{\Delta \mbox{CPU}}
\newcommand{\delcpu}{\Delta t}
\newcommand{\delcpumean}{\overline{\Delta \mbox{CPU}}}

\newcommand{\deloptkmp}{\Delta \mbox{opt}_{\text{km++}}}
\newcommand{\deloptbkm}{\Delta \mbox{opt}_{\text{bkm}}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min} 

\newcommand{\skl}{\texttt{scikit-learn}}
\newcommand{\Skl}{\texttt{Scikit-learn}}
\newcommand{\kmc}{\texttt{KMeans}}
\newcommand{\bkmc}{\texttt{BKMeans}}
\newcommand{\assign}{=}
\newcommand{\givenx}{$\mx \assign \{x_1,x_2,\dots,x_n\}, x_i \in \mathbb{R}^d$\tcc*[r]{data set}}

\newcommand{\meanx}{$\mc \assign \{c_1\}, \;\mbox{with} \; c_1=\overline{\mx}$\tcc*[r]{codebook consisting only of data set mean}}

\newcommand{\randomc}{$\mc \assign \{c_1,\ldots,c_k\}$\tcc*[r]{random seeding from $\mx$}}

\pdfstringdefDisableCommands{\let\Bbbk\relax%
\def\'{\relax}%
}   

\setlength\fboxrule{0.8pt} % default is 0.4 
% according to https://comp.text.tex.narkive.com/ckiOVO7i/fbox-thickness-of-the-lines



\newenvironment{myindentpar}[1]%
{\begin{list}{}%
		{\setlength{\leftmargin}{#1}}%
		\item[]%
	}%
	{\end{list}}

\newenvironment{myitemize}
{ \begin{itemize}
		\setlength{\itemsep}{0pt}
		\setlength{\parskip}{0pt}
		\setlength{\parsep}{0pt}     }
	{ \end{itemize}                  } 

\pdfstringdefDisableCommands{%
	\def\\{}%
	\def\texttt#1{<#1>}%
}
% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\ifjmlr
  \jmlrheading{1}{202x}{xx-yy}{xx/24}{mm/yy}{fritzke24a}{Bernd Fritzke}
\fi


\ShortHeadings{Breathing $K$-Means}{Fritzke}

% common order of figures and tables
\makeatletter
\let\ftype@table\ftype@figure
\makeatother

\begin{document}


\NewEnviron{NORMAL}{% 
    \scalebox{2}{$\BODY$} 
} 
\NewEnviron{LARGER}{% 
    \scalebox{1.5}{$\BODY$} 
} 
 
\NewEnviron{HUGE}{% 
    \scalebox{5}{$\BODY$} 
} 

\title{Breathing K-Means: Superior K-Means Solutions through Dynamic K-Values}%: 
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{\name Bernd Fritzke \email fritzke@web.de \\
	\addr 61381 Friedrichsdorf\\
    Lindenstr.~4\\
	Germany
}

\ifjmlr
\editor{First1 Last1 and First2 Last2}
\fi

\maketitle

\begin{abstract}%
    We introduce the \bkm{} algorithm, which on average significantly improves solutions obtained by the widely-known \gkmp{} algorithm, the default method for \km{} clustering in the \skl{} package. The improvements are achieved through a novel "breathing" technique, that cyclically increases and decreases the number of centroids based on local error and utility measures. We conducted experiments using \gkmp{} as a baseline, comparing it with \bkm{} and five other \km{} algorithms. Among the methods investigated, only \bkm{} and \bkmp{} consistently outperformed the baseline, with \bkm{} demonstrating a substantial lead. This superior performance was maintained even when comparing the best result of ten runs for all other algorithms to a single run of \bkm, highlighting its effectiveness and speed. Our findings indicate that the \bkm{} algorithm outperforms the other \km{} techniques, especially \gkmp{} with ten repetitions, which it dominates in both solution quality and speed. This positions \bkm{} (with the built-in initialization by a single run of \gkmp) as a superior alternative to running \gkmp{} on its own.
\end{abstract}

\begin{keywords}
 k-means, k-means++, generalized Lloyd algorithm, clustering, vector quantization, scikit-learn 
\end{keywords}

	\section{Introduction}
   This section defines the \km{} problem and describes the classic \GLA{} (a.k.a.~\km{} algorithm), the \kmp{} algorithm, and its widely-used variant, \gkmp. 
	\subsection{The \KM{} Problem}
	A common task in data analysis or compression is to describe an extensive data
	set consisting of numeric vectors by a smaller set of representative vectors,
	often called \emph{centroids}. This task is known as the \km{} problem. 
	
	We assume an integer $k$ and a set of $n$ data points $\mx
	\subset \mathbb{R}^d$. The \emph{\km{} problem} is to position a set $\mc =
	\{ c_1,c_2, \ldots\,,c_k\}$ of $k$ $d$-dimensional centroids such that the error
	function

	\begin{equation}
	\phi(\mc,\mx) = \sum_{x\in\mx}
	\min\limits_{c\in\mc} ||x-c||^2 \label{eqn:1}
	\end{equation}
	is minimized. 
We will also refer to $\phi(\mc,\mx)$ as
	\emph{Summed Squared Error} or shortly SSE. In the context of vector
	quantization, the centroid set $\mc$ is called a 
	\emph{codebook}, centroids are referred to as \emph{codebook vectors}, and 
	$\phi(\mc,\mx)$ is denoted as \emph{quantization error}.

    For each centroid $c_i$, one can determine its so-called \emph{Voronoi set},
		which is the  set  $C_i$ of data points for which $c_i$ is the nearest centroid:
		\begin{equation}\label{eqn:voro}
		C_i = \{x \in \mx\,|\; \|x-c_i\| < \|x-c_j\|\, \forall j\neq i\}
		\end{equation} \;

    A necessary but not sufficient condition for a solution $\mc$ to be optimal is the fulfillment of the \emph{centroid condition}: Each centroid $c_i \in \mc$ must be the mean of its Voronoi set $C_i$:
        \begin{equation}\label{eqn:centroid}
        c_i = \frac{1}{|C_i|}\sum_{x\in C_i} x
        \end{equation} \;

        While the term  \emph{centroid} typically refers to the mean of a Voronoi set, we will use it in this article to generally denote a codebook vector, even if is not yet the mean of its Voronoi set. We will also use the term \emph{codebook} instead of \emph{centroid set} for brevity. 

	
	Finding the optimal solution to the \km{} problem is known to be NP-hard
	\citep{Aloise2009}. Therefore, approximation algorithms are used to find a solution with an SSE as low as possible.
	
	
	Please note: In this article, we are not concerned with the general clustering problem or whether solutions to the \km{} problem lead to
	``good'' or even ``correct'' clusterings. We also do not require the data to fulfill any pre-conditions or criteria beyond the above definition of the \km{} problem. We are exclusively interested in minimizing the SSE as defined in Equation~\eqref{eqn:1} for a given data set $\mx$ and a given value of $k$. 
	
	\subsection{The Generalized Lloyd Algorithm} \label{sec:gla}

    The \GLA{}, proposed by \cite{Linde1980}, is a multidimensional version of a scalar quantization method initially proposed by John Stuart Lloyd in a 1957 technical report and published 25 years later \citep{Lloyd1982}. It differs from the \km{} algorithm proposed by MacQueen (1967) and described in Section \ref{sec:MacQueen}. Despite common misconceptions, the \GLA{} is not synonymous with 'the' \km{} algorithm, as several \km{} algorithms exist.
    

Defined in Algorithm~\ref{alg:gla}, the \GLA{} starts with the \emph{seeding} step (the initial codebook choice), followed by repeated \emph{Lloyd iterations} as long as the SSE decreases. Alternatively, it can stop when the relative SSE improvement falls below a certain threshold.


\begin{algorithm}
    \DontPrintSemicolon
    \givenx
    $\mc = \{ c_1,c_2, \ldots\,,c_k\},\, c_i \in \mathbb{R}^d$.\tcc*[r]{seeding} 
    \Repeat 
    ( /* Lloyd Iteration */ )
    {$C$ $\mbox{no longer changes}$}{
        $\bullet\;$ Determine for each centroid $c_i$ its Voronoi set $C_i$.\;
            $C_i = \{x \in \mx\,|\; \|x-c_i\| < \|x-c_j\|\, \forall j\neq i\},\;\; \forall i, i \in \{1, \ldots,\,k\}$
            \; 
        $\bullet\;
        $ Move each centroid $c_i$ to the center of gravity of its Voronoi set:
		$c_i = \frac{1}{|C_i|} \sum\limits_{x\in C_i}x,\;\; \forall i \in \{1, \ldots,\,k\}$ 
    }

    \caption{The \GLA}\label{alg:gla}
\end{algorithm}


The algorithm is proven to converge in finite steps \citep{Selim1984}, but solution quality can vary greatly depending on seeding. Hence, it is common to perform multiple runs with different seedings and select the best result \citep{Franti2019b}.
	
	\subsection{\KMp} \label{sec:kmp}
	\citet{Arthur2007} proposed \kmp, a specific way of seeding the \linebreak \gLa. Centroids are sequentially added by randomly selecting from the data set. The probability of a data point x to be selected is proportional to its quadratic distance to the nearest centroid already in the current codebook (see Algorithm~\ref{alg:kmp}).

    \begin{algorithm}
        \DontPrintSemicolon
        \givenx
        $\mc = \{c_1\}$, with $c_1$ chosen at random from $\mx$\tcc*{start with one centroid}
        \Repeat{$|C| \assign k$}{
            $\bullet\;$ for each $x \in \mx$ let $D(x)$ be the distance of $x$ to the nearest centroid $c \in \mc$\;
            $\bullet\;$ Select a new centroid $q$, choosing $x \in \mx$ with
			probability $P(x) = \frac{D(x)^2}{\sum_{x \in \mx}D(x)^2}$.

            $\bullet\;$ $ \mc \assign \mc \cup \{q\}$\tcc*{add $q$ to the set of centroids}
        }
        $\mc = \mbox{GLA}(\mc,\mx)$ \tcc*{apply the \gLa{}}
        \caption{\Kmp}\label{alg:kmp}
    \end{algorithm}
	
     \citet{Arthur2007} proved the following theorem providing an upper bound for the expected error $E[\phi]$ of a \kmp{} seeding:\\
	
	\setlength\parindent{24pt} \textsc{Theorem.} For any set of data points, $E[\phi] \le 8(\mbox{log}\, k + 2)\phi_{OPT}$\\
	
	\setlength\parindent{0pt}
Thereby, $\phi_{OPT}$ is the error of the optimal solution. 
The theorem provides a significant theoretical improvement over random initialization, which lacks an upper bound for expected error. While seeding is only the algorithm's initial phase, subsequent Lloyd iterations often lead to substantial error reduction. Yet, there is a lack of theoretical evidence quantifying the expected error reduction during this post-seeding phase.
\subsection{\GKMp}\label{sec:gkmp}

\Gkmp, a variant of \kmp{} introduced by \cite{Arthur2007}, draws multiple centroid candidates in each step, choosing the one that maximizes overall error reduction. This reduces the chance of two closely located centroids, which could limit error reduction. Despite reporting improved solution quality, the $\mathcal{O}(\log{}k)$ approximation no longer holds, as confirmed by \cite{DBLP:conf/esa/BhattacharyaER020}.

The Python library, \skl, uses \gkmp{} as its default seeding method, making it the most commonly used seeding approach. It served as our baseline for experimental evaluations. The default number of candidates drawn per step in \skl{} is $\mbox{n\_local\_trials} = 2 + \lfloor \log(k)\rfloor$, resulting for example in 4 for $k=10$, 6 for $k=100$, and 8 for $k=1000$.

	\section{\BKM}\label{sec:bkm}
In this section, we motivate and define the core components of the proposed approach before presenting the complete algorithm.
	\subsection{Algorithm~Outline}

    The \gLa{} is deterministic and only performs local movements of its centroids (by moving them to the center of gravity of their associated data points). This makes this approach very dependent on the initial seeding. To overcome this locality, we added so-called "breathing cycles" consisting of the following steps which are executed after one initial execution of the \gLa:
	
	\begin{enumerate}
		\item Insert $m$ additional centroids (``breathe in'').
		\item Run the \gLa{} on the resulting enlarged codebook of size $k+m$.
		\item Delete $m$ centroids (``breathe out'').
		\item Run the \gLa{} on the resulting  codebook of size $k$.
	\end{enumerate}

    The purpose of a breathing cycle is to position the $m$ additional centroids to minimize the SSE, and subsequently remove $m$ centroids without significantly increasing the SSE thus leading to an improved solution with $k$ centroids. Usually, the removed centroids differ from the added ones, effectively leading to non-local movements of the centroids. 
	
	Because of the periodic changes in codebook size, we refer to the new algorithm as ``\bkm.'' Several questions must be addressed to complete the description of the approach, which are covered in the following sections: 
	\begin{itemize}
		\item Where should new centroids be inserted during the ``breathe in'' step?
		\item Which centroids should be deleted in the ``breathe out'' step?
		\item When should the algorithm terminate?
	\end{itemize}

	\subsection{Breathe In: Adding Centroids Based on High Error}
	An established strategy \citep{Fritzke93, Fritzke95} for minimizing error, regardless of the underlying data distribution, involves adding new centroids near those generating significant errors in quantizing their Voronoi sets.
	Let us denote with $d(x,\mc)$ 
the \emph{quantization error} made for data point $x$ using the
	codebook $\mc$, i.e.,\ the squared distance between $x$ and the nearest centroid in $\mc$, defined as
	
	\begin{equation*}
		d(x,\mc) = \min_{c_i\in \mc} \|x-c_i\|^2.
	\end{equation*}
	
	Given a codebook $\mc$ and a data set $\mx$, we define for each centroid $c_i \in \mc$ its associated error $\phi(c_i)$
	as 
	\begin{equation}
	\label{eqn:Err}
	\phi(c_i) = \sum_{x \in C_i} d(x,\mc),
	\end{equation} 
which is the sum of all $d(x,\mc)$-values over its Voronoi set $C_i$ (defined in Equation~\ref{eqn:voro}).

We can now define the set of those $m$ centroids, which will serve as  anchors for placing new centroids as 
\begin{equation}
\mathcal{M} = (\mbox{the $m$ centroids with the largest associated error $\phi(c)$})
\end{equation}
	
\newcommand{\rmse}{\mbox{RMSE}(\mc,\mx)}
	One new centroid will be inserted near the position of each centroid in $\mathcal{M}$, modified by adding a small random offset vector $v$ to ensure
	distinct centroid values. To be independent of the scaling of the data, 
	we set the length of these offset vectors proportional to the root-mean-square error $\rmse$, defined as 
	
	$$  \rmse = \sqrt{\phi(\mc,\mx) / |\mx|}.
	$$
	

	Accordingly, we compute each offset vector $v$ as 
	\begin{equation}
	v = \epsilon \rmse u\label{eqn:offset}
	\end{equation}
	with a small constant $\epsilon$ and a random vector $u$ drawn uniformly from the
	$d$-dimensional unit hypercube centered at the origin. This leads to the following set $\mathcal{D}^+$ of new centroids
\begin{equation}
\mathcal{D}^+ = \{c+v|c\in\mathcal{M}\} \;\mbox{with each $v$ being an offset vector according to \eqref{eqn:offset}}.
\end{equation}

The set $\mathcal{D}^+$ is added to the current codebook to finalize the ``breathe in'' step:
\begin{equation}
\mc \leftarrow \mc \cup \mathcal{D}^+
\end{equation}
	
	\subsection{Breathe Out: Removing Centroids Based on Low Utility}\label{sec:breatheout}
	Removing centroids inevitably increases the SSE. To minimize this effect, we select for removal the $m$ centroids causing the smallest
	error increase. Fortunately, the subsequent run of the \gLa{} will lower the resulting SSE again to some degree.
	
	Following \cite{Fritzke1997},  we define the \emph{Utility} $U(c_i)$ of a
	given centroid $c_i$ as 
	
	\begin{equation}\label{eqn:util}
	U(c_i) = \phi(\mc\setminus \{c_i\},\mx) -
	\phi(\mc,\mx).
	\end{equation} 
The utility measures the increase in the overall error caused by removing $c_i$ from the original codebook $\mc$. If this difference is significant, then $c_i$ is considered useful.
	
	Using the definition of $\phi(\mc,\mx)$ in Eq.~(\ref{eqn:1}), the utility of a centroid $c_i$ can be expressed as

	\begin{align}
	U(c_i) & = \sum_{x \in \mx} d(x,\mc\setminus\{c_i\})- d(x,\mc)\nonumber\\
	& = \sum_{x \in C_i} d(x,\mc\setminus\{c_i\})- d(x,\mc)
\;+\; \sum_{x \notin C_i} \underbrace{d(x,\mc\setminus\{c_i\})- d(x,\mc)}_{0} \label{eqn:vororeg}\\
& = \sum_{x \in C_i} d(x,\mc\setminus\{c_i\})- d(x,\mc)\label{eqn:utifinal}.
	\end{align}

The second sum in Equation~\eqref{eqn:vororeg} contains only zero summands since for any $x$ outside the Voronoi region $\mc_i$ the following holds (and makes the terms in the second sum to be zero):
$$\mathlarger{\mathlarger{\forall}}_{x\notin\mc_i} \,\mathlarger{\mathlarger{\exists}}_{j, j\ne i} : x\in\mc_j \land \, d(x,\mc\setminus\{c_i\}) =  \|x-c_j\| = d(x,\mc).$$

 Thus, the
	utility $U(c_i)$ of a centroid $c_i$ only depends on the data points in its Voronoi set $C_i$. Moreover, the utility is always non-negative. This follows from the fact that the expression $d(x,\mc\setminus\{c_i\})- d(x,\mc)$ inside the sum in Equation~\eqref{eqn:utifinal} is non-negative since $d(x,\mc\setminus\{c_i\} \ge d(x,\mc)$.

This expression inside the sum in Equation~\eqref{eqn:utifinal} can be denoted as the utility $U_x(c_i)$ of the centroid $c_i$ for a particular data point $x\in\mx$:
\begin{equation*}
U_x(c_i) = d(x,\mc\setminus\{c_i\})- d(x,\mc)
\end{equation*}
The overall utility can now be expressed as the sum of the utilities of the individual data points in the Voronoi region of $c_i$:
\begin{equation*}
U(c_i) = \sum_{x \in C_i} U_x(c_i)
\end{equation*}



The utility of a centroid, $U_x(c_i)$, only becomes zero when another centroid, $c_j$, is at the same distance from $x$. This happens when $x$ lies on the so-called \emph{bisecting normal hyperplane} of $c_i$ and $c_j$, an event with practically zero probability assuming random positions of $X$ and $C$. Similarly, the complete utility of a centroid $c_i$, $U(c_i)$, becomes zero only when all its associated data points lie on bisecting hyperplanes, another event of virtually zero probability.
Figure~\ref{fig:util_simple} illustrates the error and utility values for a simple \km{} problem.	
\newcommand{\bibi}{0.35}
	\begin{figure}[tb]%
		\centering
		\subfloat[Error values of the centroids]
		{\makebox[\bibi\linewidth][c]
			{{\includegraphics[align=c,width=\bibi\linewidth]{imgX/err_simple.png} }}}
		~
		\subfloat[Utility values of the centroids]
		{\makebox[\bibi\linewidth][c]
			{{\includegraphics[align=c,width=\bibi\linewidth]{imgX/util_simple.png} }}}
		\caption{   
Error and utility values are shown for a problem with data from six equal Gaussian kernels and $k=6$, each centroid placed at a cluster center. While error values are similar, the utilities of centroids differ based on the distance between the nearest and second-nearest centroids. The most useful centroid is in cluster A, while the least useful is in cluster D, followed by those in B and C.}
			\label{fig:util_simple}%
	\end{figure}

	To reduce the codebook back to its original size, one might consider deleting the $m$ centroids with the lowest utility values. However, there is a fundamental flaw in  this approach: If the
	distance between two centroids, $c_i$, and $c_j$, is small, also their utility values $U(c_i)$ and $U(c_j)$ are small because they mutually act as the second-nearest centroid for their
	Voronoi sets (see below). Both seem rather ``useless.'' However, removing both $c_i$ and $c_j$ can lead to a colossal error increase, as becomes evident further below.

Let us first calculate what happens to the utility values of two centroids approaching each other:
	\begin{align*}\lim_{c_i\rightarrow c_j} U(c_i) 
& = \lim_{c_i\rightarrow c_j} \sum_{x \in C_i} \underbrace{d(x,\mc\setminus\{c_i\})}_{\le \|x-c_j\|^2}- 
\underbrace{d(x,\mc)}_{\|x-c_i\|^2}\\
& \le \lim_{c_i\rightarrow c_j} \sum_{x \in C_i} \|x-c_j\|^2 - \|x-c_i\|^2 \\
& = 0 \;\;\;\mbox{for all}\, c_i,\,c_j \in \mc, i\ne j
\end{align*}
Since also $U(c_i) \ge 0$ holds and because of symmetry reasons, the following is fulfilled:

\begin{equation*}
\lim_{c_i\rightarrow c_j} U(c_i) = \lim_{c_j\rightarrow c_i} U(c_j) = 0\;\;\;\mbox{for all}\, c_i,\,c_j \in \mc, i\ne j
\end{equation*}

As centroids move closer, their utility values decrease and become zero if they are identical, as they can perfectly substitute for each other in quantizing data points. However, removing such neighboring centroids because of low utility can drastically increase error, especially if the next nearest centroid is far away (see Figure~\ref{fig:utilex}). This often occurs in datasets with isolated smaller clusters, where data points from these close centroids are quantized by a distant centroid, leading to substantial error. To counter this, we introduce a "freezing" mechanism to prevent the concurrent removal of neighboring centroids.
\newcommand{\wii}{0.33}
\begin{figure}[t]
	\centering
	\subfloat[Two neighboring centroids with low utility values (red).]
	{\makebox[\wii\linewidth][c]
		{{\includegraphics[align=c,width=\wii\linewidth]{imgX/utilex1.png} }}}
	~
	\subfloat[Removing one of them makes the other one very useful (red).]
	{\makebox[\wii\linewidth][c]
		{{\includegraphics[align=c,width=\wii\linewidth]{imgX/utilex3.png} }}}
~
	\subfloat[Removing also the second one causes a huge overall error.]
	{\makebox[\wii\linewidth][c]
		{{\includegraphics[align=c,width=\wii\linewidth]{imgX/utilex4.png} }}}
	\caption{The problem of misleading utility values of close neighbors. a) The two centroids in the small cluster A exhibit low utilities (red). b) Eliminating one of them marginally escalates the error $\phi$, while the remaining one sees its utility spike. c)~The simultaneous removal of the second centroid from A leads to an enormous total error (226.1), and the closest centroid to A becomes highly useful (84.4).
		\label{fig:utilex}}%
    \end{figure}
\subsection{Freezing The Nearest Neighbors}\label{sec:freezing}
	How can we avoid a significant error increase in the ``Breathe out'' step because of the removal
	of neighboring centroids? One possible solution would be to remove one centroid at
	a time, run the \gLa, recompute the utility, remove the next centroid, and so on. This strategy avoids large error increases, though it may come at the cost of high computational demands due to the many required runs of the \gLa . 

To enable the simultaneous removal of multiple centroids, we take the following approach:

\begin{enumerate}
    \item Initialize empty sets for "frozen" centroids ($\mathcal{F}$) and centroids to be removed ($\mathcal{D}^-$).
    \item Rank the centroids by increasing utility.
    \item Scan through the centroids; skip "frozen" ones. Add the first non-frozen centroid to $\mathcal{D}^-$.
    \item After selecting a centroid for removal, add its nearest neighbor to $\mathcal{F}$ (i.e., "freeze" it).
    \item Repeat steps 3 and 4 until $|\mathcal{D}^-|$ equals $m$.
\end{enumerate}

One can construct cases where the above procedure would deliver less than $m$ centroids to remove since too many have been ``frozen.'' To prevent this, we perform freezing (step 4) only as long as the following condition holds:
%no numbering
\begin{equation*}
|\mathcal{F}| + m < |\mc|.
\end{equation*} 
Together with this condition, the above strategy effectively prevents the problematic case of concurrently removing two closely neighboring centroids. 

	\subsection{Ensuring Termination}
	To define a termination criterion, we demand a decrease in error after each ``breathe out'' step (the error after a
	``breathe in'' step is irrelevant because of the enlarged number of centroids). Moreover, we empirically found that once the error stops sinking for a given value of $m$, additional breathing steps with reduced $m$-values  can further lower the error.	
	The above results in the simple approach to guarantee termination shown in Algorithm~\ref{alg:termination}:
    \begin{algorithm}
        \DontPrintSemicolon

        $m \assign m_0$.\tcc*[r]{initialize the breathing size}
        $\phi_{\text{best}} \assign \infty$.\tcc*[r]{initialize the error}
        $tol = $(small positive number, e.g., 0.0001) \tcc*[r]{tolerance for error decrease}
        \Repeat
		( /* breathing cycles */ )
		{
			$m = 0$
		}{
            Perform one breathing cycle with the current $m$.\;
            Compute current error $\phi$.\;    
            \eIf(\tcc*[f]{error improved sufficiently?}){$(\phi_{best}-\phi)/\phi_{best} > \mbox{\text{tol}}$}{
                $\phi_{\text{best}} \assign \phi$.\tcc*[r]{update the best error}
                } {
                $m \assign m - 1$.\tcc*[r]{decrement breathing size}
            }
        }
        \caption{Ensuring Termination}\label{alg:termination}
    \end{algorithm}
	For each value of $m$, breathing cycles are repeated as long as the error $\phi$ strictly decreases, which each time requires finding a previously unseen solution. Since both $m_0$ and the number of partitions of the data into $k$ Voronoi sets are finite and positive, termination occurs in finitely many steps.

	\subsection{The \texorpdfstring{\BKM}{Breathing K-Means}{} Algorithm in Pseudo-Code}
	The complete algorithm in pseudo-code is shown in Figure
	\ref*{alg:bkm}. 

	\begin{algorithm}[thp]
		%\small
        \givenx
		$m\assign m_0$ (default: \mdefault) \tcc*[r]{number of centroids to add and remove} 
		$k\assign k_0$\tcc*[r]{the $k$ in \km}
		\SetKw{ini}{Seeding:}
		\SetKw{cont}{continue}
		\SetKw{break}{break}
		\DontPrintSemicolon
		%\linesnumbered
		$\mc \assign$ (result of \gkmp{} without repetition)\tcc*{seeding}
		$\mbox{tol}\assign\mbox{tol}_0$ (default: 0.0001) \tcc*[r]{tolerance to declare convergence} 		
		$\phi_{best} \assign \phi(\mc,\mx)$ \tcc*[r]{store best error so far}
		$\mc_{best} \assign C$  \tcc*[r]{store best codebook so far}
		
		\Repeat
		( /* breathing cycles */ )
		{
			$m = 0$
		}{
            {\centering \bfseries breathe in\par}

			(Compute error $\phi(c)$ for each $c \in \mc$)
			\tcc*[r]{see Eq.(\ref{eqn:Err})}
			$c_1,\,c_2,\, \ldots,\,c_{m},\,\ldots,\,c_k = \mbox{partial\_sort\_by\_error}(\mc,\mbox{``descending''},m)$\;
			\tcc*[r]{first $m$ centroids sorted}
			
			$\mathcal{M} \assign \{c_1,\,c_2,\, \ldots,\,c_{m}\}$ 
\tcc*[r]{subset of $m$ largest-error centroids}
			$\mathcal{D}^+ = \{c+v|c\in\mathcal{M}\} \;\mbox{with offset vectors $v$   according to Eq.~\eqref{eqn:offset}}$\;

			$\mc \assign \mc \cup \mathcal{D}^+$ 
			\tcc*[r]{insert $m$ additional centroids ("breathe in")}
			$\mc \assign $\gla($\mc,\mx$)
			\tcc*[r]{run the generalized Lloyd algorithm}
            {\centering \bfseries breathe out\par}

			(Compute utility $U(c)$ for each $c \in \mc$.)
			\tcc*[r]{see Eq.(\ref{eqn:util})}
			$c_1, c_2,\, \ldots,\,c_{k+m} = \mbox{sort\_by\_utility}(\mc,\mbox{``ascending''})$
			\tcc*[r]{sorted sequence}			
$\mathcal{D}^-\assign\varnothing$
			\tcc*[r]{initialize set of to-be-deleted centroids}
			$\mathcal{F}\assign\varnothing$
\tcc*[r]{initialize set of frozen centroids}
			\ForAll
			{
				$c \;\mbox{in} \;(c_1, c_2,\, \ldots,\,c_{k+m})$
			}{
				\If (\tcc*[f]{only remove un-frozen centroids}){$c \notin \mathcal{F}$}{ 


					$\mathcal{D}^-\assign\mathcal{D}^- \cup \{ c\} $
					\tcc*[r]{add centroid to to-be-deleted set}


				\If(\tcc*[f]{not yet too many centroids frozen}){$|\mathcal{F}| + m < |\mc|$}{		
	        $\hat{c} \assign \argmin_{x\in \mc\setminus\{c\}} \|	c-x\|$	
	                    \tcc*[r]{find nearest neighbor $\hat{c}$ of $c$}	                       
						$\mathcal{F}\assign\mathcal{F} \cup \{ \hat{c}\} $ 
						\tcc*[r]{freeze nearest neighbor $\hat{c}$}
					}
					
					\If(\tcc*[f]{found $m$ centroids to delete}){$|\mathcal{D}^-| = m$}{
						\break
					}
				}
				
				
			} 
			$\mc\assign\mc\setminus \mathcal{D}^-$
			\tcc*[r]{delete $m$ centroids  ("breathe out")}
			$\mc \assign $\gla($\mc,\mx$)
			\tcc*[r]{run the generalized Lloyd algorithm}
            {\centering \bfseries possibly reduce ``breathing depth''\par}
		
			\eIf{$(\phi_{best}-\phi(\mc,\mx))/\phi_{best} > \mbox{\text{tol}}$}{
				$\phi_{best} \assign \phi(\mc,\mx)$
				\tcc*[r]{improvement:~update best error} 
				$\mc_{best} \assign C$
				\tcc*[r]{update best codebook} 
			}{
				$m\assign m-1$
				\tcc*[r]{no improvement:~reduce "breathing depth"} 
			}
		}
		\Return{$\mc_{best}$}\;
		\caption{The \BKM{} Algorithm}
		
		\label{alg:bkm}	
	\end{algorithm}

\section{Related Work}\label{sec:related}
The literature on algorithms for the \km{} problem is vast and can not be fully surveyed here. In the following,  
we describe two relevant groups of approaches. The first group contains methods for finding a good seeding of the centroids before finally running the \gLa. The second group employs the \gLa{} also in intermediate phases or not at all.
\subsection{Seeding Methods}\label{sec:initialization}
\label{sec:ini}	
	Many methods proposed in the literature focus on finding a seeding used as a starting configuration for the \gLa . 
    Here several relevant examples are described in the order they were historically developed.

\subsubsection{Forgy's Method}
	\cite{Forgy65} randomly assigns each data point to a cluster and then calculates the centroids as the means of these clusters. Consequently, all centroids are typically very close together near the mean of the whole data set, and one can expect a large number of Lloyd iterations before convergence.
 
    \subsubsection{MacQueens First Method}\label{sec:macqueen1}
    In his first method, \cite{MacQueen1967} proposed using the first $k$ elements of the data set $\mx$ as initial centroids. A drawback of this method is that it may initialize all centroids to similar positions in the case of ordered data.

\subsubsection{MacQueens's Second Method}\label{sec:macqueen2}
In his second (and more popular) method, \cite{MacQueen1967}  proposed to pick random elements from the data set $\mx$. This avoids the possible problem of ordered data which his first method has. A drawback of this method is that it may initialize many centroids to similar positions, e.g., if the data set contains a large high-density cluster of data points and a smaller number of spaced-out data points.


\subsubsection{Maximin}	\label{sec:maximin}
	In the Maximin method \citep{Gonzales1985}, the first centroid $c_1$ is chosen arbitrarily. The $i$-th $(i \in {2, 3, . . . , k})$ centroid $c_i$
	is chosen to have the largest minimum distance to all previously selected centroids, i.e., $c_1, c_2, \hdots c_{i-1}$. The method can be seen as a deterministic ancestor of \kmp{} (see sections \ref{sec:kmp} and \ref{sec:kmp2}) and avoids positioning centroids close to each other even if the data contains high-density clusters.
\subsubsection{Method of Bradley and Fayyad}
\newcommand{\bfcm}{M}
\cite{Bradley1998} proposed a method to efficiently produce an initial codebook for large data sets. Initially, $J$ small random sub-samples $S_i, i \in \{1,\ldots,J\}$ are drawn from the original data set $\mc$,  and the \gLa{} is performed on each of the sub-samples $S_i$. Thereafter, the $J$ solutions $M_i, i \in \{1,\ldots,J\}$, are merged to a data set $M$ of size $J\times K$ on which the \gLa{} is run $J$ times with the solutions $M_i$ from the first step as seedings.  From all obtained solutions in the second step, the one with the smallest SSE when encoding $M$ is chosen.
\subsubsection{\KMpr}\label{sec:kmp2}
The \kmp{} algorithm \citep{Arthur2007} is described in detail in Section \ref{sec:kmp} and can be interpreted as a randomized version of the Maximin method (see Section \ref{sec:maximin})  since it uses a point's minimum distance to all previous centroids to set the probability of choosing this point as the next centroid.			
\subsubsection{Greedy \KMpr}			
	\Gkmp{} \citep{Arthur2007} differs from \kmp{} by drawing several new centroid candidates in each step and selecting the one that maximally reduces the overall error (see Section \ref{sec:gkmp}). \Gkmp{} is the default \km{} method for the \skl{} package \citep{scikit}.
 

\subsubsection{Better \KMpr}\label{sec:betterkmp}
The ``better'' \kmp{} variant \citep{Lattanzi2019} extends the \kmp{} initialization by continuing to select centroid candidates beyond $k$ and possibly replacing existing centroids if there is an improvement (see Algorithm~\ref{alg:betterkmp}). 

\SetKwFor{RepTimes}{repeat}{times}{end}
\begin{algorithm}
    \DontPrintSemicolon
    \givenx
    Initialize codebook with \kmp: $\mc \assign \{x_1,\dots,x_k\}, x_i \in \mx$\:

    \RepTimes{$Z$}{  
        $\bullet$ Select a new centroid candidate $q$, choosing $x \in \mx$ with
        probability $p(x)=\frac{D(x)^2}{\sum_{x \in \mx}D(x)^2}$ whereby $D(x)$ denotes the distance from a data point $x$ to the nearest centroid in $\mc$.
        $D(x) \assign \min_{c \in C} \|x-c\|$\;
        $\bullet$ Compute the minimal error $\phi_{min}$ resulting from replacing one of the centroids in $\mc$ with $q$:
        %\begin{equation}
            $\phi_{min} \assign \min_{i \in \{1,\dots,k\}} \phi(\mc\setminus \{c_i\} \cup \{q\} ))$  
        %\end{equation}
        \;
        $\bullet$ \If{
            $\phi_{min} < \phi(\mc,\mx)$
            }{
                Perform the replacement resulting in the minimal error $\phi_{min}$
            }
    }
    return $\mc_{best}$
    \caption{Better K-Means++}\label{alg:betterkmp}
    \end{algorithm}


\cite{Lattanzi2019} proved the following theorem guaranteeing that with a sufficiently large computational budget (parameter Z), the expected cost of the solution produced by \bkmp{} will be close to the optimal cost (within a constant factor).

\begin{theorem} 
    Let $P \subseteq \mathbb{R}^{d}$
be a set of points and C be the
output of Algorithm~1 with $Z \ge 100000k \log \log k$ then we
have $E[cost(P, C)] \in O(cost(P, C^{*}))$, 
where $C^{*}$ is the set of optimum centers. The algorithm's running time is
$O(dnk^2 \log \log k)$.
\end{theorem}
\subsection{Integrated Methods}
The approaches described here %, although often not similiar to each other,  
have in common that they cannot be described as seeding methods for the \gLa. Rather, they perform various operations on the codebook (e.g., splitting, merging, adding, removing, or replacing centroids), and most of them alternate this with Lloyd iterations.% or (largely equivalent) online adaptations. 

\subsubsection{MacQueen's \KM{}}\label{sec:MacQueen}
\cite{MacQueen1967} proposed an algorithm he called  \km{} (thereby coining the term \km) described as follows (excerpt from the article):
\begin{quote}
Informally, the \km{} procedure consists of simply starting with $k$ groups, each consisting of a single random point, and then adding each new point to the group whose mean the new point is nearest. After a point is added to a group, the mean of that group is adjusted to take the new point into account. Thus at each stage, the $k$ means are, in fact, the means of the groups they represent (hence the term \km{}).
\end{quote}

 This highly efficient algorithm recalculates the mean (centroid) by shifting towards the new point by $\frac{1}{n}$  of total distance upon adding the n-th point to a group. While the means situate at the gravity center of all nearest points when added, a full data sweep does not always ensure the centroid condition, implying each mean is not necessarily at the gravity center of its Voronoi set. As the centroid condition is key for optimality, additional Lloyd iterations often enhance MacQueen's \km{} solutions, even if only towards a local optimum.

\begin{comment}
(see Algorithm~\ref{alg:macqueen}, here for simplicity formulated with MacQueen's first initialization method, see Section \ref{sec:macqueen1}). 

\begin{algorithm}
    \DontPrintSemicolon
    \givenx
    $\mc \assign \{x_1,\dots,x_k\}, x_i \in \mx$ \tcc*{seeding with MacQueen's first method}
    $\mc_i \assign \{x_i\}|\forall i\in\{1,\dots,k\} $\tcc*{initialize clusters with centroids}
    \ForEach{$i\in k+1,\ldots,n$}{
        for data point $x_i$ determine the closest centroid $c_j, c_j \in \mc$ \;
        $\mc_j \assign \mc_j \cup \{x_i\}$\tcc*{add $x_i$ to $j$-th cluster}
        $c_j = c_j + \frac{1}{|\mc_j|} (x_i-c_j)$\tcc*{recompute centroid $c_j$}
    }

    \caption{MacQueen's K-Means} \label{alg:macqueen}
    \end{algorithm}
    
\end{comment}

\subsubsection{The Hartigan-Wong Algorithm}\label{sec:hawo}
% https://www.datanovia.com/en/lessons/k-means-clustering-in-r-algorith-and-practical-examples/
% says that the Hartigan-Wong algorithm is the standard algorithm for k-means clustering in R
The \hawo{} Algorithm \citep{Hartigan1979} skips Lloyd iterations. It starts by randomly choosing $k$ centroids, forming initial clusters using MacQueen’s Second Method (Section \ref{sec:macqueen2}). It then reassigns a random data point $x$ from its cluster $S$ to another cluster $T$ if it reduces the sum of intra-cluster variances of $S$ and $T$, choosing $T$ to maximize variance reduction. Termination occurs when no reassignment reduces overall variance (Algorithm~\ref{alg:hartigan}).

Hartigan and Wong's implementation introduces a \emph{Quick Transfer} phase, where $T$ is the centroid second-nearest to $x$, reducing computation. This phase iterates until no improvement occurs. The \emph{Optimal Transfer} phase, involving a complete search among all clusters (Algorithm 6), alternates with the Quick Transfer phase. Termination occurs when the Optimal Transfer phase finds no improvement.

This algorithm is the default \km{} algorithm in the \texttt{stats} package of R \citep{RCore2019}. While \cite{TelgarskyVattani2010} reported improvements over "online" \km{}, we generally found better results with \vkmp{} and \gkmp{} than \hawo{} (see Table \ref{tab:msee-high-D}).

\begin{algorithm}
    \DontPrintSemicolon
    \givenx
    Select $k$ centroids $c_1, c_2, \ldots, c_k$ randomly from $\mx$.\;
    Form  $k$ clusters $C_1, C_2, \ldots, C_k$ by assigníng each $x\in\mx$ to its nearest centroid.\;
    Recompute centroids $c_1, c_2, \ldots, c_k$ as means of their associated clusters.\;
    \Repeat{no error improvement for a complete sweep through all data points $x \in \mx$}{
        Select a data point $x \in \mx$.\;
        Let $S \in \{C_1, C_2, \ldots, C_k\}$ be the cluster to which $x$ is currently assigned.\;
        \ForAll{$T \in \{C_1, C_2, \ldots, C_k\}, T \ne S$}{
            Compute the error improvement $\Phi(x;S;T)$ of re-assigning $x$ from $S$ to $T$ taking into account the resulting centroid updates of $S$ and $T$.\;
            $\Phi(x;S;T) = \frac{|S|}{|S|-1} \|\mu(S)-x\|^2 - \frac{|T|}{|T|+1} \|\mu(T)-x\|^2$
        }
        \If{$\exists T \in \{C_1, C_2, \ldots, C_k\}, T \ne S| \Phi(x;S;T) > 0$}{
            Re-assign $x$ to cluster $T$ with $T = \argmax_T \Phi(x;S;T)$.\;
        }
    }
    \caption{Hartigan and Wong}\label{alg:hartigan}
\end{algorithm}

\subsubsection{LBG with Binary Splitting}				

	When the \gLa{} (a.k.a.~LBG) was proposed by \cite{Linde1980}, the authors also discussed a method to produce a series of increasingly large codebooks. In particular, given a codebook consisting of $m$ centroids, one can produce a codebook consisting of twice as many centroids by ``splitting'' each centroid, adding small offsets to enforce distinct values, and applying the \gLa{} to the resulting enlarged codebook. If one starts with a codebook of size one and performs $p$ splitting steps, the resulting codebook has the size $k=2^{p}$. 
    In each splitting step, all existing centroids are split. Thus, this method does not consider which centroids are
most suited for splitting to reduce the overall error. This can limit the quality of the results compared to approaches splitting based on error reduction. 
\begin{comment}    
The approach is shown in  Algorithm~\ref{alg:lbg}.

    % (\tcc{split current centroid})

% add figure to include one png file
% \begin{figure}[ht]
% \centering
% \includegraphics[width=0.8\textwidth]{svg/hawoexample.png}
% \caption{Example how \hawo{} can escape from a local minimum of the \GLA. a) one-dimensional data set $\mx = \{x_1,x_2,x_3,x_4\}$. Neighboring data points have a distance $d$, only the distance between $x_3$ and $x_4$ is $2d$. b) Local minimum of the \GLA: each data point is associated to its closest centroid. $V$ is the Voronoi border. c) \hawo{} has selected $x_3$ for re-assignment and has reassigned it to $c_1$ since the error after recomputing the centroids was lower than before.}\label{fig:lbg}
% \end{figure}

\begin{algorithm}
\DontPrintSemicolon
\givenx
$k \assign 2^p$ for $p\in \mathbb{N}$\tcc*{set $k$ to a power of 2}
\meanx
%$c_1 \assign \overline{\mx}$\tcc*{set $c_1$ to data set mean}
%$\mc \assign \{c_1\}$\tcc*[r]{start with a codebook of size 1}
\Repeat{$|\mc|=k$}{ 
    \For{$i=1$ \KwTo  $|\mc|$}{
    Replace the centroid $c_i$ with two new centroids $c_i'$ and $c_i''$ \\ with $c_i'=c_i+\epsilon$ and $c_i''=c_i-\epsilon$ for some random offset vector $\epsilon$\;
    }
    Perform the \gLa{} on the current codebook $\mc$\;
}
\caption{LBG Splitting algorithm}\label{alg:lbg}
\end{algorithm}

Consider, e.g., a data set with two well-separated Gaussian clusters with the same number of data points each, but one with a very small standard deviation $\sigma=0.00001$ and one with $\sigma=1.0$ and an assumed value of  $k=256$. The described splitting method would position 128 centroids in each cluster which is far from optimal.
\end{comment}
\subsubsection{LBG-U} \label{sec:lbgu}
\cite{Fritzke1997} proposed the \emph{LBG-U} algorithm to improve the \texttt{generalized Lloyd}{} \linebreak\texttt{algorithm} by non-local movements of centroids. Central to this approach is the concept of \emph{Utility} (thus the ``U'' in the name) initially defined in that work and also used for \bkm{} (see Equation~\ref{eqn:util}).  The core mechanism of LBG-U is to repeatedly move the least useful centroid to the centroid with maximum error and perform the \gLa{} after each such move. LBG-U delivered better solutions than the \gLa{} (\kmp{} was not yet invented) at the price of additional compute time. Lacking the idea of nearest neighbor freezing introduced in the current article, LBG-U could only insert and delete one centroid at a time which led to larger computational effort and smaller improvements than \bkm.
\begin{comment}
% (see the pseudo-code in Algorithm \ref{alg:lbgu}). 
%According to the results of Section \ref{sec:paramm}, this is quite crucial to find excellent solutions. 

\Bkm{} also differs in that after each multi-insertion step (``breathe in''), and after each multi-deletion step (``breathe out''), the \gLa{} is executed, which gives the modified codebooks a chance to re-organize before the subsequent modification. LBG-U made a move (equivalent to removing a centroid and inserting it elsewhere) in one step and performed the \gLa{} only afterward.% As can be seen in the next Section, there is now strong evidence that moving multiple centroids at once---a central feature of \bkm{} ---does often lead to much better solutions than those obtained by moving only one centroid at a time.
\begin{algorithm}
    \DontPrintSemicolon
    \givenx
    \randomc
    $\mc \assign \GLAS(\mc)$\tcc*[r]{run the generalized Lloyd algorithm}
    \Repeat{$\phi(\mc) > \phi(\mc_{best})$}{  
        $\mc_{\mathit{best}} \assign \mc$\;
        Determine $c_i \in \mc$ with maximal error.\;
        Determine $c_j \in \mc$ with minimal utility.\;
        $c_j \assign c_i + \epsilon$ \tcc*[r]{move min.~utility centroid to max.~error centroid}%$c_j$ to $c_i + \epsilon$}
        $\mc \assign \GLAS(\mc)$\tcc*[r]{run the generalized Lloyd algorithm}
        }
        Return $\mc_{best}$
        \caption{LBG-U}\label{alg:lbgu}
    \end{algorithm}
\end{comment}

\subsubsection{Splitting}\label{sec:split}
This algorithm \citep{Franti1997} starts with a codebook of size one and iteratively enlarges the codebook by a splitting procedure until it reaches size $k$. Different approaches for selecting the cluster to be split (largest variance, largest width, largest skewness) and for technically performing the splitting (fixed offset vector, random choice of new centroids, mutually furthest data points, local PCA) are discussed. Also the question of how to refine the partition of the data set and the centroids after the splitting is addressed.

% comment
\begin{comment}

The outline of the approach is shown in Algorithm~\ref{alg:splitting}.
    
    \begin{algorithm}
        \DontPrintSemicolon
        \givenx
        \meanx

        \Repeat{$|\mc|=k$}{  
            Select a cluster to be split.\;
            Split the selected cluster.\;
            Refine the partition of the data set and the centroids.   
        }
        \caption{Splitting Method}\label{alg:splitting}
        \end{algorithm}
    
    The paper discusses various alternatives for selecting the cluster to be split:

\begin{itemize}
    \item selecting the cluster with the largest variance
    \item selecting the widest cluster,
    i.e., the one with the maximal distance of the two mutually furthest
    data points in the cluster
    \item selecting the cluster having the largest skewness
\end{itemize}

Also, several methods are discussed for cluster splitting (replacing the old centroid $c_i$ with the two new centroids $c_i'$ and $c_i''$):
\begin{itemize}
    \item choosing a fixed offset vector $\epsilon$ and setting $c_i'=c_i+\epsilon$ and $c_i''=c_i-\epsilon$
    %\\ \emph{Note: The insertion strategy used by \bkm {} is rather similar to this in  particular when the cluster with the largest variance is selected in the algorithm description  above.}
    \item randomly choosing $c_i'$ and $c_i''$ from the cluster (Voronoi set) $\mc_i$ corresponding to $c_i$
    \item determining the two mutually furthest data points $x'$ and $x''$ in the cluster $\mc_i$ and setting $c_i'=\frac{1}{2}(c_i+x')$ and $c_i''=\frac{1}{2}(c_i+x'')$
    \item performing a (possibly approximated) principal component analysis (PCA) on the cluster $\mc_i$ and positioning the new centroids $c_i'$ and $c_i''$ using the first principal component. Different heuristics are discussed on how to exactly position the new centroids.%, the best of which considers for each data point $x$  in $\mc_i$ a separating hyperplane perpendicular to the first principal component and containing $x$. The hyperplane minimizing the overall error is chosen to split the current cluster into two.
\end{itemize}

Several approaches to the refinement of the centroids are discussed:
\begin{itemize}
    \item running the \gLa{} on the new codebook
    \item local re-mapping of the data points in $\mc_i$ to the new centroids $c_i'$ and $c_i''$ followed by re-computing $c_i'$ and $c_i''$ as the center of gravity of their associated data points. This operation is less expensive but also less accurate than the \gLa. 
\end{itemize}

The paper concludes with a comparison of the heuristics, thereby showing the trade-off between speed and solution quality for different variants of the splitting method.
\end{comment}
\subsubsection{Tabu Search}
\cite{Franti1998} proposed an algorithm adapted from a previous clustering method by \cite{AlSultan1995}. Tabu search generates new solution candidates through random operations, allowing for potentially worse solutions and possible cyclic behavior. To prevent non-termination, the algorithm employs a \emph{tabu list}—a record of previously visited solutions that helps avoid considering them multiple times.
For larger data sets, the exact recurrence of \km{} solutions is rare. To exclude \emph{similar}  solutions as well, the authors propose checking candidates for a minimum distance from all tabu list elements using a suitable distance measure. Two methods for the randomized generation of new solutions are considered: (1) randomly assigning a fraction of the data set to different (but nearby) clusters and (2) adding noise to existing cluster centers.
\begin{comment}
%This enhancement significantly impacts larger data sets and codebook sizes, as precisely overlapping solutions become highly improbable.

The basic structure of Fränti et al.'s (1998) algorithm is depicted in Algorithm~\ref{alg:tabu}. For larger data sets, the exact recurrence of solutions is rare. To exclude \emph{similar}  solutions, candidates are checked for a minimum distance from all tabu list elements using a suitable distance measure. This enhancement significantly impacts larger data sets and codebook sizes, as precisely overlapping solutions become highly improbable.

\begin{algorithm}
\DontPrintSemicolon
\givenx
$C_{init} = \{c_1,c_2,\ldots,c_k\}$\tcc*{generate using any existing algorithm}
$C_{curr} = C_{best} = C_{init}$\;
tabu-list \assign \{\};\;
\ForEach{$i \in \{1,2,\ldots,I\}$} { 
Generate a set of $s$ candidate solutions $Y=\{Y_1,\ldots,Y_s\}$ by making small random modifications to solution $C_{curr}$ followed by the \gLa .\;
$Y_{best} = \argmin\limits_{S \in Y} \phi(S,\mx)$\tcc*{find best candidate}
%calculate the distortion values of the candidate solutions $Y$.\; 
\If{$\phi(Y_{best},\mx) < \phi(C_{best},\mx)$}{ 
    $C_{curr} \assign Y_{best}$\;
}
\Else{
    $C_{curr} \assign (\mbox{best non-tabu candidate})$\;
}
\If{tabu-list is full}{ remove the oldest solution}
Insert $C_{curr}$ to the tabu list.\;
\If{$\phi(C_{curr},\mx) < \phi(C_{best},\mx)$}{set $C_{best} = C_{curr}.$ \;
}
}
\Return{$C_{best}$}
\caption{Tabu Search}\label{alg:tabu}
\end{algorithm}


Two algorithm variations are discussed, which use specific methods to generate new solution candidates. 

\begin{itemize}
\item \emph{partition-based} generation:  this variant operates on the partition of the data set, i.e., the assignment of all data points to clusters. New candidates can be generated by randomly assigning a fraction of the data points to different clusters. Since pure random assignment tends to produce poor candidates, an improvement is to assign the affected data points to near clusters with higher probability.
\item \emph{centroid-based} generation. In this case, new candidates are generated by adding noise to the existing centroids or by setting some centroids to randomly selected data points.
\end{itemize} 
%\emph{Note: There is some relation in this case to \bkm{} where also several centroids are modified at once. The modification is not at random in \bkm{}, however, but directed by large quantization error (insertions) and low utility plus freezing (deletions).}

\end{comment}


\subsubsection{Iterative Splitting and Merging}\label{sec:splitmerge}
\cite{Kaukoranta1998} describe an iterative splitting and merging algorithm for vector quantization codebook generation. 
Repeatedly the following steps are performed: (1) a cluster is selected which is split. (2) Two clusters are selected which are merged. (3) Some Lloyd iterations are performed to refine the codebook.
For (1) a local optimization strategy is applied where each cluster is tentatively split, and the one decreasing the
distortion most is chosen. For (2) the pairwise nearest neighbor (PNN) approach as described by \cite{Equitz1989} is employed. This approach determines the neighboring pair of clusters, $mc_i$ and $\mc_k$, which least increases the overall error when merged. For (3) the authors suggest performing a fixed small number (e.g., 2) of Lloyd iterations.

This approach is somewhat similar to \bkm{} with breathing depth $m=1$. 
\Bkm, however, avoids the effort to split each cluster by splitting only the cluster with the largest quantization error. Moreover, \bkm{} avoids the computation of many possible merges by always deleting the centroid with the smallest utility. 

\begin{comment}
The approach is shown in Algorithm~\ref{alg:splitmerge}.
\renewcommand{\labelenumii}{\arabic{enumi}.\arabic{enumii}} 
% enumerate the following as \labelenumii{}

\begin{algorithm}
\DontPrintSemicolon
\givenx
Generate an initial codebook by any algorithm.\;
\Repeat{no improvement is achieved.}{ 
Select a cluster to be split\;
Split the selected cluster\;
Select two clusters to be merged\;
Merge the selected clusters\;
Apply some Lloyd iterations
}
\caption{Iterative Splitting and Merging}\label{alg:splitmerge}
\end{algorithm}

Several variants of the described splitting/merging step are discussed:
\begin{itemize}
    \item One-split-one-merge: Perform a single split operation followed by a single merge
    operation (as described in Algorithm~\ref{alg:splitmerge})
    \item h-split-h-merge: Perform $h$ split operations followed by $h$ merge operations. The original size of the codebook is preserved.
    \item Adaptive split-and-merge: Perform a variable number of split-and-merge operations
    so that the size of the codebook converges finally to the desired size.
\end{itemize}

\cite{Kaukoranta1998} make the following choices for the final algorithm:
\begin{enumerate}
\item For the selection, a local optimization strategy is applied where each cluster is tentatively split, and the one decreasing the
distortion most is chosen.
\item The splitting is realized using the PCA-based approach described in section \ref{sec:split}.% approximate principal component analysis (PCA) on the cluster $\mc_i$ and the new centroids $c_i'$ and $c_i''$ are positioned using the first principal component. 
\item The merging is done using the pairwise nearest neighbor (PNN) approach described by \cite{Equitz1989}. This approach determines the neighboring pair of clusters, $mc_i$ and $\mc_k$, which least increases the overall error when merged. The merging is performed by replacing the two centroids $c_i$ and $c_k$ with a single centroid $c_i'$ positioned at the center of gravity of the associated data points.
\item Per iteration, two Lloyd iterations are performed.


\end{enumerate}

If one compares this to \bkm, two differences are apparent: 
\begin{itemize}
    \item Instead of tentatively splitting each cluster, \bkm{} uses the accumulated error of a centroid as an indicator for the SSE decrease after splitting.
    \item Instead of tentatively merging each cluster pair in the PNN graph, \bkm{} uses the utility value of a centroid as an indicator for the SSE increase after merging.
\end{itemize}
In both cases, accuracy is traded for speed. The \bkm{} approach is more efficient since it only relies on information already provided by the \gLa{} (the distances between centroids and data points), while the local operations used by \cite{Kaukoranta1998} very likely give better indications on the effects of splitting and merging than the approximations used by \bkm.
\end{comment}
\subsubsection{Bisecting K-Means}				

The \emph{bisecting \km}{} \citep{steinbach2000} has large similarities to the splitting method proposed by \cite{Franti1997}. It starts with one single cluster and iteratively ``bisects'' (i.e., ``splits'') one of the present clusters into two by performing \km{} with $k=2$ on the selected cluster. This bisection step is repeated several times (say $m$ times) with different random initializations before choosing the bisection with the lowest error. This is iterated until a predefined number of clusters is reached 
%(see Algorithm \ref{alg:bisect}), 
or the overall error falls below a threshold.
Optionally, the \gLa{} can be applied to the resulting codebook after each bisecting step.
If this optimization is \emph{not} done, the algorithm produces a hierarchical clustering (obtained by considering all intermediate codebooks).
To select the cluster to be split, \cite{steinbach2000} propose to use either the size of the cluster or the SSE of the cluster as a criterion. In the latter case, there is a similarity to the error-based insertion proposed by \cite{Fritzke93, Fritzke94c}.

\begin{comment}
\begin{algorithm}
    \DontPrintSemicolon
    \givenx
    \meanx
    \Repeat{$|\mc|=k$}{  
        Select a cluster $\mc_i$ (corresponding to $c_i \in \mc$) to be split.\;
        \RepTimes{$m$}{
            Perform \km{} (randomly initialized) with $k=2$ in the selected cluster.\;
            Memorize the best solution so far (two centroids, $c_i^1$, and $c_i^2$).\;
        }
        $\mc \assign \mc \cup \{c_i^1, c_i^2\} \setminus \{c_i\}$\tcc*{replace $c_i$ with the best solution so far}
        (optionally apply the \gLa{} to $\mc$)\;  
    }
    \caption{Bisecting K-means}\label{alg:bisect}
    \end{algorithm}

If the resulting clusters are not later optimized by a \km{} phase over all centroids, bisecting \km{} leads to a hierarchical clustering:
the clusters form a binary tree with leaves on several levels. Each inner node of this tree has an associated set of data points which is the union of the data point sets of its two children.  \cite{steinbach2000} mention the possibility of bisecting either the largest cluster or the cluster with the lowest similarity (which corresponds to the cluster with the highest error). Surprisingly, they reported little difference among the results and finally chose the largest clusters in their experiments. 

%While little discrepancies may have occurred for their specific data, both choices are principally different and will lead, in general, to quite different results. They tend to either maximize entropy (bisecting the largest cluster) or minimize error (bisecting the cluster with maximum error). Consider, e.g., a 2D data set where half of the points are positioned in a quadratic area of $1\,\mbox{mm}^2$ size and the other half uniformly in a separate much quadratic surface with an area of $1\,\mbox{km}^2$. Let us assume $k=100$. Bisecting the largest cluster will lead to a similar number of centroids in both areas. Bisecting the cluster with the largest error will lead to one centroid in the small area and 99 in the large area. In the latter case, there is a similarity to the error-based insertion proposed by \cite{Fritzke93, Fritzke94c}.
\end{comment}

\subsubsection{Genetic Algorithm with Deterministic Cross-Over}\label{sec:ga}
Genetic algorithms typically make use of a condensed ``genetic'' representation of the candidate solutions. They do attempt to simulate natural evolution by employing concepts such as selection (survival of the fittest), cross-over (recombination of several different genetic representations), and mutation (random modifications of genetic representations). Here we consider the genetic algorithm described by \cite{Franti2000}, which is specifically adapted to the problem of vector quantization.

The top-level algorithm is shown in Algorithm~\ref{alg:ga} and does not show any problem-specific properties apart from the use of the \gLa{} for fine-tuning.  

\begin{algorithm}
    Generate S initial solutions.\;
    Sort the solutions by error.\;
    \RepTimes(/* T = number of generations */){T}{
        \RepTimes(/* S = number of individuals per generation */){S}{
            Select two solutions for cross-over.\;
            Generate a new solution by crossing the selected solutions.\;
            Optionally mutate the new solution.\;
            Fine-tune the new solution by \gLa.\;
        }
        %Generate new solutions\;
        Sort the solutions by error.\;
        Store the best solution.\;
    }
    \Return{best solution}
    \caption{Genetic Algorithm}\label{alg:ga}
\end{algorithm}


The problem-specific properties proposed by \cite{Franti2000} are the representation of a solution and the cross-over operation. Each solution is represented as a pair $(C,P)$ where $C$ is a codebook, and $P$ is a corresponding partition of the data set. Maintaining both types of information makes it possible to perform the cross-over operation of two solutions $(C^1,P^1)$ and $(C^2,P^2)$ in a very effective and efficient way:
\begin{itemize}
    \item A new codebook $C^{new}$ is created as the union of $C^1$ and $C^2$: \newline $C^{new} = C^1 \cup C^2$.
    \item The partitions $P^1$ and  $P^2$ are combined to form a new partition $P^{new}$ by mapping each data point to its cluster from $P^1$ or $P^2$, depending on which corresponding centroid is closer.
    \item $C^{new}$ is updated to contain the centroids of the clusters in $P^{new}$.
    \item Empty clusters are removed from $P^{new}$.
    \item The number of clusters in $P^{new}$ is reduced to the desired number $k$ of clusters by using the pairwise-nearest-neighbor (PNN) approach on ($C^{new}$,$P^{new}$). PNN is performed on the $2\times k$ (fewer if empty clusters were removed) centroids instead of the full data set. The partition is updated accordingly by combining merged clusters.
\end{itemize}
Mutation was considered but not performed in \cite{Franti2000} because of the concentration on efficiency.
\subsubsection{Global K-Means}				
``Global \km'' \citep{Likas2003} is a deterministic method that finds an approximate solution for a given \km{} problem ($\mx,k$) by starting with the trivial solution for a codebook size of one. This solution is used to find a solution for codebook size two by combining the size-one solution sequentially with each element of the data set $\mx$ and running the \gLa{} starting from there. The best solution found is taken as the solution for size two. This is iterated for all codebook sizes until a solution for codebook size $k$ is found. The algorithm requires $k \times n$ runs of the \gLa{} leading to very high computation demand for data sets of non-trivial size. 
\begin{comment}
See Algorithm \ref{alg:globalkm} for a pseudo-code formulation.

\begin{algorithm}
    \caption{Global \Km}\label{alg:globalkm}
    \DontPrintSemicolon
    \givenx
    \meanx
    \Repeat{$|\mc|=k$}{ 
        \ForEach{$x \in \mx$}{
           Run \GLAS{} for $(\mx,\;\mc\, \cup \{x\})$ \tcc*[r]{try out $x$ as new centroid}
           Memorize the best\_solution\_so\_far.
        }
        $ \mc \assign \mbox{best\_solution\_so\_far}$\tcc*[r]{$|\mc|$ is increased by one}
    }
    Return $\mc$
\end{algorithm}
\end{comment}

\subsubsection{Iterative Shrinking}

\cite{Franti2006}  describe an iterative shrinking algorithm for vector quantization codebook generation. The method starts by assigning each data vector to its own cluster. This huge codebook is then stepwise reduced by deleting the single centroid leading to the smallest error increase. The major difference between the ``shrinking'' described in this algorithm and the ``merging'' described in \cite{Kaukoranta1998} (see section \ref{sec:splitmerge}) is the following: During ``shrinking," a centroid $c_i$ is removed, and each associated data point $x \in \mc_i$ is assigned to the respective nearest other centroid $c \in \mc \setminus \{c_i\}$, whereas during ``merging," two neighboring clusters, $\mc_i$ and $\mc_k$ are combined into a new cluster $\mc_m$, i.e., all affected data points end up in the same cluster $\mc_m$ (before any further optimizations, e.g.,  Lloyd iterations, are done.). 

\begin{comment}
The outline of the approach is shown in Algorithm~\ref{alg:shrink}.

\begin{algorithm}
\DontPrintSemicolon
\givenx
$\mc = \mx$\tcc*[r]{every data vector is a centroid}
\Repeat{$|\mc|=k$}{ 
Select a centroid $c \in \mc$ to remove.\tcc*{with minimal error increase}
$\mc = \mc \setminus \{c\}$\tcc*{remove  $c$}

Update the remaining centroids.\tcc*{see text for variants}
}
\caption{Iterative Shrinking}\label{alg:shrink}
\end{algorithm}


The article discusses various strategies to approximate (with varying accuracy) the computation of the error increase and the update of the centroids to avoid full Lloyd iterations. Moreover, the approach is combined with a genetic algorithm. This combination reportedly gives the best results compared to several other approaches investigated empirically.
\end{comment}

\subsubsection{Random Swap Clustering}\label{sec:rs}


\Rsc{} \citep{Franti2018} is based on the idea of repeatedly replacing a randomly chosen centroid $c \in \mc$ with a randomly chosen data vector $x\in \mx$. This operation, also called ``swap," is followed by a small number of Lloyd iterations. If the resulting error is lower than before the swap, the swap is ``accepted," and the algorithm continues. If the resulting error is higher than before the swap, the algorithm continues from the codebook state before the swap (basically ignoring the
 swap). The algorithm is shown in Algorithm~\ref{alg:swap}.  

 This number of required Lloyd operations after a swap operation can be reduced by locally repartitioning the data points associated with the deleted centroid and by specifically determining which data points will be assigned to the new centroid. This is merely an efficiency measure and is denoted as optional by \cite{Franti2018} if two or more Lloyd iterations are performed.
\begin{algorithm}
    \DontPrintSemicolon
    \givenx
    \randomc
    \RepTimes{$Z$}{ 
        Randomly select a centroid $c,\,c \in \mc$.\;
        Randomly select a data vector $x \in \mx$.\;
        $\mc_{new}=\mc \cup \{x\} \setminus \{c\}$\tcc*[r]{replace $c$ with $x$}
        Optional: Locally repartition data points.\tcc*{see text}
        Perform a few Lloyd iterations on $\mc_{new}$.\;
        \If {$\phi(\mc_{new},\mx) < \phi(\mc,\mx)$\tcc*[r]{is the new codebook better?}}{$\mc \assign \mc_{new}$ \tcc*[r]{Accept the new codebook.}}
    }
    \Return{$\mc$}
    \caption{Random Swap Clustering}\label{alg:swap}
    \end{algorithm}

\subsubsection{Improvement by Multiple Runs}\label{sec:multirun}	
	
	A general method to improve results for any randomized algorithm is selecting the best result from multiple repeated runs \citep{Franti2019b}. The improvements obtained depend on the variance of the results produced by the algorithm at hand. The required amount of computation is proportional to the number of repetitions.

\section{Algorithms Selected for Comparison}\label{sec:algorithms}
    The following algorithms were selected as contenders for \bkm{} (a brief reasoning is given for the inclusion of each approach):
    
    
    \begin{description}[style=nextline]
    
    \item [\gkmp] (see Section \ref{sec:gkmp}). This method was selected as the baseline algorithm for all other methods since it probably is the most widely-used algorithm for \km{} because of being the default \km{} method in the popular \skl{} package \citep{scikit}.  The implementation is the class \texttt{KMeans} of \skl.
    Sources:  \url{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html}
    
    \item [\vkmp] (see Section  \ref{sec:kmp}). The original (non-greedy) variant of \kmp. For this method, an $\mathcal{O}(\log{}k)$ upper bound was proven. The implementation stems from the \skl{} package \citep{scikit}, in particular from the function \texttt{kmeans\_plusplus} with default parameters, except for setting \texttt{n\_local\_trials=1}. 
    Sources:  \url{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.kmeans_plusplus.html}
    
    \item [\bkmp] (see Section \ref{sec:betterkmp}). A modification of \kmp{} that was selected after being independently suggested by two reviewers. The algorithm was implemented by the author in Python since no open-source (or other) implementation could be found. The parameter $Z$ for the number of additional centroid selections was set to $25$, following the experiments described in the original paper of \cite{Lattanzi2019}.
    
    \item [\hawo] (see Section~\ref{sec:hawo}). This method was selected since it is the default \km{} algorithm in the \texttt{stats} package of the R programming language \citep{RCore2019}. The approach is special since it does not rely on the \GLA. Parameter settings: \texttt{iter.max=500, nstart=1, algorithm="Hartigan-Wong"}. Implementation: Fortran code from the \texttt{stats} package of R.
    Sources: \url{https://cloud.r-project.org}
    
    \item [\ga] (see Section \ref{sec:ga}). We included this approach following a reviewer's suggestion and promising results in a pre-study. The implementation used is part of a C-Package published on github by the authors of the original publication \citep{Franti2000}. The default parameters defined by the package were used in the experiments.
    Sources: \url{https://github.com/uef-machine-learning/CBModules}
    
    \item [\rs] (see Section~\ref{sec:rs}). We included this approach following a reviewer's suggestion and promising results in a pre-study. The implementation used is part of a C-Package published on github by the authors of the original publication \citep{Franti2018}. The default parameters defined by the package were used in the experiments.
    Sources: \url{https://github.com/uef-machine-learning/CBModules}
    
    
    \end{description}
    
    

\section{Empirical Results}\label{sec:empirical}
\setlength{\fboxsep}{3pt}
We first list the algorithms selected for comparison with \bkm. Then, the \km{} problems used for the experiments are described. Finally, the empirical results are presented.


\subsection{\KM{} Problems Investigated}\label[]{sec:problems}
We used four groups of two-dimensional problems (9 problems per group) and one group of high-dimensional problems (15 members) for a total of 51 \km{} problems. These groups were selected to showcase the algorithms' strengths and weaknesses.

The problems included varied point densities, reflecting the diversity of real datasets, unlike the mixtures of identically shaped Gaussians found in many textbook examples. Of the 51 problems, 24 are from the literature, and 27 are self-generated. The following subsections describe the problems in detail.

\subsubsection{Problems with known optimum}
Each problem in this group is constructed with a known optimal solution. The datasets consist of $g$ identical, well-separated \emph{macro-blocks}, each comprising $b$ adjacent quadratic base blocks of data points, where $b$ can be 1, 3, or 4. The number of centroids $k$ is set to $k = g \times b$. The optimal solution places one centroid at the center of each base block, as shown in Figure \ref{fig:prob-opt} with optimal solutions in red.

The optimality is justified because the macro-blocks are identical and well-separated, ensuring an optimal solution distributes centroids evenly among them. Thus, the overall solution's optimality equates to that of an individual macro-block.

For $b = 1$, the optimal solution is the centroid of the single block, thus satisfying the centroid condition. For $b = 3$ or $b = 4$, it is assumed that the optimal partial solution involves placing one centroid in each base block's center. Hence, the optimal solution for the entire problem is to center one centroid in each base block across all macro-blocks.

%
% caption command definitions
%
\newcommand{\capProbLit}{...}
\newcommand{\capProbOptDetail}{...}
\newcommand{\capProbOpt}{...}
\newcommand{\capProbLitmod}{...}
\newcommand{\capProbEspiral}{...}
\newcommand{\capProbHighD}{...}

\begin{comment}
\renewcommand{\capProbOptDetail}{Small examples of problems with known optimum. The data points are shown in green, and the optimal centroids are in red, each centered in one of the base blocks (see text). The SSE of the optimal solution can be computed as the sum of quadratic distances of one centroid to all data points in its corresponding base block multiplied by the number of base blocks in the respective data set.}

\begin{figure}
	\centering
	\includegraphics[width=0.90\linewidth]{img/prob-opt-detail.png}
	\caption[short caption]{\capProbOptDetail}
	\label{fig:prob-opt-detail}
\end{figure}
\end{comment}

\renewcommand{\capProbOpt}{Problems with known optimum. The data points are shown in green, and the optimal centroids are in red.}
\newcommand{\fiwid}{1.0}
\renewcommand{\fiwid}{0.76}
% make relative

\input{img/prob-opt.tex}

\begin{comment}
This construction principle relies on the macro-blocks containing only a few base blocks. For $4\times4$ (or more) connected base blocks, one can find better solutions than the respective base block centers if the number of data points per base block is high enough. %\todo{image of a larger block with a better solution}

Knowing the optimal solution for a problem makes it possible to use it as an absolute benchmark for the quality of the solutions obtained by the investigated algorithms instead of just comparing them to each other or to a designated baseline algorithm. Since this evaluation is possible only for this problem group, we moved the corresponding results to the Appendix (see Table \ref{tab:mse-realopt}).
\end{comment}

\subsubsection{Literature Problems}\label{sec:prob-lit}
The problems in this group are based on data sets from the literature. The $k$-values were freely chosen, resulting in the problems listed in Table \ref{tab:datalit} and displayed in Figure~\ref{fig:prob-lit}. 
\begin{comment}
    We will refer to these problems as ``Literature problems'' even if the $k$-values in the source papers were different.
\end{comment}
\begin{table}
    \begin{center}
    \begin{tabular}{llrr}
    \toprule
    data set&  origin &$n$ & $k$\\ 
    \midrule
    ``Aggregation''&  \cite{Gionis2007} &788& 200 \\
    ``Compound``&  \cite{Zahn1971} & 399 & 50 \\
    ``D31''&  \cite{Veenman2002} &3100& 100 \\
    ``Flame''&  \cite{Fu2007} &240& 80 \\
    ``Jain''&  \cite{Jain2005} &373& 30 \\
    ``R15''&  \cite{Veenman2002} &600& 30 \\
    ``S2''&  \cite{Franti2006} &5000& 100 \\
    ``Spiral''&  \cite{Chang2008} &312& 80 \\
    ``Pathbased''&  \cite{Chang2008} &312& 80 \\
    \bottomrule
    \end{tabular}
    \end{center}
        \caption[short caption]{Two-dimensional data sets from the literature with chosen $k$-values. All data sets were obtained from \url{http://cs.joensuu.fi/sipu/datasets/}.}
        \label{tab:datalit}
    \end{table}

\renewcommand{\capProbLit}{\Km{} problems based on two-dimensional data sets from the literature (see Table \ref{tab:datalit}).}
\input{img/prob-lit.tex}

\subsubsection{Modified Literature Problems.}\label{sec:mod-prob-lit}
The problems in this group were generated from the ``literature problems'' in the previous section as follows.
\begin{itemize}
\item Randomly select 200 data points from the literature problem.
\item Add a very dense cluster of 4000 data points below the area occupied by the selected 200 data points.
\end{itemize}
The purpose of this modification is to test the algorithms' ability to deal with data sets having a large variation in density. 

\renewcommand{\capProbLitmod}{Modified literature problems. The data sets were constructed from the problems shown in Figure~\ref{fig:prob-lit} by taking a random subset of size 200 and adding a high-density cluster consisting of 4000 data points below the centroid of the subset. The $k$-values remain the same as in Table \ref{tab:datalit}.}

\input{img/prob-litmod.tex}
\subsubsection{``Evil Spiral'' Problems}
This group of problems (see Figure~\ref{fig:prob-espiral}) was designed to check the effect of letting an increasing fraction of the data originate from high-density clusters. One common part of all data sets consists of 500 data points located in 25 Gaussian clusters of 20 points, each positioned on a spiral. The other part of the data sets consists of one or more high-density Gaussian clusters arranged on a second spiral intertwined with the first one. The number of high-density clusters, each consisting of 500 data points, varies from one to 25 in steps of three.
The value of $k$ is always set to 100.

\renewcommand{\capProbEspiral}{``Evil Spiral'' problems. Each data set contains the same 25 wide Gaussian clusters (20 points each) arranged in a spiral (having $20\times25=500$ points) and an increasing number ($1,4,\ldots,25$) of very dense clusters arranged on a spiral intertwined with the first spiral. Each of the dense clusters consists of 500 points, i.e., as many as the complete first spiral. For all problems, $k=100$ is used.}

\input{img/prob-espiral.tex}
\subsubsection{High-Dimensional Problems}\label{sec:prob-highD}

The problems in this group are based on three data sets (see Figure~\ref{fig:prob-highD}) used in the original paper on \kmp{} \citep{Arthur2007} where the $k$-values were chosen from $k \in \{10,\,25,\,50\}$. We added two larger $k$-values resulting in $k \in \{10,\,25,\,50,\,100,\,200\}$. The data sets are described below. \newline 


\textbf{Norm25:}
This data set consists of $n=10000$ vectors of dimension $d=15$. The original data used by \cite{Arthur2007} is not publicly available anymore, but their paper contains the following description (and also states that the number of data points is 10000):
 \begin{quote}``The first data set, \emph{Norm25}, is synthetic. 25 “true” centers were drawn uniformly at random
from a 15-dimensional hypercube of side length 500.
Then points from Gaussian distributions of variance 1 around each true center were added, resulting in 
25 well-separated Gaussians with the true centers providing a close approximation to the optimal
clustering.''
 \end{quote}

 Using this information, we generated a new data set, \emph{Norm25}, with statistical properties similar to those of the original one.\newline

 \textbf{Cloud:}
 This data set consists of $n=1024$ vectors of dimension $d=10$. It is the \emph{Cloud} data set from the UCI Machine Learning Repository \citep{Dua:2019} and is available at \url{https://archive.ics.uci.edu/ml/datasets/Cloud}. The data was derived from two $512 \times 512$-pixel satellite images of clouds (one image in the visible spectrum and one in the infrared spectrum) taken with an AVHRR (Advanced Very High-Resolution Radiometer) sensor. The images were divided into 1024 super-pixels of size $16\times 16$, and from each pair of super-pixels, ten numerical features were extracted to form the final data set.\newline

 \textbf{Spam:}
 This data set consists of $n=4601$ vectors of dimension $d=58$. It is the \emph{Spam} data set from the UCI Machine Learning Repository \citep{Dua:2019} and is available at \url{https://archive.ics.uci.edu/ml/datasets/Spambase}. According to the data set description, the data was generated from spam and non-spam emails. Most of the features (48 of 58) are word frequencies from different words. Other features measure the occurrence frequencies of certain characters or capital letters.

\renewcommand{\capProbHighD}{Three high-dimensional data sets which were also used by \cite{Arthur2007}. Descriptions are in the text. The $k$-values used with each data set were 10, 25, 50, 100, and 200, resulting in 15 different problems. The figures display projections of the high-dimensional data onto two selected axes.}

\begin{figure}
	\centering
	\includegraphics[width=\fiwid\linewidth]{img/prob-highD.png}
	\caption[short caption]{\capProbHighD}
	\label{fig:prob-highD}
\end{figure}

\subsection{Solution Quality}\label{sec:results-solqual}
\setlength{\tabcolsep}{3pt}
The primary findings on solution quality are summarized below, with detailed results in Appendix \ref{sec:appendixA}. Table \ref{tab:mse-top} presents the core results, with each row representing a problem group and each column an algorithm. The values denote the mean relative MSE improvement over the baseline \gkmp{} algorithm, marked as 0.0\%. Positive values indicate improvements, while negative values denote poorer results.

\newcommand{\tabcom}{}
\renewcommand{\tabcom}{}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{img/colmap.png}
    \caption{Color mapping used for the ``SSE improvements'' tables.}% 0.0\% is shown in yellow, positive values (improvements) in shades of green, and negative values (deteriorations) in shades of red.}
    \label{fig:colorbar}
\end{figure}

\renewcommand{\tabcom}{Both \bkmp{} and \bkm{} consistently found improvements for all problem groups. With the exception of the ``Literature'' problems (where \rs{} performed best), \bkm{} always found the largest improvements. The values have been colorized according to the mapping shown in Figure~\ref{fig:colorbar}. The best results in each row are boxed.}
\input{coltables/t-summary-mse.tex}

\renewcommand{\tabcom}{The values for \bkm{} are tiny, indicating a quite homogeneous quality of the solutions.}
\input{coltables/t-summary-msestd.tex}

The following observations can be made regarding solution quality:

\begin{itemize} 
    
    \item Only  \bkm{} and \bkmp{} consistently beat the baseline algorithm for all problem groups. Thereby, the improvements found by \bkm{} were much larger than those found by \bkmp{}.
    \item \Rs{} found the best solutions for the ``Literature'' problems.
    \item \Vkmp{} (the original \kmp{} variant with the $\mathcal{O}(\log{}k)$ upper bound) found significantly worse solutions than the baseline algorithm.
    \item \hawo{} (the default \km{} algorithm in the \texttt{stats} package of the R programming language) always produced the worst results of all methods (far below the baseline algorithm).
\end{itemize}

In Table \ref{tab:msestd-top}, the average standard deviation corresponding to Table \ref{tab:mse-top} is shown. A noticeable pattern is that the standard deviations for \bkm{} are tiny, indicating a quite homogeneous quality of the solutions. Overall there seems to exist a negative correlation between SSE improvement and standard deviation: the higher the SSE improvement, the smaller the standard deviation, and vice versa.

\subsection{CPU Time Usage}\label{sec:results-cputime}
Here we present the primary findings on CPU time usage from our experiments which were performed on a Linux PC (AMD FX\textsuperscript{\texttrademark}-8300 eight-core Processor, 16GB RAM) running Ubuntu 22.04 LTS. Detailed, problem-specific results are provided in Appendix \ref{sec:appendixB}.

To reduce bias in the experimental results, we selected the most widely used open-source implementation of each algorithm. For \bkmp, no implementation was available, so we implemented it in Python based on \skl{} with reasonable effort to obtain an efficient implementation. Overall, this approach resulted in three different programming languages being used: Python, Fortran, and C. For algorithms with different implementation languages, the CPU time usage is not directly comparable. With this caveat, the observed CPU time usage is reported here to provide some efficiency indication.

Table \ref{tab:cpu-top} contains one row per problem group. It shows in the second column the mean CPU time in seconds used by the baseline algorithm, \gkmp, and in the following columns, the percentual CPU time usage relative to the baseline algorithm for all investigated approaches.

\renewcommand{\tabcom}{The column {``t(greedy km++)"} shows the mean CPU time used by the baseline algorithm. Green (red) background coloring indicates faster (slower) execution than the baseline algorithm}
\input{coltables/t-summary-cpu.tex} 

The algorithms implemented in Python (\gkmp, \vkmp, \bkmp, \bkm) are all based on the \skl{} library, which makes a comparison among them relatively meaningful.
The following observations can be made for CPU time usage of these
algorithms:

\begin{itemize}
    \item The fastest of these algorithms was \vkmp. It was about 23\% faster than the baseline algorithm, \gkmp{}, which, however,  had a much better solution quality.
    \item \Bkmp{} required about 9.5 times as much CPU time as the baseline algorithm. %We implemented it with reasonable optimization, but there may still be room for improvement.
    \item \Bkm{} required about 5.6 times as much CPU time as the baseline algorithm).
\end{itemize}    

The only algorithm implemented in Fortran, \hawo, was the fastest overall (about 40\% faster than \gkmp). It is called from within R, but the implementation language is Fortran.

Among the two algorithms implemented in C, \ga{} was on average over 30 times faster than \rs, which was by far the most compute-heavy algorithm of all we investigated.

\subsection{Effects of Multiple Runs and Running \BKM{} Only Once}\label{sec:results-multirun}

\emph{Multiple runs} is a technique to enhance algorithmic outcomes by running it several times and choosing the best result (Section \ref{sec:multirun}). Table  \ref{tab:mse_bo10-top} shows the average improvements from ten runs of all algorithms, over the baseline.
Using the same data as Table \ref{tab:mse-top} (100 runs per problem-algorithm combo), results are grouped into ten clusters of ten, selecting the best from each. Percentage SSE difference from the baseline is calculated and averaged. Positive values in the \gkmp{} column represent the improvement from ten runs.

\renewcommand{\tabcom}{The performed experimental results for each combination of problem and algorithm have been partitioned into groups of size ten, and the best result from each group was selected. 
}
\input{coltables/t-summary-mse_bo10.tex}


Table \ref{tab:mse_diff-top} shows the SSE difference between single run and best-of-ten runs. The minimal improvements from multiple runs of \bkm{} suggest that it can be effectively compared to other algorithms' best-of-ten results, even when executed only once. Table \ref{tab:wildcard-top} confirms that \bkm{} is overall superior, averaging across all problem groups.

\renewcommand{\tabcom}{This table can be computed as the difference of Table \ref{tab:mse_bo10-top} and Table \ref{tab:mse-top}. It shows the improvement obtained by selecting the best of ten runs instead of just a single run. The tiny improvements for \bkm{} suggest that performing multiple runs is optional here to obtain good results.}   

\renewcommand{\tabcom}{This table compares the relative SSE difference between a single run and the best of ten runs, computed from Tables \ref{tab:mse_bo10-top} and \ref{tab:mse-top}. 
The minor improvements for \bkm{} indicate that multiple runs are not required to achieve desirable results.}
\input{coltables/t-summary-mse_diff.tex}

\renewcommand{\tabcom}{A single run of  \bkm{} generally outperforms the best of ten runs for the analyzed competing algorithms, barring few exceptions like \rs{} and \ga. The figures represent the mean SSE improvements over the baseline algorithm from best-of-ten runs for competitors (Table \ref{tab:mse_bo10-top}) and a single run for \bkm{} (last column of Table \ref{tab:mse-top}). \Bkm{} (1 run) was the best overall, topping three individual problem groups, and ranking second and third in the remaining two.}
\input{coltables/t-summary-wildcard.tex}

CPU time for this scenario (one run of \bkm, ten runs for others) is computed by multiplying Table  \ref{tab:cpu-top}'s first column by ten and dividing the \bkm{} percentage by ten (see Table \ref{tab:cpu-one-top}). \Bkm{} is the quickest, requiring just $55.6\%$ of the CPU time needed for ten runs of \gkmp.

\renewcommand{\tabcom}{ when all algorithms including \gkmp{} are run ten times, but \bkm{} is run only once. In this case \bkm{} is the fastest algorithm, requiring only $55.6\%$ of the CPU time needed for ten runs of \gkmp.}
\input{coltables/t-summary-cpu-one.tex}

\subsection{Effect of Varying the Breathing Depth Parameter $m$}

The breathing depth parameter $m$ (default value: 5) serves as a means of balancing solution quality and computational resource demands. Higher values of $m$ typically result in improved solutions, albeit at the expense of greater computation time, and vice versa. Figure \ref{fig:errhisto} illustrates the average error improvement relative to the baseline algorithm across all test problems for varying values of $m$. Concurrently, Figure \ref{fig:cpuhisto} presents the corresponding computation time relative to the baseline algorithm. For instance, replacing the default $m = 5$ with $m = 25$ improved the average solution quality by 0.9\% for our test problems, but quadrupled the CPU time required.

\renewcommand{\tabcom}{Average error improvement across all test problems of \bkm{} over \gkmp, for varying values of the ``breathing depth'' parameter $m$. For the default value of $m=5$, the average error improvement is 8.1\% (see the lower-right value in Table \ref{tab:mse-top}).}
\begin{figure}
	\centering
	\includegraphics[width=0.70\linewidth]{img/errhisto.png}
	\caption[short caption]{\tabcom}
	\label{fig:errhisto}
\end{figure}

\renewcommand{\tabcom}{Average computation time across all test problems of \bkm{} relative to \gkmp, for varying values of the ``breathing depth'' parameter 
$m$. For the default value of $m=5$, the relative size of the CPU time is 555.6\% (see the lower-right value in Table \ref{tab:cpu-top}).}
\begin{figure}
	\centering
	\includegraphics[width=0.70\linewidth]{img/cpuhisto.png}
	\caption[short caption]{\tabcom}
	\label{fig:cpuhisto}
\end{figure}

\section{Conclusion}

We introduced the novel \bkm{} algorithm which dynamically changes the size $k$ of the codebook to improve solutions found by the \GLA{}. We empirically compared \bkm{} (initialized by \gkmp) to the baseline \gkmp{} (followed by the \GLA) and five other algorithms across diverse test problems. Our approach consistently outperformed all other methods in terms of solution quality, with only a few exceptions where it slightly lagged behind \rs{} or \ga. It also was the only approach able to find near-optimal solutions across all problems in the "Known Optimum" problem group.

While \rs{} and \ga{} did outperform the baseline for most problems, they were dramatically inferior to the baseline in several cases, making them less suitable for unknown data.
The comparison between \gkmp{} and \vkmp{} underlined the improved solution quality offered by the former, validating its use as the present default \km{} algorithm in \skl.

\hawo{}, the default algorithm in R's stats package, consistently underperformed, suggesting its use should be limited to situations where low computational cost is a priority.

Notably, \bkm{} maintained its superior performance even when other algorithms were run ten times and it was only run once. In this scenario, it continued to deliver significantly better solutions than \gkmp{} while being nearly twice as fast.

Based on these findings, we recommend using \bkm{} over \gkmp{} for improved solution quality and speed.

\bibliography{bkmshort}
\FloatBarrier
%new page
\newpage
\setcounter{table}{0}
\renewcommand*\thetable{\Alph{section}.\arabic{table}}
\renewcommand{\theHsection}{A\arabic{section}}
\appendix
\pagenumbering{roman}
%\setcounter{page}{1}

\section{Solution Quality Details}\label{sec:appendixA}

This section presents tables depicting solution quality per problem for all studied algorithms. The average SSE ($\Phi$) for the baseline algorithm, \gkmp, is shown with a grey background. The percentages for non-baseline algorithms represent mean relative SSE improvement over the baseline, with negative values indicating worse performance. The color scheme corresponds to Figure~\ref{fig:colorbar}. The best results in each row are boxed, including ties.

\renewcommand{\tabcom}{\Bkm{} dominates, closely followed by \rs{} and less closely by \ga{} and \bkmp. \Vkmp{} and \hawo{} are clearly inferior.}
\input{coltables/t-msee-opt.tex}

\renewcommand{\tabcom}{Green background marks cases where the optimum has been approached up to 0.001\% tolerance. Only \bkm{} was able to consistently find such near-optimal solutions. Shades of blue indicate varying deviations from the optimum.}
\input{coltables/t-mse-realopt.tex}

\renewcommand{\tabcom}{For all problems in this group, \rs{} found the best solutions.}
\input{coltables/t-msee-fraenti.tex}

\renewcommand{\tabcom}{For all problems, \bkm{} found the best solutions. Also, \bkmp{} was able to improve upon the baseline algorithm in all cases but by a considerably smaller margin. The other approaches produced worse solutions than the baseline algorithm for some or all problems.}
\input{coltables/t-msee-fr-mod.tex}

\renewcommand{\tabcom}{\Bkm{} is best apart from the first two problems. 
}
%\renewcommand{\tabcom}{}
\input{coltables/t-msee-espiralX.tex}

\renewcommand{\tabcom}{The problem with the \emph{Norm25} data set and $k = 25$ seems to be so simple that all algorithms, except \hawo, found the same solution (one centroid per cluster).
The dimensionality of the data sets \emph{Norm25}, \emph{Cloud}, and \emph{Spam} is 15, 10, and 58, respectively.
}
\input{coltables/t-msee-high-D.tex}
\FloatBarrier
\setcounter{table}{0}
\section{CPU Time Details}\label{sec:appendixB}

In this section, tables for each problem group illustrate the CPU time usage per problem for all investigated algorithms. The average CPU time in seconds for the baseline algorithm, \gkmp, is displayed with a blue background. Percentage values for non-baseline algorithms represent the \emph{CPU time relative to the baseline algorithm} for each specific \km{} problem. Values below 100\% (faster than the baseline) are on green backgrounds, while values above 100\% (slower than the baseline) are on red backgrounds.

\input{coltables/t-cpu-opt.tex}

\input{coltables/t-cpu-fraenti.tex}

\input{coltables/t-cpu-fr-mod.tex}

\input{coltables/t-cpu-espiralX.tex}

\renewcommand{\tabcom}{The dimensionality of the data sets \emph{Norm25}, \emph{Cloud}, and \emph{Spam} is 15, 10, and 58, respectively.}
\input{coltables/t-cpu-high-D.tex}

\end{document}

