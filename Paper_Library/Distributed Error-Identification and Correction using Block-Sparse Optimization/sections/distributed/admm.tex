ADMM uses the theory of Lagrangian duality to solve convex optimization problems. Specifically, it introduces another set of primal variables (in addition to $\hat {\mathbf x}$) to decouple the optimization problem, and then augments the Lagrangian of the problem with a quadratic penalty term. To this end, we introduce the following set of primal variables corresponding to the $i^{th}$ agent: $\lbrace \hat{\mathbf w}_i^{(j)}\in \mathbb R^{n_i} \hsp |\hspace{2pt} j \in \mathcal N_i\rbrace$, whose elements are meant to represent $|\mathcal N_i|$ duplicate copies of agent $i$'s error vector. Along with the new set of primal variables, we introduce an equal number of \textit{consistency constraints}:
\begin{align}
   \hat{\mathbf w}_i^{(j)} = \hat{\mathbf x} [i], \quad \forall j \in \mathcal N_i
   \label{eq:consistency_constraints}
\end{align}
% We will show that the newly introduced primal variables have an intuitive interpretation: they are agent $i$'s \textit{opinions} of its neighbors' error vectors. 
Define $\hat {\mathbf w}\coloneqq \lbrace \hat{\mathbf w}_i^{(j)} \hsp | \hsp i\in \mathcal V, j\in\mathcal N_i \rbrace$ as the set of all the newly introduced primal variables, and let $\mathcal E_i\coloneqq \lbrace l \hsp | \hsp i\in\mathcal E^{(l)}\rbrace$ be the set of indices of the hyperedges that contain agent $i$.
By introducing the new primal variables and constraints into Problem P2 and then augmenting the objective function with quadratic penalty terms, we arrive at the following optimization problem:
\vspace{-35pt}
\begin{align*}
% P3 & Description
\begin{array}{rl}
    \textsc{P3:}   &
    % Minimize & Equation
    \begin{array}{rl}
        \underset{
        \scalebox{0.9}{$\hat{\mathbf x}, \hat{\mathbf w}$}
        }{\textnormal{minimize\ }}  
        & 
        \begin{array}{l}
            \\\vspace{7pt}\\
            \hspace{-5pt}
            \|\mathbf x^* + \hat{\mathbf x}\|_{2,1} 
            %
            + \frac{\rho}{2}\sum_{
                \begin{subarray}{l}
                l\in\mathcal E_i\\ i\in\mathcal V \end{subarray}
            }\|\mathbf c_i^{(l)}(\hat{\mathbf x}, \hat{\mathbf w})\|^2 \bigstrut[b] \vspace{4pt}\\ 
            %
            \hspace{24.25pt} + \bigstrut[t]\frac{\rho}{2}\sum_{
                \begin{subarray}{l}
                j\in\mathcal N_i\\ i\in\mathcal V \end{subarray}
            }
            \|\mathbf d_i^{(j)}(\hat{\mathbf x}, \hat{\mathbf w})\|^2
            \vspace{8pt}
        \end{array}
        \\ % SUBJECT TO
        \textnormal{\bigstrut[t] subject to
        } \quad
        & \big\lbrace \mathbf c_i^{(l)}(\hat{\mathbf x}, \hat{\mathbf w})= \mathbf 0 \big\rbrace_{
        \begin{subarray}{l}
                l\in\mathcal E_i\\ i\in\mathcal V \end{subarray}} 
        \vspace{4pt}
        \\& 
        \big\lbrace \mathbf d_i^{(j)}(\hat{\mathbf x}, \hat{\mathbf w})=\mathbf 0\big\rbrace_{
        \begin{subarray}{l}
                j\in\mathcal N_i\\ i\in\mathcal V \end{subarray}}
    \end{array} 
\end{array}
\end{align*}
where $\rho\in\mathbb R$ is a positive real number called the penalty parameter, and 
\begin{align}
\mathbf c_i^{(l)}(\hat{\mathbf x}, \hat{\mathbf w}) &\coloneqq \mathbf R[l, i]\hat {\mathbf x}[i]-\Big(\mathbf z[l] - \sum_{j\in\mathcal N_i}\mathbf R[l,j] \hat{\mathbf w}_{j}^{(i)}\Big) 
\nonumber\\
\mathbf d_i^{(j)}(\hat{\mathbf x}, \hat{\mathbf w}) &\coloneqq \hat{\mathbf x}[i] - \hat{\mathbf w}_i^{(j)}
\label{eq:d_ij}
\end{align}
are used to represent the constraints.
The first set of constraints in P3 is a decomposed version of the constraint $\mathbf R \hat {\mathbf x} = \mathbf z$ of P2. In fact, we have the following equivalence between Problems P2 and P3.

\begin{proposition}
    Suppose $(\bar{\mathbf x}, \bar{\mathbf w})$ is a minimizer of Problem P3, then $\bar{\mathbf x}$ also minimizes Problem P2.
\end{proposition}
\begin{proof}
As a minimizer $(\bar{\mathbf x}, \bar{\mathbf w})$ of P3 must by definition be feasible, the last two terms of the objective function of P3 vanish. Since $\mathbf d_i^{(j)}(\bar{\mathbf x}, \bar {\mathbf w})=\mathbf 0$, we have that $ \bar{\mathbf w}_i^{(j)}=\bar{\mathbf x}[i]$, for all $i$ and $j$.
    Substituting this in the other constraint equation, we have 
    \begin{align}
    \mathbf c_i^{(l)}(\bar{\mathbf x}, \bar{\mathbf w})&=\sum_{j\in\mathcal N_i \cup \lbrace i\rbrace}\mathbf R[l, j]\bar{\mathbf x}[j] - \mathbf z[l]\\ 
    &= \quad \sum_{j\in\mathcal V} \quad\mathbf R[l, j]\bar{\mathbf x}[j] - \mathbf z[l] = \mathbf 0
    \label{eq:prop_used_assumption}
    \end{align}
In (\ref{eq:prop_used_assumption}), we extended the summation to include terms which are identically $\mathbf 0$, due to the corresponding blocks of $\mathbf R$ being $\mathbf 0$ (cf. Assumption \ref{ass:jacobian}). Thus, it can be seen that
\begin{align}
\big\lbrace \mathbf c_i^{(l)}(\bar{\mathbf x}, \bar{\mathbf w})= \mathbf 0 \big\rbrace_{
        \begin{subarray}{l}
                l\in\mathcal E_i\\ i\in\mathcal V \end{subarray}}
\ \Rightarrow \ \mathbf R \bar{\mathbf x} = \mathbf z
\end{align}
\end{proof}

% The penalty parameter also has an intuitive interpretation, which will be made rigorous later: it controls the threshold of the fault-identification mechanism. 

Denote the Lagrangian of P3 by $L(\hat {\mathbf x}, \hat{\mathbf w}, \boldsymbol \lambda, \boldsymbol \mu)$, where
\begin{align}
{\boldsymbol \lambda} &\coloneqq \left\lbrace \hsp
{\boldsymbol \lambda}_{i}^{(l)} \in \mathbb R^{m_l} \hsp\big|\hspace{3pt}  
i \in \mathcal V,\hspace{3pt}
l \in \lbrace 1, \dots, |\mathcal E|\rbrace,\hspace{3pt} i\in \mathcal E^{(l)}
\right\rbrace\nonumber \\
%
{\boldsymbol \mu} &\coloneqq \left\lbrace \hsp
{\boldsymbol \mu}_{i}^{(j)} \in \mathbb R^{n_i}\hsp\big|\hspace{3pt}  
i \in \mathcal V,\hspace{3pt}
j\in \mathcal N_i
\right\rbrace
\end{align}
are the sets of dual variables (or Lagrange multipliers) corresponding to the two sets of constraints in P3. In each iteration of ADMM, the Lagrangian $L(\hat {\mathbf x}, \hat{\mathbf w}, \boldsymbol \lambda, \boldsymbol \mu)$ is first minimized with respect to $\hat{\mathbf x}$, then it is minimized with respect to $\hat{\mathbf w}$, 
% (this is what is meant by the term \textit{alternating direction} \cite[p. 14]{boyd2011distributed})
and finally, a gradient ascent step is used to update the dual variables \cite{boyd2011distributed}. 
% Note that a stationary point (a point where the partial derivatives vanish) of $L(\hat{\mathbf x}, \hat{\mathbf w}, \boldsymbol \lambda, \boldsymbol \mu)$ indeed minimizes P3 (and hence, P2), as per the usual results in Lagrangian duality theory; the advantage of using ADMM is that we can perform the primal minimization steps successively, in alternating directions \cite[p. 14]{boyd2011distributed}.
Our next task of order is to show that each of the primal minimization steps can be carried out in a distributed manner. In the following, we omit the arguments of $L$, ${\mathbf c_i^{(l)}}$ and ${\mathbf d_i^{(j)}}$ for brevity.
\begin{proposition}
The Lagrangian of P3 can be expressed in the following, equivalent forms:
\begin{align}
L &=
\sum_{i\in\mathcal V}
\bigg[
\|\hat{\mathbf x}[i]+\mathbf x^*[i]\| 
+\sum_{l\in\mathcal E_i} \left(
\frac{\rho}{2}\|\mathbf c_i^{(l)}\|^2 + \boldsymbol{\lambda}_i^{(l)
\raisebox{2pt}{$\scriptstyle\top$}
}
\mathbf c_i^{(l)} \right)
\nonumber\\
&\hspace{75pt} +
\sum_{j\in\mathcal N_i} \left( \frac{\rho}{2}\|
\mathbf d_i^{(j)}
\|^2 + \boldsymbol{\mu}_i^{(j)
\raisebox{2pt}{$\scriptstyle\top$}
}
\mathbf d_i^{(j)}\right)\bigg]
\label{eq:L'_i_def}\\
&= \sum_{i\in\mathcal V}\bigg[
\ \cdots \ +
\sum_{j\in\mathcal N_i} \left( \frac{\rho}{2}\|
\mathbf d_j^{(i)}
\|^2 + \boldsymbol{\mu}_j^{(i)
\raisebox{2pt}{$\scriptstyle\top$}
}
\mathbf d_j^{(i)}\right) \bigg]
\label{eq:L'_i_def_2}
\end{align}
where `$\cdots\hspace{0.5pt}$' is used to indicate that the first two terms inside the summation $\sum_{i\in\mathcal V}(\hsp\cdot\hsp)$ are identical in both expressions.
\end{proposition}
\begin{proof}
Firstly, note that $\|\mathbf x\|_{2,1}=\sum_{i\in\mathcal V}\|\mathbf x[i]\|$, which follows from the definition of $\|\hsp\cdot\hsp\|_{2,1}$. Thus, each term of the objective function of P3 has a summation of the form $\sum_{i\in\mathcal V}(\hsp\cdot\hsp)$ around it. It is also clear by comparing P3 and (\ref{eq:L'_i_def}) how the constraints are being split between the agents, as well as the pairing between the constraints and the Lagrange multipliers. It only remains to be seen that the Lagrangian can be equivalently expressed in the two different forms that are given in (\ref{eq:L'_i_def}) and (\ref{eq:L'_i_def_2}). To see this, observe that
\begin{align}
   \sum_{i\in\mathcal V} \sum_{j\in\mathcal N_i} &\Big( \frac{\rho}{2}\|
\mathbf d_i^{(j)}
\|^2 + \boldsymbol{\mu}_i^{(j)
\raisebox{2pt}{$\scriptstyle\top$}
}
\mathbf d_i^{(j)}\Big)\nonumber\\
&=\sum_{j\in\mathcal V} \sum_{i\in\mathcal N_j} \Big( \frac{\rho}{2}\|
\mathbf d_i^{(j)}
\|^2 + \boldsymbol{\mu}_i^{(j)
\raisebox{2pt}{$\scriptstyle\top$}
}
\mathbf d_i^{(j)}\Big)
\label{eq:L'_equivalence}
\end{align}
which is true because the summations $\sum_{i\in\mathcal V}\sum_{j\in\mathcal N_i}(\hsp \cdot \hsp)$ and  $\sum_{j\in\mathcal V}\sum_{i\in\mathcal N_j}(\hsp \cdot \hsp)$ both sum over the same set of terms.
By interchanging the indices on the right-hand side of (\ref{eq:L'_equivalence}), we see that the two decompositions of $L$ are equivalent.
\end{proof}

By decomposing the Lagrangian appropriately, the agents can execute the primal minimization steps in parallel, without needing to perform any redundant computations (e.g., due to two or more agents minimizing over the same variable).
The overall distributed multi-agent FDIR protocol which combines the SCP and ADMM optimization techniques is presented in Algorithm \ref{alg:admm}. The proposed algorithm has two loops: the outer loop corresponds to SCP, and the inner loop corresponds to the ADMM subroutine (which is presented separately, in Alg. \ref{alg:inner}). 
% The outer loop is repeated $N_{\textrm{SCP}}$ times and the inner loop is repeated $N_{\textrm{ADMM}}$ times, where $N_{\textrm{SCP}}$ and $N_{\textrm{ADMM}}$ are positive integers. 
To implement the proposed FDIR algorithm, all the agents must know the penalty parameter $\rho$ and the inter-agent communications must be synchronous; these requirements can be fulfilled by using the distributed algorithms given in
\cite{olshevsky2009convergence} and \cite{sync2010}, respectively.

% Additionally, we introduce the quantities $\mathbf x^*, \bar {\mathbf x}\in\mathbb R^n$ in the algorithm, which are updated in the outer and inner loops, respectively. As $(\hat {\mathbf p} + \mathbf x^*)$ represents an updated estimate of the state of the multi-agent configuration, the measurement model $\mathbf \Phi(\hsp \cdot\hsp)$ is linearized about $(\hat {\mathbf p} + \mathbf x^*)$ at the beginning of each SCP iteration.

% \vspace{2pt}
% \begin{remark}
% A stopping condition for the overall FDIR algorithm may be introduced below step 5 of Algorithm \ref{alg:admm}, in which it is checked whether the value of $\|\bar{\mathbf x}[i]\|$ lies below a desired threshold, $\forall j\in\mathcal N_i\cup \lbrace i\rbrace$.
% As shown in \cite[Prop. 2]{khan2023recovery}, a small value of $\|\bar {\mathbf x}\|$ indicates that $\mathbf x^*$ is a local minimizer of $\|\hsp\cdot\hsp\|_{2,1}$ subject to the nonlinear constraint, $\mathbf y = \mathbf \Phi(\hat {\mathbf p}+\mathbf x^*)$.
% \end{remark}
% \vspace{2pt}

% At the start of each SCP iteration, the Jacobian matrix $J_{\mathbf \Phi}(\hsp\cdot\hsp)$ is re-linearized.  
In either of the primal minimization steps of the proposed FDIR algorithm at agent $i$ (i.e., steps 3 and 5 of Alg. \ref{alg:inner}), the agent solves a convex optimization problem whose dimension scales linearly with the size of the agent's neighborhood in $\mathcal G$, i.e., $|\mathcal N_i|$, which can be considered as a measure of how densely connected the network is.
Neither the computational cost nor the communication cost of the proposed FDIR algorithm (at a given agent) scales with the size of the graph, $|\mathcal V|$, making it an efficient solution for FDIR in large-scale multi-agent systems.
%
%
% \vspace{2pt}
% \begin{remark}[Penalty Parameter]
% The penalty parameter $\rho$ is related to the (strong) convexity of the objective function of P3, as shown in \cite[Sec. 3.2.1]{bertsekas}. By Lagrange duality theory, convexity of the primal objective function corresponds to the Lipschitz smoothness of the dual objective function \cite[Sec. 4]{goebel2008local}, whereas Lipschitz smoothness determines the step-size of gradient ascent/descent methods \cite[Prop. 1.2.3]{bertsekas}. These observations together explain why
% $\rho$ is also used as the step-size in the dual update of ADMM, as in step 11 of Alg. \ref{alg:admm}.
% \end{remark}
% \vspace{2pt}
\vspace{2pt}
\begin{remark}[Warm Start of the ADMM Loop]
In Alg. \ref{alg:admm}, the dual variables $\lbrace \boldsymbol{\lambda}_i^{(l)}\rbrace _{l\in\mathcal E_i}$ and $\lbrace \boldsymbol{\mu}_i^{(j)}\rbrace_{j\in\mathcal N_i}$ are retained between successive SCP iterations (as opposed to re-initializing these variables as $\mathbf 0$), which is referred to as a \textit{warm start} of the ADMM loop.
Using the interpretation of the dual variables given in \cite[Prop. 3.2.2.]{bertsekas}, it can be seen that the optimal values of the dual variables are not expected to change by much after re-linearization of the constraint, thereby explaining why the warm start should yield better convergence properties.
\end{remark}
\vspace{2pt}
\input{sections/distributed/admm_algorithm}
\input{sections/distributed/admm_subroutine}