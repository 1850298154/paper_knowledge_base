\section{Related Work}\label{sec:related_works}
\paragraph{Normalization methods.}
Normalization has long been a central tool for stabilizing optimization and improving convergence in deep networks. Batch Normalization (BN)~\cite{batch_normalization_ioffe2015batch} normalizes activations using mini-batch statistics and has been widely successful in convolutional architectures, but its behavior can depend on batch size and distributed synchronization. Layer Normalization (LN)~\cite{layernorm_ba2016layer} and its variants remove batch dependence by computing statistics across features within each sample and have become the standard in Transformers. Related methods such as Instance Normalization (IN)~\cite{instance_norm_ulyanov2016instance}, Group Normalization (GN)~\cite{group_norm_wu2018group}, RMSNorm~\cite{rmsnorm_zhang2019root}, and nGPT~\cite{ngpt_loshchilov2024ngpt} further tailor normalization to specific architectural or efficiency constraints.


\textbf{Normalization Placement in Transformers.}
Beyond the choice of normalization operator, its \emph{placement} within Transformer blocks plays a critical role in optimization stability. The original Transformer architecture adopted post-normalization (PostNorm), in which normalization follows residual addition~\cite{transformer_vaswani2017attention}. Subsequent large-scale practice shifted toward pre-normalization (PreNorm), placing normalization before attention and MLP sublayers to improve trainability in deep networks~\cite{prenorm_wang2019learning}.

Recent work further systematizes normalization placement and explores additional insertion points. Deeply Normalized Transformer (DNT)~\cite{dnt_qi2025dnt} categorizes multiple strategies—including InputNorm, PreNorm, MidNorm, PostNorm, and QKNorm—and motivates them through a Jacobian- and gradient-stability analysis. DNT ultimately combines InputNorm, PreNorm, MidNorm, and QKNorm, while avoiding PostNorm due to its potential training instabilities. Among these placements, QK normalization (QKNorm)~\cite{qk_norm_henry2020query} specifically targets the attention mechanism, stabilizing the geometry of query–key interactions and mitigating softmax saturation.

By treating normalization as a design space over both operator and location, these works emphasize that stability and conditioning can be targeted at specific architectural subcomponents, rather than only at block outputs. As model depth increases, normalization also interacts with residual pathways and initialization. DeepNorm~\cite{deepnorm_wang2022deepnet}, for example, modifies residual scaling and initialization to bound parameter updates and control dynamical growth with depth, complementing normalization-placement strategies.


\textbf{Normalization-free Transformers.}
Motivated by the cost/complexity of normalization and the desire for simpler training dynamics, recent work questions whether explicit normalization is necessary in Transformers.
\emph{Transformers without Normalization} shows that replacing normalization layers with a simple point-wise nonlinearity, {Dynamic Tanh (DyT)}~\cite{dyt_zhu2025transformers}, can match normalized baselines across tasks, suggesting that an appropriate bounded nonlinearity can provide much of the stability typically attributed to LN/RMSNorm. 
Building on this, \emph{Stronger Normalization-Free Transformers}~\cite{stronger_norm_chen2025stronger} studies the design of point-wise functions more broadly and reports improved normalization-free performance via a searched function family (e.g., \(\mathrm{Derf}\)), outperforming LN/RMSNorm/DyT across multiple domains. Despite being framed as normalization-free, these approaches fundamentally operate by controlling the norm of activations through bounded transformations, and can therefore be viewed as a form of implicit normalization.



\textbf{Positioning of our work.}
While our method can be viewed as a study of normalization placement in Transformers, its key distinction lies in explicitly linking architectural design to second-order optimization geometry. Rather than motivating normalization heuristically or empirically, we analyze how local normalization immediately following linear mappings stabilizes activation scale and, in turn, constrains the spectral norm of the Hessian and leads to a smoother optimization landscape. This perspective yields a principled characterization of the maximum tolerable learning rate and provides a unified theoretical explanation for optimization stability in large Transformer models.