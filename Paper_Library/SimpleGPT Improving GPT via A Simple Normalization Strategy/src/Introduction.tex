\section{Introduction}
Transformer-based large language models (LLMs)~\cite{gpt1_radford2018improving,gpt2_radford2019language, gpt3_brown2020language, llama1_touvron2023llama, llama2_touvron2023llama, llama3_dubey2024llama, palm_chowdhery2023palm, deepseek_v3_liu2024deepseek, qwen_team2023qwen} have achieved state-of-the-art performance across a wide range of tasks. As these models scale in depth and width, optimization stability increasingly constrains performance and scalability. Many architectural components in modern Transformers—such as residual connections~\cite{resnet_he2016deep}, normalization layers~\cite{batch_normalization_ioffe2015batch, layernorm_ba2016layer, rmsnorm_zhang2019root, modular_norm_large2024scalable}, and nonlinear activations~\cite{swishglu_shazeer2020glu}—are primarily introduced to stabilize training, and in some cases to increase expressivity. Understanding optimization from a principled perspective is therefore central to the design of scalable Transformer architectures.

Classical optimization theory~\cite{nesterov1983method, nesterov1998introductory, nocedal1999numerical,boyd_boyd2004convex} provides a precise connection between optimization stability and second-order geometry. For a twice-differentiable objective \( \ell(\vx) \), the local curvature is characterized by the Hessian
$\mH_{\vx\vx} = \nabla^2 \ell(\vx).$
If \( \ell \) is \(\beta\)-smooth, then
$
\|\nabla \ell(\vx) - \nabla \ell(\vy)\|_2 \le \beta \|\vx - \vy\|_2, \forall {\vx, \vy}.
$
Standard results imply that gradient descent is stable only when \textbf{the maximum tolerable learning rate} $\eta$ satisfies
\[
\eta \le \frac{2}{\beta} = \frac{2}{\sup_{\vx}\,\|\mH_{\vx\vx}\|_2},
\]
establishing the Hessian spectral norm as the fundamental quantity governing admissible learning rates and convergence behavior.

In contrast, much of the recent literature on Transformer optimization focuses on architectural heuristics without explicitly analyzing their relationship with classical optimization theory. Techniques such as normalization placement~\cite{transformer_vaswani2017attention, prenorm_wang2019learning, qk_norm_henry2020query, lipsformer_qilipsformer, stable_transformer_qi2025stabletransformer, dnt_qi2025dnt}, residual scaling~\cite{resnet_he2016deep, rezero_bachlechner2021rezero,mhc_xie2025mhc}, or modified nonlinearities~\cite{gelu_hendrycks2016gaussian, swishglu_shazeer2020glu} are typically justified empirically, while their impact on activation scale, Hessian geometry, and thereby optimal learning rates remains implicit. As a result, the theoretical relationship between network design and classical stability conditions is not well understood, despite its relevance to training very deep and large-scale models.

In this work, we bridge this gap by analyzing Transformer architectures through the central lens of Hessian-based optimization theory, while accounting for the role of activation scale. We introduce a simple normalization strategy, termed \emph{SimpleNorm}, which stabilizes intermediate activation scales through normalization \emph{immediately} following linear mappings. Building on this structural property, we analyze the Hessian of the loss with respect to network activations and show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting substantially larger stable learning rates. By grounding Transformer design in classical optimization principles~\cite{nesterov1983method,nesterov1998introductory,nestorov_nesterov2013introductory}, our framework provides a unified explanation for existing stabilization techniques and offers principled guidance for building scalable and stable models.

Our contributions can be summarized as follows:
\begin{itemize}
    \item We revisit Transformer optimization through the lens of second-order geometry and establish a direct connection between architectural design, activation scale, the Hessian, and the maximum tolerable learning rate.
    \item We introduce SimpleGPT, a new GPT architecture based on SimpleNorm, and theoretically show that this design significantly reduces $\|\mH_{\vx\vx}\|_2$, yielding a smaller Lipschitz gradient constant and enabling substantially larger stable learning rates.
    \item We demonstrate experimentally that these theoretical advantages are accompanied by consistent empirical gains across \texttt{nanoGPT}, \texttt{LLaMA2}, and \texttt{LLaMA3} architectures, for model sizes ranging from 1B to 8B parameters. Specifically, when training 7B-scale models for 60K steps, our method achieves a training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208.
\end{itemize}
