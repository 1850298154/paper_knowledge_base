The derivations of the equations appearing in both the main text and the appendix are available in the following material~\cite{matrix_computation_golub2013matrix, matrix_analysis_horn2012matrix, matrix_cookbook_petersen2008matrix, matrix_diff_magnus2019matrix, nesterov1983method, nesterov1998introductory, nestorov_nesterov2013introductory, boyd_boyd2004convex, qi_taming_transformer_qitaming, qi2023understanding, dnt_qi2025dnt}.


\section{Derivatives of  $\nabla_{\vx}\ell$ and $\nabla_{\vx}^{2}\ell$ for SimpleNorm}
\label{appendix:derivation_of_x_simplenorm}

We consider the mapping
\[
\vy
=
\vgamma \odot \sqrt{d}\,
\frac{\mW\vx}{\|\mW\vx\|_2},
\]
where $\vgamma \in\mathbb{R}^{d}$, and $\odot$ denotes elementwise multiplication and $\ell=\ell(\vy)$ is a scalar loss.




As before, let us define some intermediate variables,
\begin{equation*}
\vz=\mW\vx,
\qquad
s=\|\vz\|_2,
\qquad
\vu=\frac{\vz}{s},
\qquad
\mP=\mI-\vu\vu^{\top}, 
\qquad \mD=\operatorname{Diag}(\vgamma),
\end{equation*}
\noindent where $\mP, \mD$ are symmetric.

Let us define the normalized and scaled output as
\begin{equation*}
\vy=\sqrt{d}\,\mD\,\vu,
\end{equation*}
and let
\begin{equation*}
\vg_{\vy}:=\nabla_{\vy}\ell,
\qquad
\mH_{\vy\vy}:=\nabla^2_{\vy\vy}\ell.
\end{equation*}


Let us define the Jacobian $\mA$ of the normalization map $\vu$ with respect to $\vz$ as
\begin{equation*}
\mA:=\frac{\partial \vu}{\partial \vz}
=
\frac{1}{s}\,\mP,
\end{equation*}
and consequently
\begin{equation*}
\frac{\partial \vy}{\partial \vz}
=
\sqrt{d}\,\mD\,\mA
=
\frac{\sqrt{d}}{s}\,\mD\,\mP.
\end{equation*}
We will repeatedly use the chain rule through the path
\[
\mW \to \vz \to \vu \to \vy \to \ell,
\qquad\text{and similarly}\qquad
\vgamma \to \mD \to \vy \to \ell .
\]

%------------------------------------------------------------
\paragraph{(1) First-order derivative.}
Since $\vz=\mW\vx$, we have
\begin{equation*}
d\vz=\mW\,d\vx.
\end{equation*}
From $\vu=\vz/s$ and $\mA=\partial \vu/\partial \vz$, it follows that
\begin{equation*}
d\vu=\mA\,d\vz=\frac{1}{s}\,\mP\,d\vz.
\end{equation*}
Therefore,
\begin{equation*}
d\vy
=
\sqrt{d}\,\mD\,d\vu
=
\sqrt{d}\,\mD\,\mA\,\mW\,d\vx,
\end{equation*}
so the Jacobian is
\begin{equation*}
\mJ_{\vx}^{\vy}:=\frac{\partial \vy}{\partial \vx}
=
\sqrt{d}\,\mD\,\mA\,\mW
=
\frac{\sqrt{d}}{s}\,\mD\,\mP\,\mW.
\end{equation*}


Applying the chain rule gives
\begin{equation*}
\nabla_{\vx}\ell
=
{\mJ_{\vx}^{\vy}}^{\top}\vg_{\vy}
=
\sqrt{d}\,\mW^{\top}\mA^{\top}\mD^{\top}\vg_{\vy}.
\end{equation*}
Using $\mA^{\top}=\mA$ and $\mD^{\top}=\mD$, we obtain
\begin{equation}
\nabla_{\vx}\ell
=
\sqrt{d}\,\mW^{\top}\mA\,\mD\,\vg_{\vy}
=
\frac{\sqrt{d}}{s}\,\mW^{\top}\mP\,\mD\,\vg_{\vy}.
\end{equation}

%------------------------------------------------------------
\paragraph{(2) Second-order derivative.}

Differentiating $\nabla_{\vx}\ell=\sqrt{d}\,\mW^{\top}\mA\,\mD\,\vg_{\vy}$ yields
\begin{equation*}
d(\nabla_{\vx}\ell)
=
\sqrt{d}\,\mW^{\top}
\Bigl(\mA\,\mD\,d\vg_{\vy} + d\mA\,\mD\,\vg_{\vy}
\Bigr).
\end{equation*}
This induces the standard decomposition
\begin{equation*}
\nabla_{\vx}^2\ell
=
{\mJ_{\vx}^{\vy}}^{\top}\mH_{\vy\vy}\mJ_{\vx}^{\vy}
+
\mC,
\end{equation*}
where the two terms are computed below.

%------------------------------------------------------------
(A) Linear (Gauss--Newton) term.

Since $d\vg_{\vy}=\mH_{\vy\vy}\,d\vy$, and $d\vy=\mJ_{\vx}^{\vy}\,d\vx$, we have
\begin{equation*}
d\vg_{\vy}
=
\mH_{\vy\vy}\,\mJ_{\vx}^{\vy}\,d\vx.
\end{equation*}
Substituting into $\sqrt{d}\,\mW^{\top}\mA\,\mD\,d\vg_{\vy}$ gives
\begin{equation*}
\sqrt{d}\,\mW^{\top}\mA\,\mD\,d\vg_{\vy}
=
\sqrt{d}\,\mW^{\top}\mA\,\mD\,\mH_{\vy\vy}\,\mJ_{\vx}^{\vy}\,d\vx
=
{\mJ_{\vx}^{\vy}}^{\top}\mH_{\vy\vy}\mJ_{\vx}^{\vy}\,d\vx.
\end{equation*}
Using $\mA=\mP/s$ and $\mJ_{\vx}^{\vy}=(\sqrt{d}/s)\mD\mP\mW$, we obtain the explicit form
\begin{equation*}
{\mJ_{\vx}^{\vy}}^{\top}\mH_{\vy\vy}\mJ_{\vx}^{\vy}
=
\frac{d}{s^2}\,
\mW^{\top}\mP\,\mD\,\mH_{\vy\vy}\,\mD\,\mP\,\mW.
\end{equation*}

%------------------------------------------------------------
(B) Curvature (normalization) term.

Recall $\mA=\frac{1}{s}\mP$.
Using $d\vu=\mA\,d\vz$ and $ds=d\|\vz\|_2=\vu^{\top}d\vz$, one obtains
\begin{equation*}
d\mA
=
-\frac{1}{s^{2}}
\Bigl(
(\vu^{\top}d\vz)\mP
+\mP\,d\vz\,\vu^{\top}
+\vu\,d\vz^{\top}\mP
\Bigr).
\end{equation*}
Right-multiplying by $\mD\,\vg_{\vy}$ gives
\begin{equation*}
d\mA\,\mD\,\vg_{\vy}
=
-\frac{1}{s^{2}}
\Bigl(
(\vu^{\top}d\vz)\mP\,\mD\,\vg_{\vy}
+\mP\,d\vz\,\vu^{\top}\mD\,\vg_{\vy}
+\vu\,d\vz^{\top}\mP\,\mD\,\vg_{\vy}
\Bigr).
\end{equation*}
Equivalently, pulling the scalar contractions to the left yields the linear map in $d\vz$:
\begin{equation*}
d\mA\,\mD\,\vg_{\vy}
=
-\frac{1}{s^{2}}
\Bigl(
\mP \mD\vg_{\vy} \vu^{\top} 
+ \vu^{\top}\mD\vg_{\vy}\mP 
+ \vu \vg_{\vy}^{\top}\mD\mP
\Bigr)\,d\vz.
\end{equation*}
Substituting $d\vz=\mW\,d\vx$ and left-multiplying by $\sqrt{d}\,\mW^{\top}$ gives the curvature term
\begin{equation*}
\mC
=
-\frac{\sqrt{d}}{s^{2}}\,
\mW^{\top}
\Bigl(
\mP \mD\vg_{\vy} \vu^{\top} 
+ \vu^{\top}\mD\vg_{\vy}\mP 
+ \vu \vg_{\vy}^{\top}\mD\mP
\Bigr)\mW .
\end{equation*}
(If desired, define $\vg_1:=\mD\vg_{\vy}$ to simplify the expression, but we keep the figure's symbols explicit.)



Combining the linear and curvature terms, the Hessian with respect to $\vx$ is
\begin{equation}
\mH_{\vx\vx} = 
\nabla_{\vx}^2 \ell
=
\frac{d}{s^2}\,
\mW^{\top}\mP\,\mD\,\mH_{\vy\vy}\,\mD\,\mP\,\mW
\;-\;
\frac{\sqrt{d}}{s^{2}}\,
\mW^{\top}
\Bigl(
\mP \mD\vg_{\vy} \vu^{\top} 
+ \vu^{\top}\mD\vg_{\vy}\mP 
+ \vu \vg_{\vy}^{\top}\mD\mP
\Bigr)\mW, 
\end{equation}
where $
\vz=\mW\vx,
s=\|\vz\|_2,
\vu=\frac{\vz}{s},
\mP=\mI-\vu\vu^{\top}, \mD=\operatorname{Diag}(\vgamma).$

\ 

\ 


\ 

\section{Derivatives of $\nabla_{\mW}\ell$ and $\nabla_{\operatorname{vec}(\mW)}^{2}\ell$ for $\vy = \mW \vx$}
\label{appendix:derivation_of_W_linear}
We consider the linear mapping
\[
\vy = \mW\vx,
\]
where \(\mW\in\mathbb{R}^{d\times m}\), \(\vx\in\mathbb{R}^{m}\), and \(\vy\in\mathbb{R}^{d}\).
Let \(\ell=\ell(\vy)\) be a scalar-valued loss function.

We define the first- and second-order derivatives of the loss $l$
with respect to \(\vy\) as
\[
\vg_{\vy} \;:=\; \nabla_{\vy}\ell(\vy)\in\mathbb{R}^{d},
\qquad
\mH_{\vy\vy} \;:=\; \nabla_{\vy}^{2}\ell(\vy)\in\mathbb{R}^{d\times d}.
\]

\paragraph{(1) First-order derivative with respect to \(\mW\).}
Since \(\vy\) depends linearly on \(\mW\), a first-order variation satisfies
\[
d\vy = d\mW\,\vx.
\]
Applying the chain rule,
\[
d\ell = \vg_{\vy}^{\top} d\vy
       = \vg_{\vy}^{\top}(d\mW\,\vx)
       = \langle \vg_{\vy}\vx^{\top},\, d\mW\rangle_F .
\]
Matching coefficients under the Frobenius inner product yields the gradient
\begin{equation}
    \nabla_{\mW}\ell
    \;=\;
    \vg_{\vy}\,\vx^{\top}.
\end{equation}



\paragraph{(2) Second-order derivative with respect to \(\mW\).}
To express second-order derivatives, it is convenient to vectorize \(\mW\).
Using the standard identity for matrix--vector products,
\[
\vy
=
(\vx^{\top}\otimes \mI_d)\,\operatorname{vec}(\mW),
\]
which makes the dependence of \(\vy\) on \(\operatorname{vec}(\mW)\) explicit.

\paragraph{Jacobian with respect to \(\operatorname{vec}(\mW)\).}
From the above expression, the Jacobian of \(\vy\) with respect to
\(\operatorname{vec}(\mW)\) is
\[
\mJ_{\operatorname{vec}(\mW)}^{\vy}
\;\triangleq\;
\frac{\partial \vy}{\partial \operatorname{vec}(\mW)}
=
\vx^{\top}\otimes \mI_d
\;\in\;\mathbb{R}^{d\times (md)}.
\]

\paragraph{Hessian with respect to \(\operatorname{vec}(\mW)\).}
Because the mapping \(\mW\mapsto \vy\) is linear, it contributes no second-order term.
Hence, by the second-order chain rule, the Hessian of the loss with respect to
\(\operatorname{vec}(\mW)\) is
\[
\nabla_{\operatorname{vec}(\mW)}^{2}\ell
=
{\mJ_{\operatorname{vec}(\mW)}^{\vy}}^{\top}\,
\mH_{\vy\vy}\,
\mJ_{\operatorname{vec}(\mW)}^{\vy}.
\]
Substituting the explicit Jacobian gives us
\[
\nabla_{\operatorname{vec}(\mW)}^{2}\ell
=
(\vx\otimes \mI_d)\,\mH_{\vy\vy}\,(\vx^{\top}\otimes \mI_d).
\]

\paragraph{Kronecker-product simplification.}
Using standard identities of the Kronecker product, the Hessian simplifies to
\begin{equation}
    \nabla_{\operatorname{vec}(\mW)}^{2}\ell
    =
    (\vx\vx^{\top})\otimes \mH_{\vy\vy}.
\end{equation}

This result shows that the Hessian with respect to the weight matrix factorizes
into a Kronecker product of an input-dependent term \(\vx\vx^{\top}\) and an
output-space curvature term \(\mH_{\vy\vy}\).
Equivalently, when viewed as a block matrix with \(m\times m\) blocks of size
\(d\times d\), the \((i,j)\)-th block is
\[
\big[\nabla_{\operatorname{vec}(\mW)}^{2}\ell\big]_{(i,j)\text{ block}}
=
x_i x_j\,\mH_{\vy\vy},
\]
where \(x_i\) denotes the \(i\)-th entry of \(\vx\).

\ 

\ 

\



\section{Derivatives of  $\nabla_{\vgamma}\ell$, $\nabla_{\vgamma}^{2}\ell$, $\nabla_{\mW}\ell$ and $\nabla_{\operatorname{vec}(\mW)}^{2}\ell$ where $\vy = 
\vgamma \odot \sqrt{d}\, \frac{\mW\vx}{\|\mW\vx\|_2}$.}
\label{appendix:derivation_of_W_simplenorm}

We consider the mapping
\[
\vy = 
\vgamma \odot \sqrt{d}\, \frac{\mW\vx}{\|\mW\vx\|_2},
\]
where $\odot$ denotes elementwise multiplication and $\ell=\ell(\vy)$ is a scalar loss.
Define the intermediate variables
\[
\vz=\mW\vx,\qquad
s=\|\vz\|_2,\qquad
\vu=\frac{\vz}{s},\qquad
\mP=\mI-\vu\vu^{\top},\qquad
\mD=\operatorname{Diag}(\vgamma).
\]
We have
\[
\vy=\sqrt{d}\,\mD\vu,\qquad
\ell=\ell(\vy),\qquad
\vg_{\vy}:=\nabla_{\vy}\ell,\qquad
\mH_{\vy\vy}:=\nabla^2_{\vy\vy}\ell.
\]

% ============================================================
Remind two useful Jacobian matrices,

\[
\frac{\partial \vu}{\partial \vz}=\mA=\frac{1}{s}\mP,
\qquad
\frac{\partial \vy}{\partial \vz}
=\sqrt{d}\,\mD\mA
=\frac{\sqrt{d}}{s}\,\mD\mP.
\]

We will repeatedly use the chain rule through the path
$\mW \to \vz \to \vu \to \vy \to \ell$,
and similarly
$\vgamma \to \mD \to \vy \to \ell$.

% ============================================================
\subsection{Part I: derivatives w.r.t.\ $\vgamma$}

\paragraph{(1) First-order derivative.}

Since $\vy_i=\sqrt{d}\,\gamma_i u_i$, the Jacobian of $\vy$ w.r.t.\ $\vgamma$ is diagonal:
\[
\frac{\partial \vy}{\partial \vgamma}
=\sqrt{d}\,\operatorname{Diag}(\vu).
\]
Applying the chain rule
$\vg_{\vgamma}= \big(\frac{\partial \vy}{\partial \vgamma}\big)^{\!\top}\vg_{\vy}$,
we obtain
\begin{equation}
   \vg_{\vgamma}
=\nabla_{\vgamma}\ell
=\sqrt{d}\,\operatorname{Diag}(\vu)\,\vg_{\vy}
=\sqrt{d}\,(\vu\odot \vg_{\vy}). 
\end{equation}


\paragraph{(2) Second-order derivative.}

Because $\vy$ is linear in $\vgamma$,
$\frac{\partial^2 \vy}{\partial \vgamma^2}=0$,
hence the Hessian comes purely from $\mH_{\vy\vy}$:
\begin{equation}
  \mH_{\vgamma\vgamma}
=\nabla^2_{\vgamma\vgamma}\ell
=\Big(\frac{\partial \vy}{\partial \vgamma}\Big)^{\!\top}
\mH_{\vy\vy}
\Big(\frac{\partial \vy}{\partial \vgamma}\Big)
=
d\ \operatorname{Diag}(\vu)\,\mH_{\vy\vy}\,\operatorname{Diag}(\vu).  
\end{equation}

% ============================================================
\subsection{Part II: derivatives w.r.t.\ $\mW$}
% ============================================================
\paragraph{(1) First-order derivative.}

Step 1 ($\vy\to\vz$): gradient w.r.t.\ $\vz$

By the chain rule,
\[
\vg_{\vz}
=\Big(\frac{\partial \vy}{\partial \vz}\Big)^{\!\top}\vg_{\vy}
=\Big(\frac{\sqrt{d}}{s}\mD\mP\Big)^{\!\top}\vg_{\vy}.
\]
Using $\mP^{\top}=\mP$ and $\mD^{\top}=\mD$, this simplifies to
\[
\vg_{\vz}
=\frac{\sqrt{d}}{s}\,\mP\,\mD\,\vg_{\vy}.
\]

Step 2 ($\vz=\mW\vx\to\mW$): gradient w.r.t.\ $\mW$

We write the differential $d\vz=d\mW\,\vx$, so
\[
d\ell
=\vg_{\vz}^{\top}d\vz
=\vg_{\vz}^{\top}(d\mW\,\vx)
=\langle \vg_{\vz}\vx^{\top},\, d\mW\rangle_F.
\]
Matching coefficients under the Frobenius inner product yields
\begin{equation}
    \vg_{\mW}
=\nabla_{\mW}\ell
=\vg_{\vz}\vx^{\top}
=\frac{\sqrt{d}}{s}\,(\mP\mD\vg_{\vy})\,\vx^{\top}.
\end{equation}

If one needs the vectorized mapping,
\[
\operatorname{vec}(\vz)
=\operatorname{vec}(\mW\vx)
=(\vx^{\top}\otimes \mI)\operatorname{vec}(\mW).
\]

% ============================================================
\paragraph{(2) Second-order derivative.}

We first derive $\mH_{\vz\vz}:=\nabla^2_{\vz\vz}\ell$,
then lift it to $\operatorname{vec}(\mW)$.

Step 1: Hessian w.r.t.\ $\vz$

The second-order chain rule gives
\[
\mH_{\vz\vz}
=
\Big(\frac{\partial \vy}{\partial \vz}\Big)^{\!\top}
\mH_{\vy\vy}
\Big(\frac{\partial \vy}{\partial \vz}\Big)
+
\sum_{k=1}^{d}(\vg_{\vy})_k\,\nabla^2_{\vz\vz} y_k.
\]

The first term is the Gauss--Newton part:
\[
\Big(\frac{\partial \vy}{\partial \vz}\Big)^{\!\top}
\mH_{\vy\vy}
\Big(\frac{\partial \vy}{\partial \vz}\Big)
=
\frac{d}{s^2}\,
\mP\,\mD\,\mH_{\vy\vy}\,\mD\,\mP.
\]

For the second term, we use the bilinear form of the normalization Hessian:
\[
\nabla^2 \vu(\vz)[\va,\vb]
=
-\frac{1}{s^2}
\Big(
(\vu^{\top}\va)\,\mP\vb
+(\vu^{\top}\vb)\,\mP\va
+(\va^{\top}\mP\vb)\,\vu
\Big).
\]

Since $\vy=\sqrt{d}\mD\vu$,
\[
\sum_{k=1}^{d}(\vg_{\vy})_k\,\nabla^2 y_k[\va,\vb]
=
\vg_{\vy}^{\top}\nabla^2\vy[\va,\vb]
=
\sqrt{d}\,(\mD\vg_{\vy})^{\top}\nabla^2\vu[\va,\vb].
\]

Substituting gives
\[
\sum_{k=1}^{d}(\vg_{\vy})_k\,\nabla^2 y_k[\va,\vb]
=
-\frac{\sqrt{d}}{s^2}
\Big(
(\vu^{\top}\va)(\mD\vg_{\vy})^{\top}\mP\vb
+(\vu^{\top}\vb)(\mD\vg_{\vy})^{\top}\mP\va
+(\vu^{\top}\mD\vg_{\vy})\,\va^{\top}\mP\vb
\Big).
\]

Equivalently, the associated matrix form is
\[
\sum_{k=1}^{d}(\vg_{\vy})_k\,\nabla^2_{\vz\vz}y_k
=
-\frac{\sqrt{d}}{s^2}
\Big(
\mP \mD\vg_{\vy} \vu^{\top} 
+ \vu^{\top}\mD\vg_{\vy}\mP 
+ \vu \vg_{\vy}^{\top}\mD\mP
\Big).
\]

Combining both terms,
\[
\mH_{\vz\vz}
=
\frac{d}{s^2}\,
\mP\,\mD\,\mH_{\vy\vy}\,\mD\,\mP
-
\frac{\sqrt{d}}{s^2}
\Big(
\mP \mD\vg_{\vy} \vu^{\top} 
+ \vu^{\top}\mD\vg_{\vy}\mP 
+ \vu \vg_{\vy}^{\top}\mD\mP
\Big)
,
\qquad s=\|\mW\vx\|_2.
\]

%\subsection*{Lifted Hessian w.r.t.\ $\operatorname{vec}(\mW)$}

Using $\operatorname{vec}(\vz)=(\vx^{\top}\otimes \mI)\operatorname{vec}(\mW)$,
\begin{equation}
\begin{aligned}
    \mH_{\operatorname{vec}(\mW)\operatorname{vec}(\mW)} &=(\vx\vx^{\top})\otimes \mH_{\vz\vz} \\
&= (\vx\vx^{\top})\otimes
\left[
\frac{d}{s^2}\,
\mP\,\mD\,\mH_{\vy\vy}\,\mD\,\mP
-
\frac{\sqrt{d}}{s^2}
\Big(
\mP \mD\vg_{\vy} \vu^{\top} 
+ \vu^{\top}\mD\vg_{\vy}\mP 
+ \vu \vg_{\vy}^{\top}\mD\mP
\Big)
\right]. 
\end{aligned}
\end{equation}

\ 

\ 

\ 
\newpage



\section{Proof of \autoref{theorem:theorem_1}}
\label{appendix:proof_theorem_1}
\begin{proof}
Recall SimpleNorm:
\[
\vy = \vgamma \odot \sqrt{d}\,\frac{\mW\vx}{\|\mW\vx\|_2},
\qquad
\vz=\mW\vx,
\qquad
s:=\|\vz\|_2,
\qquad
\vu:=\frac{\vz}{s},
\qquad
\mP:=\mI-\vu\vu^\top.
\]
Assume $\mD=\operatorname{Diag}(\vgamma)=\mI$. Let $\ell=\ell(\vy)$ and define
\[
\vg_{\vy}:=\nabla_{\vy}\ell(\vy)\in\mathbb{R}^d,
\qquad
\mH_{\vy\vy}:=\nabla^2_{\vy\vy}\ell(\vy)\in\mathbb{R}^{d\times d}.
\]

By the chain rule, we have the decomposition:
\begin{equation}
\mH_{\vx\vx}
=
\nabla^2_{\vx}\ell
=
\underbrace{{\mJ_{\vx}^{\vy}}^\top \mH_{\vy\vy}\,\mJ_{\vx}^{\vy}}_{\text{Gauss--Newton  term}}
\;+\;
\underbrace{\mC}_{\text{curvature term}}
\end{equation}
Here the Jacobian is
\[
\mJ_{\vx}^{\vy}=\frac{\sqrt d}{s}\,\mD\mP\mW = \frac{\sqrt d}{s}\,\mP\mW,
\]
and therefore the Gauss--Newton term is
\begin{equation}
\mL
=
(\mJ_{\vx}^{\vy})^\top \mH_{\vy\vy}\mJ_{\vx}^{\vy}
=
\frac{d}{s^2}\,\mW^\top \mP\, \mH_{\vy\vy}\, \mP\,\mW.
\end{equation}
The curvature term (with the condition $\mD=\mI$) can be written as
\begin{equation}
\mC
=
-\frac{\sqrt d}{s^2}\,
\mW^\top
\Bigl(
(\mP\vg_{\vy})\vu^\top
+ (\vu^\top\vg_{\vy})\,\mP
+ \vu(\vg_{\vy}^\top \mP)
\Bigr)\mW,
\label{eq:C_exact}
\end{equation}
where $\vu^\top\vg_{\vy}$ is a scalar.

\paragraph{Bounding the Gauss--Newton term.}

Define
\[
\kappa :=
\frac{\sqrt d}{\|\mW\vx\|_2}\,\|\mW\|_2
\]


We show that $\kappa=\Theta(1)$ under high effective rank and typical input.
Let $\vx=\sqrt d\,\xi$ where $\xi$ is isotropic (e.g., uniform on the sphere or subgaussian). Then
\[
\mathbb{E}\|\mW\vx\|_2^2
=
d\,\mathbb{E}\,\xi^\top \mW^\top\mW\xi
=
\|\mW\|_F^2,
\]
and by standard concentration for quadratic forms,
\[
\|\mW\vx\|_2^2 \asymp \|\mW\|_F^2
\quad\Longrightarrow\quad
\|\mW\vx\|_2 \asymp \|\mW\|_F
\qquad\text{with high probability}.
\]
Thus
\[
\kappa
=
\frac{\sqrt d}{\|\mW\vx\|_2}\,\|\mW\|_2
\asymp
\sqrt d\,\frac{\|\mW\|_2}{\|\mW\|_F}
=
\sqrt{\frac{d}{r_{\mathrm{eff}}(\mW)}},
\qquad
r_{\mathrm{eff}}(\mW):=\frac{\|\mW\|_F^2}{\|\mW\|_2^2}.
\]
If $r_{\mathrm{eff}}(\mW)\asymp c d$, then $\kappa\asymp 1/\sqrt c = \Theta(1)$ with high probability.

Additionally, define
\[
\tau
:=
\frac{\big\|\widetilde{\mW}^\top \mP\,\mH_{\vy\vy}\,\mP\,\widetilde{\mW}\big\|_2}{\|\mH_{\vy\vy}\|_2}
\in[0,1],
\qquad
\widetilde{\mW}:=\frac{\mW}{\|\mW\|_2}.
\]
Under the theorem's ``non-pathological alignment'' assumption (i.e., the dominant spectral modes of $\mH_{\vy\vy}$ are not eliminated by projecting out $\mathrm{span}(\vu)$ and are represented in the range of $\mW$, which has high effective rank), we have $\tau=\Theta(1)$.

Now, it follows that
$$
\mL = \frac{d}{\|\mW\vx\|_2^2}\,\mW^\top \mP\,\mH_{\vy\vy}\,\mP\,\mW = \frac{\kappa^2}{\|\mW\|_2^2}\,\mW^\top \mP\,\mH_{\vy\vy}\,\mP\,\mW = \kappa^2 \,\widetilde{\mW}^\top \mP\,\mH_{\vy\vy}\,\mP\,\widetilde{\mW}
$$

and

\begin{equation}
    \|\mL\|_2 = \tau\,\kappa^2\,\|\mH_{\vy\vy}\|_2 \qquad \text{where } \tau = \Theta(1) \text{ and }\kappa = \Theta(1) \quad [\text{w.h.p}]
    \label{eq:L_exact_tau}
\end{equation}


\paragraph{Bounding the curvature term.}
From \autoref{eq:C_exact} and submultiplicativity,
\[
\|\mC\|_2
\le
\frac{\sqrt d}{s^2}\,\|\mW\|_2^2\,
\Big\|
(\mP\vg_{\vy})\vu^\top
+ (\vu^\top\vg_{\vy})\,\mP
+ \vu(\vg_{\vy}^\top \mP)
\Big\|_2.
\]
Now use $\|\vu\|_2=1$, $\|\mP\|_2=1$, and $\|\mP\vg_{\vy}\|_2\le \|\vg_{\vy}\|_2$:
\[
\|(\mP\vg_{\vy})\vu^\top\|_2=\|\mP\vg_{\vy}\|_2\|\vu\|_2\le \|\vg_{\vy}\|_2,
\]
\[
\|(\vu^\top\vg_{\vy})\,\mP\|_2 = |\vu^\top\vg_{\vy}|\,\|\mP\|_2 \le \|\vg_{\vy}\|_2,
\]
\[
\|\vu(\vg_{\vy}^\top \mP)\|_2 = \|\vu\|_2\,\|\mP\vg_{\vy}\|_2 \le \|\vg_{\vy}\|_2.
\]
By triangle inequality, the middle norm is at most $3\|\vg_{\vy}\|_2$, hence
\begin{equation}
\|\mC\|_2
\le
\frac{3\sqrt d}{\|\mW\vx\|_2^2}\,\|\mW\|_2^2\,\|\vg_{\vy}\|_2
=
\frac{3\kappa^2}{\sqrt d}\,\|\vg_{\vy}\|_2.
\label{eq:C_bound_final}    
\end{equation}


\paragraph{Dominance of $\mL$ over $\mC$.}
Combining \eqref{eq:L_exact_tau} and \eqref{eq:C_bound_final},
\[
\frac{\|\mC\|_2}{\|\mL\|_2}
\le
\frac{3}{\tau\sqrt d}\,\frac{\|\vg_{\vy}\|_2}{\|\mH_{\vy\vy}\|_2}.
\]
Assuming $\tau=\Theta(1)$ (non-pathological alignment) and $\|\vg_{\vy}\|_2/\|\mH_{\vy\vy}\|_2 = O(1)$ (bounded gradient-to-curvature ratio, as stated in the theorem assumptions), we obtain
\[
\frac{\|\mC\|_2}{\|\mL\|_2} = O(d^{-1/2}),
\]
so in high dimension $\|\mL\|_2 \gg \|\mC\|_2$ with high probability. Therefore the Gauss--Newton term dominates $\mH_{\vx\vx}$ in typical non-pathological regimes.

This completes the proof of \autoref{theorem:theorem_1}.
\end{proof}

\ 

\ 

\ 


\newpage




\section{Proof of \autoref{theorem:theorem_2}}
\label{appendix:proof_theorem_2}
\begin{proof}
Provided $\ell=\ell(\vy)$ is twice differentiable. Denote
\[
\vg_{\vy} := \nabla_{\vy}\ell,
\qquad
\mH_{\vy\vy} := \nabla^2_{\vy\vy}\ell,
\]
We have the two mappings:

\noindent {Linear:} \quad $\vy_1 = \mW_1 \vx$, \qquad
$\mH^{\mathrm{lin}}_{\vx\vx} = \mW_1^\top \mH_{\vy_1\vy_1}\mW_1$.

\noindent{SimpleNorm:} \quad
$\vy_2 = \mD \frac{\sqrt{d}\,\mW_2\vx}{\|\mW_2\vx\|_2}$,
where $\mD=\mathrm{Diag}(\vgamma)$ and define
\[
\vz=\mW_2\vx,\quad s=\|\vz\|_2,\quad \vu=\vz/s,\quad \mP=\mI-\vu\vu^\top.
\]
The Hessian w.r.t.\ $\vx$ admits the standard decomposition
\[
\mH^{\mathrm{sn}}_{\vx\vx} \;=\; \mL + \mC,
\qquad
\mL = {\mJ_{\vx}^{\vy_2}}^\top \mH_{\vy_2\vy_2} {\mJ_{\vx}^{\vy_2}},
\]
where $\mJ_{\vx}^{\vy_2}$ is the Jacobian of $\vy_2$ w.r.t.\ $\vx$, and $\mC$ is the
curvature term induced by the normalization.

Assuming the high-dimensional conditions stated in \autoref{theorem:theorem_1} hold
($\|\vx\|_2=\sqrt{d}$, $\mD=\mI$, $\|\mP\|_2 = 1$, $\mW_2$ has high effective rank, no pathological alignment, and $\|\vg_{\vy}\|_2/\|\mH_{\vy\vy}\|_2 = O(1)$), \autoref{theorem:theorem_1} shows that the Gauss-Newton term dominates the curvature term, i.e., \ $\|\mL\|_2 \gg \|\mC\|_2$. Therefore, we obtain the approximation $\|\mH^{\mathrm{sn}}_{\vx\vx}\|_2 \;\approx \|\mL\|_2$.

We now show an integral result: the spectral norm of $\mH^{\mathrm{lin}}_{\vx\vx}$ is directly proportional to the spectral norm of the weight matrix $\mW$ whereas the spectral norm of $\mH^{\mathrm{sn}}_{\vx\vx}$ is independent of the spectral norm of the weight matrix. In the following, we assume $\mW = \mW_1 = \mW_2$ and $\mH_{\vy_1\vy_1}\,=\, \mH_{\vy_2\vy_2}\, := \,\mH_{\vy\vy}$.

Define $\widetilde{\mW} \; := \; \frac{\mW}{\alpha}$ where $\alpha \; := \; \|\mW\|_2$. Consequently, $\|\widetilde{\mW}\|_2 = 1$ and $\mW = \alpha\widetilde{\mW}$. Now, treating $\mH^{\mathrm{lin}}_{\vx\vx}$ and $\mH^{\mathrm{sn}}_{\vx\vx}$ as functions of $\alpha$, it follows that:

\begin{equation}
\label{eq:lin-alpha}
\|\mH^{\mathrm{lin}}_{\vx\vx}(\alpha)\|_2 = \|\mW_1^\top \mH_{\vy_1\vy_1}\mW_1\|_2 = \alpha^2 \|\widetilde{\mW}^\top \mH_{\vy\vy} \widetilde{\mW}\|_2
\end{equation}

and

\begin{equation}
\label{eq:sn-alpha}
\|\mH^{\mathrm{sn}}_{\vx\vx}(\alpha)\|_2 \;\approx\; \|\mL(\alpha)\|_2 = \frac{d}{\|\mW \vx\|_2^2}\,
\|\mW^{\top}\mP\,\mD\,\mH_{\vy\vy}\,\mD\,\mP\,\mW \|_2 = 
\frac{d}{\|\widetilde{\mW} \vx\|_2^2}\,
 \|\widetilde{\mW}^{\top}\,\mP\,\mH_{\vy\vy}\,\mP\,\widetilde{\mW} \|_2
\end{equation}

Note that $\mP=\mI-\vu\vu^\top$ is independent of $\alpha$:
$$
\vu(\alpha)
=\frac{\mW\vx}{\|\mW\vx\|_2}
=\frac{\alpha\widetilde{\mW}\vx}{\|\alpha\widetilde{\mW}\vx\|_2}
=\frac{\widetilde{\mW}\vx}{\|\widetilde{\mW}\vx\|_2},
\qquad
\Longrightarrow\qquad
\mP(\alpha)=\mI-\vu(\alpha)\vu(\alpha)^\top=\mP(1).
$$
Therefore, $\|\mH^{\mathrm{lin}}_{\vx\vx}(\alpha)\|_2$ depends quadratically on $\alpha$ whereas $\|\mH^{\mathrm{sn}}_{\vx\vx}(\alpha)\|_2$ is \emph{scale-invariant} in $\alpha$.

Next, we compare the magnitudes of the two Hessians. First, recall the definition of $\kappa$ provided in \autoref{theorem:theorem_1}:
$$
\kappa
\; := \; \frac{\sqrt d}{\|\mW\vx\|_2}\|{\mW}\|_2
=
\frac{\sqrt d}{\alpha\|\widetilde{\mW}\vx\|_2}\alpha\|{\widetilde{\mW}}\|_2 = 
\frac{\sqrt d}{\|\widetilde{\mW}\vx\|_2}
$$

By definition, $\kappa^2 = \frac{d}{\|\widetilde{\mW}\vx\|_2^2}$, and \autoref{theorem:theorem_1} implies $\kappa^2 = \Theta(1)$ w.h.p.

Moreover, since $\|\widetilde{\mW}\|_2=\|\mP\|_2=1$,
\begin{equation}
\label{eq:sn-upper}
\|\mH^{\mathrm{sn}}_{\vx\vx}(\alpha)\|_2 \approx \|\mL\|_2 = \kappa^2 \, \|\widetilde{\mW}^\top \mP\,\mH_{\vy\vy}\,\mP\,\widetilde{\mW}\|_2 \le \kappa^2 \, \|\mH_{\vy\vy}\|_2
\end{equation}

For the linear module, there exists a constant $c_{\mathrm{lin}}>0$ such that

$$
\|\mH^{\mathrm{lin}}_{\vx\vx}(\alpha)\|_2 = \alpha^2\|\widetilde{\mW}^\top \mH_{\vy\vy}\widetilde{\mW}\|_2
\ge \alpha^2 c_{\mathrm{lin}}\,\|\mH_{\vy\vy}\|_2.
$$

We assume $\mathrm{Range}(\widetilde{\mW})$ is not adversarially aligned with the leading eigenspace of $\mH_{\vy\vy}$; in particular the overlap is constant-order, so the visibility constant satisfies $c_{\mathrm{lin}}=\Theta(1)$ and does not decay with $d$.




Combining all the aforementioned results, the final ratio between the two Hessians satisfies:
$$
\frac{\|\mH^{\mathrm{lin}}_{\vx\vx}(\alpha)\|_2}{\|\mH^{\mathrm{sn}}_{\vx\vx}(\alpha)\|_2}
\;\ge\;
\frac{\alpha^2 \|\widetilde{\mW}^\top \mH_{\vy\vy} \widetilde{\mW}\|_2}{\kappa^2\,\|\mH_{\vy\vy}\ \|_2} \ge \alpha^2\frac{c_{\text{lin}}}{\kappa^2}
$$

Empirically, $\alpha = \|\mW\|_2$ typically grows to tens or hundreds during training,
so $\alpha^2c_{\text{lin}} \gg \kappa^2$ (recall $\kappa=\Theta(1)$ w.h.p.). Hence:

$$
\|\mH^{\mathrm{lin}}_{\vx\vx}\|_2 \gg \|\mH^{\mathrm{sn}}_{\vx\vx}\|_2
\qquad \text{(with high probability)}.
$$

In summary, in non-pathological cases, the gradient Lipschitz constant of $\|\mH^{\mathrm{lin}}_{\vx\vx}\|_2$ is much larger than  the gradient Lipschitz constant of $\|\mH^{\mathrm{sn}}_{\vx\vx}\|_2$.

This completes the proof of \autoref{theorem:theorem_2}. 

\end{proof}



\section{Detailed Experimental Settings}
\label{appendix:exp_settings}
Our \textbf{SimpleGPT} models are built on Llama2, Llama3, and nanoGPT (a GPT-2 implementation). We apply the SimpleNorm operator to all Transformer blocks except the embedding layer and classification layer.
Our implementations are based on the Adam-mini~\footnote{\url{https://github.com/zyushun/Adam-mini}}~\cite{adam_mini_zhang2024adam} and nanoGPT~\footnote{\url{https://github.com/karpathy/nanoGPT}}~\cite{nanogpt_Karpathy2022}. Below we briefly describe the main architectural and training settings for each backbone. 

\textbf{nanoGPT} is a lightweight and efficient implementation of the GPT-2 architecture. It uses the GELU activation function and a byte-pair encoding (BPE) tokenizer~\cite{bpe_gage1994new} consistent with GPT-2~\cite{gpt2_radford2019language}, with an expanded vocabulary size of 50{,}257 tokens. 2{,}000 steps are used for learning rate warmup. Our training data on nanoGPT models is OpenWebText.

\textbf{Llama2} adopts the SwiGLU~\cite{swishglu_shazeer2020glu} activation function in the feed-forward networks, which improves expressivity and parameter efficiency. Positional information is encoded using Rotary Positional Embeddings (RoPE)~\cite{rope_su2023enhanced}. Llama2 also introduces Grouped-Query Attention (GQA) to reduce inference-time memory usage and computational cost. The model uses a SentencePiece-based BPE tokenizer with a vocabulary size of 32K tokens. In our experiments,  1\% of the total steps are allocated for learning rate warmup. Our training data on Llama2 models is C4.

\textbf{Llama3} follows the dense Transformer design of Llama2, while introducing several targeted changes. It continues to use GQA with eight key-value heads to improve decoding efficiency and reduce key-value cache size. A major difference lies in the tokenizer: Llama 3 adopts a significantly larger vocabulary of 128K tokens, combining tokens from the tiktoken tokenizer with additional multilingual tokens, which improves compression rates and language coverage. To better support long contexts, the RoPE base frequency is increased to 500{,}000. In our experiments,  1\% of the total steps are allocated for learning rate warmup. Our training data on Llama3 models is C4.

Across all experiments, we adopt the AdamW optimizer~\cite{adam_kingma2014adam, adamw_IlyaLoshchilov2018FixingWD} with $\beta_1 = 0.9$ and $\beta_2 = 0.95$. Since we know that weight decay is associated with the learning rate, and our method permits the use of larger learning rates, we accordingly adjust the weight decay. Unless otherwise stated, a weight decay value of 0.1 is used throughout our experiments. Additional hyperparameter configurations are summarized in \autoref{tab:model_configs}.

All models are trained using PyTorch~\citep{pytorch_paszke2019pytorch} with bfloat16 precision on A800 GPUs. We employ a cosine learning rate schedule for all training runs.

\section{Parameters and configurations of SimpleGPT}\label{appendix:model_configs}





\begin{table}[h]
\centering
\caption{Model configurations for different scales of SimpleGPT. The models 1B and 7B are based on Llama 2, the model 8B is based on Llama 3, and the model 1.4B is based on nanoGPT.}
\label{tab:model_configs}
\begin{tabular}{l|cccc}
\toprule
 & SimpleGPT \textbf{1B} & SimpleGPT \textbf{7B} & SimpleGPT \textbf{8B} & SimpleGPT \textbf{1.4B}\\
\midrule
Origin from & Llama2 & Llama2 & Llama3 & nanoGPT(GPT2)\\
Layers & 18 & 32 & 32 & 48\\
Model Dimension & 2{,}048 & 4{,}096 & 4{,}096 & 1{,}536 \\
FFN Dimension & 5{,}632 & 11{,}008 & 14{,}336 & 6{,}144 \\
Attention Heads & 16 & 32 & 32 & 24\\
Key / Value Heads & 16 & 32 & 8 & 24  \\

Activation Function & SwiGLU & SwiGLU&  SwiGLU & GeLU\\
Vocabulary Size & 32{,}000 & 32{,}000 & 128{,}000 & 50,304 \\
Positional Embeddings (RoPE) &  $\theta = 10{,}000$ & $\theta = 10{,}000$ & $\theta = 500{,}000$ & No\\
Batch Size & $512\times 256$ & $2048\times 192$ & $2048\times 192$ & $1024\times 512$\\
Training Steps & 200K & 20K/40K/60K & 20K  & 100K\\
Warmup Steps & 1\% & 1\% & 1\% & 2000\\
\bottomrule
\end{tabular}
\end{table}



\newpage 
\section{More experiments on SimpleGPT 7B}
Furthermore, we evaluate the SimpleGPT 7B  models using different learning rates or weight decay values. Results are shown in~\autoref{fig:simplegpt_7b_smaller_lr} and~\autoref{fig:simplegpt_7b_larger_lr}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{Figures/simplegpt/big_model/7B/Llama2_7B_lr3e_4_wd0.1_20K_smooth.pdf}
    \caption{The training loss curves of Llama2 7B, Llama2 7B with QKNorm and SimpleGPT 7B with learning rate 3e-4 and weight decay 0.1.}
    \label{fig:simplegpt_7b_smaller_lr}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{Figures/simplegpt/big_model/7B/Llama2_7B_lr3e_3_wd0.03_20K_smooth.pdf}
    \caption{The training loss curves of Llama2 7B, Llama2 7B with QKNorm and SimpleGPT 7B with learning rate 3e-3 and weight decay 0.03.}
    \label{fig:simplegpt_7b_larger_lr}
\end{figure}


\ 

\ 

\ 

\section{More experiments on SimpleGPT 8B}
Furthermore, we evaluate the SimpleGPT 8B  models using different learning rates or weight decay values. Results are shown in~\autoref{fig:simplegpt_8b_smaller_lr} and~\autoref{fig:simplegpt_8b_large_lr_small_wd}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{Figures/simplegpt/big_model/8B/Llama3_8B_lr3e_4_wd0.1_20K_smooth.pdf}
    \caption{The training loss curves of Llama3 8B, Llama3 8B with QKNorm and SimpleGPT 8B with learning rate 3e-4 and weight decay 0.1.}
    \label{fig:simplegpt_8b_smaller_lr}
\end{figure}




\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{Figures/simplegpt/big_model/8B/Llama3_8B_lr3e_3_wd0.03_20K_smooth.pdf}
    \caption{The training loss curves of Llama3 8B, Llama3 8B with QKNorm and SimpleGPT 8B with learning rate 3e-3 and weight decay 0.03.}
    \label{fig:simplegpt_8b_large_lr_small_wd}
\end{figure}


\newpage 



\section{More experiments on weight decays}
 We conduct experiments on the SimpleGPT 8B model using two different weight decay values to evaluate robustness to regularization.
Across all tested settings, SimpleGPT 8B consistently outperforms LLaMA2 8B with QKNorm.
These results indicate that the benefits of SimpleNorm are not sensitive to the choice of weight decay, further demonstrating its robustness in large-scale training. Results are shown in~\autoref{fig:llama2_8B_weight_decay}.


\begin{figure}[H]
    \centering
    % ---------- First row ----------
    \begin{subfigure}{0.44\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/simplegpt/big_model/8B/Llama3_8B_lr1e_3_wd0.1_20K_smooth.pdf}
        \caption{8B with wd=0.1.}
        \label{fig:simplegpt_8b_wd01}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/simplegpt/big_model/8B/Llama3_8B_lr1e_3_wd0.05_20K_smooth.pdf}
        \caption{8B with wd=0.05.}
        \label{fig:simplegpt_8b_wd005}
    \end{subfigure}
    \caption{
        Overall comparison across Llama3 8B with QKNorm and SimpleGPT 1B under two different weight decay values.
    }
    \label{fig:llama2_8B_weight_decay}
\end{figure}



\ 

\ 

\
















