\section{Methodology}

\begin{figure*}[ht]
\tiny
     \centering
     	\begin{subfigure}[b]{\textwidth}
        \centering

            \begin{tikzpicture}[
                font=\normalsize,
                >=Latex,
                node distance=10mm and 14mm,
                block/.style={draw, rounded corners=2pt, minimum height=7mm, minimum width=9mm, align=center},
                smallblock/.style={draw, rounded corners=2pt, minimum height=7mm, minimum width=9mm, align=center},
                circ/.style={draw, circle, minimum size=7mm, inner sep=0pt},
                plus/.style={circ},
                mult/.style={circ},
                line/.style={-Latex, line width=0.9pt},
            ]
            
            % ----- Left part (Attention) -----
            
            \node[block] (pren1) {PreN};
            
            
            \node[smallblock, right=8mm of pren1, yshift=11mm] (wq) {$\mW_q$};
            
            \node[smallblock, right=8mm of pren1]     (wk) {$\mW_k$};
            \node[smallblock, right=8mm of pren1, yshift=-11mm]    (wv) {$\mW_v$};
            
            \node[circ, right=7mm of wk, yshift=5.5mm] (att) {Attn};
            
            \node[mult, right=5mm of att] (mul) {$\times$};
            
            \node[smallblock, right=5mm of mul] (wo) {$\mW_o$};
            
            \node[plus, right=5mm of wo] (add1) {$+$};
            
            % wiring: pren1 splits to wq,wk,wv
            \draw[line] (pren1) -- (9mm,0) coordinate (split1);
            \draw[line] (split1) |- (wq.west);
            \draw[line] (split1) |- (wk.west);
            \draw[line] (split1) |- (wv.west);
            
            % wq,wk -> att
            \draw[line] (wq.east) -- (att.west);
            \draw[line] (wk.east) -- (att.west);
            
            % att -> mul (top input)
            \draw[line] (att.east) -- (mul.west);
            
            % wv -> mul (bottom input)
            \draw[line] (wv.east) -| ($(mul.south)+(0,-6mm)$) -- (mul.south);
            % mul -> wo -> add1
            \draw[line] (mul.east) -- (wo.west);
            \draw[line] (wo.east) -- (add1.west);
            
            % local residual into add1 (from pren1 input stream)
            %\draw[line] (pren1.west) ++(-7mm,0) coordinate (x0);
            %\draw[line] (x0) -- (pren1.west);

            %\draw[line] (pren1.west) ++(-7mm,0) coordinate (x0);
            \draw[line] (pren1.west) ++(-7mm,0) coordinate (x0) -- (pren1.west);
            \draw[line] (x0)+(3mm,0) |- ($(add1.north)+(0,8mm)$) -- (add1.north);



            
            
            % ----- Right part (MLP) -----
            \node[block, right=6mm of add1] (pren2) {PreN};
            \node[smallblock, right=5mm of pren2] (w1) {$\mW_1$};
            \node[block, right=5mm of w1] (relu) {ReLU};
            \node[smallblock, right=5mm of relu] (w2) {$\mW_2$};
            \node[plus, right=5mm of w2] (add2) {$+$};
            
            % main forward
            \draw[line] (add1.east)  -- (pren2.west);
            \draw[line] (pren2.east) -- (w1.west);
            \draw[line] (w1.east) -- (relu.west);
            \draw[line] (relu.east) -- (w2.west);
            \draw[line] (w2.east) -- (add2.west);
            \draw[line] (add2.east) -- ++(4mm,0);
            
            % residual into add2 (from add1 output)
            \draw[line] (add1.east)+(2mm,0) |- ($(add2.north)+(0,8mm)$) -- (add2.north);
            

            
            \end{tikzpicture}

        \caption{GPT. GPT adopts a pre-normalization architecture. Linear projections $\mW_q$, $\mW_k$,  $\mW_v$, $\mW_o$, $\mW_1$ and $\mW_2$ are applied. }
    \end{subfigure}


        \begin{subfigure}[b]{\textwidth}
            \centering
                \begin{tikzpicture}[
        font=\normalsize,
        >=Latex,
        node distance=10mm and 14mm,
        block/.style={draw, rounded corners=2pt, minimum height=7mm, minimum width=9mm, align=center},
        myblock/.style={draw, rounded corners=2pt, minimum height=7mm, text=blue, minimum width=9mm, align=center},
        smallblock/.style={draw, rounded corners=2pt, minimum height=7mm, minimum width=9mm, align=center},
        circ/.style={draw, circle, minimum size=7mm, inner sep=0pt},
        plus/.style={circ},
        mult/.style={circ},
        line/.style={-Latex, line width=0.9pt},
    ]
        
        % --- define the input coordinate properly ---
        \coordinate (x0) at (0,0);
        \draw[line] (-8mm,0) -- (x0);
        
        \node[myblock, right=8mm of x0, yshift=11mm] (wq) {$\mPsi_q$};
        \node[myblock, right=8mm of x0]            (wk) {$\mPsi_k$};
        \node[myblock, right=8mm of x0, yshift=-11mm] (wv) {$\mPsi_v$};
        
        \node[circ, right=9mm of wk, yshift=5.5mm] (att) {Attn};
        \node[mult, right=7mm of att] (mul) {$\times$};
        \node[myblock, right=7mm of mul] (wo) {$\mPsi_o$};
        \node[plus, right=7mm of wo] (add1) {$+$};
        
        % wiring: x0 splits to wq,wk,wv
        \draw[line] (x0) |- (wq.west);
        \draw[line] (x0) |- (wk.west);
        \draw[line] (x0) |- (wv.west);
        
        % wq,wk -> att
        \draw[line] (wq.east) -- (att.west);
        \draw[line] (wk.east) -- (att.west);
        
        % att -> mul
        \draw[line] (att.east) -- (mul.west);
        
        % wv -> mul (bottom input)
        \draw[line] (wv.east) -| ($(mul.south)+(0,-6mm)$) -- (mul.south);
        
        % mul -> wo -> add1
        \draw[line] (mul.east) -- (wo.west);
        \draw[line] (wo.east) -- (add1.west);
        
        % residual into add1 (from input)
        \draw[line] ($(x0)+(-4mm,0)$) |- ($(add1.north)+(0,8mm)$) -- (add1.north);
        
        % --- MLP part ---
        \node[myblock, right=7mm of add1] (w1) {$\mPsi_1$};
        \node[block, right=7mm of w1] (relu) {ReLU};
        \node[myblock, right=7mm of relu] (w2) {$\mPsi_2$};
        \node[plus, right=7mm of w2] (add2) {$+$};
        
        \draw[line] (add1.east) -- (w1.west);
        \draw[line] (w1.east) -- (relu.west);
        \draw[line] (relu.east) -- (w2.west);
        \draw[line] (w2.east) -- (add2.west);
        \draw[line] (add2.east) -- ++(6mm,0);
        
        \draw[line] ($(add1.east)+(2mm,0)$) |- ($(add2.north)+(0,8mm)$) -- (add2.north);
        
        \end{tikzpicture}

                    
        \caption{SimpleGPT. SimpleGPT replaces all linear layers with \textbf{SimpleNorm} operator, denoted by $\color{blue}{\mPsi}$. In SimpleGPT, we do not use prenorm.}
    \end{subfigure}
    \caption{SimpleGPT vs. GPT. This figure compares the standard GPT block with the proposed SimpleGPT block, highlighting the structural simplifications introduced by SimpleNorm.}
    \label{fig:simplegpt}
\end{figure*}





\subsection{SimpleNorm: A Unified Normalization Strategy}
\textbf{Definition of SimpleNorm.}
We define \emph{SimpleNorm} as placing a normalization operator \emph{immediately} after a linear mapping.
Given an input vector $\vx \in \mathbb{R}^m$ and a linear transformation $\mW \in \mathbb{R}^{d \times m}$, we abstract SimpleNorm as a primitive operator
\begin{equation}
    \mPsi(\vx) = 
    \operatorname{Norm}({\mW\vx}),
\qquad
\end{equation}
where $\operatorname{Norm}(\cdot)$ is a normalization operator such as LayerNorm or RMSNorm. SimpleNorm is motivated by a simple yet effective \emph{placement} strategy, rather than algebraic complexity. In contrast to existing normalization techniques that typically operate at the level of residual blocks,
hidden states, or parameter reparameterization,
SimpleNorm enforces normalization \emph{locally and immediately} after linear mapping, treating ``linear mapping immediately followed by a normalization'' as a single, unified operator.

\textbf{Definition of SimpleGPT.} As illustrated in \autoref{fig:simplegpt}, \emph{SimpleGPT} uses
SimpleNorm  as a fundamental building block.
SimpleNorm is systematically inserted wherever a linear layer appears,
including MLP projections, attention projections (Q, K, V),
output projections, and gating or memory-related modules. Take \autoref{fig:simplegpt} as an example, normalization is inserted after the $\mW_q$, $\mW_k$, $\mW_v$, $\mW_o$, $\mW_1$, and $\mW_2$ projections. In architectures that employ SwiGLU~\cite{swishglu_shazeer2020glu} instead of MLP, SimpleGPT inserts normalization after $\mW_q$, $\mW_k$, $\mW_v$, $\mW_o$, $\mW_1$, $\mW_2$, and $\mW_3$.

\paragraph{Instantiating SimpleNorm with RMSNorm} In this work, we instantiate $\operatorname{Norm(\cdot)}$ with RMSNorm~\citep{rmsnorm_zhang2019root}. Hence, SimpleNorm is now defined as: 
\begin{equation}
    \mPsi(\vx; \mW, \vgamma) = \vgamma \odot \sqrt{d}\,
\frac{\mW\vx}{\norm{\mW\vx}},
\qquad
\end{equation}
where $\vx \in \mathbb{R}^m$, $\mW \in \mathbb{R}^{d\times m}$, and $\mW, \vgamma$ are learnable parameters, and $\odot$ denotes element-wise multiply.

For later analysis, we define the intermediate variables
\[
\begin{aligned}
\vz &= \mW\vx, &
s &= \|\vz\|_2, &
\vu &= \frac{\vz}{s},\\
\mP &= \mI-\vu\vu^{\top}, &
\mD &= \operatorname{Diag}(\vgamma),
\end{aligned}
\]


so that $\mPsi(\vx; \mW, \vgamma)=\sqrt d\,\mD\vu$.

\paragraph{Core Properties of SimpleNorm} The following  sections prove two important mechanisms of SimpleNorm: SimpleNorm directly stabilizes the scale of activations to be on the order of $\sqrt{d}$, and SimpleNorm constrains the spectral norm of the Hessian of the loss w.r.t. the activations, smoothing the loss landscape and enabling larger learning rates. Moreover, we present a hypothesis for why, in addition to the predicted optimization stability, SimpleNorm also exhibits strong empirical performance.

\subsection{Mechanism I: Stable Activation Scale}
By construction, SimpleNorm stabilizes the scale of intermediate activations by normalizing \emph{immediately} after each linear mapping. Recall that
\[
\mPsi(\vx;\mW,\vgamma)=\sqrt d\,\mD\vu \qquad \text{ with } \|\vu\|_2=1.
\]
Then
\[
\|\mPsi(\vx;\mW,\vgamma)\|_2=\sqrt d\,\|\mD\vu\|_2.
\]
In particular, letting $\gamma_{\min}=\min_i |\gamma_i|$ and $\gamma_{\max}=\max_i |\gamma_i|$, we have the bound
\[
\gamma_{\min}\sqrt d \;\le\; \|\mPsi(\vx)\|_2 \;\le\; \gamma_{\max}\sqrt d.
\]
Thus, up to the learned per-dimension scaling $\vgamma$, each projection is rescaled to have norm on the order of $\sqrt d$. Consequently, SimpleNorm prevents intermediate representation norms from drifting with depth or weight growth, eliminating a common source of activation explosion. Mechanism II shows how, in addition to activations, SimpleNorm also stabilizes curvature via the gradient Lipschitz constant.


\subsection{Mechanism II: Smoother Loss Landscape}
\textbf{Smoothness and the Hessian.}
The smoothness of the objective directly constrains optimization stability: larger curvature implies smaller safe learning rates. Given a twice-differentiable $\beta$-smooth objective $\ell(x)$, we quantify local curvature by the activation Hessian $\mH_{\vx\vx}=\nabla^2_{\vx\vx}\ell$. Specifically, the supremum of the spectral norm of the Hessian upper bounds local curvature and governs gradient stability:
\[
\beta \; = \; \sup_{\vx}\|\mH_{\vx\vx}(\vx)\|_2.
\]
To show SimpleNorm yields a smoother landscape, we prove two results: (i) the SimpleNorm Hessian decomposes as $\mH_{\vx\vx}=\mL+\mC$ and in high dimension $\|\mC\|_2\ll \|\mL\|_2$; (ii) compared to a linear projection whose curvature scales as $\|\mW\|_2^2$, the SimpleNorm curvature is scale-invariant with respect to $\|\mW\|_2$. Combined, these imply $\|\mH^{\mathrm{sn}}_{\vx\vx}\|_2 \ll \|\mH^{\mathrm{lin}}_{\vx\vx}\|_2$ since $\|\mW\|_2$ generally grows during training.

\textbf{SimpleNorm Derivatives.}
First, we compute the first-order gradient and second-order Hessian of $\ell$ with respect to $\vx$.

Given $\vy=\sqrt{d}\,\mD\vu$, let
\begin{equation*}
\ell=\ell(\vy),
\qquad
\vg_{\vy}:=\nabla_{\vy}\ell,
\qquad
\mH_{\vy\vy}:=\nabla^2_{\vy\vy}\ell.
\end{equation*}

\emph{First-order derivative.}
The Jacobian of the normalization  satisfies
$
\frac{\partial \vu}{\partial \vz} = \frac{1}{s}\mP,$
which yields the Jacobian of $\vy$ with respect to $\vx$:
\[
\mJ_{\vx}^{\vy}:=\frac{\partial \vy}{\partial \vx}
=
\frac{\sqrt{d}}{s}\,\mD\,\mP\,\mW.
\]
Applying the chain rule, the gradient is
\begin{equation}\label{eq:first_order_simplenorm}
\nabla_{\vx}\ell
=
{\mJ_{\vx}^{\vy}}^{\top}\vg_{\vy}
=
\frac{\sqrt{d}}{s}\,
\mW^{\top}\mP\,\mD\,\vg_{\vy}.
\end{equation}

\emph{Second-order derivative.}
Differentiating the gradient leads to the standard decomposition
\begin{equation}\label{eq:second_order_simplenorm}
\mH_{\vx\vx}
=
\nabla^2_{\vx}\ell
=
\underbrace{{\mJ_{\vx}^{\vy}}^\top \mH_{\vy\vy}\,\mJ_{\vx}^{\vy}}_{\text{Gauss--Newton  term}}
\;+\;
\underbrace{\mC}_{\text{curvature term}},
\end{equation}
where the first term is the Gauss--Newton component
\begin{equation}\label{eq:linear_term}
{\mJ_{\vx}^{\vy}}^{\top}\mH_{\vy\vy}\mJ_{\vx}^{\vy}
=
\frac{d}{s^2}\,
\mW^{\top}\mP\,\mD\,\mH_{\vy\vy}\,\mD\,\mP\,\mW,
\end{equation}
and the second term is from the curvature of the normalization,
\begin{small}
   \begin{equation}\label{eq:curvature_term}
    \mC =
-\frac{\sqrt{d}}{s^{2}}\,
\mW^{\top}
\Bigl(
\mP \mD\vg_{\vy} \vu^{\top} 
+ \vu^{\top}\mD\vg_{\vy}\mP 
+ \vu \vg_{\vy}^{\top}\mD\mP
\Bigr)\mW.
\end{equation}   
\end{small}
Please see \autoref{appendix:derivation_of_x_simplenorm} for a detailed derivation of $\nabla_{\vx}\ell$ and $\nabla_{\vx}^{2}\ell$ for SimpleNorm. For completeness, derivations of $\nabla_{\vgamma}\ell$, $\nabla_{\vgamma}^{2}\ell$, $\nabla_{\mW}\ell$ and $\nabla_{\operatorname{vec}(\mW)}^{2}\ell$ with $\vy = \vgamma \odot \sqrt{d}\, \frac{\mW\vx}{\|\mW\vx\|_2}$ and $\nabla_{\mW}\ell$ and $\nabla_{\operatorname{vec}(\mW)}^{2}\ell$ with $\vy = \mW \vx$ are also provided in \autoref{appendix:derivation_of_W_simplenorm} and \autoref{appendix:derivation_of_W_linear} 


\textbf{Gauss--Newton Term Dominates in High Dimension.}
Next, we show that under standard high-dimensional and non-pathological conditions,
the Gauss--Newton term dominates the curvature induced by normalization.

\begin{theorem}[Gauss--Newton dominance for SimpleNorm]
\label{theorem:theorem_1}
Let $\mH_{\vx\vx}=\nabla^2_{\vx}\ell$ denote the activation Hessian induced by
SimpleNorm for a twice-differentiable objective $\ell(\vy)$. Then, the Hessian decomposes as
\[
\mH_{\vx\vx}=\mL+\mC,
\qquad
\mL=(\mJ_{\vx}^{\vy})^\top \mH_{\vy\vy}\mJ_{\vx}^{\vy},
\]
where $\mC$ is the curvature term induced by normalization.

Assume $\|\vx\|_2=\sqrt d$, $\mD = \mI$, $\mW\in\mathbb{R}^{d\times d}$ has high effective rank
$\|\mW\|_F^2/\|\mW\|_2^2\ge c\,d$, and the input and loss derivatives are not
pathologically aligned with $\mW$. Define
\[
\kappa:=\left\|\frac{\sqrt d\,\mW}{\|\mW\vx\|_2}\right\|_2 .
\]
Then $\kappa=\Theta(1)$ with high probability, and there exists a constant
$\tau=\Theta(1)$ such that
\[
\|\mL\|_2=\tau\,\kappa^2\,\|\mH_{\vy\vy}\|_2,
\qquad
\|\mC\|_2\le \frac{3\kappa^2}{\sqrt d}\,\|\vg_{\vy}\|_2 .
\]

In particular, if $\|\vg_{\vy}\|_2=O(\|\mH_{\vy\vy}\|_2)$, then
\[
\|\mC\|_2\ll \|\mL\|_2
\]
so the Gauss--Newton term dominates the Hessian w.h.p.
\end{theorem}

A complete proof is given in \autoref{appendix:proof_theorem_1}.

\textbf{SimpleNorm Hessian is Weight Scale-Invariant.}
Finally, we compare the SimpleNorm Hessian's magnitude to that of a plain linear projection. We show that linear curvature grows quadratically with the weight matrix spectral norm $\|\mW\|_2$, whereas SimpleNorm removes this dependence.

\begin{theorem}[Linear curvature scales with $\|\mW\|_2^2$ while SimpleNorm does not]
\label{theorem:theorem_2}
Let $\ell=\ell(\vy)$ be twice differentiable, with
$\mH_{yy}=\nabla^2_{\vy\vy}\ell$ and $\vg_{\vy}=\nabla_{\vy}\ell$.

Consider the linear mapping with its Hessian
\[
\vy_1=\mW_1\vx,\qquad
\mH^{\mathrm{lin}}_{xx}=\mW_1^\top \mH_{yy}\mW_1,
\]
and the SimpleNorm mapping with its Hessian
\[
\vy_2=\mD \frac{\sqrt{d}\,\mW_2\vx}{\|\mW_2\vx\|_2},\qquad 
\mH^{\mathrm{sn}}_{xx}=\mL+\mC .
\]
Assume $\mH_{y_1y_1} = \mH_{y_2y_2} := \mH_{yy}$, $\mW_1 = \mW_2 := \mW$, and that the conditions of \autoref{theorem:theorem_1} hold, such that $\|\mL\|_2 \gg \|\mC\|_2$. Then, with high-probability,
\[
\|\mH^{\mathrm{sn}}_{\vx\vx}\|_2
=
\Theta\!\left(\kappa^2\,\|\mH_{\vy\vy}\|_2\right),
\qquad
\kappa^2=\frac{d}{\|\widetilde{\mW}\vx\|_2^2}=\Theta(1),
\]
where $\widetilde{\mW}=\mW/\|\mW\|_2$.

Moreover, if the range of $\widetilde{\mW}$ is not adversarially aligned with the leading eigenspace of $\mH_{\vy\vy}$, then there
exists a constant $c_{\mathrm{lin}}=\Theta(1)$ such that
\[
\|\mH^{\mathrm{lin}}_{\vx\vx}\|_2
=
\|\mW^\top \mH_{\vy\vy}\mW\|_2
\;\ge\;
c_{\mathrm{lin}}\,\|\mW\|_2^2\,\|\mH_{\vy\vy}\|_2.
\]

Consequently, as $\|\mW\|_2$ grows during training,
\[
\|\mH^{\mathrm{lin}}_{\vx\vx}\|_2
\;\gg\;
\|\mH^{\mathrm{sn}}_{\vx\vx}\|_2
\qquad\text{(with high probability)}.
\]
\end{theorem}
Intuitively, SimpleNorm removes the dependence of curvature on weight scale by normalizing activations, whereas a linear projection amplifies curvature as $\|\mW\|_2$ grows. We provide a proof for \autoref{theorem:theorem_2} in  \autoref{appendix:proof_theorem_2}.

\textbf{SimpleNorm Enables Larger Learning Rates.}
For a twice-differentiable $\beta$-smooth objective,
the maximum stable learning rate of gradient descent is inversely proportional to $\beta$, the Lipschitz constant of the gradient, which is equivalent to the supremum of the spectral norm of the Hessian:
$\eta \;\le\; \frac{2}{\beta}$ where $\beta = \sup_{\vx}\|\mH_{\vx\vx}(\vx)\|_2$


\autoref{theorem:theorem_1} and \autoref{theorem:theorem_2} establish that, under standard high-dimensional and non-pathological conditions, the SimpleNorm Hessian is invariant to the spectral norm of the weight matrix whereas the Hessian of a linear projection scales quadratically with the weight norm.

Consequently, since the weight spectral norm generally grows throughout training, the SimpleNorm-based SimpleGPT architecture has a smoother loss landscape that can tolerate significantly larger learning rates compared to methods based on direct linear projections.



\subsection{Interpretation: Beyond Optimization Stability}
We have established two core properties of SimpleNorm:
(i) it stabilizes activation scale at $\Theta(\sqrt{d})$, and
(ii) it smooths the loss landscape by constraining the spectral norm of the
activation Hessian, enabling larger and more stable learning rates.
Although these properties explain the improved optimization stability of SimpleNorm,
they do not fully account for the strong empirical performance observed in \autoref{sec:exp}.

We hypothesize that SimpleNorm provides additional benefits at a more \textit{global} representational level. By normalizing immediately after each linear projection,
SimpleNorm ensures that every layer induces a genuinely nonlinear transformation,
even in regimes where the surrounding network would otherwise behave nearly
linearly. This effectively increases the depth of nonlinear interactions and
enhances expressive capacity without increasing parameter count.

Under this view, SimpleNorm improves performance through a dual effect:
locally, by improving optimization geometry via reduced curvature variability;
and globally, by increasing expressiveness through pervasive
normalization-induced nonlinearity. We believe this combination explains why
SimpleNorm yields consistent empirical gains beyond what would be expected from
learning-rate stability alone.

\subsection{Use \texttt{torch.compile} to Speedup Training}
Normalization layers are memory-bound and frequently executed, making them a potential bottleneck. By fusing reduction and pointwise operations and leveraging \texttt{torch.compile}, SimpleNorm's increased normalization overhead is largely amortized, resulting in around a 3\% training-time increase compared to GPT with QKNorm.




