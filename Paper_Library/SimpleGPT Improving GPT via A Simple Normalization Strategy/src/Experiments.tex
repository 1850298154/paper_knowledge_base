\section{Experiments}\label{sec:exp}
\textbf{Experimental settings.} We evaluate SimpleNorm on three Transformer backbones: nanoGPT, Llama2, and Llama3.
SimpleNorm is applied to all Transformer blocks, excluding the embedding and output
layers. All models are trained using the AdamW optimizer~\cite{adam_kingma2014adam, adamw_IlyaLoshchilov2018FixingWD} with cosine learning-rate
scheduling with bfloat16 precision. Learning rates are tuned for each method.
Since SimpleNorm permits significantly larger stable learning rates, we adjust weight decay accordingly. Additional architectural, hyperparameter, and training details are provided in \autoref{appendix:exp_settings} and \autoref{appendix:model_configs}.



\subsection{Largest Tolerable Learning Rate}
We evaluate the largest tolerable learning rate by comparing optimization stability across different normalization schemes while keeping all other training settings fixed. As shown in \autoref{fig:simplegpt_max_learning_rate}, PreNorm already exhibits convergence issues at a learning rate of $2\times 10^{-3}$.
In contrast, PreNorm+QKNorm remains stable at $2\times 10^{-3}$ and $2\times 10^{-2}$, but becomes unstable when the learning rate is increased to $2\times 10^{-1}$.
SimpleNorm shows stable convergence at both $2\times 10^{-3}$ and $2\times 10^{-2}$, and is notably more stable than PreNorm+QKNorm at $2\times 10^{-1}$.
Overall, these results suggest that SimpleNorm consistently tolerates larger learning rates, indicating improved optimization robustness.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/simplegpt/lr_torlance/Llama_2_1B_lrtorlance.pdf}
    \caption{The largest admissible learning rate for Llama2-B, Llama2-1B with QKNorm, and SimpleGPT-1B.}
    \vspace{-6pt}
    \label{fig:simplegpt_max_learning_rate}
\end{figure}

\subsection{SimpleGPT 1B based on Llama2}
In this subsection, we evaluate SimpleGPT 1B and compare it against the standard  Llama2 1B as well as  Llama2 1B with QKNorm. We train the model for 200K steps (following~\cite{adam_mini_zhang2024adam}), with a global batch size of 256 and a sequence length of 512, resulting in approximately 26B training tokens.
We train all models on the C4 dataset following the same training recipe as their corresponding baselines.
The results are presented in \autoref{fig:simplegpt_1b}. The loss curve is smoothed by 80\% in Tensorboard. For all experiments, we report the training loss of the last step.

In \autoref{fig:simplegpt_1b}, SimpleGPT 1B achieves notable improvement over Llama2 1B with QKNorm.
Specifically, the training loss is reduced from 2.478 to 2.446, corresponding to an absolute improvement of 0.032.
Hence, SimpleGPT provides measurable gains, even at the small 1B scale.


\begin{figure*}[t]
    \centering
    % ---------- First row ----------
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/simplegpt/big_model/7B/Llama2_7B_lr1e_3_wd0.05_20K_smooth.pdf}
        \caption{SimpleGPT 7B with 20K steps.}
        \label{fig:simplegpt_7b_20k}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/simplegpt/big_model/7B/Llama2_7B_lr1e_3_wd0.05_40K_smooth.pdf}
        \caption{SimpleGPT 7B with 40K steps.}
        \label{fig:simplegpt_7b_40k}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/simplegpt/big_model/7B/Llama2_7B_lr1e_3_wd0.05_60K_smooth.pdf}
        \caption{SimpleGPT 7B with 60K steps.}
        \label{fig:simplegpt_7b_60k}
    \end{subfigure}
    \caption{
        The training loss curves of Llama2 7B, Llama2 7B with QKNorm and SimpleGPT 7B under 20K, 40K and 60K training steps.
    }
    \label{fig:simplegpt_7b_several_steps}
\end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/simplegpt/big_model/1B/Llama2_1B_lr2e_3_200K_smooth.pdf}
    \caption{The training loss curves of Llama2 1B, Llama2 1B with QKNorm and SimpleGPT 1B under 200K training steps.}
    \vspace{-6pt}
    \label{fig:simplegpt_1b}
\end{figure}

\subsection{SimpleGPT 7B based on Llama2}
We compare SimpleGPT 7B against the standard Llama2 7B and Llama2 7B with QKNorm in \autoref{fig:simplegpt_7b_several_steps}. We train the models  for 20K, 40K, and 60K steps, corresponding to approximately 8B, 16B, and 24B tokens, respectively. All models are trained on the C4 dataset following the same training recipe as their corresponding baselines. SimpleGPT 7B uses a 0.001 learning rate, which is $3\times$ larger than that used in  Llama2~\cite{llama2_touvron2023llama} 7B model.



\begin{figure*}[t!]
    \centering
    % ---------- First row ----------
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/simplegpt/big_model/1B/Llama2_1B_lr2e_4_200K_smooth.pdf}
        \caption{SimpleGPT 1B with lr=2e-4.}
        \label{fig:simplegpt_1b_4}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/simplegpt/big_model/1B/Llama2_1B_lr2e_3_200K_smooth.pdf}
        \caption{SimpleGPT 1B with lr=2e-3.}
        \label{fig:simplegpt_1b_3}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/simplegpt/big_model/1B/Llama2_1B_lr2e_2_200K_smooth.pdf}
        \caption{SimpleGPT 1B with lr=2e-2.}
        \label{fig:simplegpt_1b_2}
    \end{subfigure}
    \caption{
        Overall comparison across  Llama2 1B, Llama2 1B with QKNorm and SimpleGPT 1B under three different learning rates. Adam-mini uses a $2\times 10^{-4}$ learning rate. In SimpleGPT, we enable a $10\times$ learning rate and obtain better performance.
    }
    \label{fig:six_subfigures}
\end{figure*}

We make the following observations.
First, the performance gain of SimpleGPT over Llama2+QKNorm is consistently significant throughout training: the improvement reaches 0.062 at 20K steps, increases to 0.077 at 40K steps, and remains at a comparable level (0.082) at 60K steps.
Second, SimpleGPT maintains more stable training dynamics compared to Llama2 with QKNorm.
Third, as training progresses and more tokens are observed, the relative improvement does not diminish, indicating that the advantage of SimpleGPT is stable rather than a transient early-training effect.
Finally, we observe a clear scaling trend with respect to model size.
While the 1B model trained on 26B tokens achieves a modest improvement of approximately 0.03, the 7B model trained on 24B tokens exhibits a substantially larger gain of 0.08.


\begin{figure}[t]
    \centering
    \vspace{-4pt}
    \includegraphics[width=0.95\linewidth]{Figures/simplegpt/big_model/8B/Llama3_8B_lr1e_3_wd0.05_20K_smooth.pdf}
    \caption{The training loss curves of Llama3 8B, Llama3 8B with QKNorm and SimpleGPT 8B.}
    \vspace{-6pt}
    \label{fig:simplegpt_8b}
\end{figure}

\begin{figure}[t]
    \centering
    \vspace{-4pt}
    \includegraphics[width=0.98\linewidth]{Figures/simplegpt/big_model/simplegpt_1.4B_seq1024.pdf}
    \caption{The validation loss curves of GPT2 1.4B, GPT2 1.4B with QKNorm and SimpleGPT 1.4B under 100K training steps.}
    \vspace{-6pt}
    \label{fig:simplegpt_1.4b}
\end{figure}


\subsection{SimpleGPT 8B  based on Llama3}
At the 8B scale, our experiments are based on the Llama3 8B architecture.
We train both SimpleGPT 8B and Llama3 8B on the C4 dataset with a global batch size of 192 and a sequence length of 2048.
We conduct training for 20K steps, corresponding to approximately 8B training tokens. We do not train for more steps due to compute constraints. SimpleGPT 8B employs a $3\times$ larger learning rate than Llama3 8B, and as shown in \autoref{fig:simplegpt_8b}, achieves a substantially lower training loss. Moreover, the magnitude of the performance gain is consistent with that observed for the 7B model, suggesting that our method exhibits favorable scaling behavior with increasing model size.


\subsection{SimpleGPT 1.4B  based on nanoGPT}
Finally, we evaluate SimpleGPT 1.4B on the nanoGPT code base.
All models are trained for 100K steps, corresponding to approximately 50B tokens. SimpleGPT 1.4B is trained using a learning rate that is $3\times$ larger than the baseline. 


We report validation losses in \autoref{fig:simplegpt_1.4b}.
Note that, since validation loss is recorded once every 1{,}000 steps, the curves in \autoref{fig:simplegpt_1.4b} appear different compared to earlier figures.



We observe that GPT-2 with QKNorm achieves nearly identical performance to the original GPT-2, indicating that QKNorm alone provides limited benefits in this setting.
Consistent with the results on LLaMA2 1B, SimpleGPT 1.4B based on nanoGPT yields an improvement of approximately 0.043.
These findings suggest that the gains introduced by SimpleNorm are stable across architectures.

\subsection{Ablation Study}
\emph{Different learning rates.} 
As shown in \autoref{fig:six_subfigures}, we conduct experiments on a 1B model using three different learning rates.
Under the learning rate $2\times10^{-4}$, LLaMA2 1B with QKNorm only slightly outperforms the original LLaMA2 1B.
When the learning rate is increased to $2\times10^{-3}$, the improvement from QKNorm becomes more pronounced, which we attribute to the smoother optimization landscape induced by QKNorm in comparison to PreNorm.
Importantly, across all learning rates, SimpleGPT achieves consistent improvement over LLaMA2 1B with QKNorm.
For a fair comparison, reported results are obtained under the best-performing configuration of LLaMA2 1B with QKNorm.


\subsection{Discussion about training time}
We compare the training speed of our SimpleGPT 8B model with that of Llama3 8B with QKNorm. On average, the Llama3 8B model requires 1553 ms per training step while our SimpleGPT 8B model takes 1603 ms per step. This corresponds to a reasonable slowdown of around 3\%, which can likely be further reduced by more clever kernel design or swapping the normalization operator to a more fusion-friendly point-wise functions like \(\mathrm{Derf}\)~\citep{stronger_norm_chen2025stronger}.