\section{Preliminaries}
We consider the unconstrained convex optimization problem
$\min_{\vx \in \mathbb{R}^d} f(\vx),$
where \(f:\mathbb{R}^d \to \mathbb{R}\) is \emph{differentiable}.  

\subsection{Convex and Smoothed Optimization}

\textbf{Lipschitz gradient smoothness.}
If \(f\) is twice differentiable, its second-order Taylor expansion at point $\vx$ is
\begin{align*}
    f(\vy)
\approx f(\vx)
&+ \langle \nabla f(\vx), \vy - \vx \rangle \\
&+ \frac{1}{2} (\vy - \vx)^\top \nabla^2 f(\vx) (\vy - \vx).
\end{align*}
The second-order term captures the local curvature.

\begin{definition}[$\beta$-smoothness]
The function \(f\) is said to be \(\beta\)-smooth if
\[
\|\nabla f(\vy)-\nabla f(\vx)\|_2 \le \beta \|\vy-\vx\|_2,
\forall \vy,\vx.
\]
\end{definition}

For convex and differentiable functions, \(\beta\)-smoothness is equivalent to the following quadratic upper bound:
\[
f(\vy) \le f(\vx) + \langle \nabla f(\vx), \vy-\vx \rangle
+ \frac{\beta}{2}\|\vy-\vx\|_2^2,
 \forall \vy,\vx .
\]
This inequality plays a central role in step-size (learning rate) selection for optimization methods.

\textbf{Gradient descent and learning rate.}
Consider the standard gradient descent iteration
\[
\vx_{k+1} = \vx_k - \eta \nabla f(\vx_k),
\]
where \(\eta>0\) is the learning rate. We wish to understand how the choice of \(\eta\) depends on the smoothness constant \(\beta\), and how this choice affects convergence.

\textbf{Descent condition.}
Evaluating the quadratic upper bound with
$\vy = \vx_{k+1} = \vx_k - \eta \nabla f(\vx_k)$ and $\vx = \vx_{k}$, we obtain
\[
f(\vx_{k+1})
\le f(\vx_k)
- \eta \|\nabla f(\vx_k)\|_2^2
+ \frac{\beta \eta^2}{2}\|\nabla f(\vx_k)\|_2^2 .
\]
Rearranging terms gives
\[
f(\vx_{k+1})
\le f(\vx_k)
- \left(\eta - \frac{\beta \eta^2}{2}\right)
\|\nabla f(\vx_k)\|_2^2 .
\]

A sufficient condition for monotone decrease of the objective is therefore
\begin{equation}
    0 < \eta \le \frac{2}{\beta}.
\end{equation}
This bound characterizes the \emph{stability region} of gradient descent for convex \(\beta\)-smooth functions. $\frac{2}{\beta}$ is usually known as the maximum tolerable learning
rate. 


For convex $\beta$-smooth problems, among all fixed learning rates that ensure descent, the canonical choice is typically $\eta = \frac{1}{\beta}$. This choice balances progress and stability and leads to the sharpest worst-case guarantees.

\subsection{Gradient and Hessian of a Linear Projection}
Given linear projection $\vy=\mW\vx$ and loss function $\ell$, the gradient and Hessian of $\ell$ with respect to $\vy$ are,
\[
\vg_{\vy} := {\frac{\partial \ell}{\partial \vy}}^{\top},
\qquad
\mH_{\vy\vy} := \frac{\partial^2 \ell}{\partial \vy \partial \vy^\top}.
\]
The Jacobian of $\vy$ with respect to $\vx$ is
$\mJ_{\vx}^{\vy}  = \frac{\partial \vy}{\partial \vx} = \mW.$
Hence, according to the chain rule, we have
\[
\vg_{\vx} = {\frac{\partial \ell}{\partial \vx}}^{\top}
=
{\mJ_{\vx}^{\vy}}^\top \vg_{\vy}
=
\mW^\top \vg_{\vy},
\]
and the Hessian matrix with respect to $\vx$ is
\begin{equation}
    \mH_{\vx\vx} = \frac{\partial^2 \ell}{\partial \vx \partial \vx^\top}
=
{\mJ_{\vx}^{\vy}}^\top \mH_{\vy\vy} \mJ_{\vx}^{\vy}
=
\mW^\top \mH_{\vy\vy} \mW.
\end{equation}
