\section{Conclusion}
\label{sec:conclusion}
In this work, we revisit Transformer optimization from a second-order perspective and establish a direct connection between architectural design, the Hessian matrix, and optimization stability. By introducing SimpleNorm and analyzing its induced Hessian structure, we show that reducing the Hessian norm of the activation with respect to the loss enables substantially larger admissible learning rates. The resulting model, \textbf{SimpleGPT}, reliably supports learning rates up to 3$\times$-10$\times$ larger than strong baselines while maintaining stable optimization. Across extensive experiments on \texttt{nanoGPT}-, \texttt{Llama2} and \texttt{Llama3}-style models, spanning parameter scales from 1B to 8B, our method consistently achieves substantially stronger performance than GPT with QKNorm. Importantly, these gains are obtained with minimal additional computational overhead. 
