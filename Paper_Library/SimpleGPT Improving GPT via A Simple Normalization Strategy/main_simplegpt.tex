%%%%%%%% ICML 2026 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2026} with \usepackage[nohyperref]{icml2026} above.
\usepackage{hyperref}


% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2026}

% For preprint, use
\usepackage[preprint]{icml2026}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2026}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


% \usepackage{algorithm}
% %\usepackage{algorithmicx}
% \usepackage{algorithmic}

% \usepackage{algpseudocode}

% \usepackage{algorithm}
% \usepackage{algpseudocode}
% \usepackage{amsmath}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}


\usepackage{float}
\usepackage{cancel}





\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}

\usepackage[algo2e]{algorithm2e}
\usepackage{hyperref}

%\usepackage[linesnumbered,algoruled,boxed,lined]{algorithm2e}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


\usepackage{amsmath,amssymb,amsfonts,bm}
\usepackage{mathtools}


\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{tikz} 

\usetikzlibrary{arrows.meta,positioning,calc}



\newcommand{\va}{\boldsymbol{a}}
\newcommand{\mA}{\boldsymbol{A}}
\newcommand{\vb}{\boldsymbol{b}}
\newcommand{\mB}{\boldsymbol{B}}
\newcommand{\vc}{\boldsymbol{c}}
\newcommand{\mC}{\boldsymbol{C}}
\newcommand{\vd}{\boldsymbol{d}}
\newcommand{\mD}{\boldsymbol{D}}

\newcommand{\mE}{\boldsymbol{E}}
\newcommand{\ve}{\boldsymbol{e}}
\newcommand{\mF}{\boldsymbol{F}}
\newcommand{\vf}{\boldsymbol{f}}
\newcommand{\mG}{\boldsymbol{G}}
\newcommand{\vg}{\boldsymbol{g}}
\newcommand{\mH}{\boldsymbol{H}}
\newcommand{\vh}{\boldsymbol{h}}
\newcommand{\mI}{\boldsymbol{I}}
\newcommand{\vi}{\boldsymbol{i}}
\newcommand{\mJ}{\boldsymbol{J}}
\newcommand{\vj}{\boldsymbol{j}}
\newcommand{\mK}{\boldsymbol{K}}
\newcommand{\vk}{\boldsymbol{k}}
\newcommand{\mL}{\boldsymbol{L}}
\newcommand{\vl}{\boldsymbol{l}}
\newcommand{\mM}{\boldsymbol{M}}
\newcommand{\vm}{\boldsymbol{m}}
\newcommand{\mN}{\boldsymbol{N}}
\newcommand{\vn}{\boldsymbol{n}}
\newcommand{\mO}{\boldsymbol{O}}
\newcommand{\vo}{\boldsymbol{o}}
\newcommand{\mP}{\boldsymbol{P}}
\newcommand{\vp}{\boldsymbol{p}}
\newcommand{\mQ}{\boldsymbol{Q}}
\newcommand{\vq}{\boldsymbol{q}}
\newcommand{\mR}{\boldsymbol{R}}
\newcommand{\vr}{\boldsymbol{r}}
\newcommand{\mS}{\boldsymbol{S}}
\newcommand{\vs}{\boldsymbol{s}}
\newcommand{\mT}{\boldsymbol{T}}
\newcommand{\vt}{\boldsymbol{t}}
\newcommand{\mU}{\boldsymbol{U}}
\newcommand{\vu}{\boldsymbol{u}}
\newcommand{\mV}{\boldsymbol{V}}
\newcommand{\vv}{\boldsymbol{v}}
\newcommand{\mW}{\boldsymbol{W}}
\newcommand{\vw}{\boldsymbol{w}}
\newcommand{\mX}{\boldsymbol{X}}
\newcommand{\vx}{\boldsymbol{x}}
\newcommand{\mY}{\boldsymbol{Y}}
\newcommand{\vy}{\boldsymbol{y}}
\newcommand{\mZ}{\boldsymbol{Z}}
\newcommand{\vz}{\boldsymbol{z}}


\newcommand{\mDiag}{\mathrm{Diag}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert_2}

\newcommand{\vgamma}{\boldsymbol{\gamma}}


\newcommand{\mPsi}{\boldsymbol{\Psi}}

\newcommand{\valpha}{\boldsymbol{\alpha}}

\newcommand{\cty}[1]{\textcolor{magenta}{{[cty: #1]}}\xspace}
\newcommand{\qxb}[1]{\textcolor{red}{{[qxb: #1]}}\xspace}





% Capitalize autoref
\renewcommand{\sectionautorefname}{Section}
\renewcommand{\subsectionautorefname}{Subsection}
\renewcommand{\subsubsectionautorefname}{Subsubsection}
\renewcommand{\appendixautorefname}{Appendix}
\renewcommand{\algorithmautorefname}{Algorithm}

% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}




% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{SimpleGPT: Improving GPT via A Simple Normalization Strategy}


\begin{document}



\twocolumn[
  \icmltitle{SimpleGPT: Improving GPT via A Simple Normalization Strategy}

  % It is OKAY to include author information, even for blind submissions: the
  % style file will automatically remove it for you unless you've provided
  % the [accepted] option to the icml2026 package.

  % List of affiliations: The first argument should be a (short) identifier you
  % will use later to specify author affiliations Academic affiliations
  % should list Department, University, City, Region, Country Industry
  % affiliations should list Company, City, Region, Country

  % You can specify symbols, otherwise they are numbered in order. Ideally, you
  % should not use this facility. Affiliations will be numbered in order of
  % appearance and this is the preferred way.
  \icmlsetsymbol{equal}{*}
  \icmlsetsymbol{comp}{$\dagger$}

  \begin{icmlauthorlist}
    \icmlauthor{Marco Chen}{yyy,equal}
    \icmlauthor{Xianbiao Qi}{xxx,equal,comp}
    \icmlauthor{Yelin He}{xxx}
    \icmlauthor{Jiaquan Ye}{xxx}
    \icmlauthor{Rong Xiao}{xxx}

  \end{icmlauthorlist}

  %\icmlaffiliation{yyy}{Tsinghua University, Beijing, China}
  %\icmlaffiliation{xxx}{Intellifusion Inc., Shenzhen, China}
  \icmlaffiliation{yyy}{Tsinghua University}
  \icmlaffiliation{xxx}{Intellifusion Inc.}

  \icmlcorrespondingauthor{Xianbiao Qi}{qixianbiao@gmail.com}


  % You may provide any keywords that you find helpful for describing your
  % paper; these are used to populate the "keywords" metadata in the PDF but
  % will not be shown in the document
  \icmlkeywords{Machine Learning, ICML}

  \vskip 0.3in
]




% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column listing the
% affiliations and the copyright notice. The command takes one argument, which
% is text to display at the start of the footnote. The \icmlEqualContribution
% command is standard text for equal contribution. Remove it (just {}) if you
% do not need this facility.

% Use ONE of the following lines. DO NOT remove the command.
% If you have no special notice, KEEP empty braces:
%\printAffiliationsAndNotice{}  % no special notice (required even if empty)
% Or, if applicable, use the standard equal contribution text:
\printAffiliationsAndNotice{\icmlEqualContribution}

\begin{abstract}
    In this work, we revisit Transformer optimization through the lens of second-order geometry and establish a direct connection between architectural design, activation scale, the Hessian matrix, and the maximum tolerable learning rate. We introduce a simple normalization strategy, termed SimpleNorm, which stabilizes intermediate activation scales by construction. Then, by analyzing the Hessian of the loss with respect to network activations, we theoretically show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting larger stable learning rates. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B. Empirically, SimpleGPT, our SimpleNorm-based network, tolerates learning rates 3$\times$-10$\times$ larger than standard convention, consistently demonstrates strong optimization stability, and achieves substantially better performance than well-established baselines. Specifically, when training 7B-scale models for 60K steps, SimpleGPT achieves a training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208. Our source code will be released at \url{https://github.com/Ocram7/SimpleGPT}.
\end{abstract}


\input{src/Introduction}
\input{src/RelatedWorks}
\input{src/Preliminaries}
\input{src/Methodology}
\input{src/Experiments}
\input{src/Conclusion}

\newpage
\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine
Learning. There are many potential societal consequences of our work, none
which we feel must be specifically highlighted here.




% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliography{ref}
\bibliographystyle{icml2026}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn



\input{src/Appendix}


\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
