\section{Appendix}

In the appendix we provide further qualitative analysis and implementation details, including the prompts used in our experiments.

\subsection{Qualitative Examples}

We discuss several other qualitative examples from the agent with search.


\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{images/qualitative_wa_14.pdf}
    \vspace{-0.2in}
    \caption{WA task \#14 is an example where performing more exploration helps the model to identify a trajectory that is likely to be more successful than others.}
    \label{fig:qualitative_14}
\end{figure*}


\paragraph{Enabling exploration} A significant advantage of models with search is their ability to explore larger parts of the environment compared to models without search. Fig.~\ref{fig:qualitative_14} part of the search tree for WebArena task \#14 (in the CMS environment), where the model is able to take multiple plausible actions at the first step (actions 1, 2, 3, and 4 in the graph), and expand the search tree to find the best trajectory ($3 \rightarrow 5 \rightarrow 6 \rightarrow 10$, which achieves the highest value of 0.68). In this case, the model terminates after hitting the search budget $c$ (rather than finding a state with value of 1.0), committing to the best found trajectory thus far, which is successful. This also highlights that our value function does not need to be perfect for search to be helpful.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/qualitative_96.pdf}
    \vspace{-0.2in}
    \caption{VWA shopping task \#96 is another example where search allows the model to be more robust to sampling bad actions. On this task, the baseline agent without search failed, but the agent with search is able to prune less promising trajectories (faded nodes in the figure) to identify the successful one.}
    \label{fig:qualitative_96}
\end{figure*}


\paragraph{Improving robustness} As discussed in Sec.~\ref{sec:qualitative}, the baseline agent can be prone to selecting bad samples from the language model due to randomness from nucleus sampling. Search allows the agent to explore each possibility and identify the best trajectories. VWA shopping task \#96 (shown in Fig.~\ref{fig:qualitative_96}) is another example. The baseline agent fails on this task, but the agent with search avoids the first two trajectories (ending at actions 3 and 4) due to low values assigned after exploring the subsequent states. It is able to prune these and identify a successful trajectory (highlighted in Fig.~\ref{fig:qualitative_96}).


\subsection{Additional Ablations}

\subsubsection{Value Function Ablations} \label{appendix:vf_ablations}

\begin{table*}[t]
\centering
\resizebox{1.0\linewidth}{!}{%
\begin{tabular}{lllcccccc}
  \toprule
   & \textbf{Agent Model} & \textbf{Value Function} & \textbf{Max Steps}  & \textbf{No Search} & \textbf{+ Search} & \textbf{$\Delta$} \\
  \midrule
  \multirow{7}{*}{VWA} & Llama-3-70B-Instruct~\citep{koh2024visualwebarena} & - & \multirow{2}{*}{30}  & 9.8\% & - & - \\
  % & GPT-4o-mini + SoM & - & & 7.3\% & - & - \\
  & GPT-4o + SoM~\citep{koh2024visualwebarena} & - & & 19.8\% & - & - \\
  \cmidrule(lr){2-7}
  & Llama-3-70B-Instruct + captions & LLaVA-1.6-34B &  \multirow{5}{*}{5} & 7.6\% & 13.5\% & +77.6\% \\
  & Llama-3-70B-Instruct + captions & GPT-4o & & 7.6\% & 16.7\% & +119.7\% \\
  & Llama-3.1-70B-Instruct + captions & GPT-4o & & 9.1\% & 16.2\% & +78.0\% \\
  & GPT-4o-mini + SoM & GPT-4o-mini & & 9.1\% & 14.4\% & +58.2\% \\
  & GPT-4o + SoM & GPT-4o &   & 18.9\% & \textbf{26.4\%} & +39.7\% \\
  \midrule
  \multirow{10}{*}{WA}  
  & GPT-4o~\citep{zhou2023webarena} & - & \multirow{6}{*}{30} & 13.1\% & - & - \\
  % & GPT-4~\citep{zhou2023webarena} & - &  & 14.9\% & - & - \\
  & GPT-4 + Reflexion~\citep{pan2024autonomous} & - &  & 15.6\% & - & - \\
  & AutoWebGLM~\citep{lai2024autowebglm} & - &  & 18.2\% & - & - \\
  & AutoEval~\citep{pan2024autonomous} & - &  & 20.2\% & - & - \\
  & BrowserGym~\citep{drouin2024workarena} & - &  & 23.5\% & - & - \\
  & SteP~\citep{sodhi2024step} & - &  & \textbf{35.8\%} & - & - \\
  \cmidrule(lr){2-7}
  & Llama-3-70B-Instruct & GPT-4o & \multirow{2}{*}{5} & 7.6\% & 10.1\% & +32.3\% \\
  % & GPT-4o-mini & GPT-4o-mini &  & \jykoh{X\%} & \jykoh{X\%} & \jykoh{+X\%} \\
  & GPT-4o & GPT-4o &  & 15.0\% & 19.2\% & +28.0\% \\
  \bottomrule
\end{tabular}
}
\caption{Success rates (SR) and relative change ($\Delta$) for baseline models and models that employ search on the VisualWebArena (VWA)~\citep{koh2024visualwebarena} and WebArena (WA)~\citep{zhou2023webarena} benchmarks. We also show other published approaches. Search substantially improves our baseline models, setting a new state-of-the-art on VWA.}
\label{tab:vf_results}
\end{table*}

In Sec.~\ref{sec:results} of the main paper, we experimented with using gpt-4o as our value function. In Tab.~\ref{tab:vf_results}, we present results using different language models as the agent models and the value functions. We observe that our tree search algorithm is effective across a range of different model sizes and capabilities. In particular, our approach applied to the Llama-3-70B-Instruct and LLaVA-1.6-34B value function yields a 77.6\% relative improvement over the baseline Llama-3-70B-Instruct agent on VWA (7.6\% to 13.5\%), and is a fully open sourced and reproducible baseline. For the GPT-4o-mini model (a relatively weaker model compared to GPT-4o) we also observed improvements when it is used as both the agent model and the value function, improving performance by 58.2\% over the no-search baseline on VWA (9.1\% to 14.4\%).



\subsection{Implementation Details}  \label{appendix:implementation}
\subsubsection{Language Model Agents}

\input{prompts/system_message.tex}
\input{prompts/som_prompt.tex}
\input{prompts/llama_prompt.tex}
\input{prompts/wa_prompt.tex}

For all experiments, we use a webpage viewport width of 1280, a viewport height of 2048, and truncate text observations to 3840 tokens. We sample from models using nucleus sampling with a temperature of 1.0 and a temperature of 1.0 and a top-p of 0.95. The system message used in all our experiments is provided in Fig.~\ref{fig:som_system_message}. This instructs the agent with the guidelines for the web navigation task, and list out all the possible actions that it can perform.

For the GPT-4o agent on VWA, we use the same prompt with SoM prompting from \cite{koh2024visualwebarena}, reproduced in Fig.~\ref{fig:som_prompt}. The model is provided with 3 in-context examples. A similar prompt (without the image screenshots) is used for the caption-augmented Llama-3-70B-Instruct agent which takes the caption-augmented accessibility tree as input (shown in Fig.~\ref{fig:llama_prompt}). 
On WA, the agents take the accessibility tree as input, and we use the same prompt from \cite{zhou2023webarena} that includes 2 in-context examples (reproduced in Fig.~\ref{fig:wa_prompt}).

\subsubsection{Value Function} \label{appendix:vf_prompt}

\input{prompts/vf_prompt.tex}
As described in Sec.~\ref{sec:method_vf}, we implement the value function $f_v$ by prompting a multimodal language model with all current and previously seen observations $\{ o_1, \ldots, o_p\}$. We use a prompt similar to the one from \cite{pan2024autonomous}, but make several modifications:
\begin{itemize}
    \item Instead of just the current screenshot, we include the last-$d$ screenshots of the evaluated trajectory, to enable the value function to more accurately compute success or failure for tasks that involve multi-step reasoning (e.g., whether the final observation corresponds to the second item in the second row of the second last observation).
    \item We modify the instructions to include more detailed instructions about what constitutes a failure or a success crtieria. This is necessary as our search occurs over a denser graph (compared to generating and re-ranking trajectories), and requires a more accurate value function. We refer readers to \cite{chen2024tree} for more discussion.
    \item Rather than a binary output, we instruct the model to produce whether the given observations have succeeded at the task or failed. If it fails, we further prompt the model to output if it is possibly on the right track to success. This allows us to collect scores in $`\{0, 0.5, 1\}$, enabling more finegrained value outputs (in addition to the averaging of multiple reasoning paths described in Sec.~\ref{sec:implementation}).
\end{itemize} 

The full system message and prompt for the value function is provided in Tab.~\ref{fig:vf_prompt}. We also note that our value function is heavily visual, which may be one explanation for why our method is more effective on the multimodal VWA benchmark than on WA (Sec.~\ref{sec:experiments}). Including more finegrained textual information about the trajectory on top of the screenshots, such as the accessibility tree representations of each page, may further improve its performance (at greater compute and API cost).

\subsection{Search Algorithm} \label{appendix:search_algorithm}

\begin{algorithm*}[t]
\caption{Our proposed search algorithm at step $t$}  \label{alg:search}
\let\AND\relax
\begin{algorithmic}[1]
\REQUIRE depth $d$, branching factor $b$, search budget $c$, start state $s_t$
\STATE Initialize frontier $\mathcal{F} \leftarrow \{\}$ as a max priority queue
\STATE Initialize best state $\hat{s}_t \leftarrow s_t$
\STATE Initialize the best score $\hat{v}_t \leftarrow -\infty$
\STATE Initialize the search counter $s \leftarrow 0$
\WHILE{$s < c$}
    \STATE $s_p, v_{\text{prev}} \leftarrow \text{pop}(\mathcal{F})$
    \STATE Backtrack and execute new actions to get to state $s_p$
    \STATE Compute the score $v_p = f_v(I, \{ o_1, \ldots, o_p\})$ from current and previous observations
    \STATE $s \leftarrow s + 1$
    \IF{$v_p \geq \hat{v}_t$}
        \STATE $\hat{v}_t \leftarrow v_p$
        \STATE $\hat{s}_t \leftarrow s_p$
    \ENDIF
    \IF{$v_p \geq \theta$}
        \STATE \textbf{break} \COMMENT{Found a likely successful state}
    \ENDIF
    \IF{$s \geq c$}
        \STATE \textbf{break} \COMMENT{Search budget exceeded}
    \ENDIF
    \IF{$| s_0, ..., s_{p} | < d$}
    \STATE Sample $b$ candidates for the next action from the LM: $\{a_p^1, ..., a_p^b\} \sim f_{\theta}(o_p)$
    \FOR{$i \leftarrow 1 \text{ to } b$}
        \STATE Execute $a_p^i$ to get to state $s_p^i$
        \STATE Add new candidate state and the current value to the frontier: $\mathcal{F} \leftarrow \mathcal{F} \cup (s_p^i, v_p)$
    \ENDFOR
    \ENDIF
\ENDWHILE
\STATE Reset $\mathcal{F} \leftarrow \{\}$ and $s \leftarrow 0$
\STATE Go to the best state $\hat{s}_t$
\STATE Set $t \leftarrow t + \text{(\#actions to get from $s_t$ to $\hat{s}_t$)}$
\end{algorithmic}
\end{algorithm*}

Our search procedure described in Sec.~\ref{sec:method_search} is summarized in Algorithm.~\ref{alg:search}.

\subsubsection{Environment Reset} \label{appendix:reset}

In this section, we describe the implementation details of the backtracking used in our search procedure:

\begin{enumerate}
    \item We maintain a max priority queue that contains sequences of actions and their score $v$ (from the value function). Each element is a sequence of actions that the agent has to sequentially execute starting from the initial state (task dependent, but often the website homepage) to get to state $s$ that has the corresponding score $v$.
    \item After we execute a new action (L23 of Algorithm.~\ref{alg:search}), we append this action to the sequence of actions and add the new sequence to the priority queue with its corresponding score $v$.
    \item In order to reset the environment to get a clean slate for the next node to explore, we reset to the initial state again, and repeat the execution of the next sequence of actions starting from step 1.
\end{enumerate}

We implemented backtracking in this fashion, as we found that this was a substantially more complete way of resetting the state, as opposed to simply clicking the ``back'' button on the browser for example, as this does not persist certain web states such as the scroll offset, or retain text in text inputs. While our implementation does improve fidelity of backtracking and resets, it however does add significant overhead in terms of time (see Sec.~\ref{sec:limitations} for more discussion).

%The exact code implementation details can be found within the \texttt{removed\_for\_review} file of our publicly available code at \texttt{removed\_for\_review}.
