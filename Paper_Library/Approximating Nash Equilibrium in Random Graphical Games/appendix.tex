\section{Feasibility of Convex Hierarchy}

\begin{lemma}
Let $(G, \{f_{ij}\}_{i,j \in [N]})$ be a polymatrix game defined on graph $G$ with rescaled payoffs $\{f_{ij}\}_{i,j \in [N]}$ that are $L$-smooth.  If $G$ is a random graph with edge density $\frac{k^8}{\epsilon^4} \ln^3(N)$ we have the constraints of \pref{alg:convex} are feasible for $\ell = \frac{L^4\ln(k)}{\epsilon^4}$.  In particular, any mixed nash equilibrium of the game is a feasible pseudodistribution.  
\end{lemma}

\begin{proof}
Let $\zeta \in \calP(N,k)$ be the product distribution over $[k]^N$ corresponding to a mixed nash equilibrium.  Since $\zeta$ is a distribution we immediately satisfy $\pE_\zeta[1] = 1$ and $\pE_\zeta[p(\bold{a})] \geq 0$ for all $p(\bold{a}) \in SoS_{\ell}(\{a_{ir}\}_{i \in [N], r \in [k]})$. Furthermore, it is immediate that  $\pE[(a_{ip}^2 - a_{ip})\prod_{a_{uv} \in S}a_{uv}] = 0$  for all $i \in [N]$ and $p \in [k]$, and for all $S \subset \bold{a}$ of size $|S| \leq \ell - 2$.  It is also immediate that $\pE[(\sum_{p=1}^k a_{ip} - 1)\prod_{a_{uv} \in S}a_{uv}] = 0$ for all $i \in [N]$ and for all $S \subset \bold{a}$ of size $|S| \leq \ell - 1$.  Next we have 
\[\E_\zeta[(\sum_{j \in N(i)} \sum_{p,q \in [k]^2}f_{ij}(p,q)a_{ip}a_{jq} - \sum_{j \in N(i)} \sum_{q \in [k]}f_{ij}(w,q)a_{jq} + 2\epsilon)\prod_{a_{uv} \in S}a_{uv}] \]

Let $R \subseteq S$ where for all $a_{uv} \in R$ we have $u \in N(i)$.  This enables us to write    
\[= \E_\zeta[(\sum_{j \in N(i)} \sum_{p,q \in [k]^2}f_{ij}(p,q)a_{ip}a_{jq} - \sum_{j \in N(i)} \sum_{q \in [k]}f_{ij}(w,q)a_{jq} + 2\epsilon)\prod_{a_{uv} \in R}a_{uv}\prod_{a_{uv} \in S/R}a_{uv}] \]
Using the fact that $\zeta$ is a product distribution we obtain 

\begin{equation} \label{eq:feasible1}
= \E_\zeta[(\sum_{j \in N(i)} \sum_{p,q \in [k]^2}f_{ij}(p,q)a_{ip}a_{jq} - \sum_{j \in N(i)} \sum_{q \in [k]}f_{ij}(w,q)a_{jq} + 2\epsilon)\prod_{a_{uv} \in R}a_{uv}] \E_\zeta[\prod_{a_{uv} \in S/R}a_{uv}]     
\end{equation}
Since $\prod_{a_{uv} \in S/R}a_{uv}$ is a sum of squares, we have  $\E_\zeta[\prod_{a_{uv} \in S/R}a_{uv}] \geq 0$.  So to show \pref{eq:feasible1} is nonnegative, it suffices to show  

\begin{equation} \label{eq:feasible2}
\E_\zeta[(\sum_{j \in N(i)} \sum_{p,q \in [k]^2}f_{ij}(p,q)a_{ip}a_{jq} - \sum_{j \in N(i)} \sum_{q \in [k]}f_{ij}(w,q)a_{jq} + 2\epsilon)\prod_{a_{uv} \in R}a_{uv}] \geq 0    
\end{equation}
Moving on, we break $N(i)$ into $N(i)/R$ and $R$ and write \pref{eq:feasible2} accordingly, 
\begin{multline}
  \pref{eq:feasible2} = \E_\zeta[(\sum_{j \in N(i)/R} \sum_{p,q \in [k]^2}f_{ij}(p,q)a_{ip}a_{jq} - \sum_{j \in N(i)/R} \sum_{q \in [k]}f_{ij}(w,q)a_{jq})\\ + \sum_{j \in R} \sum_{(p,q) \in [k]^2}f_{ij}(p,q)a_{ip}a_{jq} - \sum_{j \in R}\sum_{q \in [k]}f_{ij}(w,q)a_{jq} + 2\epsilon) \prod_{a_{uv} \in R}a_{uv}]  
\end{multline}

\begin{multline}
  = \E_\zeta[\big(\sum_{j \in N(i)/R} \sum_{p,q \in [k]^2}f_{ij}(p,q)a_{ip}a_{jq} - \sum_{j \in N(i)/R} \sum_{q \in [k]}f_{ij}(w,q)a_{jq}\big)\prod_{a_{uv} \in R}a_{uv}\\ + \sum_{j \in R} \sum_{(p,q) \in [k]^2}f_{ij}(p,q)a_{ip}a_{jq}\prod_{a_{uv} \in R}a_{uv} - \sum_{j \in R}\sum_{q \in [k]}f_{ij}(w,q)a_{jq}\prod_{a_{uv} \in R}a_{uv} + 2\epsilon \prod_{a_{uv} \in R}a_{uv}]  
\end{multline}
By linearity 
\begin{multline}
  = \E_\zeta[\big(\sum_{j \in N(i)/R} \sum_{p,q \in [k]^2}f_{ij}(p,q)a_{ip}a_{jq} - \sum_{j \in N(i)/R} \sum_{q \in [k]}f_{ij}(w,q)a_{jq}\big)\prod_{a_{uv} \in R}a_{uv}]\\ + \pE_\zeta[\sum_{j \in R} \sum_{(p,q) \in [k]^2}f_{ij}(p,q)a_{ip}a_{jq}\prod_{a_{uv} \in R}a_{uv} - \sum_{j \in R}\sum_{q \in [k]}f_{ij}(w,q)a_{jq}\prod_{a_{uv} \in R}a_{uv} + 2\epsilon \prod_{a_{uv} \in R}a_{uv}]  
\end{multline}

Using the fact that $\zeta$ is a product distribution we obtain 
\begin{multline} \label{eq:feasible3}
  = \E_\zeta[\big(\sum_{j \in N(i)/R} \sum_{p,q \in [k]^2}f_{ij}(p,q)a_{ip}a_{jq} - \sum_{j \in N(i)/R} \sum_{q \in [k]}f_{ij}(w,q)a_{jq}\big)] \pE_\zeta[\prod_{a_{uv} \in R}a_{uv}]\\ + \pE_\zeta[\sum_{j \in R} \sum_{(p,q) \in [k]^2}f_{ij}(p,q)a_{ip}a_{jq}\prod_{a_{uv} \in R}a_{uv} - \sum_{j \in R}\sum_{q \in [k]}f_{ij}(w,q)a_{jq}\prod_{a_{uv} \in R}a_{uv} + 2\epsilon \prod_{a_{uv} \in R}a_{uv}]  
\end{multline}

Again we use the fact that $\prod_{a_{uv} \in R}a_{uv}$ is a sum of squares so $\pE_\zeta[\prod_{a_{uv} \in R}a_{uv}] \geq 0$.  We also use the fact that $\zeta$ is a mixed nash equilibrium so  

\[\E_\zeta[\big(\sum_{j \in N(i)/R} \sum_{p,q \in [k]^2}f_{ij}(p,q)a_{ip}a_{jq} - \sum_{j \in N(i)/R} \sum_{q \in [k]}f_{ij}(w,q)a_{jq}\big)] \geq -\epsilon \]

Plugging both these facts back into the first term of \pref{eq:feasible3} we obtain  

\begin{equation} \label{eq:feasible4} 
   \pref{eq:feasible3} \geq \pE_\zeta[\sum_{j \in R} \sum_{(p,q) \in [k]^2}f_{ij}(p,q)a_{ip}a_{jq}\prod_{a_{uv} \in R}a_{uv} - \sum_{j \in R}\sum_{q \in [k]}f_{ij}(w,q)a_{jq}\prod_{a_{uv} \in R}a_{uv} + \epsilon \prod_{a_{uv} \in R}a_{uv}]      
\end{equation}

By $L$-smoothness we have $f_{ij}(p,q) \geq -\frac{L}{d}$.  Therefore, 
the first term $\pE_\zeta[\sum_{j \in R} \sum_{(p,q) \in [k]^2}f_{ij}(p,q)a_{ip}a_{jq}\prod_{a_{uv} \in R}a_{uv}]$ is lower bounded by $\pE_\zeta[\frac{-L}{d}\sum_{j \in R} \sum_{(p,q) \in [k]^2}a_{ip}a_{jq}\prod_{a_{uv} \in R}a_{uv}]$.  Similarly, using the fact that $f_{ij}(p,q) \leq \frac{L}{d}$, we conclude that the second term $- \pE_\zeta[\sum_{j \in R}\sum_{q \in [k]}f_{ij}(w,q)a_{jq}\prod_{a_{uv} \in R}a_{uv}]$ is lower bounded by $- \pE_\zeta[\frac{L}{d}\sum_{j \in R}\sum_{q \in [k]}a_{jq}\prod_{a_{uv} \in R}a_{uv}]$.  Plugging both relations into \pref{eq:feasible4} we obtain 

\begin{equation} 
   \geq  \pE_\zeta[\frac{-L}{d}\sum_{j \in R} \sum_{(p,q) \in [k]^2}a_{ip}a_{jq}\prod_{a_{uv} \in R}a_{uv} - \frac{L}{d}\sum_{j \in R}\sum_{q \in [k]}a_{jq}\prod_{a_{uv} \in R}a_{uv} + \epsilon \prod_{a_{uv} \in R}a_{uv}]
\end{equation}

\begin{equation} 
   =  \pE_\zeta[\big(\frac{-L}{d}\sum_{j \in R} \sum_{(p,q) \in [k]^2}a_{ip}a_{jq} - \frac{L}{d}\sum_{j \in R}\sum_{q \in [k]}a_{jq} + \epsilon\big) \prod_{a_{uv} \in R}a_{uv}]
\end{equation}
Using the booleanity constraint we lower bound by 
\begin{equation} \label{eq:feasible5}
   \geq \pE_\zeta[\big(-\frac{L|R|(k^2 + k)}{d} + \epsilon\big) \prod_{a_{uv} \in R}a_{uv}] 
\end{equation}
   
Plugging $d = \frac{Lk^8 \ln^3(N)}{\epsilon^5}$ and $|R| = \frac{\ln(k)}{\epsilon^4}$ we obtain 
\[
   -\frac{L|R|(k^2 + k)}{d} + \epsilon \geq 0\]
Since $\prod_{a_{uv} \in R}a_{uv}$ is a sum of squares we conclude $\pref{eq:feasible5} \geq 0$ as desired.      
TODO: Change $d = \frac{Lk^8 \ln^3(N)}{\epsilon^5}$
\end{proof}

\section{Explicit Convex Constraints} \label{sec:explicit}

\paragraph{Moment Matrices: }

Let $M_{\ell}(\{a_{ir}\}_{i \in [N], r \in [k]})$ be the moment matrix with rows indexed by subsets $I \subseteq \{a_{ir}\}_{i \in [N], r \in [k]}$ for $|I| \leq \ell$.  Similarly, columns are indexed by subsets $J \subseteq \{a_{ir}\}_{i \in [N], r \in [k]}$ for $|J| \leq \ell$.
We require that 
\[M_{\ell}(\bold{a}) \succeq 0\]
\[M_{\ell}(\bold{a})_{\emptyset,\emptyset} = 1\]
Note that the PSD constraint implies via the cholesky decomposition that
\[M_{\ell}(\bold{a}) = V^TV\]
for a matrix $V \in \R^{{Nk \choose \ell} \times {Nk \choose \ell}}$ with columns equal to $v_H$ for all $H \subset \{a_{ir}\}_{i \in [N], r \in [k]}$ satisfying $|H| \leq \ell$.  Now, given the vectors   $\{v_H\}_{H \subset \{a_{ir}\}_{i \in [N], r \in [k]}, |H| \leq \ell}$ we add the linear constraints that 
\[\pE[\prod_{a_{uv} \in H} a_{uv}] \defeq \langle v_{H_1}, v_{H_2} \rangle \]
for all $H_1$ and $H_2$ satisfying $H_1 \cup H_2 = H$.  Henceforth we may refer to $\pE[\prod_{(u,v) \in H} a_{uv}]$ without ambiguity.  Also note that we have satisfied the $\pE[1] = 1$ and $\pE[p(\bold{a})] \geq 0$ constraints for all $p(\bold{a}) \in SoS_{\ell}(\bold{a})$.  At this point, we can define the linear functional $\pE: \R_{\leq \ell}[\bold{a}] \rightarrow \R$ from the degree less than $\ell$ polynomials to the reals so that $\pE[\sum_{i} \alpha_i p_i(a)] = \sum_{i} \alpha_i \pE[p_i(a)]$.       

The booleanity and color constraints are relaxed in the manner prescribed by the Sherali-Adams hierarchy.  We add linear constraints for all $ i \in [N]$ and $p \in [k]$

\[ \pE[(a_{ip}^2 - a_{ip})\prod_{a_{uv} \in S}a_{uv}] = \pE[a_{ip}^2\prod_{a_{uv} \in S}a_{uv}] - \pE[a_{ip}\prod_{a_{uv} \in S}a_{uv}] = 0\]
for all $S \subset \{a_{ir}\}_{i \in [N], r \in [k]}$ with size $|S| \leq \ell - 2$.  Additionally, we add linear constraints for all $i \in [N]$

\[\pE[(\sum_{r=1}^k a_{ir} - 1)\prod_{a_{uv} \in S}a_{uv}] = \sum_{r \in [k]}\pE[ a_{ir}\prod_{a_{uv} \in S}a_{uv}] - \pE[\prod_{a_{uv} \in S}a_{uv}] = 0\]
for all $S \subset \{a_{ir}\}_{i \in [N], r \in [k]}$ satisfying $|S| \leq \ell - 1$.  Finally, we add the linear constraints for conditioning preserving correlated equilibrium.  For all $i \in [N]$ and for all $w \in [k]$

\[\pE[\big(\sum_{j \in N(i)} \sum_{p,q \in [k]^2}f_{ij}(p,q)a_{ip}a_{jq}\big)\prod_{a_{uv} \in S}a_{uv}] - \pE[\big(\sum_{j \in N(i)} \sum_{q \in [k]}f_{ij}(w,q)a_{jq} \big)\prod_{a_{uv} \in S}a_{uv}] + \epsilon \geq 0\]

for all $S \subset \{a_{jr}\}_{j \in [N]/i, r \in [k]}$ with size $|S| \leq \ell - 2$.  




\section{Best Response Pursuit Lemmas} \label{sec:brplemmas}
\constraintcorr*

\begin{proof}
We begin with the definition 
\[\Phi_{\Gamma}(i) = \max_{w \in [k]}\Big(\sum_{j \in N(i)} \sum_{q \in [k]} f_{ij}(w,q) \pE[a_{jq}] - \sum_{j \in N(i)} \sum_{(p,q) \in [k]^2} f_{ij}(p,q) \pE[a_{ip}]\pE[a_{jq}]\Big) \]

Let $w^*$ be the argmax over $w \in [k]$ in the equation above.  Then we rewrite the equation as 

\begin{multline*}
= \sum_{j \in N(i)} \sum_{(p,q) \in [k]^2} f_{ij}(p,q) (\pE[a_{ip}a_{jq}] - \pE[a_{ip}]\pE[a_{jq}])\\  + \sum_{j \in N(i)} \sum_{q \in [k]} f_{ij}(w^*,q) \pE[a_{jq}] - \sum_{j \in N(i)} \sum_{(p,q) \in [k]^2} f_{ij}(w^*,q) \pE[a_{ip}a_{jq}]     
\end{multline*}

The second line is the conditioning preserving correlated equilibrium constraint which we know is less than $\epsilon$.  Therefore we upper bound by  
\[\leq \epsilon + \sum_{j \in N(i)} \sum_{(p,q) \in [k]^2} f_{ij}(p,q) (\pE[a_{ip} a_{jq}]) - \pE[a_{ip}]\pE[a_{jq}] ) \]
We upper bound via $\ell_{1,\infty}$ holders and use the fact that $f_{i,j}(a,b) \leq \frac{L}{d}$.   
\[\leq \epsilon + \frac{L}{d}\sum_{j \in N(i)} \sum_{(p,q) \in [k]^2} |\pE[a_{ip} a_{jq}] - \pE[a_{ip}]\pE[a_{jq}]|]  = \epsilon + L\Delta(i)\]
\end{proof}

\smooth*

\begin{proof}
We have by definition 

\[\Phi_{\Gamma_Q \bigcup \Gamma_{\overline{Q}}}(i) \defeq \max_{w \in [k]} \E_{\Gamma_Q \bigcup \Gamma_{\overline{Q}}}[ \sum_{j \in N_{Q \bigcup \overline{Q}}(i)}\sum_{q \in [k]} f_{i,j}(w, q)a_{jq} - \sum_{j \in N_{Q \bigcup \overline{Q}}(i)}\sum_{(p,q) \in [k]^2} f_{i,j}(p,q)a_{ip}a_{jq}]\]

Using the fact that $N_{Q \bigcup \overline{Q}}(i)  = N_Q(i) \bigcup N_{\overline{Q}}(i)$ we obtain 

\begin{multline}
     = \max_{r \in [k]} \E_{\Gamma_Q \bigcup \Gamma_{\overline{Q}}}[ \sum_{j \in N_Q(i)}\sum_{p \in [k]} f_{i,j}(w, p)a_{jp} + \sum_{j \in N_{\overline{Q}}(i)}\sum_{p \in [k]} f_{i,j}(w,p) a_{jp}\\ - (\sum_{j \in N_Q(i)}\sum_{(p,q) \in [k]^2} f_{i,j}(p,q)a_{ip}a_{jq} + \sum_{j \in N_{\overline{Q}}(i)}\sum_{(p,q) \in [k]^2} f_{i,j}(p,q)a_{ip}a_{jq})]
\end{multline}


Using the $L$-smooth assumption we obtain 

\[ \leq \max_{w \in [k]} \E_{ \Gamma_Q \bigcup \Gamma_{\overline{Q}}}[ \sum_{j \in N_Q(i)} \sum_{p \in [k]}f_{i,j}(w, p)a_{jp} - \sum_{j \in N_Q(i)} \sum_{(p,q) \in [k]^2}f_{i,j}(p,q)a_{ip}a_{jq} + 2L\frac{|N_{\overline{Q}}(i)|}{|N(i)|}]\]

\[ = \max_{w \in [k]} \E_{\Gamma_Q}[ \sum_{j \in N_Q(i)}\sum_{p \in [k]} f_{i,j}(w,p) a_{jp} - \sum_{j \in N_Q(i)}\sum_{(p,q)\in [k]^2} f_{i,j}(p,q)a_{ip}a_{jq}] + 2L\frac{|N_{\overline{Q}}(i)|}{|N(i)|}\]
Using the definition of $\Phi_Q(i)$ we conclude 
\[ = \Phi_Q(i) + 2L\frac{|N_{\overline{Q}}(i)|}{|N(i)|}\]
as desired.  
\end{proof}

To prove \pref{lem:append-lemma} we will need the following supporting lemma.  
\begin{lemma} \label{lem:append-exponent}
For any subset of nodes $H \subset [N]$, the number of nodes $i \in \overline{H}$ satisfying $|N_H(i)| \geq \max(10 \frac{|H|}{N} d, 10\ln(n))$ is less than $\frac{H}{4}$.    
\end{lemma}

\begin{proof}
We proceed by an application of the chernoff and union bound.  Let's fix the set $H$.  Then for any node $i \in [N]$, the probability $|N_H(i)| \geq \max(10 \frac{|H|}{N} |E(i)|, 10\ln(n))$ is upper bounded as follows.  Let $e_j$ be the following random variable with $e_j = 1$ if $j \in H$ and $e_j = 0$ otherwise.  Here the randomness is taken with respect to the randomness in generating the graph.  Let $\mu \defeq \E[\frac{1}{N}\sum_{j=1}^N e_j] = \frac{|H|}{N}\frac{d}{N}$.  We define a constant $\delta > 0$ such that $(1+\delta)\mu N = \max(10 \frac{|H|}{N} d, 10\ln(n)) \geq 10 \frac{|H|}{N} d$.  This implies $\delta > 9$. Now let $\tau \defeq \mathbb{P}[\frac{1}{N}\sum_{j=1}^N e_j \geq (1+\delta)\mu]$.  Using the chernoff bound we obtain.   
\[\tau \defeq \mathbb{P}[\frac{1}{N}\sum_{j=1}^N e_j \geq (1+\delta)\mu] \leq \exp(-\delta \mu N) \leq \exp(-9\mu N)\]
If $\mu N \geq \ln(N)$ then we have $\tau \leq \exp(-9 \ln(N))$.  On the other hand, if $\mu N \leq \ln(N)$ we define $\delta$ such that 
$(1+\delta)\mu N = \max(10 \frac{|H|}{N} d, 10\ln(n)) \geq 10 \ln(N)$ which implies $\delta \mu N \geq 9 \ln(N)$.  Thus we have in this case 

\[\tau \defeq \mathbb{P}[\frac{1}{N}\sum_{j=1}^N e_j \geq (1+\delta)\mu] \leq \exp(-\delta \mu N) \leq \exp(-9\ln(N))\]
Now let $H^* \defeq \{i \in \overline{H}| |N_H(i)| \geq \max(10 \frac{|H|}{N} d, 10\ln(n))\}$.  We need to show $|H^*| \leq \frac{|H|}{4}$.  Let $w_i$ for $i \in [N]$ be a random variable where $w_i = 1$ if $i \in H^*$ and $w_i = 0$ otherwise. Let $\mu' \defeq \mathbb{E}[\frac{1}{N}\sum_{i=1}^N w_i] = \tau$.  Let $\delta' > 0$ be defined such that $(1+\delta')\mu' N = \frac{|H|}{4}$ then by chernoff bound we have
\[\mathbb{P}(\frac{1}{N}\sum_{i=1}^N w_i \geq \mu'(1+\delta')) \leq \Big(\frac{e^{\delta'}}{(1+\delta')^{(1+\delta')}}\Big)^{\mu'N} = \exp((\delta'-(1+\delta') \ln(1+\delta')) \mu' N)  
\]
Using the fact that $(1+\delta')\mu' N = \frac{|H|}{4}$ we obtain
\begin{equation}
\leq \exp(\frac{|H|}{4} -\ln(1+\delta') \frac{|H|}{4})  
\end{equation}
Next we use the fact that $\delta' = \frac{|H|}{4\mu' N} - 1 \geq \frac{1}{4N} \exp(9 \ln(N)) - 1 \geq \frac{N^8}{4} - 1$.  Plugging $\delta' \geq \frac{N^8}{4} - 1$ into 
\[\leq \exp(-8\ln(N) \frac{|H|}{4}) \leq N^{-2|H|}    \]
Since there are no more than ${N \choose |H|} \leq N^{|H|}$ subsets of size $|H|$  we conclude via union bound that  $|H^*| \leq \frac{|H|}{4}$ with probability $ 1 - N^{-2|H|} N^{|H|} \geq  1 - N^{-|H|} \geq 1 - N^{-\ln^3(N)}$ as desired. 


\end{proof}



\appendlemma*

\begin{proof}
Let $H_1 \defeq H$.  Then we have by \pref{lem:append-exponent} that $Flag(H_1,G)$ iteratively appends $H_2,...,H_y$ to $H_1$ where $H_i \leq \frac{1}{4^{j-1}}H_1$ for all $j \in [2,y]$.  Thus $|W| \leq \frac{4}{3}|H|$.  Furthermore, each iteration of of $Flag(H_1,G)$ ensures that for all $i \in \overline{W}$ 
\begin{equation} \label{eq:app-exp-1}
\frac{|N_{W}(i)|}{d} = \frac{1}{d}|N_{\bigcup_{j \in [y]} H_j}(i)| = \sum_{j=1}^y \frac{|N_{H_j}(i)|}{d} \leq \sum_{j=1}^y 10\max \Big(\frac{|H_j|}{N}, \frac{\ln(N)}{d}\Big)    
\end{equation}



\[\leq 10\sum_{j=1}^y \Big(\frac{|H_j|}{N} + \frac{\ln(N)}{d}\Big)\leq \frac{40}{3N}|H_1| + 10\frac{\ln^2(N)}{d}\] 

Here the first inequality follows from the fact that at each iteration $Flag(H_1,G)$ performs the following operation.  For $W = \bigcup_{r=1}^j H_r$ we define $H_{j+1}$ to be 
\[H_{j+1} \defeq \Big\{i \in \overline{W}\Big| \frac{|N_{H_{j}}(i)|}{d} \geq \max \Big(10 \frac{|H_{j}|}{N}, 10\frac{\ln(n)}{d}\Big)\Big\}\]
Thus for all $i \in \overline{W}$ we have 

\[\frac{|N_{H_{j}}(i)|}{d} \leq \max \Big(10 \frac{|H_{j}|}{N}, 10\frac{\ln(n)}{d}\Big)\]
Continuing to bound \pref{eq:app-exp-1}, we separate the max into a sum to conclude
\[\leq 10\sum_{j=1}^y \Big(\frac{|H_j|}{N} + \frac{\ln(N)}{d}\Big)\leq \frac{40}{3N}|H_1| + 10\frac{\ln^2(N)}{d}\] 
We have established $\frac{|N_{W}(i)|}{d} \leq \frac{40 |H|}{3N}+ 10\frac{\ln^2(N)}{d}$ for all $i \in \overline{W}$ as desired.  
\end{proof}

\bestresponselemma*

\begin{proof}

First, using \pref{lem:smooth} we have $\max_{i \in \overline{W}} \Phi_{\hat{\Gamma}}(i) \leq \zeta + L\gamma$ which concludes the lemma for $i \in \overline{W}$.  To conclude the lemma it suffices to show that $BestRespond(W,\Gamma)$ sets a subset $A \subseteq W$ to best respond to $\Gamma_{\overline{W}}$, and that for all $i \in A$, $\frac{|N_W(i)|}{d} \leq 10\epsilon$.  

To show this, let $\kappa \in \R^+$ be a constant such that $|W| = \kappa N$.  Let $e_1,...,e_{\kappa^2N^2}$ be the indicator random variables distributed $Bern(p)$ for the presence of an edge between any pair of nodes in $W$.  Let $\mu = p$, and let $\delta \mu = \frac{10\epsilon d}{\kappa N} = \frac{10\epsilon}{\kappa} p = \frac{10\epsilon}{\kappa} \mu$ which implies $\delta = \frac{10\epsilon}{\kappa}$.  Then for $|W| \leq \epsilon N$ we have $\delta \geq 10$.  Therefore, we invoke the upper chernoff bound to obtain  
\[\mathbb{P}[\frac{1}{\kappa^2 N^2} \sum_{i=1}^{\kappa^2 N^2} e_i \geq \frac{10\epsilon d}{\kappa N} ] \leq \exp(-\frac{9\epsilon d}{\kappa N} \kappa^2 N^2) = \exp(-9\epsilon d \kappa N) = \exp(-9 \frac{1}{\epsilon^3}\ln^3(N) \kappa N)\]

We also know there are no more than ${N \choose \kappa N} \leq N^{\kappa N}$ subsets of $V$ of size $\kappa N$.  Therefore, applying union bound we conclude that $W$ has fewer than $10 \epsilon d \kappa N$ edges with high probability $1 - N^{-\ln^3(N)}$.  Furthermore, via markov we conclude that no more than $\frac{1}{10}|W|$ nodes $i \in W$ satisfy $N_W(i) \geq 100 \epsilon d$.  Equivalently, there exists a subset $A \subseteq W$ of size $|A| \geq \frac{9|W|}{10}$ such that $\frac{|N_W(i)|}{d} \leq 100 \epsilon$ for all $i \in A$. By \pref{lem:smooth}, we conclude $\max_{i \in A} \Phi_{\hat{\Gamma}}(i) \leq 100L\epsilon$.  

Putting everything together, for $BestResponsd(W,\Gamma)$ outputs $B = W/A$ such that  $\overline{B} = \overline{W} \bigcup A$.  Thus we conclude,
$$\max_{i \in \overline{B}} \Phi_{\hat{\Gamma}}(i) = \max_{i \in \overline{W} \bigcup A} \Phi_{\hat{\Gamma}}(i) = \max(\max_{i \in \overline{W}} \Phi_{\hat{\Gamma}}(i), \max_{i \in A} \Phi_{\hat{\Gamma}}(i)) \leq \max(\zeta + L\gamma, 100 L\epsilon )$$  
as desired.  

\end{proof}

\section{Miscellaneous}
\begin{lemma} \label{lem:jackson}(Bernstein, Jackson, folklore) 
There exists a series of polynomials $\{g_p(x)\}_{p=1}^n$ where  $g_{p}(x) \defeq \sum_{r=1}^{p} \alpha_r x^r$ and $\{\alpha_r\}_{r=1}^n$ is a series of coefficients satisfying  $\alpha_{r} \leq \frac{18}{\sqrt{\pi}} r^{-3/2}$ and $|g_n(x) - |x|| \leq \frac{6}{n}$.  
\end{lemma}

\begin{lemma}[\cite{RT12}] \label{lem:global_correlation} 
There exists $t \leq k$ such that $E_{i_1,...,i_t \sim W}E_{i,j \sim W}[I(X_i,X_j|X_{i_1}, ..., X_{i_t})] \leq \frac{\ln(q)}{k-1}$ where $q$ is alphabet size.  
\end{lemma}

\begin{proof}
Linearity of expectation we have for any $t \leq k-2$

\[E_{i,i_1,...i_t\sim W} [H(X_i| X_{i_1},..., X_{i_t})] =E_{i,i_1,...i_t\sim W} [H(X_i| X_{i_1},..., X_{i_{t-1}}) - E_{i_1,...,i_{t-1} \sim W}E_{i,i_t \sim W}[I(X_i,X_{i_t}|X_{i_1}, ..., X_{i_{t-1}})] \]

adding equalities from $t=1 $ to $k-2$ we get 

\[E[H(X_i)] - E_{i_1,...,i_{k-2} \sim W}[H(X_i|X_{i_1},...,X_{i_{k-2}})] = \sum_{i\leq t\leq k-1} E_{i,j,X_{i_1},..., X_{i_{t-1}} \sim W}[I(X_i,X_j|X_{i_1}, ..., X_{i_t})]\]

Lemma follows from $H(X) \leq \ln(q)$
\end{proof}

\begin{definition} (Payoff Rescaling) For each game $p_{ij}$ for $(i,j) \in E$, let 
\[U_i \defeq \max_{a_1,a_2,...,a_N \in [k]} \sum_{j \in N(i)} p_{ij}(a_i, a_j)\]

and let 

\[L_i \defeq \min_{a_1, a_2,...,a_N \in [k]} \sum_{j \in N(i)}p_{ij}(a_i, a_j)\]

Then let $f_{ij}(a_i,a_j) = (p_{ij}(a_i,a_j) - \frac{L_i}{|N(i)|}) \frac{1}{U_i - L_i}$ such that $\sum_{j \in N(i)} f_{ij}(a_i,a_j) \in [0,1]$ for all $\vec{a} \in [k]^N$.    
\end{definition}


\begin{fact} Let $x_1,...,x_N$ be i.i.d bernoulli random variables with $\mu = \E[x_i]$.    
Upper chernoff bound, for any $\delta > 0$  

\[\mathbb{P}(\frac{1}{N}\sum_{i=1}^N x_i \geq (1+\delta)\mu) \leq \Big(\frac{e^\delta}{(1+\delta)^{(1+\delta)}}\Big)^{\mu N}\]
Lower chernoff bound.  For $\delta \in [0,1]$ 
\[\mathbb{P}(\frac{1}{N}\sum_{i=1}^N x_i \geq (1-\delta)\mu) \leq \exp\Big(\frac{-\delta^2 \mu N}{2}\Big)\]
\end{fact}

\begin{comment}
\begin{restatable}[Goldbach's conjecture]{lem}{goldbach}
\label{lem:goldbach}
Every even integer greater than 2 can be expressed as the sum of two primes.
\end{restatable}
We recall \pref{lem:goldbach}:
\goldbach*
\end{comment}


