\section{Undecidability of Four Liveness Properties} %Lock-Freedom and Wait-Freedom}
\label{sec:undecidability of lock-freedom and wait-freedom}

In this section we propose our undecidability proof of lock-freedom, wait-freedom, deadlock-freedom and starvation-freedom on TSO.

\forget{
In this section we propose our undecidability proof of lock-freedom,
wait-freedom, deadlock-freedom and starvation-freedom %for
on TSO.
We first introduce the notion of lossy channel machines, the cyclic post
correspondence problem (CPCP) \cite{DBLP:journals/acta/Ruohonen83}, and a known
result of Abdulla \emph{et al.} \cite{DBLP:journals/iandc/AbdullaJ96a} which
reduces checking CPCP into checking if a specific lossy channel machine has
infinite executions of a particular form.
%
Then, we generate a specific library for this lossy channel machine, and further
reduce checking %acceptance on
the existence of such executions of the lossy channel machines into checking
lock-freedom, deadlock-freedom, wait-freedom and starvation-freedom for this
library.
%Then, we generate two specific libraries for this lossy channel machine, and further reduce checking this lossy channel machine into checking lock-freedom and deadlock-freedom for one library, as well as wait-freedom and starvation-freedom for the other library.
}

\subsection{Perfect/Lossy Channel %system
Machines}
\label{subsec:perfect/lossy channel system}

A channel machine
\cite{DBLP:journals/iandc/AbdullaJ96a,DBLP:conf/popl/AtigBBM10} is a finite
control machine equipped with channels of unbounded size. It can perform send
and receive operations on its channels. A lossy channel machine is a channel
machine where arbitrarily many items in its channels may be lost
non-deterministically at any time and without any notification.

Let $\mathcal{CH}$ be the finite set of channel names and $\Sigma_{\mathcal{CH}}$ be a finite alphabet of channel contents.
The content of a channel is a finite sequence over $\Sigma_{\mathcal{CH}}$. A
channel operation is either a send operation $c!a$ sending the value $a$ over
channel $c$, a receive operation $c?a$ receiving $a$ over $c$, or a silent operation $nop$.
We associate with each channel operation a relation over words as follows:
%{\color{orange} GP: I think there is a type problem in the following sentence.}
Given $u \in \Sigma_{\mathcal{CH}}^*$, we have $\llbracket nop \rrbracket(u,u)$, $\llbracket c!a \rrbracket(u,a \cdot u)$ and $\llbracket c?a \rrbracket(u \cdot a,u)$.
%$\llbracket nop \rrbracket(u,u)$, $\llbracket c!a \rrbracket(u,a \cdot u(c))$ and $\llbracket c?a \rrbracket(u(c) \cdot a,u)$.
%\[
%  \llbracket nop \rrbracket(u,u)\qquad
%  \llbracket c!a \rrbracket(u,a \cdot u(c)) \qquad
%  \llbracket c?a \rrbracket(u(c) \cdot a,u)
%\]
\forget{
Let $\textit{Op}(\mathcal{CH})$ be the set of channel operations over $\mathcal{CH}$.
Given $u,u' \in \mathcal{CH} \rightarrow \Sigma_{\mathcal{CH}}^*$ two functions
that record the contents of each channel, we define the channel operation
relation, relating a channel before and after, as follows:\\[-5pt]
\[
  \llbracket nop \rrbracket(u,u)\qquad
  \llbracket c!a \rrbracket(u,u[c: a \cdot u(c)]) \qquad
  \llbracket c?a \rrbracket(u'[c: u'(c) \cdot a],u')
\]
}
% \begin{centerline}
%   \(
%   \begin{array}{ccc}
%     \llbracket nop \rrbracket(u,u') & \mathtt{if} & u=u'\\
%     \llbracket c!a \rrbracket(u,u') & \mathtt{if} & u'= u[c: a \cdot u(c)]\\
%     \llbracket c?a \rrbracket(u,u') & \mathtt{if} & u=u'[c: u'(c) \cdot a]
%   \end{array}
%   \)
% \end{centerline}\\
% we have $\llbracket nop \rrbracket(u,u')$ if $u=u'$, $\llbracket c!a \rrbracket(u,u')$ if $u'= u[c: a \cdot u(c)]$, and $\llbracket c?a \rrbracket(u,u')$ if $u=u'[c: u'(c) \cdot a]$.
A channel operation over a finite channel name set $\mathcal{CH}$ is a
mapping that associates, with each channel of $\mathcal{CH}$, a channel
operation. Let $\textit{Op}(\mathcal{CH})$ be the set of channel operations
over $\mathcal{CH}$. The relation of channel operations is extended to channel
operations over $\mathcal{CH}$ as follows: given a channel operation
$\textit{op}$ over $\mathcal{CH}$ and two functions $u,u' \in \mathcal{CH}
\rightarrow \Sigma_{\mathcal{CH}}^*$, we have $\llbracket \textit{op} \rrbracket(u,u')$, if $\llbracket \textit{op}(c) \rrbracket(u(c),u'(c))$ holds for each $c \in \mathcal{CH}$.

A $\textit{channel machine}$ is formally defined as a tuple $\textit{CM} = (Q,\mathcal{CH},\Sigma_{\mathcal{CH}},\Lambda,\Delta)$, where (1) $Q$ is a finite set of states, (2) $\mathcal{CH}$ is a finite set of channel names, (3) $\Sigma_{\mathcal{CH}}$ is a finite alphabet for channel contents, (4) $\Lambda$ is a finite set of transition labels, and (5) $\Delta \subseteq Q \times (\Lambda\cup\{\epsilon\}) \times \textit{Op}(\mathcal{CH}) \times Q$ is a finite set of transitions.
When $\textit{CM}$ is considered as a perfect channel machine, its semantics is defined as an LTS $(\textit{Conf}, \Lambda,\rightarrow,\textit{initConf})$.
A configuration of $\textit{Conf}$ is a pair $(q,u)$ where $q \in Q$ and $u:\mathcal{CH} \rightarrow \Sigma_{\mathcal{CH}}^*$. $\textit{initConf}$ is the initial configuration and all its channels are empty.
The transition relation $\rightarrow$ is defined as follows: given $q,q' \in Q$ and $u,u' \in \mathcal{CH} \rightarrow \Sigma_{\mathcal{CH}}^*$,
$(q,u) \overset{\alpha}{\longrightarrow} (q',u')$,
if there exists $op$, such that $(q,\alpha,\textit{op},q') \in \Delta$ and $\llbracket \textit{op} \rrbracket (u,u')$.
When $\textit{CM}$ is considered as a lossy channel machine, its semantics is defined as another LTS $(\textit{Conf}, \Lambda,\rightarrow',\textit{initConf})$, with transition relation $\rightarrow'$ defined as follows:
$(q,u) \overset{\alpha}{\longrightarrow}' (q',u')$,
if there exists $v,v' \in \mathcal{CH} \rightarrow \Sigma_{\mathcal{CH}}^*$,
such that (1) for each $c \in \mathcal{CH}$, $v(c)$ is a sub-word of $u(c)$, (2) $(q,v) \overset{\alpha}{\longrightarrow} (q',v')$ and (3) for each $c \in \mathcal{CH}$, $u'(c)$ is a sub-word of $v'(c)$.
Here a sequence $l_1 = a_1 \cdot \ldots \cdot a_u$ is a sub-word of another sequence $l_2=b_1 \cdot \ldots \cdot b_v$, if there exists $i_1 < \ldots < i_u$, such that $a_j=b_{i_j}$ for each $1 \leq j \leq u$.




\subsection{The Lossy Channel Machine for CPCP of Abdulla et al. \cite{DBLP:journals/iandc/AbdullaJ96a}}
\label{subsec:the lossy channel machine of CPCP in Abbdulla}

Given two sequences $l$ and $l'$, let $l =_c l'$ denote that there exists
sequences $l_1$ and $l_2$, such that $l=l_1 \cdot l_2$ and $l'=l_2 \cdot l_1$.
Given two finite sequences $\alpha_1,\ldots,\alpha_m$ and $\beta_1,\ldots,\beta_m$, where each
$\alpha_i$ and $\beta_i$ is a finite sequence over a finite alphabet, a solution of $\alpha_1,\ldots,\alpha_m$ and $\beta_1,\ldots,\beta_m$ is a nonempty sequence of
indices $i_1 \cdot \ldots \cdot i_k$, such that $\alpha_{i_1} \cdot \ldots \cdot \alpha_{i_k}$ $=_c$ $\beta_{i_1} \cdot \ldots \cdot \beta_{i_k}$.
The cyclic post correspondence problem (CPCP)
\cite{DBLP:journals/acta/Ruohonen83}, known to be undecidable, requires to answer given
$\alpha_1,\ldots,\alpha_m$ and $\beta_1,\ldots,\beta_m$, whether there exists one such solution.

Given two finite sequences $A = \alpha_1,\ldots,\alpha_m$ and $B = \beta_1,\ldots,\beta_m$ of finite sequences,
Abdulla \emph{et al.} \cite{DBLP:journals/iandc/AbdullaJ96a} generate the lossy channel machine $\textit{CM}_{(A,B)}$ %that is
shown in \figurename \ref{fig:M_{(A,B)}, original}.
Moreover, they prove that CPCP has a solution for $A$ and $B$, if and only if
$\textit{CM}_{(A,B)}$ has an infinite execution that visits state $s_1$ infinite times.
We point the readers to \cite{DBLP:journals/iandc/AbdullaJ96a} for
  an explanation on how %this channel machine
  $\textit{CM}_{(A,B)}$ solves CPCP.


\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.5\textwidth]{PIC_M-CPCP.pdf}
%\vspace{-5pt}
  \caption{The lossy channel machine $\textit{CM}_{(A,B)}$.}
  \label{fig:M_{(A,B)}, original}
%\vspace{-10pt}
\end{figure}


%\gpnote{Either explain how the machine solves the CPCP or point the reader
%  to~\cite{DBLP:journals/iandc/AbdullaJ96a} for an explanation.}
$\textit{CM}_{(A,B)}$ contains two channels $c_1$ and $c_2$.
We use $c_1 ! \alpha_1$ to represent inserting the contents of $\alpha_1$ into $c_1$ one by one, and use $c_1 ? \beta_1$ to represent receiving the content of $\beta_1$ from $c_1$ one by one.
We use $c_1 ! \alpha_1 \cdot c_1 ? \beta_1$ to represent first do $c_1 ! \alpha_1$ and then do $c_1 ? \beta_1$.
Each execution of $\textit{CM}_{(A,B)}$ can be divided into (at most) two phases.
The first phase, called the guess phase, is a self-loop of state $s_0$, and is used to guess a solution of CPCP.
The second phase, called the check phase, goes from $s_0$ to $s_1$ and then repeatedly ``checks the content of $c_1$ and $c_2$''.
%When an execution comes to the second phase, its channel content is finite.
%{\color {red}Abdulla \emph{et al.} prove that, from each solution of CPCP, we could generate an execution of $M_{(A,B)}$ that visits $s_1$ infinite times.
%They also prove that, if there is an execution that visits $s_1$ infinitely many times, then according to Higman's Theorem \cite{Graham1952Ordering}, one could find a solution of CPCP.}
%To make the construction of the library of next subsection more clear, we assume that there is a special state $s_{\textit{trap}}$ in $\textit{CM}_{(A,B)}$.
%When the transitions of the second phase
%When any receive transition fails to receive the intended content, the execution goes to $s_{\textit{trap}}$ and stops there.
%Therefore, any possible cases has a transition rule.
%For clarity we do not draw $s_{\textit{trap}}$ and such transitions in \figurename \ref{fig:M_{(A,B)}, original}.

Based on $\textit{CM}_{(A,B)}$ we generate the lossy channel machine $\textit{CM}'_{(A,B)}$ which uses only one channel $c$ and works in a similar way.
To simulate one transition of $\textit{CM}_{(A,B)}$, $\textit{CM}'_{(A,B)}$ stores the content of $c_1$ followed by the content of $c_2$ (as well as new delimiter symbols) in its channel. Then it scans each symbol in its channel, modifies it (if necessary) and puts it back into its buffer, until the contents of $c_1$ and $c_2$ have all been dealt with.

\forget{
Based on $\textit{CM}_{(A,B)}$ we generate a lossy channel machine $\textit{CM}'_{(A,B)}$ which uses only one channel $c$ and works in a similar way as $\textit{CM}_{(A,B)}$. We introduce additional symbols $\bot_1$ and $\bot_2$. One transition of $\textit{CM}_{(A,B)}$ from a configuration $(q,u)$ corresponds to a sequence of transitions in $\textit{CM}'_{(A,B)}$ from configuration $(q,u')$, where $u'$ maps $c$ into $\bot_2 \cdot u(c_2) \cdot \bot_2 \cdot \bot_1 \cdot u(c_1) \cdot \bot_1$. In detail:
\begin{itemize}
\item[-] $c_1 ? a$ of $\textit{CM}_{(A,B)}$ corresponds to (1) do $c ? \bot_1 \cdot c ! \bot_1 \cdot c ? a$, then repeatedly receive non-$\bot_1$ data from channel $c$ and put it back into channel $c$, until receiving a $\bot_1$ from channel $c$ and put it back into channel $c$, and (2) do $c ? \bot_2 \cdot c ! \bot_2$, then repeatedly receive a non-$\bot_2$ data from channel $c$ and put it back into channel $c$, until receiving a $\bot_2$ and put it back into channel $c$.

\item[-] $c_1 ! a$ of $\textit{CM}_{(A,B)}$ corresponds to (1) do $c ? \bot_1 \cdot c ! \bot_1$, then repeatedly receive non-$\bot_1$ data from channel $c$ and put it back into channel $c$, until receiving a $\bot_1$ from channel $c$, and then do $c ! a \cdot c ! \bot_1$, the remaining part is the same as step (2) of $c_1 ? a$.

\item[-] The case of $c_2 ? a$ and $c_2 ! a$ is similar and we omit its detailed description here.
\end{itemize}
}

% The channel content of the initial configuration of $\textit{CM}'_{(A,B)}$ is $\bot_2 \cdot \bot_2 \cdot \bot_1 \cdot \bot_1$.
We could depict $\textit{CM}'_{(A,B)}$ similarly to \figurename~\ref{fig:M_{(A,B)},
  original}, and each transition of $\textit{CM}'_{(A,B)}$ is now a ``extended version transition''
as we discussed above. Therefore, there are ``$\textit{CM}'_{(A,B)}$'s versions'' of $s_0$, $s_1$ and
$s_{\textit{trap}}$, and when no confusion is possible we also call them $s_0$, $s_1$ and
$s_{\textit{trap}}$, respectively.
Note that if some new delimiter symbols are lost during transition, then such paths can not complete the simulation of one transition of $\textit{CM}_{(A,B)}$, and thus, do not influence the proof of the following lemma.
\forget{Note that items, including $\bot_1$ and
$\bot_2$, may be lost during transitions of $\textit{CM}'_{(A,B)}$. However,
such paths of $\textit{CM}'_{(A,B)}$ can not reach $s_1$, and thus they do not influence the proof of the following lemma.}
Based on above discussion, we reduce CPCP of $A$ and $B$ into an infinite execution problem of the lossy channel machine $\textit{CM}'_{(A,B)}$, as stated by the following lemma.

\begin{lemma}
  \label{lemma:reducing CPCP into a infinite execution problem of lossy channel machine M'{(A,B)}}
  There is a CPCP solution %between
  for sequences $A$ and $B$ of finite sequences, if and only if there is an infinite execution of $\textit{CM}'_{(A,B)}$ that visits $s_1$ infinitely often.
\end{lemma}

\forget{
We can generate a lossy channel machine $\textit{CM}'_{(A,B)}$ from $\textit{CM}_{(A,B)}$, such that $\textit{CM}'_{(A,B)}$ works in a similar way as $\textit{CM}_{(A,B)}$ and has only one channel $c$. $\textit{CM}'_{(A,B)}$ is generated from $\textit{CM}_{(A,B)}$ as follows: We introduce additional symbols $\bot_1$ and $\bot_2$, such that if the channel content of $c_1$ and $c_2$ is $l_1$ and $l_2$ in $\textit{CM}_{(A,B)}$, respectively, then the channel content of $c$ is $\bot_2 \cdot l_2 \cdot \bot_2 \cdot \bot_1 \cdot l_1 \cdot \bot_1$ in $\textit{CM}'_{(A,B)}$. Each execution of $\textit{CM}_{(A,B)}$ is transformed into an execution of $\textit{CM}'_{(A,B)}$ as follows:
In the beginning, $\textit{CM}'_{(A,B)}$ first puts %$\bot_2 \cdot \bot_2 \cdot \bot_1 \cdot \bot_1$ into $c$.
{\color {red}two $\bot_1$ and then two $\bot_2$ into $c$.}
Then, it translates each transition of $\textit{CM}_{(A,B)}$ as follows:

\begin{itemize}
\item[-] $c_1 ? a$ is translated into (1) do $c ? \bot_1 \cdot c ! \bot_1 \cdot c ? a$, then repeatedly receive non-$\bot_1$ data from channel $c$ and put it back into channel $c$, until receiving a $\bot_1$ from channel $c$ and put it back into channel $c$, and (2) do $c ? \bot_2 \cdot c ! \bot_2$, then repeatedly receive a non-$\bot_2$ data from channel $c$ and put it back into channel $c$, until receiving a $\bot_2$ and put it back into channel $c$.

\item[-] $c_1 ! a$ is translated into (1) do $c ? \bot_1 \cdot c ! \bot_1$, then repeatedly receive non-$\bot_1$ data from channel $c$ and put it back into channel $c$, until receiving a $\bot_1$ from channel $c$, and then do $c ! a \cdot c ! \bot_1$, the remaining part is the same as step (2) of $c_1 ? a$.

\item[-] The case of $c_2 ? a$ and $c_2 ! a$ is similar and we omit its detailed description here.
\end{itemize}


Therefore, we could draw $\textit{CM}'_{(A,B)}$ similarly as \figurename \ref{fig:M_{(A,B)},
  original}, and each transition of $\textit{CM}'_{(A,B)}$ is now a ``extended version transition''
as we discussed above. Therefore, there are ``$\textit{CM}'_{(A,B)}$'s version of $s_0$, $s_1$ and
$s_{\textit{trap}}$'', and when no confusion is possible we also call them $s_0$, $s_1$ and
$s_{\textit{trap}}$, respectively.
%Therefore, we can also assume that there are states $s_0$, $s_1$ and $s_{\textit{trap}}$ in $\textit{CM}'_{(A,B)}$.
The following lemma states that, the CPCP of %sequences
$A$ and $B$ can be reduced into an infinite execution problem of the lossy channel
machine $\textit{CM}'_{(A,B)}$. The proof is direct from
\cite{DBLP:journals/iandc/AbdullaJ96a} jointly with $\textit{CM}'_{(A,B)}$ and therefore
we omit it here.


\begin{lemma}
\label{lemma:reducing CPCP into a infinite execution problem of lossy channel machine M'{(A,B)}}
There is a CPCP solution %between
for sequences $A$ and $B$ of finite sequences, if and only if there is an infinite execution of $\textit{CM}'_{(A,B)}$ that visits $s_1$ infinitely often.
\end{lemma}
}

\forget{
The following lemma states that the CPCP of $A$ and $B$ can be reduced into an infinite execution problem of the lossy channel machine $\textit{CM}_{(A,B)}$. This lemma is directly obtained from %Theorem 3.6 of
\cite{DBLP:journals/iandc/AbdullaJ96a}.


\begin{lemma}
\label{lemma:reducing CPCP into a infinite execution problem of lossy channel machine M'{(A,B)}}
There is a CPCP solution for sequences $A$ and $B$ of finite sequences, if and only if there is an infinite execution of $\textit{CM}_{(A,B)}$ that visits $s_1$ infinitely often.
\end{lemma}
}



\subsection{Libraries for Four Liveness Properties}%Lock-Freedom and Deadlock-Freedom}
\label{subsec:concurrent data structures for lock-freedom and deadlock-freedom}

In this subsection, we propose our library $\mathcal{L}(A,B)$ that is generated from $\textit{CM}'_{(A,B)}$ %$\textit{CM}_{(A,B)}$
and simulates the executions of $\textit{CM}'_{(A,B)}$. %$\textit{CM}_{(A,B)}$.
This library contains two methods $M_1$ and $M_2$. Similarly to \cite{DBLP:conf/popl/AtigBBM10,DBLP:conf/atva/WangLW15}, we use the collaboration of two methods to simulate a lossy channel. %Our concurrent system contains two processes.
% The library executions simulating infinite executions of $\textit{CM}'_{(A,B)}$ require one process only run $M_1$ while another process only run $M_2$. Therefore, our library requires to ensure each method be fixed to some process.
Our library requires that each method be fixed to a single process when simulating infinite execution of $\textit{CM}'_{(A,B)}$.
Methods of our library work differently when simulating lossy channel machine transitions of different phases. %In the next subsection we will show that this reduces CPCP into violation of liveness properties.

Let us now explain in detail the construction of $\mathcal{L}(A,B)$.
$\mathcal{L}(A,B)$ uses %five
the following memory locations: %$x$, $y$,
$x_1$, $y_1$, $x_2$, $y_2$, $\textit{phase}$, $\textit{failSimu}$ and $\textit{firstM1}$.
$\textit{phase}$ stores the phase of $\textit{CM}'_{(A,B)}$, and its initial value is $\textit{guess}$.
$\textit{failSimu}$ is a flag indicating the failure of the simulation of $\textit{CM}'_{(A,B)}$, and its initial value is $\textit{false}$.
$\textit{firstM1}$ is used to indicate the first execution of $M_1$, and its initial value is $\textit{true}$.

The pseudo-code of $M_1$ and $M_2$ are shown in Algorithms \ref{Method1OfLockFreedom} and \ref{Method2OfLockFreedom}, respectively.
$\bot_s$ and $\bot_e$ are two new symbols not contained in $\textit{CM}'_{(A,B)}$.
For brevity, we use the following notations.
We use $\textit{writeOne}(x,a)$ to represent the sequence of commands writing $a$ followed by $\sharp$ into $x$.
We use $\textit{writeSeq}(x,a_1 \cdot \ldots \cdot a_k)$ to represent the sequence of commands writing $a_1 \cdot \sharp \cdot \ldots \cdot a_k \cdot \sharp$ into $x$.
$\sharp$ is a delimiter that ensures one update of a memory location will not be read twice.
\forget{For example, channel content $a \cdot b$ will be transformed into $(x,a) \cdot
(x,\sharp) \cdot (x,b) \cdot (x,\sharp)$ for some memory location $x$ in the store buffer.}
We use $v:=\textit{readOne}(x)$ to represent the sequence of commands reading $e$ followed by $\sharp$ from $x$ for some $e \neq \sharp$ and then assigning $e$ to $v$.
Moreover, if the values read do not correspond with $e$ followed by $\sharp$ we set $\textit{failSimu}$ to $\textit{true}$ and then let the current method return. This will terminate the simulation procedure.
Similarly, $v:=\textit{readRule}(x)$ reads a transition rule followed by $\sharp$ from $x$, and assign the rule to $v$.
We use $\textit{transportData}(z_1,z_2)$ to represent repeatedly using
$v=\textit{readOne}(z_1)$ to read an update of $z_1$ and using $\textit{writeOne}(z_2,v)$ to write it to $z_2$, until reading $\bot_e$ from $z_1$ and writing $\bot_e$ to $z_2$.
Given a transition rule $r$, let $valueRead(r)$ and $valueWritten(r)$ be the value received and sent by $r$, respectively. The symbols $s_0$ and $s_1$ in the pseudo-code of $M_1$ represent the corresponding state of $\textit{CM}_{(A,B)}$.

\redt{
\begin{algorithm}[t]
\KwIn {an arbitrary argument}
\While {$\textit{true}$} {
If $\textit{failSimu}$, then \KwRet; \\
\If {$\textit{firstM1}$}{
%if $\textit{cas}(\textit{exlM1},0,1)$ fails, then set $\textit{failSimu}$ to $\textit{true}$ and \KwRet; \\
guess a transition rule $r_1$ that starts from $s_0$; \\
$\textit{writeSeq}(x_1,r_1 \cdot \bot_s \cdot \bot_e)$; \\
$\textit{firstM1}=\textit{false}$; \\
}
\Else {
$r_1:=\textit{readRule}(y_2)$; \\
let $z_1:=valueRead(r_1)$ and $z_2:=valueWritten(r_1)$; \\
$\textit{readOne}(y_2,\bot_s)$; \\
if $z_1 \neq \epsilon$, then $\textit{readOne}(y_2,z_1)$; \\
guess a transition rule $r_2$ starts from the destination state of $r_1$; \\
$\textit{writeSeq}(x_1,r_2 \cdot \bot_s)$; \\
\While{$\textit{true}$}{
$tmp:=\textit{readOne}(y_2)$; \\
if $tmp=\bot_e$, then break; \\
$\textit{writeOne}(x_1,tmp)$; \\
}
$\textit{writeSeq}(x_1,z_2 \cdot \bot_e)$; \\
}
if $\textit{phase}=\textit{guess}$ and the destination state of $r_2$ is $s_1$, then set $\textit{phase}$ to $\textit{check}$; \\
$\textit{transportData}(y_1,x_2)$; \\
if $\textit{phase}=\textit{guess}$, then \KwRet; \\
}
\caption{$M_1$}
\label{Method1OfLockFreedom}
\end{algorithm}
}

\noindent\begin{algorithm}[!h]
\KwIn {an arbitrary argument}
%\If {$\textit{firstM2}$}{
%if $\textit{cas}(\textit{exlM2},0,1)$ fails, then set $\textit{failSimu}$ to $\textit{true}$ and \KwRet; %\\
%$\textit{firstM2}=\textit{false}$; \\
%}
\While {$\textit{true}$} {
if $\textit{failSimu}$, then \KwRet; \\
$\textit{transportData}(x_1,y_1)$; \\
$\textit{transportData}(x_2,y_2)$; \\
if $\textit{phase}=\textit{guess}$, then \KwRet; \\
}
\caption{$M_2$}
\label{Method2OfLockFreedom}
\end{algorithm}


\begin{figure}[tbp]
  \centering
  \includegraphics[width=1.0\textwidth]{PIC_SimluateFiniteTimesForLockFree.pdf}
%\vspace{-1pt}
  \caption{One execution of $\mathcal{L}(A,B)$.
  %Execution of $\mathcal{L}(A,B)$ for an execution of $M'_{(A,B)}$ that has one round of check successes and one round of check fails. Here each transition rule $r_i$ is of the form
  %$(q_{\textit{i-1}},\_,\_,q_i,\_)$.
  }
  \label{fig:execution for lock-freedom with one check success and one check fail}
%\vspace{-10pt}
\end{figure}

\figurename~\ref{fig:execution for lock-freedom with one check success and one
  check fail} illustrates a possible execution of $\mathcal{L}(A,B)$.
$M_1$ and $M_2$ work differently in the different phases. In the guess phase, $M_1$ and $M_2$ return after simulating one lossy channel machine transition, while in the check phase, $M_1$ and $M_2$ keep working until the simulation procedure fails.
% In this case the guess phase expands over several returns of $M_1$ and $M_2$, and then the check phase successfully simulates several lossy channel machine transition. This is the long spanning call to $M_1$ and $M_2$.

%\redt{Both $M_1$ and $M_2$ work differently in different phases. In the guess phase, $M_1$ (resp., $M_2$) returns after simulating one lossy channel machine transition, while in the check phase, $M_1$ (resp., $M_2$) keeps working until the simulation procedure fails.
%}





Assume that $M_1$ (resp., $M_2$) runs on process $P_1$ (resp., process $P_2$). %and $M_2$ runs on process $P_2$.
%Assume that the current channel content of $\textit{CM}'_{(A,B)}$ is $l$. Before simulating one lossy channel machine transition, $r_1 \cdot \bot_s \cdot l \cdot \bot_e$ are stored in $P_2$'s store buffer as buffered items of memory location $y_2$, where $r_1$ is a transition rule of $\textit{CM}'_{(A,B)}$, and $\bot_s$ and $\bot_e$ are additional symbols indicating the start and end of channel content of $\textit{CM}'_{(A,B)}$, respectively.
To simulate one lossy channel machine transition with channel content $l$, we first store $r_1 \cdot \bot_s \cdot l \cdot \bot_e$ in process $P_2$'s store buffer as buffered items of %memory location
$y_2$, where $r_1$ is the transition rule of this transition, and $\bot_s$ and $\bot_e$ are additional symbols indicating the start and end of channel content of $\textit{CM}'_{(A,B)}$, respectively. Then, the procedure for simulating one lossy channel machine transition is as follows:

%the channel content of $\textit{CM}'_{(A,B)}$ and a transition rule of the next transition are stored in $P_2$'s store buffer as buffered items of memory location $y_2$.
%The procedure for simulating one lossy channel machine transition is as follows:
\begin{itemize}
\item $M_1$ reads the transition rule $r_1$ and channel content of $\textit{CM}'_{(A,B)}$ by reading all the updates of $y_2$. After reading $r_1$, $M_2$ non-deterministically chooses a transition rule $r_2$ of the lossy channel machine. Such rule should begin from the destination state of $r_1$.

\item There are four points for information update and transfer
between $M_1$ and $M_2$: (1) According to transition rule $r_1$, $M_1$ modifies and writes all the updates of $y_2$ into %a memory location
    $x_1$, (2) then $M_2$ reads all the updates of $x_1$ and writes all the updates into %a memory location
    $y_1$, (3) then $M_1$ reads all the updates of $y_1$ and writes all the updates into %a memory location
    $x_2$, and (4) finally, $M_2$ reads all the updates of $x_2$ and writes all the updates into $y_2$. To read all the updates of a memory location, we need to repeatedly read until read $\bot_e$, which indicates the end of channel content.

    Since there is no item in the buffer at the beginning of the simulation procedure, to simulate the first lossy channel machine transition $M_1$ directly writes $r \cdot \sharp \cdot \bot_s \cdot \sharp \cdot \bot_e \cdot \sharp$ to $x_1$ and does not need to read updates from $y_2$, where $r$ is a transition rule from $s_0$. This is the reason for using $\textit{firstM1}$.

\item $M_1$ is also responsible for modifying the phase (stored in the memory location $\textit{phase}$). If the last lossy channel machine transition simulated belongs to the guess phase and the destination state of $r_1$ is $s_1$, $M_1$ changes the memory location $\textit{phase}$ to check.
\end{itemize}
%This completes the simulation procedure of one lossy channel machine transition.
%To ensure that one update of a memory location $x$ will not be read twice, we use a symbol $\sharp$ as delimiter. For example, channel content $a \cdot b$ will be transformed into $(x,a) \cdot (x,\sharp) \cdot (x,b) \cdot (x,\sharp)$ in buffer.
%Any failure of simulation procedure will set a memory location $\textit{failSimu}$ to $\textit{true}$ and finishes simulation. %At the beginning of our simulation procedure, there is no items in process $P_2$'s buffer. Therefore, to simulate the first lossy channel machine transition $M_1$ directly writes $r \cdot \sharp \cdot \bot_s \cdot \sharp \cdot \bot_e \cdot \sharp$ to $x_1$ and do not need to read updates from $y_2$, where $r$ is a transition rule from $s_0$.

%Let us explain how our library simulate one transition of lossy channel machine $\textit{CM}'_{(A,B)}$.
The reason why we need to update and transfer information between $M_1$ and $M_2$
is to deal with the case when an update of $\bot_e$ %for $y_2$
is not captured. %by $M_1$.
Let us first consider a simple but infeasible solution: $M_1$ reads updates from
$y_2$ (until reading $\bot_e$), and modifies and writes all the updates into
$x_1$; while $M_2$ repeatedly reads an update of $x_1$ and writes it into $y_2$,
until reading $\bot_e$. This solution can not deal with the case when updates of
$\bot_e$ for $y_2$ is not seen by $M_1$, and will make $M_1$ and $M_2$ fall into
infinite loop that violates liveness.
This may happen
in simulating each lossy channel machine transition, and thus, introduces ``false negatives'' to four liveness properties.
%This may happen in simulating each lossy channel machine transition. %, no matter whether there is a solution for CPCP or not.
To deal with this case, we need to break the infinite loop and avoid directly
writing the updates of $y_2$ back into $x_1$. Instead, in our update and
transfer points, we exhaust the updates of $y_2$, which are written to $x_1$,
and later written to $y_1$ instead of $y_2$. Therefore, there is no infinite
loop even if updates of $\bot_e$ for $y_2$ are lost.

%\redt{To fix each method to some process, we need to ensure that the executions of the following two cases can not be used to simulate infinite executions of the lossy channel machine $\textit{CM}'_{(A,B)}$. In the first case, the first method of the two processes are both $M_1$, or are both $M_2$. In the second case, some process previously runs $M_1$ (resp., $M_2$) and then runs $M_2$ (resp., $M_1$). There is a memory location $\textit{failSimu}$ as flag for failed simulation, and executions of both cases will set this flag to $\textit{true}$.
%}

%\redt{Let us explain how to deal with the first case. A memory location $\textit{firstM1}$ is used to indicate the first invocation of $M_1$. If $\textit{firstM1}$ is $\textit{true}$, then $M_1$ attempts to use $\textit{cas}$ commands to set a memory location $\textit{exlM1}$ from $0$ to $1$. If this attempt successes, then $\textit{firstM1}$ is set to $\textit{false}$; otherwise, $\textit{failSimu}$ is set to $\textit{false}$ and the simulation ends, since the fail of this attempt results from a concurrent $M_1$. $\textit{exlM1}$ is used to ensure the exclusive of the first $M_1$. The case for $M_2$ is similar.
%}

Assume that we can successfully simulate one transition of
$\textit{CM}'_{(A,B)}$ with one $M_1$ running on process $P_1$ and one $M_2$
running on process $P_2$. Then the most general client on process $P_1$
(resp., on process $P_2$) can call $M_1$ and $M_2$. Perhaps surprisingly, the
only possible way to simulate the second transition of $\textit{CM}'_{(A,B)}$
is to let $M_1$ and $M_2$ to continue to run on processes $P_1$ and $P_2$, respectively. Let us explain why other choices fail to simulate the second transition: (1) If both processes run method $M_1$, then they both %write $x_1$ and
require reading updates of $y_1$. Since there is no buffered item for $y_1$, and none of them write to $y_1$, both $M_1$ fail the simulation.
% the only possible buffered $y_2$ items are in process $P_2$'s buffer, according to the TSO memory model, $M_1$ on process $P_2$ always read a same value of $y_2$ and thus fails to do $\textit{readOne}(y_2,\_)$. Thus, this $M_1$ fails the simulation.
% , but neither of them writes to $y_2$. Thus, both $M_1$ fails simulation.
(2) If both processes run $M_2$, we arrive at a similar situation. (3) If $M_1$
and $M_2$ run on processes $P_2$ and $P_1$, respectively. $M_1$ requires reading
the updates on $y_2$, and the only possible buffered $y_2$ items are in process $P_2$'s buffer. According to the TSO memory model, $M_1$ always reads the same value for $y_2$ and thus fails to do $\textit{readOne}(y_2,\_)$. Thus, $M_1$ fails the simulation.
% Similarly, $M_1$ fails the simulation.
Therefore, we essentially ``fix methods to processes'' without adding specific commands for checking process id.

%Let us now explain how to associate each method to a single process. Two concurrent $M_1$s can not simulate lossy channel machine transitions, since they both write $x_1$ and require reading updates from $y_2$, but neither of them writes to $y_2$. Similarly, two concurrent $M_2$s can not simulate lossy channel machine transitions. Moreover, suppose that $M_1$ and $M_2$ run on processes $P_1$ and $P_2$, respectively; and later $M_1$ and $M_2$ run on processes $P_2$ and $P_1$, respectively; in this case they can not simulate lossy channel machine transitions.
%
%The reason is as follows: after the execution of the first $M_1$ and $M_2$, the buffered items of $y_2$ are in process $P_2$'s buffer. According to the TSO memory model, the second $M_1$ can only read the newest value of $y_2$ and can not read other items of $y_2$ in its buffer, and thus, fails to read the updates from $y_2$.










\forget{
\noindent\begin{algorithm}[!h]
\KwIn {two memory location $z_1$ and $z_2$}
$tmp:=\textit{readOne}(z_1)$; \\
\While{$tmp \neq \bot_e$}{
$\textit{writeOne}(z_2,tmp)$; \\
$tmp:=\textit{readOne}(z_1)$; \\
}
$\textit{writeOne}(z_2,tmp)$; \\
\caption{$\textit{TransportData}(y,u)$}
\label{Method5OfLockFreedom}
\end{algorithm}
}

\forget{
This library contains five methods $M_1$, $M_2$, $M_3$, $M_4$ and $M_5$.
Similarly to \cite{DBLP:conf/popl/AtigBBM10}, we use the collaboration of two
methods to simulate each lossy channel. Since $\mathcal{L}(A,B)$ has two
channels $c_1$ and $c_2$, we use the collaboration of $M_1$ and $M_2$ to
simulate $c_1$, and use the collaboration of $M_3$ and $M_4$ to simulate $c_2$.
$M_5$ is used to determine the following lossy channel machine transition rule
to be simulated, communicates to $M_1$, $M_2$, $M_3$ and $M_4$ the current phase of
$\textit{CM}_{(A,B)}$, and synchronizes with $M_1$ and $M_3$. We assume that
each $M_i$ ($1 \leq i \leq 5$) runs on its own process $i$, and use the
$\textit{checkPID}$ command to ensure that a $M_i$ runs on process $i$,
otherwise returning immediately. Hence, in the case there $\textit{checkPID}$
fails, on $M_i$, this method call does not influence the simulation of the lossy
channel machine.


The procedure for simulating one lossy channel machine transition is as
follows:
\begin{itemize}
\item First, $M_5$ non-deterministically chooses a transition rule of the
  lossy channel machine and stores it in the memory location $\textit{rule}$. Such
  rule should begin from the destination state of the rule of the last lossy
  channel machine transition simulated by our library. $M_5$ is also
  responsible for modifying the phase (stored in the memory location
  $\textit{phase}$). If the last lossy channel machine transition simulated
  belongs to the guess phase and the destination state of $\textit{rule}$ is
  $s_1$, $M_5$ changes the memory location $\textit{phase}$ to check.
\item Then, $M_1$ (resp., $M_3$) reads the updated transition rule. $M_1$ and
  $M_2$ (resp., $M_3$ and $M_4$) collaborate to simulate the channel operation
  of $c_1$ (resp., of $c_2$) of this transition rule (if any).
\item After that, $M_1$ (resp., $M_3$) writes $1$ into a memory location
  $\textit{ack1}$ (resp., $\textit{ack2}$), which notifies to $M_5$ the end of
  one channel operation. $M_5$ waits until receiving the updated
  $\textit{ack1}$ and $\textit{ack2}$.
\end{itemize}
This completes the simulation procedure of one lossy channel machine
transition.




\begin{algorithm}[t]
\KwIn {an arbitrary argument}
%If $\textit{getPID}() \neq 1$ then \KwRet; \\
If not $\textit{checkPID}(1)$ then \KwRet; \\
%If $tflag = true$ then \KwRet; \\
%guess a transition rule $(s_0,\_,\_,\_,\_) \in \Delta$ and assign it to $rule$; \\
%{\color {red}non-deterministically choose a transition rule $(s_0,\_,\_,\_,\_) \in \Delta$ and assign it to $rule$; \\}
\While {$\textit{tflag}=false$} {%true} {
%If $tflag = true$ then \KwRet; \\
do $r:=\textit{readOne}(\textit{rule})$ and reads a rule in $\Delta$ (Otherwise, \KwRet); \\
let $u_1:=valueRead(r,c_1)$ and $v_1:=valueWritten(r,c_1)$; \\
If $u_1 \neq \epsilon$ then $\textit{readOne}(y_1,u_1)$; \\
If $v_1 \neq \epsilon$ then $\textit{WriteOne}(x_1,v_1)$; \\
$\textit{writeOne}(\textit{ack1},1)$; \\
%If $dst(rule) = s_1$ and $phase=guess$ then set $phase$ to $check$; \\
%If $dst(rule) = s_{\textit{trap}}$ then set $tflag$ to $true$ and \KwRet; \\
%let $rule$ be a random transition rule $(dst(rule),\_,\_,\_,\_) \in \Delta$;\\
%{\color {red}non-deterministically let $rule$ be a transition rule $(dst(rule),\_,\_,\_,\_) \in \Delta$;\\}
If $\textit{phase}$ = guess then \KwRet; \\
}
\KwRet; \\
\caption{$M_1$}
\label{Method1OfLockFreedom}
\end{algorithm}


\noindent\begin{algorithm}[!h]
\KwIn {an arbitrary argument}
%If $\textit{getPID}() \neq 2$ then \KwRet; \\
If not $\textit{checkPID}(2)$ then \KwRet; \\
%If $tflag=true$ then \KwRet; \\
\While {$\textit{tflag}=false$} {%true} {
$tmp:=\textit{readOne}(x_1)$; \\
$\textit{writeOne}(y_1,tmp)$; \\
If $\textit{phase}$=guess then \KwRet; \\
}
%\If {$phase=guess$} {
%$tmp:=\textit{readOne}(x)$; \\
%$\textit{writeOne}(y,tmp)$; \\
%\KwRet;
%}
%\Else {
%\While {$tflag=false$} {
%$tmp:=\textit{readOne}(x)$; \\
%$\textit{writeOne}(y,tmp)$; \\
%}
%}
\KwRet;
\caption{$M_2$}
\label{Method2OfLockFreedom}
\end{algorithm}



\noindent\begin{algorithm}[!h]
\KwIn {an arbitrary argument}
If not $\textit{checkPID}(5)$ then \KwRet; \\
If $tflag=true$ then \KwRet; \\
\If {$\textit{rule}=\bot$} {
non-deterministically choose a transition rule $r=(s_0,\_,\_,\_,s',\_,\_) \in \Delta$ for some lossy channel machine state $s'$, and do $\textit{writeOne}(\textit{rule},r)$; \\
If $\textit{phase}=guess \wedge s'=s_1$ then set $phase$ to $check$; \\
%If $s'=s_{\textit{trap}}$ then set $tflag$ to $true$ and \KwRet; \\
$\textit{readOne}(\textit{ack1},1)$; \\
$\textit{readOne}(\textit{ack2},1)$; \\
\KwRet; \\
}
\Else {
\While{$\textit{tfalg}=false$}{
assume that $\textit{rule}=(s,u_1,u_2,\alpha,s',v_1,v_2)$ for some lossy channel machine states $s$ and $s'$; \\
non-deterministically choose a transition rule $r=(s',\_,\_,\_,s'',\_,\_) \in \Delta$ for some lossy channel machine state $s''$ and do $\textit{writeOne}(\textit{rule},r)$;\\
If $\textit{phase}=guess \wedge s'=s_1$ then set $phase$ to $check$; \\
%If $s'=s_{\textit{trap}}$ then set $tflag$ to $true$ and \KwRet; \\
$\textit{readOne}(\textit{ack1},1)$; \\
$\textit{readOne}(\textit{ack2},1)$; \\
If $\textit{phase}$=guess then  \KwRet; \\
}
\KwRet; \\
}
\caption{$M_5$}
\label{Method5OfLockFreedom}
\end{algorithm}




Throughout, $M_1$ reads values from memory location $y_1$, and
writes values into memory location $x_1$; while $M_2$ reads values from $x_1$
and writes values into $y_1$. We take the channel contents of $c_1$ to be the
buffered values of $x_1$ in the store buffer of process $1$ concatenated with the
buffered values of $y_1$ in the store buffer of process $2$.
% $M_1$ contains a loop, and it simulates one channel operation (and synchronizes with $M_5$) in each round of the loop.
To simulate a send operation $c_1!a$, $M_1$ writes $a$ to $x_1$, while to
simulate a receive operation $c_1?a$, $M_1$ reads $a$ from $y_1$. Both $M_1$
and $M_2$ work differently depending on the phase. In the guess phase, $M_1$
returns after it reads an updated rule, simulates one channel operation and
synchronizes with $M_5$; and $M_2$ returns after it reads a value from $x_1$
and writes it back into $y_1$. In the check phase, $M_1$ repeatedly reads an
updated rule, simulates one channel operation and synchronizes with $M_5$,
until the simulation of $\textit{CM}_{(A,B)}$ %finishes or
fails; while $M_2$ repeatedly reads values from $x_1$ and writes them into
$y_1$, until the simulation of $\textit{CM}_{(A,B)}$ %finishes or
fails.
%$M_2$ works in two phases as follows: In a first guess phase, $M_2$ returns after it reads a value from $x_1$ and writes it back into $y_1$. In a second check phase, $M_2$ repeatedly reads values from $x_1$ and writes them into $y_1$, until the simulation of $\textit{CM}_{(A,B)}$ finishes or fails.
%Thus, the buffered values of $y$ in the store buffer of process $2$ are the ``oldest contents of channel $c$''.
In this way, we simulate the non-lossy part of channel operations of $c_1$. The
case when $M_1$ (resp., $M_2$) misses one or more updates of $y_1$ (resp. of
$x_1$) simulates the loss of information in channel. In this way, we simulate
the channel operations on lossy channel $c_1$.
%
Similarly, $M_3$ and $M_4$ use memory locations $x_2$ and $y_2$ to simulate
channel operations on the lossy channel $c_2$, and work differently depending on the phase.
%
$M_5$ also operates differently depending on the phases. In the guess phase,
each call to $M_5$ returns after it updates $\textit{rule}$ (and possibly
$\textit{phase}$), and reads the updates from $\textit{ack1}$ and
$\textit{ack2}$. In the check phase, $M_5$ repeatedly updates $\textit{rule}$ %(and possibly $\textit{phase}$)
and reads the updates from $\textit{ack1}$ and
$\textit{ack2}$, until the simulation of $\textit{CM}_{(A,B)}$ %finishes or
fails.







\forget{
$M_1$ reads values from {\color {red}a memory location} %a global variable
$y$, and writes values into {\color {red}a memory location} %a global
%variable %to
$x$; while $M_2$ reads from $x$ and writes into $y$.
{\color {red}We take the channel contents of $\textit{CM}'_{(A,B)}$ to be %the values buffered in the store buffer of $x$ for process 1.
the buffered values of $x$ in the store buffer of process $1$.
}
%$M_1$ works as follows:
$M_1$ contains a loop, and {\color {red}it simulates one lossy channel machine transition %channel operation
in each
round of the loop.} {\color {red}The procedure of simulating one lossy channel machine transition is as follows: First, $M_1$ non-deterministically chooses a transition rule of the lossy channel machine and stores it in memory location $\textit{rule}$. Such rule should begin from the destination state of the rule of the last lossy channel machine transition simulated by $M_1$. $M_1$ then simulate channel operation of this transition rule.} %A silent operation is simulated by $M_1$ changing the control state of $\textit{CM}'_{(A,B)}$.
{\color {red}A send operation $c!a$ (resp., receive operation $c?a$) %additionally
require $M_1$ writing $a$ to $x$ (resp., reading $a$ from $y$).}
\forget{
$M_1$ stores {\color {red}the current transition rule that is being simulated in a variable $\textit{rule}$.} %the control state
{\color{orange} GP: what does this
  mean?} %{\color{red} (in the form of storing transition rule)} of $\textit{CM}'_{(A,B)}$.
  We take the channel contents of $\textit{CM}'_{(A,B)}$
to be {\color {red}the values buffered in the store buffer of $x$ for process
1}.\footnote{Notice that %since $M_1$ only writes to $x$, all buffer contents are to $x$.
{\color{red}$M_1$ writes to $x$, $\textit{rule}$, and possibly $\textit{tflag}$.}} $M_1$ contains a loop, and it simulates one channel operation in each
round of the loop. A silent operation is simulated by $M_1$ changing the control
state of $\textit{CM}'_{(A,B)}$. A send operation $c!a$ (resp., receive
operation $c?a$) additionally require $M_1$ writing $a$ to $x$ (resp., reading
$a$ from $y$).
}
%
$M_2$ works in two phases as follows:
In a first  guess phase, $M_2$ returns after it reads a value
from $x$ and writes it back into $y$.
In a second check phase, $M_2$ repeatedly reads values from $x$ and writes them into
$y$, until the simulation of $\textit{CM}'_{(A,B)}$ finishes or fails.
Thus, the buffered values of $y$ in the store buffer of process $2$ are the ``oldest
contents of channel $c$''.
%
In this way, we simulate the non-lossy part of $\textit{CM}'_{(A,B)}$. The
case when $M_1$ (resp., $M_2$) misses one or more updates of $y$ (resp. of $x$)
simulates the loss of information in channel.
%
In this way, we simulate the lossy channel machine $\textit{CM}'_{(A,B)}$.
}

\forget{
  $M_1$ works as follows: initially $M_1$ writes $s_0 \cdot \bot_s \cdot \bot_e$
  into the buffer and randomly guesses a transition to simulate %the first step of the lossy channel machine transition.
  the first lossy channel machine transition.
  This represents that the initial ``channel content'' of $\textit{CM}'_{(A,B)}$ is empty.
  Here $\bot_s$ and $\bot_e$ are additional symbols, where $\bot_s$ indicates the
  start of channel content of $\textit{CM}'_{(A,B)}$, and $\bot_e$ indicates the end of the
  channel content of the $\textit{CM}'_{(A,B)}$.
  The guessed transition rule is stored in a predefined memory location $rule$.
  Then, $M_1$ begins a loop, where each round of the loop simulates %one step of the lossy channel machine.
  one transition of the lossy channel machine.
  In each round, $M_1$ ``modifies the channel content'' according to the
  transition rule stored in $rule$. At the end of each round, $M_1$ finishes %simulating this step of the lossy channel machine
  simulating this lossy channel machine transition and randomly guesses a transition rule for the next %step of the lossy channel machine.
  transition of the lossy channel machine. $M_2$ works as follows: in the guess phase, $M_2$ returns after it reads an element from $x$ and writes
  it into $y$. In the check phase, $M_2$ repeatedly reads elements from $x$ and writes them into $y$.
}


\forget{
At the beginning of execution, $M_1$ writes $s_0 \cdot \bot_s \cdot \bot_e$ into
$x$. This represents that the initial ``channel content'' of $M'_{(A,B)}$ is empty.
Here $\bot_s$ and $\bot_e$ are additional symbols, where $\bot_s$ indicates the
start of channel content of $M'_{(A,B)}$, and $\bot_e$ indicates the end of
channel content of $M'_{(A,B)}$. Additionally, $M_1$ is responsible for choosing
a transition rule (stored in a predefined memory location $rule$) and changing
the channel content according to the transition rule stored therein.
}

\forget{
If the lossy channel machine execution that is simulated is of infinite length,
in a %successfully simulated library execution on this lossy channel machine,
library execution that successfully simulates this lossy channel machine execution,
$M_1$ does not return, while $M_2$ returns finitely many times.
This makes the library execution still lock-free if the lossy channel machine
execution infinitely loops in the guess phase, and makes the library execution
not lock-free %(that is locked)
if the lossy channel machine infinitely loops in
the check phase.
}

Distinguishing the different phases of $M_1$, $M_2$, $M_3$, $M_4$
and $M_5$ enables us to encode lock-freedom. Given
a library execution $t_1$ which successfully simulates an infinite lossy
channel machine execution $t_2$ that infinitely loops in the guess phase,
according to the behavior of $M_1$, $M_2$, $M_3$, $M_4$ and $M_5$ in the guess
phase, we can see that $t_1$ is lock-free and contains infinite number of
return actions. %of $M_1$, $M_2$, $M_3$, $M_4$ and $M_5$.
If such $t_2$ infinitely loops in the check phase, since $M_1$, $M_2$, $M_3$,
$M_4$ and $M_5$ never return in the check phase, if the simulation continues
indefinitely, we can see that $t_1$ violates lock-freedom since none of $M_1$,
$M_2$, $M_3$, $M_4$ or $M_5$ returns.
%{\color {orange} GP: the rest of this paragraph is not very clear to me.}
Thus, we relate the existence of solutions of CPCP with existence of lock-freedom violations. %Since $t_2$ visits $s_1$ infinitely many times if and only if $t_2$ loops infinitely in the check phase, we can relate the CPCP problem with lock-freedom when we successfully simulate infinite executions of the lossy channel machine.
%
On the other hand, we have to make every execution that fails to simulate the lossy
channel machine satisfy lock-freedom.
%
To that end, we set a flag $tflag$ (for terminate), upon which $M_1$, $M_2$, $M_3$, $M_4$ and
$M_5$ return trivially.

\forget{
Distinguishing the different phases of $M_2$ enables us to {\color {red}encode lock-freedom.}
{\color {red}Given a library execution $t_1$ which successfully simulates an infinite lossy
channel machine execution $t_2$, since (1) each transition of $\textit{CM}_{(A,B)}$ is simulated by $\textit{CM}'_{(A,B)}$ with several send and receive operations, and (2) in guess phase, each receive operation requires at least one $M_2$, we can see that $t_1$ is lock-free if $t_2$ infinitely loops in
the guess phase.} {\color {red}However}, $t_1$ is not lock-free if $t_2$ infinitely loops in the check phase {\color {red}and visits $s_1$ infinitely many times.}
%{\color {red}Since each transition of $\textit{CM}_{(A,B)}$ is simulated by $\textit{CM}'_{(A,B)}$ with several send and receive operations, and in check phase each receive operation requires a $M_2$},
%Given a library execution $t_1$ which successfully simulates an infinite lossy channel machine execution $t_2$, $t_1$ is lock-free if $t_2$ infinitely loops in the guess phase, and it is not lock-free if $t_2$ infinitely loops in the check phase {\color {red}and visits $s_1$ infinitely many times}.
%
%{\color {orange} GP: the rest of this paragraph is not very clear to me.}
{\color {red}Thus, we relate the existence of solutions of CPCP with existence of lock-freedom violations.}%Since $t_2$ visits $s_1$ infinitely many times if and only if $t_2$ loops infinitely in the check phase, we can relate the CPCP problem with lock-freedom when we successfully simulate infinite executions of the lossy channel machine.
%
On the other hand, we should make every execution that either %(1)
simulates a finite lossy channel machine execution {\color {red}end in $s_{\textit{trap}}$},
or %(2)
fails to simulate the lossy channel machine transitions to satisfy
lock-freedom. In any of these two cases we set a flag $tflag$ and subsequently
$M_1$ and $M_2$ return trivially.}

\figurename \ref{fig:execution for lock-freedom with one check success and one
  check fail} illustrates a possible execution of $\mathcal{L}(A,B)$. In this
case the guess phase expands over several returns of $M_1$, $M_2$, $M_3$, $M_4$
and $M_5$. We specifically draw how $M_1$, $M_3$ and $M_5$ communicate to
simulate the first lossy channel machine transition. Here we assume that the
first lossy channel machine transition does a send operation on channel $c_2$
and thus, $M_3$ writes $x_2$. Then, $M_4$ reads from $x_2$ and writes to $y_2$.
Note that since there is no write to $x_1$, $M_2$ does no work when simulating
the first lossy channel machine transition. The check phase is the long spanning
set of calls to $M_1$, $M_2$, $M_3$, $M_4$ and $M_5$. The check phase
successfully simulates one lossy channel machine transition, and fails when
simulating the second lossy channel machine transition. After this point $M_1$,
$M_2$, $M_3$, $M_4$ and $M_5$ return trivially, represented with the short
method calls at the end.

\forget{
\figurename \ref{fig:execution for lock-freedom with one check success and one
  check fail} illustrates a possible execution of $\mathcal{L}(A,B)$. In this
case the guess phase expands over several returns of $M_2$, and then the check
phase successfully simulates one lossy channel machine transition. This is the
long spanning call to $M_2$. The execution illustrates a later fail when
simulating the second lossy channel machine transition, and after this point both
$M_1$ and $M_2$ return trivially, represented with the short method calls at the
end.}

\forget{
If a library execution successfully simulates an infinite lossy channel machine execution, then the library execution is lock-free if the lossy channel machine execution infinitely loops in the guess phase, and is not lock-free if the lossy channel machine execution infinitely loops in the check phase.
Note that the set of infinite executions of $\textit{CM}'_{(A,B)}$
that visits $s_1$ infinitely many times is just the set of infinite executions
of $\textit{CM}'_{(A,B)}$ that loops infinitely in the check phase.
If the lossy channel machine execution that is simulated is an finite execution,
it will end in state $s_{\textit{trap}}$.
Any library execution which reaches $s_{\textit{trap}}$ should finish the
simulation.
To ensure such %library
execution is still lock-free, we make $M_1$ and $M_2$ only able to
trivially return after we reach $s_{\textit{trap}}$.

On the other hand, we should make every execution that fails to simulate the lossy
channel machine transitions satisfies lock-freedom.
When we find that the simulation procedure fails, we %force $M_1$ and $M_2$ to trivially return.
force $M_1$ and $M_2$ to trivially return after that point.
\figurename \ref{fig:execution for lock-freedom with one check success and one
  check fail} shows an execution of $\mathcal{L}(A,B)$. It finishes its guess
phase with several returns of $M_2$, and in the check phase it successfully
simulates one lossy channel machine transition, and it fails when simulating the
second lossy channel machine transition.
%it pass one round, and fails in the second round.
After this point, both $M_1$ and $M_2$ result in a method that trivially return.
}


%Similar to \cite{DBLP:journals/iandc/AbdullaJ96a} and our previous work \cite{DBLP:conf/atva/WangLW15}, we use the collaboration of two processes to simulate one lossy channel. Process $P_1$ repeatedly reads a value from $y$ and then writes it into $x$, while process $P_2$ repeatedly reads a value from $x$ and then writes it into $y$. The possible case when process $P_1$ (resp., process $P_2$) misses several updates of $y$ (resp., several updates of $x$) simulates the lose of information in channel.

%When the channel content of $M'_{(A,B)}$ is $l$, the ``channel content of $\mathcal{L}(A,B)$'' is $q \cdot \bot_s \cdot l \cdot \bot_e$, where $q$ is a state of $M'_{(A,B)}$ and represents the ``current state'' of $M'_{(A,B)}$, $\bot_s$ indicates the start of channel content of $M'_{(A,B)}$, and $\bot_e$ indicates the end of channel content of $M'_{(A,B)}$.

\forget{
We require $\mathcal{L}(A,B)$ to contain two methods, $M_1$ and $M_2$. We also require $M_1$ and $M_2$ to be fixed to process $P_1$ and $P_2$, respectively. Or we can say, if $M_1$ (resp., $M_2$) runs in a non-$P_1$ process (resp., a non-$P_2$ process), it will trivially return. This can be done by using $\textit{getPID}()$ commands.
%Methods $m_1$ and $m_2$ collaborate to simulate transitions of $M'_{(A,B)}$.
\figurename \ref{fig:execution for lock-freedom with one check success and one check fail} shows how $\mathcal{L}$ simulates one execution $t$ of $M'_{(A,B)}$.
%We draw a downward arrow and a upward arrow to represent $m_1$ and $m_2$ collaborate to simulate one transition of $M'_{(A,B)}$.
A downward arrow (resp., upward arrow) represents a modification of $x$ (resp., of $y$) done by $M_1$ (resp., done by $M_2$) is detected by $M_2$ (resp., by $M_1$).
%and a upward arrow to represent $m_1$ and $m_2$ collaborate to simulate one transition of $M'_{(A,B)}$.
$M_1$ additionally modify ``channel content'' according to transition rules.
To comply with lock-freedom and make $M_2$ not blocked when in guess phase, in the guess phase, $M_2$ returns as long as it modify $y$ one time.
In the check phase, $M_2$ keeps working until the simulation of lossy channel machine can not proceed and a flag $bflag$ is set to true.
In the check phase of $t$, we can see when $M_2$ reads $s_{\textit{trap}}$ from $x$, it returns.
%In the check phase of $t$, when $t$ does not go to $s_{\textit{trap}}$, $m_2$ never return;
%while when $t$ goes to $s_{\textit{trap}}$, $m_2$ set $z$ to $terminate$, which makes the afterwards $m_1$ and $m_2$ trivially return.
%When the simulation process fails, $z$ is also set to terminates, which makes the afterwards $m_1$ and $m_2$ trivially return.
%Note that $m_1$ never return.
}

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.738\textwidth]{PIC_SimluateFiniteTimesForLockFree.pdf}
\vspace{-10pt}
  \caption{One execution of $\mathcal{L}(A,B)$.
  %Execution of $\mathcal{L}(A,B)$ for an execution of $M'_{(A,B)}$ that has one round of check successes and one round of check fails. Here each transition rule $r_i$ is of the form
  %$(q_{\textit{i-1}},\_,\_,q_i,\_)$.
  }
  \label{fig:execution for lock-freedom with one check success and one check fail}
\vspace{-15pt}
\end{figure}

Let us now explain in detail the construction of $\mathcal{L}(A,B)$.
$\mathcal{L}(A,B)$ uses %five
the following memory locations: %$x$, $y$,
$x_1$, $y_1$, $x_2$, $y_2$, $\textit{ack1}$, $\textit{ack2}$, $\textit{phase}$,
$\textit{tflag}$ and $\textit{rule}$. Memory location $phase$ is used by $M_5$
to communicate to $M_1$, $M_2$, $M_3$ and $M_4$ whether the current phase is
guess or check, and its initial value is $guess$. Memory location
$\textit{tflag}$ is a flag indicating the failure of the simulation of the lossy
channel machine. Its initial value is $false$. Memory location $\textit{rule}$
is used by $M_5$ to stores the lossy channel machine transition rule that is
being simulated. The initial value of $\textit{rule}$ is a special value
$\bot$.

\forget{
%\gpnote{It would be useful to have variable names, and values use a different font.}
Let us now explain in detail the construction of $\mathcal{L}(A,B)$.
$\mathcal{L}(A,B)$ uses five memory locations: $x$, $y$, $phase$, $tflag$ and $rule$.
%$z$ is used to tell $m_2$ the current phase, and is a flag for $s_1$, $s_{\textit{trap}}$ or failed simulation.
%The initial value of $z$ is $phase1$;
%if the value of $z$ is $phase2$, then the simulated execution goes into check phase;
%if the value of $z$ is $terminate$, then $m_1$ and $m_2$ trivially return.
Location $phase$ is used to tell $M_2$ {whether the current phase is guess or
check,} and its initial value is $guess$.
Location $tflag$ stores a flag with initial value $false$. If the value is $true$, then the simulation of the lossy channel machine has already finished or failed.
Location $rule$ stores the current transition rule that is being simulated.
Note that, $x$ and $rule$ can only be written by $M_1$, while $y$ and $phase$
can only be written by $M_2$. $tflag$ can be written by both $M_1$ and $M_2$,
and the only possible update to $tflag$ is setting it to $true$.
}

%We now present the two methods in the pseudo-code, shown in Methods \ref{Method1OfLockFreedom} and \ref{Method2OfLockFreedom}.
We now present methods $M_1$, $M_2$ and $M_5$ in the pseudo-code, shown in Algorithms \ref{Method1OfLockFreedom}, \ref{Method2OfLockFreedom} and \ref{Method5OfLockFreedom}, respectively.
For brevity, we use the following notations.
We use $\textit{writeOne}(x,a)$ to represent the sequence of commands writing $\sharp$ followed by $a$ %followed by $\sharp$
into $x$. The symbol $\sharp$ is used as the delimiter to ensure that a value will
not be read twice.
We use $v:=\textit{readOne}(x)$ to represent the sequence of commands reading
%$e$ followed by$\sharp$
$\sharp$ followed by a value $e$ from $x$ for some $e \neq \sharp$ and then assigning $e$ to $v$.
Moreover, if the values read does not correspond with $\sharp$ followed by some
$e$, we set $\textit{tflag}$ to $true$ and then let the current method return. %, terminate the current method, and return.
We uniformly write a transition rule as $(q,u_1,u_2,\alpha,q',v_1,v_2)$, where $u_1$, $u_2,$ $v_1$ and $v_2$ can be either an value or $\epsilon$. % (but can not both be a value).
$(q,u_1,u_2,\alpha,q',v_1,v_2)$ represents a transition rule that
changes state from $q$ to $q'$ with transition label $\alpha$, receives $u_1$
from channel $c_1$ (if any), receives $u_2$ from channel $c_2$ (if any), sends
$v_1$ to channel $c_1$ (if any) and sends $v_2$ to channel $c_2$ (if any). We can see that $u_1$ and $v_1$ can not both be a value, and $u_2$ and $v_2$ can not both be a value. Given a transition rule $r=(q,u_1,u_2,\alpha,q',v_1,v_2)$, we define $valueRead(r,c_1)=u_1$, $valueRead(r,c_2)=u_2$, $valueWritten(r,c_1)=v_1$ and $valueWritten(r,c_2)=v_2$. %, and let $dst(r)$ denote the destination state $q'$ of $r$.
This library is designed to run on
five processes, and thus, the %$\textit{getPID}$ command
$\textit{checkPID}$ command only considers process
identifiers $1$ to $5$.
The symbols $s_0$ and $s_1$ in the pseudo-code of $M_5$ represent
the states of the lossy channel machine $\textit{CM}_{(A,B)}$.

The pseudo-code of $M_3$ (omitted) is obtained from $M_1$ by transforming
$\textit{checkPID}(1)$, $c_1$, $x_1$, $y_1$ and $\textit{ack1}$ into
$\textit{checkPID(3)}$, $c_2$, $x_2$, $y_2$ and $\textit{ack2}$, respectively.
The pseudo-code of $M_4$ (omitted) is obtained from $M_2$ by transforming
$\textit{checkPID}(2)$, $x_1$ and $y_1$ into $\textit{checkPID}(4)$, $x_2$ and
$y_2$, respectively.
}








%\vspace{-5pt}
\subsection{Undecidability of Four Liveness Properties}
\label{subsec:undecidability of four liveness properties}

The following theorem states that lock-freedom, wait-freedom,
deadlock-freedom and starvation-freedom are all undecidable on TSO for a
bounded number of processes. Perhaps surprisingly, we prove this theorem with
the same library $\mathcal{L}(A,B)$. %The detailed proof of this theorem can be found in Appendix \ref{sec:appendix proof of section sec:undecidability of lock-freedom and wait-freedom}.

\begin{theorem}
  \label{theorem:lock-freedom is undecidable}
  The problems of checking lock-freedom, wait-freedom, deadlock-freedom and starvation-freedom of a given library for a bounded number of processes %is
  are undecidable on TSO.
\end{theorem}
\begin {proof}(Sketch)
  For each infinite execution $t$ of $\llbracket \mathcal{L}(A,B),2 \rrbracket$, assume that it simulates an execution of $\textit{CM}'_{(A,B)}$, or it intends to do so. There are three possible cases for $t$ shown as follows:

  \begin{itemize}
  \item[-] Case $1$: The simulation fails because some $\textit{readOne}$ does not read
    the intended value.

  \item[-] Case $2$: The simulation procedure succeeds, and $t$ infinitely loops in the guess phase.

  \item[-] Case $3$: The simulation procedure succeeds, and $t$ infinitely loops in the check phase and visits $s_1$ infinitely many times.
  \end{itemize}

  In case $1$, since $\textit{failSimu}$ is set to $\textit{true}$, each method
  returns immediately. Therefore, $t$ satisfies wait-freedom and thus, satisfies
  lock-freedom. $t$ can be either fair or unfair. In case $2$, since each method
  returns after finite number of steps in the guess phase, $t$ satisfies wait-freedom and thus, satisfies lock-freedom. %Since $M_1$ and $M_2$ coordinate when simulating each transition of $\textit{CM}'_{(A,B)}$, $t$ must be fair.
  In case $3$, since $M_1$ and $M_2$ do not return in the check phase, $t$ violates lock-freedom and thus, violates wait-freedom. Since $M_1$ and $M_2$ coordinate when simulating each transition of $\textit{CM}'_{(A,B)}$, in case $2$ and $3$, $t$ must be fair.

  Therefore, we reduce the problem of checking whether $\textit{CM}'_{(A,B)}$ has an execution that visits $s_1$ infinitely often into the problem of checking whether $\mathcal{L}(A,B)$ has an infinite execution of case $3$ (which is fair and violates both wait-freedom and lock-freedom). By Lemma \ref{lemma:reducing CPCP into a infinite execution problem of lossy channel machine M'{(A,B)}}, we can see that the problems of checking lock-freedom, wait-freedom, deadlock-freedom and starvation-freedom of a given library for bounded number of processes are undecidable.
\end {proof}

% Our TSO memory model do not consider liveness condition on store buffers.
We remark here that~\cite{DBLP:journals/corr/abs-2012-01067} considers
imposing liveness condition on store buffers, and requires buffered items to be
eventually flushed. Our undecidability results on liveness properties on TSO
still hold when imposing such liveness condition on store buffers, since in case
$3$ of the proof of Theorem \ref{theorem:lock-freedom is undecidable}, each item
put into buffer will eventually be flushed.





