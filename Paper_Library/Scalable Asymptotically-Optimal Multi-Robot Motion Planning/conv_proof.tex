\subsection{Proof of Convergence}
\label{apx:conv_proof}

This appendix examines the result of Theorem~\ref{thm:ao_drrt}, and
formally proves the convergence of the \drrtstar\ tree toward
containing all optimal paths.




\begin{lemma}[Optimal Tree Convergence of \drrtstar]
\label{lem:tree_conv}
Consider an arbitrary optimal path $\pi^*$ originating from $v_0$ and 
ending at $v_{t}$, then let $O^{(m)}_k$ be the event such that after 
$m$ iterations of \drrtstar, the search tree $\tree$ contains the 
optimal path up to segment $k$.  Then, $$ \liminf_{m \to \infty} \pr 
\big( O^{(m)}_t \big) = 1.$$
\end{lemma}

% = = = = = = = = = = = = 
%  MCMC Proof
% = = = = = = = = = = = =

{\bf Proof.} This property will be proven using a theorem from Markov
chain literature \cite[Theorem~11.3]{Snell2012:intro_prob}. 
Specifically, the properties of absorbing Markov chains can be 
exploited to show that $\drrtstar$ will eventually contain the optimal
path over $\mmgimp$ for a given query.  An absorbing Markov chain is
one such that there is some subset of states in which the transition
matrix only allows that state to transition to itself.

The proof follows by showing that the $\drrtstar$ method can be 
described as an absorbing Markov chain, where the target state of a
query is represented as an absorbing state in a Markov chain, 
re-stated here.

\begin{theorem}[Thm 11.3 in Grinstead \& Snell]
\label{thm:grinstead}
In an absorbing Markov chain, the probability that the process will be 
absorbed is 1 (i.e., $Q(m) \to 0$ as $n \to \infty$), where $Q(m)$ is
the transition submatrix for all non-absorbing states.
\end{theorem}

There are two steps to using this proof.  First, that the $\drrtstar$
search can be cast as an absorbing Markov chain, and second, that the
probability of transition for from each state to the next in this 
chain is nonzero (i.e. that each state can eventually be connected to
the target).

For query $(S, T)$, let the sequence $V = \{ v_1, v_2, \dots, 
v_{\textup{t}}\}$ of length $t$ represent the vertices of $\mmgimp$ 
corresponding to the optimal path through the graph which connects 
these points, where $v_{\textup{t}}$ corresponds to the target vertex,
and furthermore, let $v_{\textup{t}}$ be an absorbing state.  
Theorem~\ref{thm:grinstead} operates under the assumption that each 
vertex $v_{\textup{i}}$ is connected to an absorbing state 
($v_{\textup{t}}$ in this case).

Then, let the transition probability for each state have two values, 
one for each state transitioning to itself, which corresponds to the
$\drrtstar$ search expanding along some other arbitrary path.  The 
other value is a transition probability from $v_{\textup{i}}$ to 
$v_{\textup{i}+1}$.  This corresponds to the method sampling within
the volume $\textup{Vol}(v_{\textup{i}})$.

Then, as the second step, it must be shown that this volume has a 
positive probability of being sampled in each iteration.  It is 
sufficient then to argue that $\frac{\mu(\textup{Vol}
(s_{\textup{i}}))} {\mu(\cfree)} > 0$.  Fortunately, for any finite 
$n$, previous work has already shown that this is the case given 
general position assumptions \cite[Lemma~2]{SoloveySH16:ijrr}.

Given these results, the $\drrtstar$ is cast as an absorbing Markov
chain which satisfies the assumptions of \ref{thm:grinstead}, and 
therefore, the matrix $Q(m) \to 0$.  This implies that the optimal
path to the goal has been expanded in the tree, and therefore 
$ \liminf_{m \to \infty} \pr \big( O^{(m)}_t \big) = 1.$ \qed

% = = = = = = = = = = = = 
%  Old, quite long proof
% = = = = = = = = = = = =

% {\bf Proof.}  The proof will be presented in two parts.  First, a 
% bound on $\pr( O^{(m)}_k )$ for finite iteration $m$ is drawn.  Then,
% using this bound, an inductive step is given to show this holds for 
% any segment $k$ given it holds for segment $k-1$.  It is finished by
% considering the base case $\pr(O^{(j)}_0) = 1$.

% First a bound on $\pr( O^{(m)}_k )$ is derived.  Let $A^{(m)}_k$ be 
% the event that at iteration $m$, \drrtstar\ makes the optimal 
% connection from vertex $v_{k-1}$ to vertex $v_k$.  Clearly, the event 
% that the optimal path to vertex $v_k$ is not in $\tree$ corresponds to 
% when the connection fails after $m$ consecutive iterations, i.e.:

% $$ \neg O^{(m)}_k = \neg A^{(1)}_k \cap \neg A^{(2)}_k \cap \dots \cap 
% \neg A^{(m)}_k$$

% \begin{multline}
% \pr (\neg O^{(m)}_k) = \pr(\neg A^{(1)}_k) \cdot \pr(\neg A^{(2)}_k | 
% \neg A^{(1)}_k) \cdot \\ \dots \cdot \pr\Big( \neg A^{(m)}_k | 
% \bigcap^{m-1}_{j=1} \neg A^{(j)}_k \Big)
% \end{multline}

% $$ \pr (\neg O^{(m)}_k) = \prod^{m}_{i=1} \Big( \pr \Big( \neg 
% A^{(i)}_k | \bigcap^{i-1}_{j=1} \neg A^{(j)}_k \Big) \Big)$$

% \noindent
% Next, the conditional probability of $\neg A^{(i)}_k$ given 
% $\bigcap^{i-1}_{j=1} \neg A^{(j)}_k$ can be described in terms of the 
% probability of having generated an optimal path to vertex $v_{k-1}$, 
% i.e.:

% \begin{multline}
% \pr \Big( \neg A^{(m)}_k | \bigcap^{m-1}_{j=1} \neg A^{(j)}_k \Big) = 
% \pr( \neg O^{(m)}_{k-1} ) \\ + \pr( O^{(m)}_{k-1} ) \cdot \pr( \{ 
% \text{fail to generate}(v_{k-1}, v_k) \} ).
% \end{multline}

% \begin{multline}
% \pr \Big( \neg A^{(m)}_k | \bigcap^{m-1}_{j=1} \neg A^{(j)}_k \Big) 
% \leq \pr( \neg O^{(m)}_{k-1} ) \\ + \pr( O^{(m)}_{k-1} ) \cdot \pr( 1 
% - \frac{\mu(Vol(v_{k-1}))}{\mu(\cfree)} ).
% \end{multline}

% $$\pr \Big( \neg A^{(m)}_k | \bigcap^{m-1}_{j=1} \neg A^{(j)}_k \Big) 
% \leq 1 - \pr( O^{(m)}_{k-1} ) \cdot \frac{\mu(Vol(v_{k-1}))}
% {\mu(\cfree)}$$

% \noindent
% Then, combining these results, we attain a bound on the probability of 
% successfully having the optimal path to vertex $v_k$ as 

% $$ \pr( O^{(m)}_k ) \geq 1 - \prod^{m}_{i=1} \Big( 1 - \pr( 
% O^{(m)}_{k-1} ) \cdot \frac{\mu(Vol(v_{k-1}))}{\mu(\cfree)} \Big). $$

% % = - = - = - = - = - = - = - = - = - = - = - = - = 

% \noindent
% {\bf Inductive step.} Next it will be shown that given $\lim_{m \to 
% \infty} \pr( O^{(m)}_{k-1} ) = 1$, then $\lim_{m \to \infty} \pr( 
% O^{(m)}_k ) = 1$.  Set $y^{(m)}_k = \prod^{m}_{j=1} (1 - 
% \pr( O^{(j)}_{k-1} ) \cdot \frac{\mu(Vol(v_{k-1}))}{\mu(\cfree)} )$.  
% The logarithm of $y^{(m)}_k$ behaves as follows:

% $$ \log y^{(m)}_k = \log \prod^{m}_{j=1} \Big( 1 - \pr(O^{(j)}_{k-1}) 
% \cdot \frac{\mu(Vol(v_{k-1}))}{\mu(\cfree)} \Big) $$

% $$ \log y^{(m)}_k = \sum^{m}_{j=1} \log \Big( 1 - \pr(O^{(j)}_{k-1}) 
% \cdot \frac{\mu(Vol(v_{k-1}))}{\mu(\cfree)} \Big) $$

% $$ \log y^{(m)}_k < - \sum^{m}_{j=1} \pr(O^{(j)}_{k-1}) \cdot
% \frac{\mu(Vol(v_{k-1}))}{\mu(\cfree)} $$

% \noindent
% While it is known that $\mu(Vol(v_{k}))$ for any $k$ is monotonically 
% nonincreasing in $m$, it is also known that this volume is nonzero.
% That is, let $\lim_{m \to \infty} \mu(Vol(v_{k})) = c_1$, for some 
% small constant $c_1 > 0$.  Then $\frac{\mu(Vol(v_{k}))}{\mu(\cfree)}
% \geq c_2$ can be treated as a constant giving:

% $$ \log y^{(m)}_k < - c_2 \sum^{m}_{j=1} \pr(O^{(j)}_{k-1}) $$

% \noindent
% Since it is assumed that $\lim_{m \to \infty} \pr(O^{(m)}_{k-1}) = 1$,
% it is clear that:

% \begin{multline}
% \lim_{m \to \infty} \log y^{(m)}_k < \lim_{m \to \infty} - c_2 
% \sum^{m}_{j=1} \pr(O^{(j)}_{k-1}) = - \infty \\ \iff \lim_{m \to 
% \infty} y^{(m)}_k = 0
% \end{multline}

% \noindent
% Then, by substitution,

% $$ \pr( O^{(m)}_k ) \geq 1 - y^{(m)}_k $$

% $$ \lim_{m \to \infty} \pr( O^{(m)}_k ) \geq 1 - \lim_{m \to \infty} 
% y^{(m)}_k = 1 - 0 = 1.$$

% % = - = - = - = - = - = - = - = - = - = - = - = - = 

% \noindent
% Therefore, the inductive step holds.  With the base case of 
% $\pr(O^{(j)}_0) = 1$ since the root is always in the tree, it is 
% true that

% $$ \lim_{m \to \infty} \pr( O^{(m)}_k ) = 1 \Longrightarrow
% \liminf_{m \to \infty} \pr( O^{(m)}_t ) = 1,$$

% \noindent
% thus proving Lemma \ref{lem:tree_conv}. \qed
