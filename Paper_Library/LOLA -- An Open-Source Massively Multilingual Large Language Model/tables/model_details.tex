\begin{table*}[]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lrrrrr@{}}
\toprule
\textbf{Model} & \textbf{Params (B)} & \textbf{Consumed Tokens (T)} & \textbf{Max Seq. Length} & \textbf{Languages} & \textbf{Category} \\ \midrule
Glot500m \cite{imanigooghari-etal-2023-glot500}     & 0.39 & -     & 512       & 500 & 1  \\
XLM-R Large \cite{conneau-etal-2020-unsupervised}  & 0.55 & 6     & 512       & 100 & 1  \\
mBART \cite{liu2020multilingual}        & 0.68 & 1.8   & 1024      & 25 & 1   \\
BLOOM-1B1 \cite{bloom2022}    & 1.10 & 0.341 & Arbitrary & 48 & 1  \\
MT5 Large \cite{xue2021mt5}   & 1.20 & 1     & Arbitrary & 101 & 1 \\
mGPT \cite{shliazhko-etal-2024-mgpt}         & 1.30 & 0.440 & 2048      & 61 & 1  \\
BLOOM-1B7 \cite{bloom2022}    & 1.70 & 0.341 & Arbitrary & 48 & 1  \\ \midrule
XLM-R XL \cite{conneau-etal-2020-unsupervised}     & 3.50 & 6     & Arbitrary & 100 & 2 \\
MT5 XL \cite{xue2021mt5}       & 3.70 & 1     & Arbitrary & 101 & 2 \\
UMT5 XL \cite{chung2023unimax}      & 3.70 & 1     & Arbitrary & 107 & 2 \\ \midrule
TowerBase 7B \cite{alves2024tower} & 6.74 & 2     & Arbitrary & 10 & 3  \\
Mistral v0.3 \cite{jiang2023mistral7b} & 7.00 & -     & 32768     & 5  & 3  \\
Falcon \cite{almazrouei2023falconseriesopenlanguage}       & 7.00 & 1.5   & 2048      & 2 & 3   \\
BLOOM-7B1 \cite{bloom2022}    & 7.10 & 0.366 & Arbitrary & 48 & 3  \\
SeaLLM v2 \cite{nguyen-etal-2024-seallms}    & 7.38 & -     & Arbitrary & 10 & 3  \\
SeaLLM v2.5 \cite{nguyen-etal-2024-seallms}  & 7.38 & -     & Arbitrary & 10 & 3  \\
Breeze \cite{hsu2024breeze7btechnicalreport}       & 7.49 & -     & Arbitrary & 2  & 3  \\ \midrule
LOLA (Our Model)    & 1.3  & 0.465 & 2048      & 167  & 1 \\ \bottomrule
\end{tabular}
}
\caption{Characteristics of models used for comparison in the evaluation, including model names, active parameter sizes (in billions), the number of consumed tokens (in trillions), maximum sequence length, and the number of languages each model was trained on. The models are grouped by their size categories (see appendix \autoref{fig:model-size-comparison}).}
\label{tab:model-det}
\end{table*}