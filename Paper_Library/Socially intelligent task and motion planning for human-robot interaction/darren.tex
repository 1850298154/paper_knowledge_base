\documentclass[conference]{IEEEtran}
\usepackage{times}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}
\usepackage{pdfpages}
\usepackage{float}
\usepackage{caption}
\usepackage{balance}


\pdfinfo{
   /Author (Darren M. Chan and Laurel D. Riek)
   /Title  (Unsupervised Salient Object Discovery for Robots)
   /CreationDate (D:20101201120000)
   /Subject (Robotics)
   /Keywords (Robotics)
}
\IEEEoverridecommandlockouts
\begin{document}

% paper title
\title{Unsupervised Salient Object Discovery for Robots}

% You will get a Paper-ID when submitting a pdf file to the conference system
%\author{Darren M. Chan and Laurel D. Riek \\
%Department of Computer Science and Engineering, University of California, San Diego \\
%\{dcc012,lriek\}@eng.ucsd.edu
%}

%\author{Darren M. Chan and Laurel D. Riek}
\author{\parbox{18cm}{\centering
     Darren M. Chan  and Laurel D. Riek\thanks{Some research reported in this article is based upon work supported by the National Science Foundation under Grant IIS-1527759.}\\
     Computer Science and Engineering, UC San Diego\\}
     }
%\author{\authorblockN{Michael Shell}
%\authorblockA{School of Electrical and\\Computer Engineering\\
%Georgia Institute of Technology\\
%Atlanta, Georgia 30332--0250\\
%Email: mshell@ece.gatech.edu}
%\and
%\authorblockN{Homer Simpson}
%\authorblockA{Twentieth Century Fox\\
%Springfield, USA\\
%Email: homer@thesimpsons.com}
%\and
%\authorblockN{James Kirk\\ and Montgomery Scott}
%\authorblockA{Starfleet Academy\\
%San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212\\
%Fax: (888) 555--1212}}


% avoiding spaces at the end of the author lines is not a problem with
% conference papers because we don't use \thanks or \IEEEmembership


% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\authorblockN{Michael Shell\authorrefmark{1},
%Homer Simpson\authorrefmark{2},
%James Kirk\authorrefmark{3}, 
%Montgomery Scott\authorrefmark{3} and
%Eldon Tyrell\authorrefmark{4}}
%\authorblockA{\authorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: mshell@ece.gatech.edu}
%\authorblockA{\authorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\authorblockA{\authorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\authorblockA{\authorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}


\maketitle

\IEEEpeerreviewmaketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Within the foreseeable future, robots will become ubiquitous in our everyday lives, where they will operate in complex, real-world environments \cite{riek2014social, nigam2015social, riek2017healthcare}.
Facing the same challenges as humans, they will encounter situations where the scene, context, people, and entities around them are not immediately obvious \cite{o2015detecting}.
Consequently, robots will need to be able to rectify these uncertainties so that they can behave appropriately, as well as learn from and adapt to their surroundings \cite{chen2017deep,kubotaactivity}.

One key challenge is designing robots to perceive novel objects, which will play a fundamental role in how robots make sense of visual information.
While research in object detection has rapidly advanced to a greater degree than anticipated, they are still primarily constrained to the problem of detecting objects of known classes \cite{miller2018dropout, scheirer2014probability, sunderhauf2018limits}.
Consequently, they are unable detect novel objects and do little to improve a robot's ability to learn about, or even perceive novel objects.

Enabling robots to perceive novel objects is critical because it will ultimately affect how they perform and behave in the real world. 
For instance, robots can most efficiently execute tasks if they are able to learn about unfamiliar objects that might be helpful to them (e.g., retrieval of uncommon items, investigating novel environments with unfamiliar objects, identifying entities that can be manipulated, etc.).
Furthermore, by exploring and interacting with objects that they have not seen before, robots can gain an understanding about object affordances, and how they functionally relate to people, places, and other objects in the world \cite{do2018affordancenet, shen2018scaling}.

Some researchers address the problem of detecting arbitrary objects in videos or image sequences by leveraging \textit{saliency} \cite{chan2017faster}, which aims to infer image or video regions that attract high visual attention while also obeying object boundaries. This concept is often applied together with unsupervised learning, where the goal is to discover objects without needing manual annotation or initialization, a problem called \textit{unsupervised object discovery}.
With the advent of faster and denser optical flow algorithms \cite{brox2010object, sundaram2010dense}, the top-performing methods use some form of motion boundary detection in their pipeline \cite{papazoglou2013fast, wang2018saliency}.
However, these methods are computationally expensive, typically taking on the order of minutes to discover objects, which can be prohibitively slow for real-time robotics applications.

Recently, advances in object proposal algorithms (OPAs) show promise in addressing key challenges in salient object discovery for robots.
In particular, when an OPA is applied to images or video, it can potentially produce object candidates, or \textit{general object proposals} (GOPs), which can be useful for accurately recovering the locations of arbitrary objects.

One of the greatest assets of OPAs is that they are fast, typically taking less than a second to compute hundreds to thousands of GOPs per image or video frame. 
However, OPAs can also be problematic because  majority of these GOPs do not contain useful information (e.g., contain background regions or partial objects) \cite{chavali2016object}.
Consequently, it is often difficult to determine which object proposals correspond to real objects.

To this end, we introduce our recent work, Unsupervised Foraging of Objects (UFO), a novel method that automatically filters out non-salient object proposals to address unsupervised salient object discovery for robot vision. The key insight of our approach is that when an OPA is applied to a video or a set of spatiotemporal images, GOPs corresponding to non-objects appear randomly, typically due to camera noise, changes in illumination, or the presence of image artifacts. In contrast, GOPs correlating to real objects contain a higher degree of saliency and appear more consistently, making it possible to discover salient objects by detecting strong correspondences across image sequences.
As a result, UFO can infer arbitrary objects using only a few observation samples, making it faster than similar, competitive methods.

By designing our method to quickly and accurately perceive novel objects, we reach one step closer to designing robots that can
learn about novel objects, leading to improved autonomy.

\begin{figure}[t]
\captionsetup{font=scriptsize}
\centerline{\includegraphics[width=0.5\textwidth]{output_ex.png}}
\caption{Sample outputs in a challenging sequence (i.e., ''\textit{mallard-fly}'') from the Davis 2016 dataset. Our method, UFO, is robust to dynamic lighting, and fast camera and object motion, which is difficult for methods that rely on optical flow or motion boundaries.}
\label{fig:outputex}
\hspace{-0.15in}
\vspace{-0.25in}
\end{figure}


\begin{figure*}[t]
\captionsetup{font=scriptsize}
\centerline{\includegraphics[width=1.0\textwidth]{darren_v3.png}}
\caption{UFO is composed of six processes: object proposal generation, saliency scoring, saliency-aware non-maximum suppression, feature extraction, sliding window graph update, and path selection.}
\label{fig:overview}
\hspace{-0.15in}
\vspace{-0.20in}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{UFO}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Figure 1 shows an overview of UFO and each of its aspects, which include: object proposal generation, saliency scoring, saliency-aware non-maximum suppression, feature extraction, sliding window graph update, path selection, and object proposal prediction.

First, an OPA (e.g., \cite{pinheiro2015learning}) is applied to the first frame of a sequence of images to generate a set of GOPs, each consisting of a bounding box and objectness confidence score.
In parallel to this procedure, we apply the Minimum Barrier Distance transform \cite{zhang2015minimum} to the image to extract saliency predictions for each pixel (i.e., saliency map).

To reduce computation time in the later part of the pipeline, we apply a saliency-aware non-maximum suppression strategy (NMS) to remove redundantly overlapping GOPs. For each
GOP we measure the number of enclosed salient pixels to derive a normalized saliency score, allowing our NMS procedure to favor the selection of those that are more salient.
The GOPs are then passed to a pre-trained convolutional neural network (i.e., VGG-19 \cite{simonyan2014very}), to extract general feature embeddings.

Transforming GOPs to vertices, we frame salient object discovery as a graph problem.
With each pair of frame-adjacent GOPs, we use their feature embeddings to compute pairwise similarity scores.
Using these similarity scores, we construct GOP correspondances by applying bipartite matching to form edges.
Taking into account the similarity and objectness confidence scores for each GOP, we apply greedy path selection, where the path with the highest score corresponds to the most salient object in the image sequence. The GOP corresponding to the latest entry of this path (i.e., the bounding box of a discovered object) is then output.

To avoid the problem of an exponentially growing graph as the image sequence grows larger, we leverage a sliding window graph, only keeping memory of the most recent GOPs.
This enables the past history of objects to propagate over time, both reducing computational load and memory usage.

Lastly, in the event that the OPA fails to generate GOPs of previously discovered objects in the follow frame, UFO makes object proposal predictions.
This is achieved by generating an image template, using repeated entries GOP entries (i.e., those with stronger correspondences).
The template is then cross-correlated with the following frame to determine the most likely location of the previously discovered object.

\begin{table}[t]
\captionsetup{font=scriptsize}
\centering
\resizebox{.5\textwidth}{!}
{%
\vspace*{2cm}\begin{tabular}{lllllll}
\hline
\textbf{Method}                                    & $t(s) \downarrow$ & Precision $\uparrow$ & Recall $\uparrow$ & F-score $\uparrow$ & $CorLoc \uparrow$ & $mAP \uparrow$ \\ \hline
UFO                                                & \textbf{4.52}     & \textbf{0.662}       & 0.645             & \textbf{0.654}     & \textbf{0.486}    & 0.568          \\
SAL \cite{wang2018saliency}  & 35.7              & 0.517                & 0.597             & 0.597              & 0.425             & 0.517          \\
FST \cite{papazoglou2013fast} & 29.4              & 0.659                & \textbf{0.647}    & 0.653              & 0.485             & \textbf{0.586} \\ \hline
\end{tabular}%
}
\caption{Summary of results: Precision, Recall, F-score, correct localization ($CorLoc$), mean average precision ($mAP$) measured at intersection over union threshold of 0.5. We report the average end-to-end computation time in seconds per frame ($t(s)$). $\uparrow$ indicates that higher is better and $\downarrow$ indicates that lower is better.}
\label{tab:results}
\vspace{-0.15in}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results and Discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


We evaluated UFO on the DAVIS 2016 dataset \cite{perazzi2016benchmark}, which consists of 50 spatiotemporal image sequences, each depicting a unique salient object.
Each sequence is captured from a moving camera under different lighting conditions, clutter, and occlusion, making it a testbed that is representative of challenges in robot vision.

In our evaluation, we measured precision, recall, F-measure, correct localization ($CorLoc$), mean average precision ($mAP$), and computation time per frame in seconds. \footnote{For a comprehensive summary of these metrics, see Tang et al. \cite{tang2014co}}.

We present a summary of our results in Table \ref{tab:results}, along with sample outputs in Figure \ref{fig:outputex}.
Comparing precision, recall, F-measure, and CorLoc, we found that UFO scored similarly to FastSeg, while Geodesic scored lower for all metrics.

Comparing end-to-end computation time, we found that UFO was approximately 6.5 times faster than SAL (which took on average 35.7 seconds compute per iteration) and FST (which took on average 29.4 seconds). 
Since these methods require multiple frames and iterations to make object discovery inferences, we show that optical flow-based methods take on the order of \textit{minutes}, while UFO is able to reduce this time to seconds.
To our knowledge, and at the time of writing, UFO is the fastest unsupervised salient object discovery method. 

Achieving state-of-the-art precision and recall on the DAVIS dataset, our results suggest that UFO is robust to real-world robot vision challenges, including moving cameras and moving objects, detractor objects, motion blur, and occlusion.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In our future work we plan to expand our method to address other problems in robotics.
For instance, we would like to explore how UFO can be used in conjunction with active vision to investigate unfamiliar objects in novel environments, such as deep sea or space exploration, where it is difficult for humans to reliably be in the loop to support robot missions \cite{driess2017active, shi2017underwater}.

We also plan to improve UFO and deploy it on a live robot, which will help us gather data in unconstrained environments and enable it to learn about novel object classes to construct recognition models in real-time.
This will ultimately allow us to build a scalable object detection framework that can learn on-the-fly, which will enable robots to one day become more seamlessly integrated into real-world environments.

\balance
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}