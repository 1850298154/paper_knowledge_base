\section{Related Work}
\label{sec:related}

In the AI planning community, it has shown that it is possible to formalize ``model synchronization'' as a meta-search problem, where traditional search and classical planning algorithms explore explanations with differing properties \cite{chakraborti1802plan,chakraborti2018explicability}. One key insight of model synchronization is that explanations are generally needed to identify mismatches in planning models produced by different information sources. This is critically important when the distance between different descriptions of a planning domain   cannot capture a cohesive model sufficiently. While explanations can certainly deviate from our actual methods of decision making \cite{klein08} they nevertheless represent how humans are trained and acculturated to providing rationalizations for our decision making. In that sense, we believe this line of work is complementary to our approach in this paper: two approaches can be combined in order to formalize and reason about properties such as social trust, analytic trust, communication frequencies, and others. This approach can balance the trade-offs between explicability and explanations for social interactions. In particular, an “optimal” AI agent might generate an estimate of the state of the world that is inexplicable to humans and model synchronization and provenance tracing will enable an AI agent to choose a less optimal model of the state that would enable an easier explanation to the human users and is close to (but not the same as) the AI agent’s actual domain models \cite{chakraborti2018explicability}.

Provenance-tracking is well-established as a practical tool across source domains \cite{gehani2010mendel,pasquier2016information} for decision support \cite{singh2018decision}, complex multi-agent workflows \cite{toniolo2015supporting,friedman_tapp_2020}, and lineage-tracking for databases \cite{benjelloun2008databases}.
These previous works have not been applied to the domain of planning, so we believe this work is the first to investigate the explainability of automated planning using provenance.

Label propagation has been used to detect persistent security threats in real time \cite{han2020unicorn} by propagating information through network flows.
We have previously used label propagation for
plan recognition with support for refutation, similar to what we demonstrate here \cite{primrose2018}, but this previous approach did not use formal provenance notation or operate on forward-generated plans.
