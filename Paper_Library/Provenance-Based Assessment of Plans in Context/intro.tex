\section{Introduction}
\label{sec:intro}

In complex, dynamic, and uncertain environments, it is critical that human operators understand machine-generated plans, including their sensitivity to world changes, their reliance on individual actors, their diversity of information sources, their core assumptions, and how risky they are. % their operational risk.
This paper contributes an approach to dynamically explain and explore machine-generated single- or multi-agent, single- or multi-goal plans using \emph{provenance-based} analysis and visualization strategies.

Most prior work on explainable planning focuses on inspecting algorithms (\ie{}, explicating the decision-making process), synchronizing mental models (\eg{}, because the user views the problem differently than the planner), and improving usability (\eg{}, making complex plans more interpretable) \cite{ChakrabortiEtAlIJCAI2020} and assumed fixed background domain knowledge.
In contrast, our provenance-based approach treats the plan as a tripartite dependency graph that helps explain the foundations, reliability, and sensitivity of the information that comprises the plan's states and actions.

We use the definition of ``provenance'' from the Provenance Data Model (PROV-DM): ``information about entities, activities, and people involved in producing a piece of data or thing, which can be used to form assessments about its quality, reliability or trustworthiness'' ~\cite{Moreau:13:PTP}.
We describe the formal PROV-DM relationships among these elements later, as shown in \figref{prov}.
% \ldots{}. PROV-DM is organized in six components, respectively dealing with: (1) entities and activities, and the time at which they were created, used, or ended; (2) derivations of entities from entities; (3) agents bearing responsibility for entities that were generated and activities that happened;  \ldots % (4) a notion of bundle, a mechanism to support provenance of provenance; (5) properties to link entities that refer to the same thing; and, (6) collections forming a logical structure for its members
%  .~\cite{Moreau:13:PTP}
% \end{quotation}
% \noindent{}
We have augmented the Hierarchical Task Network (HTN) planner \shop{}~\cite{GoldmanKuter:SHOP3ELS} with the ability to annotate its plans with provenance by recording, on the fly,
\begin{enumerate*}[label=(\arabic*)]
\item causal dependencies,
\item dependencies from plan components onto aspects of the model (domain) from which they derive, and
\item sources of information used by the planner in checking preconditions and deriving beliefs.
\end{enumerate*}

The provenance of the \shop{} plan feeds into our downstream provenance analysis, which uses PROV-DM to represent beliefs, planned activities, and actors, and the recent DIVE ontology \cite{friedman_tapp_2020} to represent assumptions, confidence, and likelihood of those PROV-DM elements.
Our approach combines truth maintenance \cite{forbus1993building} and provenance propagation \cite{singh2018decision,gehani2010mendel,pasquier2016information} to estimate the confidence in the correctness of planned actions, and counterfactually assess the \textit{sensitivity} of the plan to [the absence of] various data sources, actors, events, and beliefs.

Our central claim is that tracking and analyzing a plan's provenance can improve the interpretation of plans--- along dimensions of confidence, information dependency, risk, and sensitivity--- without reducing the efficiency of the planner or the complexity of the search space.
To support this claim, we demonstrate our approach within a provenance visualization environment \cite{friedman_tapp_2020}.
This provenance-based approach is especially useful for explaining plans with multiple goals and for plans with multiple actions to achieve a given goal.
While our demonstration uses provenance analysis after planning completes, we identify future avenues for using provenance \emph{within} a planner to advise search heuristics, mixed-initiative planning, contingency planning, and replanning.

%\from{ugur}{We should have some reference to the explanation generation, explicability works in the AI planning community (and this workshop's previous publications) here. I added a paragraph on that to the Related Work section and please see my comment in the Assessing Explainability section as well. I think we can grab a sentence or two from that paragraph and rephrase them here.}

We continue with a review of relevant background in provenance-tracking and HTN planning.
We then describe our approach using provenance as a platform for plan explanation and assessment, qualifying the types of planning questions that our approach addresses. %addressed by our approach.
We demonstrate our system facilitating plan assessment, and we review the results and outline future work in our conclusion.


% While organization standards and metrics exist for integrity, confidence, quality \cite{icd_203}, pertinence \cite{icd_206}, and rigor of analysis \cite{zelik2010measuring}, these measures have largely been ignored for automated planning systems and their generated plans.

% This paper contributes an approach to dynamically validate and explore machine-generated single- or multi-agent, single- or multi-goal plans using provenance-based analysis and visualization strategies.
% We build upon well-established provenance frameworks \cite{lebo2013prov} and truth-maintenance algorithms \cite{forbus1993building,de1986assumption}, and we extend these with novel classes and semantic constraints to represent the derivation and rationale for beliefs and planned actions, the appraisals of various agents on those beliefs and actions, and the propagation of confidence and trust from sources to conclusions and goals.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
