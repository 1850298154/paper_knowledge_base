%%
%% This is file `sample-acmsmall.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `acmsmall')
%%
%% IMPORTANT NOTICE:
%%
%% For the copyright see the source file.
%%
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall.tex.
%%
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%%
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.

\documentclass[acmsmall]{acmart}

\usepackage{booktabs} % For formal tables
%\usepackage{ltexpprt}
\usepackage{graphicx}
%\usepackage{balance}  % for  \balance command ON LAST PAGE  (only there!)
\usepackage{epsfig}
%\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{array}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{subfigure}
\usepackage{color}
\usepackage{tcolorbox}
%\usepackage[boxed]{algorithm2e}
\usepackage{enumerate}
\usepackage{diagbox}
\usepackage{makecell}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2020}
\acmYear{2020}
\acmDOI{0000}


%%
%% These commands are for a JOURNAL article.
\acmJournal{TIST}
\acmVolume{00}
\acmNumber{0}
\acmArticle{000}
\acmMonth{0}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Graph Neural Networks: Taxonomy, Advances and Trends}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Yu Zhou}
\authornote{Corresponding Author.}
\email{zhouyu@tyut.edu.cn}
\orcid{0000-0002-0304-0863}
\affiliation{%
  \institution{College of Data Science/Shanxi Spatial Information Network Engineering Technology Research Center, Taiyuan University of Technology}
  \streetaddress{No. 79 Yingze West Street, WanBaiLin District}
  \city{Taiyuan}
  \state{Shanxi}
  \country{China}
  \postcode{030024}
}

\author{Haixia Zheng}
\authornote{Corresponding Author.}
\email{zhenghaixia@tyut.edu.cn}
%\orcid{1234-5678-9012}
\affiliation{%
  \institution{College of Data Science/Shanxi Spatial Information Network Engineering Technology Research Center, Taiyuan University of Technology}
  \streetaddress{No. 79 Yingze West Street, WanBaiLin District}
  \city{Taiyuan}
  \state{Shanxi}
  \country{China}
  \postcode{030024}
}

\author{Xin Huang}
%\authornote{Both authors contributed equally to this research.}
\email{huangxin@tyut.edu.cn}
%\orcid{1234-5678-9012}
\affiliation{%
  \institution{College of Data Science, Taiyuan University of Technology}
  \streetaddress{No. 79 Yingze West Street, WanBaiLin District}
  \city{Taiyuan}
  \state{Shanxi}
  \country{China}
  \postcode{030024}
}

\author{Shufeng Hao}
%\authornote{Both authors contributed equally to this research.}
\email{haoshufeng@tyut.edu.cn}
%\orcid{1234-5678-9012}
\affiliation{%
	\institution{College of Data Science, Taiyuan University of Technology}
	\streetaddress{No. 79 Yingze West Street, WanBaiLin District}
	\city{Taiyuan}
	\state{Shanxi}
	\country{China}
	\postcode{030024}
}

\author{Dengao Li}
%\authornote{Both authors contributed equally to this research.}
\email{lidengao@tyut.edu.cn}
%\orcid{1234-5678-9012}
\affiliation{%
  \institution{College of Data Science/Shanxi Spatial Information Network Engineering Technology Research Center, Taiyuan University of Technology}
  \streetaddress{No. 79 Yingze West Street, WanBaiLin District}
  \city{Taiyuan}
  \state{Shanxi}
  \country{China}
  \postcode{030024}
}

\author{Jumin Zhao}
%\authornote{Both authors contributed equally to this research.}
\email{zhaojumin@tyut.edu.cn}
%\orcid{1234-5678-9012}
\affiliation{%
  \institution{College of Information and Computer/Shanxi Intelligent Perception Engineering Research Center, Taiyuan University of Technology}
  \streetaddress{No. 79 Yingze West Street, WanBaiLin District}
  \city{Taiyuan}
  \state{Shanxi}
  \country{China}
  \postcode{030024}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Yu Zhou, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Graph neural networks provide a powerful toolkit for embedding real-world graphs into low-dimensional spaces according to specific tasks.
  Up to now, there have been several surveys on this topic. However, they usually lay emphasis on different angles so that the readers can not see a panorama of the graph neural networks.
  This survey aims to overcome this limitation, and provide a systematic and comprehensive review on the graph neural networks. First of all, we provide a novel taxonomy for the graph neural networks,
  and then refer to up to 250 relevant literatures to show the panorama of the graph neural networks. All of them are classified into the corresponding categories.
  In order to drive the graph neural networks into a new stage, we summarize four future research directions so as to overcome the facing challenges.
  It is expected that more and more scholars can understand and exploit the graph neural networks, and use them in their research community.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010257.10010293.10010294</concept_id>
       <concept_desc>Computing methodologies~Neural networks</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010257.10010293.10010319</concept_id>
       <concept_desc>Computing methodologies~Learning latent representations</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Neural networks}
\ccsdesc[500]{Computing methodologies~Learning latent representations}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Graph Convolutional Neural Network, Graph Recurrent Neural Network, Graph Pooling Operator, Graph Attention Mechanism, Graph Neural Network}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Graph, as a complex data structure, consists of nodes (or vertices) and edges (or links). It can be used to model lots of complex systems in real world, e.g. social networks, protein-protein interaction networks,
brain networks, road networks, physical interaction networks and knowledge graph etc. Thus, Analyzing the complex networks becomes an intriguing research frontier.
With the rapid development of deep learning techniques, many scholars employ the deep learning architectures to tackle the graphs.
Graph Neural Networks (GNNs) emerge under these circumstances. Up to now, the GNNs have evolved into a prevalent and powerful computational framework for tackling irregular data such as graphs and manifolds.

The GNNs can learn task-specific node/edge/graph representations via hierarchical iterative operators so that the traditional machine learning methods can be employed
to perform graph-related learning tasks, e.g. node classification, graph classification, link prediction and clustering etc. Although the GNNs has attained substantial success over the graph-related learning tasks,
they still face great challenges. Firstly, the structural complexity of graphs incurs expensive computational cost on large graphs.
Secondly, perturbing the graph structure and/or initial features incurs sharp performance decay. Thirdly, the Wesfeiler-Leman (WL) graph isomorphism test impedes the performance improvement of the GNNs.
At last, the blackbox work mechanism of the GNNs hinders safely deploying them to real-world applications.

In this paper, we generalize the conventional deep architectures to the non-Euclidean domains,
and summarize the architectures, extensions and applications, benchmarks and evaluation pitfalls and future research directions of the graph neural networks.
Up to now, there have been several surveys on the GNNs. However, they usually discuss the GNN models from different angles and with different emphasises.
To the best of our knowledge, the first survey on the GNNs was conducted by Michael M. Bronstein et al\cite{1}.
Peng Cui et al\cite{2} reviewed different kinds of deep learning models applied to graphs from three aspects: semi-supervised learning methods including
graph convolutional neural networks, unsupervised learning methods including graph auto-encoders, and recent advancements including graph recurrent neural networks and graph reinforcement learning.
This survey laid emphasis on semi-supervised learning models, i.e. the spatial and spectral graph convolutional neural networks, yet comparatively less emphasis on the other two aspects.
Due to the space limit, this survey only listed a few of key applications of the GNNs, but ignored the diversity of the applications.
Maosong Sun et al\cite{3} provided a detailed review of the spectral and spatial graph convolutional neural networks from three aspects: graph types, propagation step and training method,
and divided its applications into three scenarios: structural scenarios, non-structural scenarios and other scenarios. However, this article did not involve the other GNN architectures
such as graph auto-encoders, graph recurrent neural networks and graph generative networks.
Philip S. Yu et al\cite{4} conducted a comprehensive survey on the graph neural networks, and investigated available datasets, open-source implementations and practical applications.
However, they only listed a few of core literatures on each research topic.
Davide Bacciu et al\cite{367} gives a gentle introduction to the field of deep learning for graph data. The goal of this article is to introduce the main concepts
and building blocks to construct neural networks for graph data, and therefore it falls short of an exposition of recent works on graph neural networks.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{GNN-Panorama.pdf}
  \caption{The architecture of this paper.}
  \label{GNN Panorama}
\end{figure}

It  is noted that all of the aforementioned surveys do not concern capability and interpretability of GNNs, combinations of the probabilistic inference and GNNs,
and adversarial attacks on graphs. In this article, we provide a panorama of GNNs for readers from 4 perspectives:
architectures, extensions and applications, benchmarks and evaluations pitfalls, future research directions,
as shown in Fig. \ref{GNN Panorama}. For the architectures of GNNs, we investigate the studies on graph convolutional neural networks (GCNNs), graph pooling operators, graph attention mechanisms and
graph recurrent neural networks (GRNNs). The extensions and applications demonstrate some notable research topics on the GNNs through integrating the above architectures.
Specifically, this perspective includes the capabilities and interpretability, deep graph representation learning, deep graph generative models, combinations of the Probabilistic Inference (PI) and the GNNs,
adversarial attacks for GNNs, Graph Neural Architecture Search and graph reinforcement learning and applications. In summary, our article provides a complete taxonomy for GNNs,
and comprehensively review the current advances and trends of the GNNs. These are our main differences from the aforementioned surveys.

\textbf{Contributions.} Our main contributions boils down to the following three-fold aspects.
\begin{enumerate}
  \item We propose a novel taxonomy for the GNNs, which has three levels. The first includes architectures, benchmarks and evaluation pitfalls, and applications. The architectures are classified into 9 categories, the benchmarks and evaluation pitfalls into 2 categories, and the applications into 10 categories. Furthermore, the graph convolutional neural networks, as a classic GNN architecture, are again classified into 6 categories.
  \item We provide a comprehensive review of the GNNs. All of the literatures fall into the corresponding categories. It is expected that the readers not only understand the panorama of the GNNs, but also comprehend the basic principles and various computation modules of the GNNs through reading this survey.
  \item We summarize four future research directions for the GNNs according to the current facing challenges, most of which are not mentioned the other surveys. It is expected that the research on the GNNs can progress into a new stage by overcoming these challenges.
\end{enumerate}

\textbf{Roadmap.} The remainder of this paper is organized as follows. First of all, we provide some basic notations and definitions that will be often used in the following sections.
Then, we start reviewing the GNNs from 4 aspects: architectures in section 3, extensions and applications in section 4, benchmarks and evaluation pitfalls in section 5
and future research directions in section 6. Finally, we conclude our paper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we introduce relevant notations so as to conveniently describe the graph neural network models.
A simple  graph can be denoted by $G=(V,E)$ where $V$ and $E$ respectively denote the set of $N$ nodes (or vertices) and $M$ edges.
Without loss of generality, let $V=\left\{v_1,\cdots,v_N\right\}$ and $E=\left\{e_1,\cdots,e_M\right\}$. Each edge $e_j\in E$ can be denoted by $e_j=\left(v_{s_j},v_{r_j}\right)$ where $v_{s_j}, v_{r_j}\in V$.
Let $A_G$ denote the adjacency matrix of $G$ where $A_G(s,r)=1$ iff there is an edge between $v_s$ and $v_r$. If $G$ is edge-weighted, $A_G(s,r)$ equals the weight value of the edge $\left(v_s,v_r\right)$.
If $G$ is directed, $\left(v_{s_j},v_{r_j}\right)\neq\left(v_{r_j}, v_{s_j}\right)$ and therefore $A_G$ is asymmetric.
A directed edge $e_j=\left(v_{s_j},v_{r_j}\right)$ is also called an arch, i.e. $e_j=\left\langle v_{s_j},v_{s_j}\right\rangle$.
Otherwise $\left(v_{s_j},v_{r_j}\right)=\left(v_{r_j}, v_{s_j}\right)$ and $A_G$ is symmetric. For a node $v_s\in V$, let $N_G(v_s)$ denote the set of neighbors of $v_s$, and $d_G(v_s)$ denote the degree of $v_s$.
If $G$ is directed, let $N_G^+(v_s)$ and $N_G^-(v_s)$ respectively denote the incoming and outgoing neighbors of $v_s$, and $d_G^+(v_s)$ and $d_G^-(v_s)$ respectively denote the incoming and outgoing degree of $v_s$.
Given a vector $a=(a_1,\cdots,a_N)\in\mathbb{R}^N$, $\text{diag}(a)$ (or $\text{diag}(a_1,\cdots,a_N)$) denotes a diagonal matrix consisting of the elements $a_n, n=1,\cdots,N$.

A vector $x\in\mathbb{R}^{N}$ is called a 1-dimensional graph signal on $G$. Similarly, $X\in\mathbb{R}^{N\times d}$ is called a $d\text{-dimensiaonl}$ graph signal on $G$.
In fact, $X$ is also called a feature matrix of nodes on $G$. Without loss of generality, let $X[j,k]$ denote the $(j,k)\text{-th}$ entry of the matrix $X\in\mathbb{R}^{N\times d}$,
$X[j,:]\in\mathbb{R}^d$ denote the feature vector of the node $v_j$ and $X[:,j]$ denote the $1\text{-dimensional}$ graph signal on $G$.
Let $\mathbb{I}_N$ denote a $N\times N$ identity matrix. For undirected graphs, $L_G=D_G-A_G$ is called the Laplacian matrix of $G$,
where $D_G[r,r]=\sum_{c=1}^N A_G[r,c]$. For a 1-dimensional graph signal $x$, its smoothness $s(x)$ is defined as
\begin{equation}\label{SmoothnessOfGraphSignal}
s(x)=x^TL_Gx=\frac{1}{2}\sum_{r,c=1}^NA_G(r,c)\left(x[r]-x[c]\right)^2.
\end{equation}
The normalization of $L_G$ is defined by $\overline{L}_G=\mathbb{I}_N-D_G^{-\frac{1}{2}}A_GD_G^{-\frac{1}{2}}$.
$\overline{L}_G$ is a real symmetric semi-positive definite matrix. So, it has $N$ ordered real non-negative eigenvalues $\left\{\lambda_n:n=1,\cdots,N\right\}$
and corresponding orthonormal eigenvectors $\left\{u_n:n=1,\cdots,N\right\}$, namely $\overline{L}_G=U\Lambda U^T$ where $\Lambda=\text{diag}(\lambda_1,\cdots,\lambda_N)$
and $U=\left(u_1,\cdots,u_N\right)$ denotes a orthonormomal matrix. Without loss of generality, $0=\lambda_1\leq\lambda_2\leq\cdots\leq\lambda_N=\lambda_{\text{max}}$.
The eigenvectors $u_n, n=1,\cdots,N$ are also called the graph Fourier bases of $G$. Obviously, the graph Fourier basis are also the 1-dimensional graph signal on $G$.
The graph Fourier transform\cite{5} for a given graph signal $x$ can be denoted by
\begin{equation}\label{GraphFourierTransform}
\hat{x}\triangleq\mathcal{F}(x)=U^Tx.
\end{equation}
The inverse graph Fourier transform can be correspondingly denoted by
\begin{equation}\label{InverseGraphFourierTransform}
x\triangleq\mathcal{F}^{-1}(\hat{x})=U\hat{x}.
\end{equation}
Note that the eigenvalue $\lambda_n$ actually measures the smoothness of the graph Fourier mode $u_n$.
Throughout this paper, let $\rho(\cdot)$ denote an activation function, $\bowtie$ denote the concatenation of at least two vectors, and $\left<\right>$ denote the inner product of two vectors/matrices.
We somewhere use the function $\text{Concat}(\cdot)$ to denote the concatenation of two vectors as well.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Architectures}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Graph Convolutional Neural Networks (GCNNs)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The GCNNs play pivotal roles on tackling the irregular data (e.g. graph and manifold). They are motivated by the Convolutional Neural Networks (CNNs) to learn hierarchical representations of irregular data.
There have been some efforts to generalize the CNN to graphs \cite{27,31,48}. However, they are usually computationally expensive and cannot capture spectral or spatial features.
Below, we introduce the GCNNs from the next 6 aspects: spectral GCNNs, spatial GCNNs, Graph wavelet neural networks and GCNNs on special graphs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Spectral Graph Convolution Operators}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[htb]
  \centering
  \includegraphics[width=1.0\textwidth]{Spectral-GCNN.pdf}
  \caption{Computational framework of the spectral GCNN.}
  \label{Spectral GCNN}
\end{figure}

The spectral graph convolution operator is defined via the graph Fourier transform. For two graph signals $x$ and $y$ on $G$,
their spectral graph convolution $x\ast_Gy$ is defined by
\begin{equation}\label{Spectral Graph Convolution}
\begin{array}{rcl}
x\ast_G y & = & \mathcal{F}^{-1}(\mathcal{F}(x)\circledast\mathcal{F}(y)) \\ [2mm]
          & = & U\left(U^Tx\circledast U^Ty\right) \\[2mm]
          & = & U\text{diag}(U^Ty)U^Tx,
\end{array}
\end{equation}
where $\circledast$ denotes the element-wise Hadamard product \cite{7,18,27}.
The spectral graph convolution can be rewritten as
\begin{displaymath}
x\ast_G f_{\theta}=Uf_{\theta}U^Tx,
\end{displaymath}
where $f_{\theta}$ is a diagonal matrix consisting of the learnable parameters.
That is, the signal $x$ is filtered by the spectral graph filter (or graph convolution kernel) $f_{\theta}$.
For a $d^{(l)}\text{-dimensional}$ graph signal $X^{(l)}$ on $G$,
the output $X^{(l+1)}$ yielded by a graph convolution layer, namely $d^{(l+1)}\text{-dimensional}$ graph signal on $G$,
can be written as
\begin{equation}
\label{Spectral GCNN Formula}
X^{(l+1)}[:,k]=\rho\left(\sum_{j=1}^{d^{(l)}}Uf_{\theta,j,k}^{(l)}U^TX^{(l)}[:,j]\right),
\end{equation}
where $f_{\theta,j,k}^{(l)}$ is a spectral graph filter, i.e. a $N\times N$ diagonal matrix consisting of learnable parameters corresponding to the $j\text{-th}$ graph signal
at $l\text{-th}$ layer and the $k\text{-th}$ graph signal at $(l+1)\text{-th}$ layer.
The computational framework of the spectral GCNN in Eq. (\ref{Spectral GCNN Formula}) is demonstrated in Fig. \ref{Spectral GCNN}.
It is worth noting that the calculation of the above graph convolution layer takes $O(N^3)$ time and $O(N^2)$ space
to perform the eigendecomposition of $\overline{L}_G$ especially for large graphs.
The article \cite{179} proposes a regularization technique, namely GraphMix, to augment the vanilla GCNN with a parameter-sharing Fully Connected Network (FCN).

\textbf{Spectral Graph Filter.} Many studies \cite{27} focus on designing different spectral graph filters.
In order to circumvent the eigendecomposition, the spectral graph filter $f_{\theta}$ can formulated as a $K\text{-localized}$ polynomial of the eigenvalues
of the normalized graph Laplacian $\overline{L}_G$ \cite{6,8,78},
i.e.
\begin{equation}\label{Chebyshev Spectral Filter}
f_{\theta}=f_{\theta}(\Lambda)\triangleq\sum_{k=0}^{K-1}\theta_k\Lambda^k.
\end{equation}
In practice, the $K\text{-localized}$ Chebyshev polynomial \cite{78} is a favorable choice of formulating the spectral graph filter, i.e.
\begin{displaymath}
f_{\theta}(\Lambda)=\sum_{k=0}^{K-1}\theta_kT_k(\widetilde{\Lambda}),
\end{displaymath}
where the Chebyshev polynomial is defined as
\begin{equation}\label{Chebeshev Polynomial}
T_0(x)=1,\quad T_1(x)=x,\quad T_k(x)=2xT_{k-1}(x)-T_{k-2}(x)
\end{equation}
and $\widetilde{\Lambda}=\frac{2}{\lambda_{\text{max}}}\Lambda-\mathbb{I}_N$.
The reason why $\widetilde{\Lambda}=\frac{2}{\lambda_{\text{max}}}\Lambda-\mathbb{I}_N$ is because it can map eigenvalues $\lambda\in\left[0,\lambda_{\text{max}}\right]$ into $[-1,1]$.
This filter is $K\text{-localized}$ in the sense that it leverages information from nodes which are at most $K\text{-hops}$ away.
In order to further decrease the computational cost, the $1\text{st-order}$ Chebyshev polynomial is used to define the spectral graph filter. Specifically, it lets $\lambda_{\text{max}}\approx 2$
(because the largest eigenvalue of $\overline{L}_G$ is less than or equal to 2 \cite{374}) and $\theta=\theta_0=-\theta_1$.
Moreover, the renormalization trick is used here to mitigate the limitations of the vanishing/exploding gradient,
namely substituting $\widetilde{D}_G^{-\frac{1}{2}}\widetilde{A}_G\widetilde{D}_G^{-\frac{1}{2}}$ for $\mathbb{I}_N+D_G^{-\frac{1}{2}}A_GD_G^{-\frac{1}{2}}$ where $\widetilde{A}_G=A_G+\mathbb{I}_N$
and $\widetilde{D}_G=\text{diag}\left(\sum_{k=1}^{N}\widetilde{A}[1,k],\cdots,\sum_{k=1}^{N}\widetilde{A}[N,k]\right)$.
As a result, the Graph Convolutional Network (GCN) \cite{6,375} can be defined as
\begin{equation}\label{GCN-Layer}
X^{(l+1)}=\rho\left(\widetilde{D}_G^{-\frac{1}{2}}\widetilde{A}_G\widetilde{D}_G^{-\frac{1}{2}}X^{(l)}\Theta^{(l)}\right).
\end{equation}
The Chebyshev spectral graph filter suffers from a drawback that the spectrum of $\overline{L}_G$ is linearly mapped into $\left[-1,1\right]$.
This drawback makes it hard to specialize in the low frequency bands.
In order to mitigate this problem, Michael M. Bronstein et al \cite{12} proposes the Cayley spectral graph filter
via the $\text{order-}r$ Cayley polynomial $f_{c,h}(\lambda)=c_0+2\text{Re}\left(\sum_{j=0}^{r}c_h\mathcal{C}(\lambda)^j\right)$ with
the Cayley transform $\mathcal{C}(\lambda)=\frac{\lambda-i}{\lambda+i}$.
Moreover, there are many other spectral graph filters, e.g. \cite{8,14,19,22,30,37,38,39,44,90}.
In addition, some studies employ the capsule network \cite{400} to construct capsule-inspired GNNs \cite{52,53,54}.

\textbf{Overcoming Time and Memory Challenges.} A chief challenge for GCNNs is that their training cost is strikingly expensive, especially on huge and sparse graphs.
The reason is that the GCNNs require full expansion of neighborhoods for the feed-forward computation of each node,
and large memory space for storing intermediate results and outputs.
In general, two approaches, namely sampling \cite{9,10,11,25} and decomposition \cite{23,24}, can be employed to mitigate the time and memory challenges for the spectral GCNNs.

\textbf{Depth Trap of Spectral GCNNs.} A bottleneck of GCNNs is that their performance maybe decease with ever-increasing number of layer.
This decay is often attributed to three factors: (1) overfitting resulting from the ever-increasing number of parameters;
(2) gradient vanishing/explosion during training; (3) oversmoothing making vertices from different clusters more and more indistinguishable.
The reason for oversmoothing is that performing the Laplacian smoothing many times forces the features of vertices
within the same connected component to stuck in stationary points \cite{16}.
There are some available approaches, e.g. \cite{33,373,379,398,399}, to circumvent the depth trap of the spectral GCNNs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Spatial Graph Convolution Operators}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Original spatial GCNNs \cite{47,76,77,380} constitutes a transition function, which must be a contraction map in order to ensure the uniqueness of states, and an update function.
In the following, we firstly introduce a generic framework of the spatial GCNN, and then investigate its variants.

Graph networks (GNs) as generic architectures with relational inductive bias \cite{85} provide an elegant interface
for learning entities, relations and structured knowledge.
Specifically, GNs are composed of GN blocks in a sequential, encode-process-decode or recurrent manner. GN blocks contain three kinds of update functions,
namely $\phi^{e}(\cdot),\phi^{v}(\cdot),\phi^{u}(\cdot)$, and three kinds of aggregation functions, namely $\psi^{e\rightarrow v}(\cdot), \psi^{e\rightarrow u}(\cdot), \psi^{v\rightarrow u}(\cdot)$.
The iterations are described as follows.
\begin{equation}\label{GN-BLOCK}
\begin{array}{lll}
e'_k=\phi^{e}\left(e_k,v_{r_k},v_{s_k},u\right), & \bar{e}'_i=\psi^{e\rightarrow v}\left(E'_i\right), & v'_i=\phi^{v}\left(v_i,\bar{e}'_i,u\right), \\[2mm]
\bar{e}'=\psi^{e\rightarrow u}\left(E'\right), & u'=\phi^{u}\left(u,\bar{e}',\bar{v}'\right), & \bar{v}'=\psi^{v\rightarrow u}\left(V'\right)
\end{array}
\end{equation}
where $e_k$ is an arch from $v_{s_k}$ to $v_{r_k}$, $E'_i=\left\{(e'_k,s_k,r_k):r_k=i,k=1,\cdots,M\right\}$, $V'=\left\{v'_i:i=1,\cdots,N\right\}$ and $E'=\left\{(e'_k,s_k,r_k):k=1,\cdots,M\right\}$,
see Fig. \ref{Spatial GCNN}. It is noted that the aggregation functions should be invariant to any permutations of nodes or edges.
In practice, the GN framework can be used to implement a wide variety of architectures in accordance with three key design principles,
namely flexible representations, configuable within-block structure and flexible multi-block architectures.
Below, we introduce three prevalent variants of the GNs, namely Message Passing Neural Networks (MPNNs) \cite{93}, Non-local Neural Networks (NLNNs) \cite{102} and GraphSAGE \cite{104}.

\begin{figure}[htb]
  \centering
  \includegraphics[width=1.0\textwidth]{Spatial-GCNN.pdf}
  \caption{Computational framework of the spatial GCNN.}
  \label{Spatial GCNN}
\end{figure}

\textbf{Variants of GNs------MPNNs.} MPNNs \cite{93} have two phases, a message passing phase and a readout phase. The message passing phase is defined by a message function $M_l$
(playing the role of the composition of the update function $\psi^{e\rightarrow v}(\cdot)$ and the update function $\phi^{e}(\cdot)$)
and a vertex update function $U_l$ (playing the role of the update function $\phi^{v}(\cdot)$). Specifically,
\begin{displaymath}
m_v^{(l+1)}=\displaystyle\sum_{u\in N_G(v)}M_l\left(x_v^{(l)},x_u^{(l)},e_{v,u}\right),\quad x_v^{(l+1)}=U_l\left(x_v^{(l)},m_v^{(l+1)}\right)
\end{displaymath}
where $e_{v,u}$ denotes the feature vector of the edge with two endpoints $v$ and $u$.
The readout phase computes a universal feature vector for the whole graph using a readout function $R(\cdot)$, i.e. $u=R\left(\left\{x_v^{(L)}:v\in V\right\}\right)$.
The readout function $R(\cdot)$ should be invariant to permutations of nodes.
A lot of GCNNs can be regarded as special forms of the MPNN, e.g. \cite{103,122,336,195}.

\textbf{Variants of GNs------NLNNs.} NLNNs \cite{102} give a general definition of non-local operations \cite{381} which is a flexible building block and can be easily integrated into convolutional/recurrent layers.
Specifically, the generic non-local operation is defined as
\begin{equation}\label{NLNN}
y_s=\frac{1}{\mathcal{C}(x_s)}\sum_{t}f(x_s,x_t)g(x_t),
\end{equation}
where $f(\cdot,\cdot)$ denotes the affinity between $x_s$ and $x_t$, and $\mathcal{C}(x_s)=\sum_{t}f(x_s,x_t)$ is a normalization factor.
The affinity function $f(\cdot,\cdot)$ is of the following form
\begin{enumerate}
  \item Gaussian: $f(x_s,x_t)=e^{x_s^Tx_t}$;
  \item Embedded Gaussian: $f(x_s,x_t)=e^{\theta(x_s)^T\eta(x_t)}$, where $\theta(x_s)=W_{\theta}x_s$ and $\eta(x_t)=W_{\eta}x_t$;
  \item Dot Product: $f(x_s,x_t)=\theta(x_s)^T\eta(x_t)$;
  \item Concatenation: $f(x_s,x_t)=\text{ReLU}(w_f^T\left[\theta(x_s),\eta(x_t)\right])$.
\end{enumerate}
The non-local building block is defined as $z_s=W_zy_s+x_s$ where "$+x_s$" denotes a residual connection.
It is noted that $f(\cdot,\cdot)$ and $g(\cdot)$ play the role of $\phi^e(e_k,v_{r_k},v_{s_k},u)$, and the summation in Eq. (\ref{NLNN}) plays the role of $\psi^{e\rightarrow v}(E'_i)$.

\textbf{Variants of GNs------GraphSAGE.} GraphSAGE (\textsc{SAmple} and \textsc{aggreGatE}) \cite{104} is a general inductive framework capitalizing on node feature information to efficiently generate node embedding vectors for previously unseen nodes.
Specifically, GraphSAGE is composed of an aggregation function $\text{\textsc{Aggregate}}^{(l)}(\cdot)$ and an update function $\text{\textsc{Update}}^{(l)}$, i.e.
\begin{displaymath}
\begin{array}{l}
x_{N_G(v)}^{(l)}=\text{\textsc{Aggregate}}^{(l)}\left(\left\{x_u^{(l-1)}: u\in N_G(v)\right\}\right) \\[5mm]
x_v^{(l)}=\text{\textsc{Update}}^{(l)}\left(\left\{x_v^{(l-1)},x_{N_G(v)}^{(l)}\right\}\right)
\end{array}
\end{displaymath}
where $N_G(v)$ denotes a fixed-size set of neighbors of $v$ uniformly sampling from its whole neighbors.
The aggregation function is of the following form
\begin{enumerate}
  \item Mean Aggregator: $x_v^{(l)}=\sigma\left(W\cdot\text{\textsc{Mean}}\left(\left\{x_v^{(l-1)}\right\}\cup\left\{x_u^{(l-1)}:u\in N_G(v)\right\}\right)\right)$;
  \item LSTM Aggregator: applying the LSTM \cite{160} to aggregate the neighbors of $v$;
  \item Pooling Aggregator: $\text{\textsc{Aggregate}}^{(l)}=\max\left(\left\{\sigma\left(Wx_u^{(l)}+b\right): u\in N_G(v)\right\}\right)$.
\end{enumerate}
Note that the aggregation function and update function play the role of $\psi^{e\rightarrow v}(E'_i)$ and $\phi^{v}(v_i,\overline{e}'_i,u)$ in formula (\ref{GN-BLOCK}) respectively.

\textbf{Variants of GNs------Hyperbolic GCNNs.} The Euclidean GCNNs aim to embed nodes in a graph into a Euclidean space. This will incur a large distortion
especially when embedding real-world graphs with scale-free and hierarchical structure. Hyperbolic GCNNs pave an alternative way of embedding with little distortion.
The $n\text{-dimensional}$ hyperbolic space \cite{69,193}, denoted as $\mathbb{H}_K^n$, is a unique, complete, simply connected $d\text{-dimensional}$ Riemannian manifold
with constant negative sectional curvature $-\frac{1}{K}$, i.e.
\begin{displaymath}
\mathbb{H}_K^n=\left\{x\in\mathbb{R}^{n+1}:\langle x,x\rangle_{\mathcal{M}}=-K,x_0>0\right\},
\end{displaymath}
where the Minkowski inner produce $\langle x,y\rangle_{\mathcal{M}}=-x_0y_0+\sum_{j=1}^dx_jy_j,\forall x,y\in\mathbb{R}^{n+1}$.
Its tangent space centered at point $x$ is denoted as $\mathcal{T}_x\mathbb{H}_K^n=\left\{v\in\mathbb{R}^{n+1}:\langle x,v\rangle_{\mathcal{M}}=0\right\}$.
Given $x\in\mathbb{H}_K^n$, let $u\in\mathcal{T}_x\mathbb{H}_K^n$ be unit-speed. The unique unit-speed geodesic $\gamma_{x\rightarrow u}(\cdot)$ such that $\gamma_{x\rightarrow u}(0)=x$
and $\dot{\gamma}_{x\rightarrow u}(0)=u$ is denoted as $\gamma_{x\rightarrow u}(t)=\cosh\left(\frac{t}{\sqrt{K}}\right)x+\sqrt{K}\sinh\left(\frac{t}{\sqrt{K}}\right), u, t>0.$
The intrinsic distance between two points $x,y\in\mathbb{H}_K^n$ is then equal to
\begin{displaymath}
d_{\mathcal{M}}^K\left(x,y\right)=\sqrt{K}\text{arcosh}\left(-\frac{\langle x,y\rangle_{\mathcal{M}}}{K}\right).
\end{displaymath}
Therefore, the above $n\text{-dimensional}$ hyperbolic space with constant negative sectional curvature $-\frac{1}{K}$ is usually denoted as $\left(\mathbb{H}_K^n,d_{\mathcal{M}}^K(\cdot,\cdot)\right)$.
In particular, $\mathbb{H}_1^n$, i.e. $K=1$, is called the hyperboloid model of the hyperbolic space.
Hyperbolic Graph Convolutional Networks (HGCN) \cite{15} benefit from the expressiveness of both GCNNs and hyperbolic embedding.
It employs the exponential and logarithmic maps of the hyperboloid model, respectively denoted as $\exp_x^K(\cdot)$ and $\log_x^K(\cdot)$, to realize the mutual transformation
between Euclidean features and hyperbolic ones. Let $\|v\|_{\mathcal{M}}=\langle v,v\rangle_{\mathcal{M}},v\in\mathcal{T}_x\mathbb{H}_K^n$. The $\exp_x^K(\cdot)$ and $\log_x^K(\cdot)$ are respectively defined to be
\begin{displaymath}
\begin{array}{l}
\exp_x^K(v)=\cosh\left(\frac{\|v\|_{\mathcal{M}}}{\sqrt{K}}\right)x+\sqrt{K}\sinh\left(\frac{\|v\|_{\mathcal{M}}}{\sqrt{K}}\right)\frac{v}{\|v\|_{\mathcal{M}}} \\[2mm]
\log_x^{K}(y)=d_{\mathcal{M}}^K(x,y)\dfrac{y+\frac{1}{K}\langle x,y\rangle_{\mathcal{M}}x}{\left\|y+\frac{1}{K}\langle x,y\rangle_{\mathcal{M}}x\right\|_{\mathcal{M}}},
\end{array}
\end{displaymath}
where $x\in\mathbb{H}_K^n$, $v\in\mathcal{T}_x\mathbb{H}_K^n$ and $y\in\mathbb{H}_K^n$ such that $y\neq 0$ and $y\neq x$.
The HGCN architecture is composed of three components: a Hyperbolic Feature Transform (HFT), an Attention-Based Aggregation (ABA) and a Non-Linear Activation with Different Curvatures (NLADC).
They are respectively defined as
\begin{displaymath}
\begin{array}{lr}
h_j^{H,l}=\left(W^{(l)}\otimes^{K_{l-1}}x_j^{H,l-1}\right)\oplus^{K_{l-1}}b^{(l)}    &\quad \text{(HFT)}, \\[2mm]
y_j^{H,l}=\text{\textsc{Aggregate}}^{K_{l-1}}(h^{H,l})_j                            &\quad \text{(ABA)}, \\[2mm]
x_j^{H,l}=\exp_o^{K_l}\left(\rho\left(\log_o^{K_{l-1}}\left(y_j^{H,l}\right)\right)\right) &\quad \text{(NLADC)},
\end{array}
\end{displaymath}
where $o=\left(\sqrt{K},0,\cdots,0\right)\in\mathbb{H}_K^n$, the subscript $j$ denotes the indices of nodes, the superscript $l$ denotes the layer of the HGCN.
The linear transform in hyperboloid manifold is defined to be $W\otimes^K x^H=\exp_o^K\left(W\log_o^K(x^H)\right)$ and $x^H\oplus^K b=\exp_{x^H}^K\left(P_{o\rightarrow x^H}^K(b)\right)$,
where $P_{o\rightarrow x^H}^K(b)$ is the parallel transport from $\mathcal{T}_o\mathbb{H}_K^n$ to $\mathcal{T}_{x^H}\mathbb{H}_K^{n}$.
The attention-based aggregation is defined to be $\text{\textsc{Aggregate}}^K(x^H)_j=\exp_{x_j^H}^K\left(\sum_{k\in N_G(j)}\omega_{j,k}\log_{x_j^H}^K\left(x_k^H\right)\right)$,
where the attention weight $\omega_{j,k}=\text{Softmax}_{k\in N_G(j)}\left(\text{MLP}\left(\log_o^K\left(x_j^H\right)\bowtie\log_o^K\left(x_k^H\right)\right)\right)$.

\textbf{Higher-Order Spatial GCNNs.} the aforementioned GCNN architectures are constructed from the microscopic perspective.
They only consider nodes and edges, yet overlook the higher-order substructures and their connections, i.e subgraphs consisting of at least 3 nodes.
Here, we introduce the studies on the $k\text{-dimensional}$ GCNNs \cite{88}. Specifically, they take higher-order graph structures at multiple scales into consideration by leveraging the $k\text{-Weisfeiler-Leman}$
($k\text{-WL}$) graph isomorphism test so that the message passing is performed directly between subgraph structures rather than individual nodes. Let $\{\hspace{-1.2mm}\{\cdots\}\hspace{-1.2mm}\}$
denote a multiset, $\text{\textsc{Hash}}(\cdot)$ a hashing function and $C_{l,k}^{(l)}(s)$ the node coloring (label) of $s=(s_1,\cdots,s_k)\in V^k$ at the $l\text{-th}$ time. Moreover, let $N_G^j(s)=\left\{(s_1,\cdots,s_{j-1},r,s_{j+1},\cdots,s_k):r\in V\right\}$.
The $k\text{-WL}$ is computed by
\begin{displaymath}
C_{l,k}^{(l+1)}(s)=\text{\textsc{Hash}}\left(C_{l,k}^{(l)}(s),\left(c_1^{(l+1)}(s),\cdots,c_k^{(l+1)}(s)\right)\right),
\end{displaymath}
where $c_{j}^{(l+1)}=\text{\textsc{Hash}}\left(\left\{\hspace{-1.2mm}\left\{C_{l,k}^{(l)}(s'):s'\in N_G^j(s)\right\}\hspace{-1.2mm}\right\}\right)$.
The $k\text{-GCNN}$ computes new features of $s\in V^k$ by multiple computational layers. Each layer is computed by
\begin{displaymath}
X_k^{(l+1)}[s,:]=\rho\left(X_k^{(l)}[s,:]W_1^{(l)}+\sum_{t\in N_G(s)}X_k^{(l)}[t,:]W_2^{(l)}\right).
\end{displaymath}
In practice, the local $k\text{-GCNNs}$ is often employed to learn the hierarchical representations of nodes in order to scale to larger graphs and mitigate the overfitting problem.

\textbf{Other Variants of GNs.} In addition to the aforementioned GNs and its variants, there are still many other spatial GCNNs which is defined from other perspectives, e.g.
Diffusion-Convolutional Neural Network (DCNN) \cite{20}, Position-aware Graph Neural Network (P-GNN) \cite{46}, Memory-based Graph Neural Network (MemGNN) and Graph Memory Network (GMN) \cite{84},
Graph Partition Neural Network (GPNN) \cite{35}, Edge-Conditioned Convolution (ECC) \cite{43}, DEMO-Net \cite{386}, Column network \cite{29}, Graph-CNN \cite{32}.

\textbf{Invariance and Equivariance.} Permutation-invariance refers to that a function $f:\mathbb{R}^{n^k}\rightarrow\mathbb{R}$ (e.g. the aggregation function)
is independent of any permutations of node/edge indices \cite{50,389}, i.e. $f(P^TA_GP)=f(A_G)$ where $P$ is a permutation matrix and $A_G\in\mathbb{R}^{n^k}$ is
a $k\text{-order}$ tensor of edges or multi-edges in the (hyper-)graph $G$. Permutation-equivariance refers to that a function $f:\mathbb{R}^{n^k}\rightarrow\mathbb{R}^{n^l}$
coincides with permutations of node/edge indices \cite{50,389}, i.e. $f(P^TA_GP)=P^Tf(A_G)P$ where $P$ and $A_G$ are defined as similarly as the permutation-invariance.
For permutation-invariant aggregation functions, a straightforward choice is to take $\text{sum}/\max/\text{average}/\text{concatenation}$ as heuristic aggregation schemes \cite{50}. Nevertheless,
these aggregation functions treat all the neighbors of a vertex equivalently so that they cannot precisely distinguish the structural effects of different neighbors to the target vertex.
That is, the aggregation functions should extract and filter graph signals aggregated from neighbors of different hops away and different importance.
GeniePath \cite{34} proposes a scalable approach for learning adaptive receptive fields of GCNNs. It is composed of two complementary functions,
namely adaptive breadth function and adaptive depth function. The former learns the importance of different sized neighborhoods,
whereas the latter extracts and filters graph signals aggregated from neighbors of different hops away. More specifically, the adaptive breadth function is defined as follows.
\begin{displaymath}
h_{v_j}^{\text{temp}}=\tanh\left((W^{(t)})^T\sum_{v_k\in N_G(v_j)\cup\{v_j\}}\alpha(h_{v_j}^{(t)},h_{v_k}^{(t)})\cdot h_{v_k}^{(t)}\right),
\end{displaymath}
where $\alpha(x,y)=\text{Softmax}_y\left(\alpha^T\tanh\left(W_x^Tx+W_y^Ty\right)\right)$.
The adaptive depth function is defined as a LSTM \cite{160}, i.e.
\begin{equation}\label{LSTM Formulation}
\begin{array}{ll}
i_{v_j}=\sigma\left(\left(W_i^{(t)}\right)^Th_{v_j}^{\text{temp}}\right) & \qquad f_{v_j}=\sigma\left(\left(W_f^{(t)}\right)^Th_{v_j}^{\text{temp}}\right)     \\
o_{v_j}=\sigma\left(\left(W_o^{(t)}\right)^Th_{v_j}^{temp}\right)        & \qquad \widetilde{C}_{v_j}=\tanh\left(\left(W_c^{(t)}\right)^Th_{v_j}^{(temp)}\right) \\
C_{v_j}^{(t+1)}=f_{v_j}\circledast C_{v_j}^{(t)}+i_{v_j}\circledast\widetilde{C}_{v_j} & \qquad h_{v_j}^{(t+1)}=o_{v_j}\circledast\tanh\left(C_{v_j}^{(t+1)}\right).
\end{array}
\end{equation}

\textsc{Geom-GCN} \cite{17} proposes a novel permutation-invariant geometric aggregation scheme consisting of three modules,
namely node embedding, structural neighborhood, and bi-level aggregation. This aggregation scheme does not lose structural information of nodes and fail to capture long-range dependencies in disassortative graphs.
For the permutation-invariant graph representations, PiNet \cite{41} proposes an end-to-end spatial GCNN architecture that utilizes the permutation equivariance of graph convolutions.
It is composed of a pair of double-stacked message passing layers, namely attention-oriented message passing layers and feature-oriented message passing layers.

\textbf{Depth Trap of Spatial GCNNs.} Similar to the spectral GCNNs, the spatial GCNNs is also confronted with the depth trap.
As stated previously, the depth trap results from oversmoothing, overfitting and gradient vanishing/explosion.
In order to escape from the depth trap, some studies propose some available strategies, e.g. DeepGCN \cite{13} and Jumping Knowledge Network \cite{89}.
The jumping knowledge networks \cite{89} adopt neighborhood aggregation with skip connections to integrate information from different layers.
The DeepGCN \cite{13} apply the residual/dense connections \cite{382,383} and dilated aggreagation \cite{384} in the CNNs to construct the spatial GCNN architecture.
They has three instantiations, namely ResGCN, DenseGCN and dilated graph convolution.
ResGCN is inspired by the ResNet \cite{382}, which is defined to be
\begin{displaymath}
\begin{array}{lcl}
G^{(l+1)} & \triangleq & \mathcal{H}(G^{(l)},\Theta^{(l)}) \\ [2mm]
          & = & \mathcal{F}(G^{(l)},\Theta^{(l)})+G^{(l)},
\end{array}
\end{displaymath}
where $\mathcal{F}(\cdot,\cdot)$ can be computed by spectral or spatial GCNNs.
DenseGCN collectively exploit information from different GCNN layers like the DenseNet \cite{383}, which is defined to be
\begin{displaymath}
\begin{array}{lcl}
G^{(l+1)} & \triangleq & \mathcal{H}(G^{(l)},\Theta^{(l)}) \\ [2mm]
          & =          & \text{Concat}\left(\mathcal{F}(G^{(l)},\Theta^{(l)}),G^{(l)}\right) \\ [2mm]
          & =          & \text{Concat}\left(\mathcal{F}(G^{(l)},\Theta^{(l)}),\cdots,\mathcal{F}(G^{(0)},\Theta^{(0)}),G^{(0)}\right).
\end{array}
\end{displaymath}
The dilated aggregation \cite{384} can magnify the receptive field of spatial GCNNs by a dilation rate $d$. More specifically, let $N_G^{(k,d)}(v)$ denote the set of $k$ $d\text{-dilated}$ neighbors of vertex $v$ in $G$.
If $\left(u_1,u_2,\cdots,u_{k\times d}\right)$ are the first sorted $k\times d$ nearest neighbors, then $N_G^{(k,d)}(v)=\left\{u_1,u_{1+d},\cdots,u_{1+(k-1)d}\right\}$.
Thereby, we can construct a new graph $G^{(k,d)}=\left(V^{(k,d)},E^{(k,d)}\right)$ where $V^{(k,d)}=V$ and $E^{(k,d)}=\left\{\left\langle v,u\right\rangle:v\in V,u\in N_G^{(k,d)}\right\}$.
The dilated graph convolution layer can be obtained by running the spatial GCNNs over $G^{(k,d)}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Graph Wavelet Neural Networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As stated previously, the spectral and spatial GCNNs are respectively inspired by the graph Fourier transform and message-passing mechanism.
Here, we introduce a new GCNN architecture from the perspective of the Spectral Graph Wavelet Transform (SGWT) \cite{189}.
First of all, the SGWT is determined by a graph wavelet generating kernel $g:\mathbb{R}^+\rightarrow\mathbb{R}^+$ with the property $g(0)=0,g(+\infty)=\lim_{x\rightarrow\infty}g(x)=0$.
A feasible instance of $g(\cdot)$ is parameterized by two integers $\alpha$ and $\beta$, and two positive real numbers $x_1$ and $x_2$ determining the transition regions, i.e.
\begin{displaymath}
g(x;\alpha,\beta,x_1,x_2)=\left\{
\begin{array}{ll}
x_1^{-\alpha}x^{\alpha} & x<x_1 \\
s(x)                    & x_1\leq x\leq x_2 \\
x^{-\beta}x_2^{\beta}   & x>x_2,
\end{array}
\right.
\end{displaymath}
where $s(x)$ is a cubic polynomial whose coefficients can be determined by the continuity constraints $s(x_1)=s(x_2)=1$, $s'(x_1)=\frac{\alpha}{x_1}$ and $s'(x_2)=-\frac{\beta}{x_2}$.
Given the graph wavelet generating kernel $g(\cdot)$ and a scaling parameter $s\in\mathbb{R}^+$, the spectral graph wavelet operator $\Psi_g^s$ is defined to be $\Psi_g^s=Ug(s\Lambda)U^T$
where $g(s\Lambda)=g\left(\text{diag}\left(s\lambda_1,\cdots,s\lambda_N\right)\right)$.
A graph signal $x\in\mathbb{R}^N$ on $G$ can thereby be filtered by the spectral graph wavelet operator, i.e. $\mathcal{W}_{g,s}^x=\Psi_g^sx\in\mathbb{R}^N$.
The literature \cite{190} utilizes a special instance of the graph wavelet operator $\Psi_g^s$ to construct a graph scattering network,
and proves its covariance and approximate invariance to permutations and stability to graph operations.

\textbf{Graph Wavelet Neural Networks.} The above spectral graph wavelet operator $\Psi_g^s$ can be employed to construct a Graph Wavelet Neural Network (GWNN) \cite{191}.
Let $\Psi_g^{-s}\triangleq\left(\Psi_g^s\right)^{-1}$. The graph wavelet based convolution is defined to be
\begin{displaymath}
x*_Gy=\Psi_g^{-s}\left(\Psi_g^sx\circledast\Psi_g^sy\right).
\end{displaymath}
The GWNN is composed of multiple layers of the graph wavelet based convolution. The structure of the $l\text{-th}$ layer is defined as
\begin{equation}\label{Graph Wavelet based Convolution in vector form}
X^{(l+1)}[:,j]=\rho\left(\sum_{k=1}^{d^{(l)}}\Psi_g^{-s}\Theta_{j,k}^{(l)}\Psi_g^sX^{(l)}[:,k]\right),
\end{equation}
where $\Theta_{j,k}^{(l)}$ is a diagonal filter matrix learned in spectral domain.
Eq. (\ref{Graph Wavelet based Convolution in vector form}) can be rewritten as a matrix form, i.e. $X^{(l+1)}=\rho\left(\Psi_g^{-s}\Theta\Psi_g^{s}X^{(l)}W^{(l)}\right)$.
The learnable filter matrix $\Theta$ can be replaced with the $K\text{-localized}$ Chebyshev Polynomial so as to eschew the time-consuming eigendecomposition of $\overline{L}_G$.

\subsubsection{Summary}

The aforementioned GCNN architectures provide available ingredients of constructing the GNNs.
In practice, we can construct our own GCNNs by assembling different modules introduced above.
Additionally, some scholars also study the GCNNs from some novel perspectives,
e.g. the parallel computing framework of the GCNNs \cite{40}, the hierarchical covariant compositional networks \cite{213},
the transfer active learning for GCNNs \cite{45} and quantum walk based subgraph convolutional neural network \cite{51}.
They are closely related to the GCNNs, yet fairly different from the ones introduced above.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Graph Pooling Operators}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Graph pooling operators are very important and useful modules of the GCNNs, especially for graph-level tasks such as the graph classification.
There are two kinds of graph pooling operators, namely global graph pooling operators and hierarchical graph pooling operators.
The former aims to obtain the universal representations of input graphs, and the latter aims to capture adequate structural information for node representations.

\subsubsection{Global Graph Pooling Operators.}
Global graph pooling operators pool all of representations of nodes into a universal graph representation.
Many literatures \cite{8,21,41} apply some simple global graph pooling operators, e.g. max/average/concatenate graph pooling, to performing graph-level classification tasks.
Here, we introduce some more sophisticated global graph pooling operators in contrast to the simple ones.
Relational pooling (RP) \cite{98} provides a novel framework for graph representation with maximal representation power.
Specifically, all node embeddings can be aggregated via a learnable function to form a global embedding of $G$.
Let $X^{(v)}\in\mathbb{R}^{N\times d_v}$ and $\underline{X}^{(e)}\in\mathbb{N}^{N\times N\times d_e}$ respectively denote node feature matrix and edge feature tensor.
Tensor $\underline{A}_G\in\mathbb{R}^{N\times N\times (1+d_e)}$ combines the adjacency matrix $A_G$ of $G$ with its edge feature tensor $\underline{X}^{(e)}$,
i.e. $\underline{A}_G[u,v,:]=\mathbb{I}_{(u,v)\in E_G}\bowtie\underline{X}^{(e)}[u,v,:]$.
After performing a permutation on $V_G$, the edge feature tensor $\underline{A}_G^{(\pi,\pi)}\left[\pi(r),\pi(c),d\right]=\underline{A}_G[r,c,d]$ and the node feature matrix $X_{\pi}^{(v)}[\pi(r),c]=X^{(v)}[r,c]$.
The joint RP permutation-invariant function for directed or undirected graphs is defined as
\begin{displaymath}
\bar{\bar{f}}(G)=\frac{1}{N!}\sum_{\pi\in\Pi_{|V|}}\overrightarrow{f}(\underline{A}_G^{\pi,\pi},X_{\pi}^{(v)}),
\end{displaymath}
where $\Pi_{|V|}$ is the set of all distinct permutations on $V_G$ and $\overrightarrow{f}(\cdot,\cdot)$ is an arbitrary (possibly permutation-sensitive) vector-valued function.
Specifically, $\overrightarrow{f}(\cdot,\cdot)$ can be denoted as Multi-Layer Perceptrons (MLPs), Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs) or Graph Neural Networks (GNNs).
The literature \cite{98} proves that $\bar{\bar{f}}(G)$ has the most expressive representation of $G$ under some mild conditions, and provides approximation approaches to making RP computationally tractable.
In addition, there are some other available global graph pooling operators, e.g. SortPooling \cite{92} and function space pooling \cite{97}.

\subsubsection{Hierarchical Graph Pooling Operators.}
Hierarchical graph pooling operators group a set of proximal nodes into a super-node via graph clustering methods.
Consequently, the original graph is coarsened to a new graph with coarse granularity. In practice, the hierarchical graph pooling operators are interleaved with the vanilla GCNN layers.
In general, there are three kinds of approaches to performing the graph coarsening operations, namely invoking the existing graph clustering algorithms (e.g. spectral clustering \cite{7} and Graclus \cite{99}),
learning a soft cluster assignment and selecting the first $k$ top-rank nodes.

\textbf{Invoking existing graph clustering algorithms.}
The graph clustering aims to assign proximal nodes to the same cluster and in-proximal nodes to different clusters.
The coarsened graph regards the resulting clusters as super-nodes and connections between two clusters as super-edges.
The hierarchical graph pooling operators aggregate the representations of nodes in super-nodes via aggregation functions such as max pooling and average pooling \cite{91}
to compute the representations of the super-nodes. The literature \cite{82} proposes the EigenPooling method and presents the relationship between the original and coarsened graph.
In order to construct a coarsened graph of $G$, a graph clustering method is employed to partition $G$ into $K$ disjoint clusters, namely $\left\{G_k:k=1,\cdots,K\right\}$.
Suppose each cluster $G_k$ has $N_k$ nodes, namely $\left\{v_{k,1},\cdots,v_{k,N_k}\right\}$, and its adjacency matrix is denoted as $A_{G_k}$.
The coarsened graph $G_{\text{coar}}$ of $G$ can be constructed by regarding the clusters $G_k,k=1,\cdots,K$ as super-nodes and connections between two super-nodes as edges.
For $G_k$, its sampling matrix $C_k$ of size $\left(N\times N_k\right)$ is defined by
\begin{displaymath}
C_k(s,t)=\left\{
\begin{array}{lll}
1 && \text{if node $v_{k,s}$ in $G_k$ is identical to vertex $v_{t}$ in $G$} \\ [2mm]
0 && \text{otherwise}
\end{array}
\right.
\end{displaymath}
On one hand, $C_k$ can be used to down-sample a $1\text{-dimensional}$ graph signal $x$ on $G$ to obtain an contracted graph signal $x_{G_k}$ on $G_k$, i.e. $x_{G_k}=C_k^Tx$.
On the other hand, $C_k$ can also be used to up-sample a graph signal $x_{G_k}$ on $G_k$ to obtain a dilated graph $G$, i.e. $x=C_kx_{G_k}$.
Furthermore, the adjacency matrix $A_{G_k}$ of $G_k$ can be computed by
\begin{displaymath}
A_{G_k}=C_k^TA_GC_k.
\end{displaymath}
The intra-subgraph adjacency matrix of $G$ is computed by $A_{\text{intra}}=\sum_{k=1}^KC_kA_{G_k}C_k^T$.
Thereby, the inter-subgraph adjacency matrix of $G$ can be computed by $A_{\text{inter}}=A_G-A_{\text{intra}}$.
Let $M_{\text{coar}}\in\mathbb{R}^{N\times K}$ denote the assignment matrix from $G$ to $G_{\text{coar}}$. Its $(j,k)\text{-th}$ entry is defined as
\begin{displaymath}
M_{\text{coar}}[j,k]=\left\{
\begin{array}{lll}
1 && \text{if $v_j$ in $G$ is grouped into $G_k$ in $G_{\text{coar}}$} \\ [2mm]
0 && \text{otherwise} \\
\end{array}
\right.
\end{displaymath}
As a result, the adjacency matrix $A_{\text{coar}}$ of the coarsened graph $G_{\text{coar}}$ is computed by $A_{\text{coar}}=M_{\text{coar}}^TA_{\text{inter}}M_{\text{coar}}$.
In fact, $A_{\text{coar}}$ can be written as $A_{\text{coar}}=f\left(M_{\text{coar}}^TA_GM_{\text{coar}}\right)$ as well,
where $f(\widetilde{a}_{i,j})=1$ if $\widetilde{a}_{i,j}>0$ and $f(\widetilde{a}_{i,j})=0$ otherwise.
As stated previously, $X$ is a $d\text{-dimensional}$ graph signal on $G$.
Then, a $d\text{-dimensional}$ graph signal $X_{\text{coar}}$ on $G_{\text{coar}}$ can be computed by $X_{\text{coar}}=M_{\text{coar}}^TX$.
EigenPooling \cite{82} employs spectral clustering to obtain the coarsened graph, and then up-sample
the Fourier basis of subgraphs $G_k,k=1,\cdots,K$. These Fourier basis are then organized into pooling operators with regard to ascending eigenvalues.
Consequently, the pooled node feature matrix is obtained via concatenating the pooled results.
The literature \cite{42} proposes a novel Hierarchical Graph Convolutional Network (H-GCN) consisting of graph coarsening layers and graph refining layers.
The former employs structural equivalence grouping and structural similarity grouping to construct the coarsened graph,
and the latter restores the original topological structure of the corresponding graph.

\textbf{Learning a soft cluster assignment.}
\textsc{StructPool} \cite{390}, as a structured graph pooling technique, regards the graph pooling as a graph clustering problem so as to learn a
 cluster assignment matrix
via the feature matrix $X$ and adjacency matrix $A_G$. Learning the cluster assignment matrix can formulated as a Conditional Random Field (CRF) \cite{391} based probabilistic inference.
Specifically, the input feature matrix $X$ is treated as global observation, and $Y=\left\{Y_1,\cdots,Y_N\right\}$ is a random field where $Y_i\in\{1,\cdots,K\}$ is a random variable
indicating which clusters the node $v_i$ is assigned to. As a result, $\left(Y,X\right)$ can be characterized by a CRF model, i.e.
\begin{displaymath}
\begin{array}{lll}
\mathbb{P}(Y|X) & = & \dfrac{1}{Z(X)}\exp\left(-\mathcal{E}(Y|X)\right) \\[2mm]
                & = & \dfrac{1}{Z(X)}\exp\left(\displaystyle\sum_{C\in\mathcal{C}_G}\psi_C(Y_C|X)\right)
\end{array}
\end{displaymath}
where $\mathcal{E}(Y|X)=-\sum_{C\in\mathcal{C}_G}\psi_C(Y_C|X)$ is called an energy function, $\mathcal{C}_G$ is a set of cliques, $\psi_C(Y_C|X)$ is a potential function and $Z(X)$ is a partition function.
The energy function $\mathcal{E}(Y|X)$ can be characterized by an unary energy $\psi_u(\cdot)$ and a pairwise energy $\psi_p(\cdot,\cdot)$, i.e.
\begin{displaymath}
\mathcal{E}(Y|X)=\sum_{s=1}^N\psi_u(y_s|X)+\sum_{s\neq t}\psi_p(y_s,y_t|X)a_{s,t}^l,
\end{displaymath}
where $a_{s,t}^l$ denotes the $(s,t)\text{-th}$ entry of the $l\text{-hop}$ adjacency matrix $A_G^l$.
The unary energy matrix $\Psi_u=\left(\psi_u(y_s|X)\right)_{N\times K}$ can be obtained by a GCNN taking the global observation $X$ and the adjacency $A_G$ as input.
The pairwise energy matrix $\Psi_p=\left(\psi_p(y_s,y_t|X)\right)_{K\times K}$ can be obtained by
\begin{displaymath}
\psi_p(y_s,y_t|X)=\mu(y_s,y_t)\frac{x_s^Tx_t}{\sum_{j\neq s}x_s^Ts_j},
\end{displaymath}
where $\mu(y_s,y_t)$ is a learnable compatibility function. Minimizing the energy function $\mathcal{E}(Y|X)$ via mean-field approximation
results in the most probable cluster assignment matrix $M$ for a give graph $G$. As a result, we obtain a new graph $A_{\text{coar}}=f\left(M^TA_GM\right)$ and $X_{\text{coar}}=M^TX$.
\textsc{DiffPool} \cite{94} is a differentiable graph pooling operator
which can generate hierarchical representations of graphs and can be incorporated into various GCNNs in an end-to-end fashion.
It maps an adjacency matrix $A_{G^{(l)}}$ and embedding matrix $Z^{(l)}$ at the $l\text{-th}$ layer
to a new adjacency matrix $A_{G^{(l+1)}}$ and a coarsened feature matrix $X^{(l+1)}$, i.e. $\left(A_{G^{(l+1)}},X^{(l+1)}\right)=\text{\textsc{DiffPool}}\left(A_{G^{(l)}},Z^{(l)}\right)$.
More specifically, $X^{(l+1)}=\left(M^{(l)}\right)^TZ^{(l)}$, $A_{G^{(l+1)}}=\left(M^{(l)}\right)^TA_{G^{(l)}}M^{(l)}$.
Note that the assignment matrix $M^{(l)}$ and embedding matrix $Z^{(l)}$ are respectively computed by two separate GCNNs, namely embedding GCNN and pooling GCNN, i.e.
$Z^{(l)}=\text{GCNN}_{\text{embed}}\left(A_{G^{(l)}},X^{(l)}\right)$, $M^{(l)}=\text{Softmax}\left(\text{GCNN}_{\text{pool}}\left(A_{G^{(l)}},X^{(l)}\right)\right)$.

\textbf{Selecting the first $k$ top-rank nodes.}
The literature \cite{86} proposes a novel Self-Attention Graph Pooling operator (abbreviated as SAGPool).
Specifically, SAGPool firstly employs the GCN \cite{6} to calculate the self-attention scores, and then invokes the top-rank function to select the top $\lceil kN\rceil$ node indices, i.e.
\begin{displaymath}
\begin{array}{lll}
Z=\rho\left(\widetilde{D}_G^{-\frac{1}{2}}\widetilde{A}_G\widetilde{D}_G^{-\frac{1}{2}}X\Theta\right), &  \text{idx}=\text{top-rank}\left(Z,\lceil kN\rceil\right), & Z_{\text{mask}}=Z_{\text{idx}}  \\
X'=X_{\text{idx}}, & X_{\text{out}}=X'\circledast Z_{\text{mask}}, & A_{\text{out}}=A_{\text{idx},\text{idx}}.
\end{array}
\end{displaymath}
As a result, the selected $\text{top-}\lceil kN\rceil$ node indices are employed to extract the output adjacency matrix $A_{\text{out}}$ and feature matrix $X_{\text{out}}$.
In order to exploit the expressive power of an encoder-decoder architecture like U-Net \cite{72}, the literature \cite{95} proposes a novel graph pooling (gPool) layer and a graph unpooling (gUnpool) layer.
The gPool adaptively selects $\text{top-}k$ ranked node indices by the down-sampling technique to form a coarsened graph ($A_{\text{coar}}\in\mathbb{R}^{N\times N}$ and $X_{\text{coar}}\in\mathbb{R}^{N\times d}$)
based on scalar projection values on a learnable projection vector, i.e.
\begin{displaymath}
\begin{array}{lll}
y=\frac{Xp}{\|p\|}, & \text{idx}=\text{top-rank}(y,k), & \widetilde{y}=\tanh(y_{\text{idx}}) \\
\widetilde{X}_{\text{coar}}=X_{\text{idx},:}, & A_{\text{coar}}=A_{\text{idx},\text{idx}}, & X_{\text{coar}}=\widetilde{X}_{\text{coar}}\circledast(\widetilde{y}\mathbf{1}_C^T),
\end{array}
\end{displaymath}
where $y\in\mathbb{R}^{d}$. The gUnpool performs the inverse operation of the gPool layer so as to restore the coarsened graph into its original structure.
To this end, gUnpool records the locations of nodes selected in the corresponding gPool layer, and then restores the selected nodes to their original positions in the graph.
Specifically, let $X_{\text{refine}}=\text{Distribute}(0_{N\times d},X_{\text{coar}},\text{idx})$,
where the function $\text{Distribute}(\cdot,\cdot,\cdot)$ distributes row vectors in $X_{\text{coar}}$ into $0_{N\times d}$ feature matrix according to the indices $\text{idx}$.
Note that row vectors of $X_{\text{refine}}$ with indices in $idx$  are updated by the ones in $X_{\text{coar}}$, whereas other row vectors remain zero.
It is worth noting that the literature \cite{96} adopts the similar pooling strategy as gPool to learn the hierarchical representations of nodes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Graph Attention Mechanisms}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Attention mechanisms, firstly introduced in the deep learning community, guide deep learning models to focus on the task-relevant part of its inputs so as to make precise predictions or inferences \cite{73,306,393}.
Recently, applying the attention mechanisms to GCNNs has gained considerable attentions so that various attention techniques have been proposed.
Below, we summarize the graph attention mechanisms on graphs from the next 4 perspectives \cite{394}, namely softmax-based graph attention, similarity-based graph attention,
spectral graph attention and attention-guided walk. Without loss of generality, the neighbors of a given node $v_0$ in $G$ are denoted as $v_1,\cdots,v_{d_0}$,
and their current feature vectors are respectively denoted as $x_0,x_1,\cdots,x_{d_0}$, where $d_0=d_{G}(v_0)$.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.8\textwidth]{GAT.pdf}
  \caption{Two kinds of graph attention mechanisms.}
  \label{GAT}
\end{figure}

\subsubsection{Concatenation-based Graph Attention.}
The softmax-based graph attention is typically implemented by employing a softmax with learnable weights \cite{66,110}
to measure the relevance of $v_j,j=1,\cdots,d_G(v_0)$ to $v_0$. More specifically, the softmax-based attention weights between $v_0$ and $v_j$ can be defined as
\begin{equation}\label{Softmax-based Attention}
\begin{array}{lll}
\omega_{0,j} & = & \text{Softmax}\left(\left[e_{0,1},\cdots,e_{0,d_G(v_0)}\right]\right) \\[2mm]
             & = & \dfrac{\exp\left(\rho\left(a^T(Wx_0\bowtie Wx_j)\right)\right)}{\sum_{k=1}^{d_G(v_0)}\exp\left(\rho\left(a^T(Wx_0\bowtie W_k)\right)\right)},
\end{array}
\end{equation}
where $e_{0,j}=\exp\left(\rho\left(a^T(Wx_0\bowtie Wx_j)\right)\right)$, $a$ is a learnable attention vector and $W$ is a learnable weight matrix, see Fig. \ref{GAT}(a).
As a result, the new feature vector of $v_0$ can be updated by
\begin{equation} \label{Single_Graph_Attention}
x'_{0}=\rho\left(\sum_{j=1}^{d_G(v_0)}\omega_{0,j}Wx_j\right).
\end{equation}
In practice, multi-head attention mechanisms are usually employed to stabilize the learning process of the single-head attention \cite{66}.
For the multi-head attention, assume that the feature vector of each head is $x_0^{(h)}=\rho\left(\sum_{j=1}^{d_G(v_0)}\omega_{0,j}^{(h)}W^{(h)}x_j\right)$.
The concatenation based multi-head attention is computed by
$x'_0=\bowtie_{h=1}^H x_0^{(h)}=\bowtie_{h=1}^H \rho\left(\sum_{j=1}^{d_G(v_0)}\omega_{0,j}^{(h)}W^{(h)}x_j\right)$.
The average based multi-head attention is computed by $x'_0=\rho\left(\frac{1}{H}\sum_{h=1}^H\sum_{j=1}^{d_G(v_0)}\omega_{0,j}^{(h)}W^{(h)}x_j\right)$.

The conventional multi-head attention mechanism treats all the attention heads equally so that feeding the output of an attention that captures a useless representation maybe mislead the final prediction of the model.
The literature \cite{74} computes an additional soft gate to assign different weights to heads, and gets the formulation of the gated multi-head attention mechanism.
The Graph Transformer (GTR) \cite{110} can capture long-range dependencies of dynamic graphs with softmax-based attention mechanism by propagating features within the same graph structure via an intra-graph message passing.
The Graph-BERT \cite{367} is essentially a pre-training method only based on the graph attention mechanism without any graph convolution or aggregation operators.
Its key component is called a graph transformer based encoder, i.e. $X^{(l+1)}=\text{Softmax}\left(\frac{QK^T}{\sqrt{d_h}}\right)V$,
where $Q=X^{(l)}W_Q^{(l+1)}$, $K=X^{(l)}W_K^{(l+1)}$ and $V=X^{(l)}W_V^{(l+1)}$.
The Graph2Seq \cite{106} is a general end-to-end graph-to-sequence neural encoder-decoder model converting an input graph to a sequence of vectors with the attention based LSTM model.
It is composed of a graph encoder, a sequence decoder and a node attention mechanism. The sequence decoder takes outputs (node and graph representations) of the graph encoder as input, and employs the softmax-based
attention to compute the context vector sequence.

\subsubsection{Similarity-based Graph Attention.}
The similarity-based graph attention depends on the cosine similarities of the given node $v_0$ and its neighbors $v_j,j=1,\cdots,d_G(v_0)$.
More specifically, the similarity-based attention weights are computed by
\begin{equation}\label{Similarity-based Attention}
\omega_{0,j}=\dfrac{\exp\left(\beta\cdot\cos\left(Wx_0,Wx_j\right)\right)}{\sum_{k=1}^{d_G(v_0)}\exp\left(\beta\cdot\cos\left(Wx_0,Wx_k\right)\right)},
\end{equation}
where $\beta$ is learnable bias and $W$ is a learnable weight matrix, see Fig. \ref{GAT}(b). It is well known that $\cos\left(x,y\right)=\frac{\langle x,y\rangle}{\|x\|_2\|y\|_2}$.
Attention-based Graph Neural Network (AGNN) \cite{67} adopts the similarity-based attention to construct the propagation matrix $P^{(l)}$ capturing the relevance of $v_j$ to $v_i$.
As a result, the output hidden representation $X^{(l+1)}$ at the $(l+1)\text{-th}$ layer is computed by
\begin{displaymath}
X^{(l+1)}=\rho\left(P^{(l)}X^{(l)}W^{(l)}\right),
\end{displaymath}
where $P^{(l)}(i,j)\triangleq\omega_{i,j}$ is defined in formula (\ref{Similarity-based Attention}).

\subsubsection{Spectral Graph Attention.}
The Spectral Graph Attention Networks (SpGAT) aims to learn representations for different frequency components \cite{362}.
The eigenvalues of the normalized graph Laplacian $\overline{L}_G$ can be treated as frequencies on the graph $G$. As stated in the Preliminary section, $0=\lambda_1\leq\lambda_2\leq\cdots\leq\lambda_N=\lambda_{\max}$.
The SpGAT firstly extracts the low-frequency component $B_L=\left\{u_1,\cdots,u_{n}\right\}$ and the high-frequency component $B_H=\left\{u_{N-n+1},\cdots,u_N\right\}$
from the graph Fourier bases $\left\{u_1,u_2,\cdots,u_N\right\}$.
So, we have
\begin{equation}\label{Spectral Graph Attention Networks}
\begin{array}{l}
X_L=X^{(l)}\Theta_L,\quad X_H=X^{(l)}\Theta_H \\
X^{(l+1)}=\rho\left(\text{\textsc{Aggregate}}\left(B_LF_LB_L^TX_L,B_HF_HB_H^TX_H\right)\right),
\end{array}
\end{equation}
where $F_L$ and $F_H$ respectively measure the importance of the low- and high-frequency.
In practice, we exploit a re-parameterization trick to accelerate the training. More specifically, we replace $F_L$ and $F_H$ respectively with the learnable attention weights
$\Omega_L=\text{diag}\left(\omega_L,\cdots,\omega_L\right)$ and $\Omega_H=\text{diag}\left(\omega_H,\cdots,\omega_H\right)$ so as to reduce the number of learnable parameters.
To ensure that  $\omega_L$ and $\omega_H$ are positive and comparable, we normalize them by the softmax function, i.e.
$\omega_L=\frac{\exp\left(\omega_L\right)}{\exp\left(\omega_L\right)+\exp\left(\omega_H\right)},\quad \omega_H=\frac{\exp\left(\omega_H\right)}{\exp\left(\omega_L\right)+\exp\left(\omega_H\right)}$.
In addition to the attention weights, another important issue is how to choose the low- and high-frequency components $B_L$ and $B_H$.
A natural choice is to use the graph Fourier bases, yet the literatures \cite{191,194} conclude that utilizing the spectral graph wavelet operators can achieve better embedding results than the graph Fourier bases.
Therefore, we substitute $B_L$ and $B_H$ in Eq. (\ref{Spectral Graph Attention Networks}) for the spectral graph wavelet operator $\Psi_{L,g}^s$ and $\Psi_{H,g}^s$, i.e.
\begin{displaymath}
X^{(l+1)}=\rho\left(\text{\textsc{Aggregate}}\left(\left(\Psi_{L,g}^s\right)F_L\left(\Psi_{L,g}^s\right)^{-1}X_L,\left(\Psi_{H,g}^s\right)F_H\left(\Psi_{H,g}^s\right)^{-1}X_H\right)\right).
\end{displaymath}

\subsubsection{Attention-guided Walk.}
The two aforementioned kinds of attention mechanisms focus on incorporating task-relevant information from the neighbors of a given node into the updated representations of the pivot.
Here, we introduce a new attention mechanism, namely attention-guided walk \cite{81}, which has different purpose from the softmax- and similarity-based attention mechanisms.
Suppose a walker walks along the edges of the graph $G$ and he currently locates at the node $v_t$.
The hidden representation $x^{(t)}$ of $v_t$ is computed by a recurrent neural network $f_x\left(\cdot\right)$
taking the step embedding $s^{(t)}$ and internal representation of the historical information from the previous step $x^{(t-1)}$ as input, i.e.
\begin{displaymath}
x^{(t)}=f_x\left(s^{(t)},x^{(t-1)};\Theta_x\right).
\end{displaymath}
The step embedding $s^{(t)}$ is computed by a step network $f_s\left(r^{(t-1)},x_{v_t};\Theta_s\right)$ taking the ranking vector $r^{(t-1)}$
and the input feature vector $x_{v_t}$ of the top-priority node $v_t$ as input, i.e.
\begin{displaymath}
s^{(t)}=f_s\left(r^{(t-1)},x_{v_t};\Theta_s\right).
\end{displaymath}
The hidden representation $x^{(t)}$ is then feeded into a ranking network $f_r\left(x^{(t)};\Theta_r\right)$ and a predicting network $f_p\left(x^{(t)};\Theta_p\right)$, i.e.
\begin{displaymath}
r^{(t)}=f_r\left(x^{(t)};\Theta_r\right), \quad\hat{l}^{(t)}=f_p\left(x^{(t)};\Theta_p\right).
\end{displaymath}
The ranking network $f_r\left(x^{(t)};\Theta_r\right)$ determines which neighbors of $v_t$ should be prioritized in the next step, and the predicting network $f_p\left(x^{(t)};\Theta_p\right)$
makes a prediction on graph labels. Now, $x^{(t)}$ and $r^{(t)}$ are feeded into the next node to compute its hidden representations.
Fig. \ref{attention-guided-walk} shows the computational framework of the attention-guided walk.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.8\textwidth]{attention-guided-walk.pdf}
  \caption{The computational framework of the attention-guided walk.}
  \label{attention-guided-walk}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Graph Recurrent Neural Networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The Graph Recurrent Neural Networks (GRNNs) generalize the Recurrent Neural Networks (RNNs) to process the graph-structured data.
In general, the GRNN can be formulated as $h_{v_j}^{(l+1)}=\text{GRNN}\left(x_{v_j}^{(l)},\left\{h_{v_k}^{(l)}:v_k\in N_G(v_j)\cup\left\{v_j\right\}\right\}\right)$.
Below, we introduce some available GRNN architectures.

\subsubsection{Graph LSTM}
The Graph Long Short Term Memroy (Graph LSTM) \cite{117,118,119,120,121,123,143} generalizes the vanilla LSTM for the sequential data to the ones for general graph-structured data.
Specifically, the graph LSTM updates the hidden states and cell states of nodes by the following formula,
\begin{displaymath}
\begin{array}{ll}
i_{v_j}^{(l+1)}=\sigma\left(W_ix_{v_j}^{(l)}+\sum_{v_k\in N_G(v_j)\cup\left\{v_j\right\}}U_ih_{v_k}^{(l)}+b_i\right), \\ [2mm]
o_{v_j}^{(l+1)}=\sigma\left(W_ox_{v_j}^{(l)}+\sum_{v_k\in N_G(v_j)\cup\left\{v_j\right\}}U_oh_{v_k}^{(l)}+b_o\right) \\ [2mm]
\widetilde{c}_{v_j}^{(l+1)}=\tanh\left(W_cx_{v_j}^{(l)}+\sum_{v_k\in N_G(v_j)\cup\left\{v_j\right\}}U_ch_{v_k}^{(l)}+b_c\right), \\ [2mm]
f_{v_j,v_k}^{(l+1)}=\sigma\left(W_fx_{v_j}^{(l)}+U_fh_{v_k}^{(l)}+b_f\right) \\ [2mm]
c_{v_j}^{(l+1)}=i_{v_j}^{(l+1)}\circledast\widetilde{c}_{v_j}^{(l+1)}+\sum_{v_k\in N_G{v_j}\cup\left\{v_j\right\}}f_{v_j,v_k}^{(l+1)}\circledast c_{v_k}^{(l)}, \\ [2mm] h_{v_j}^{(l+1)}=o_{v_j}^{(l+1)}\circledast\tanh(c_{v_j}^{(l+1)}).
\end{array}
\end{displaymath}
see the Fig. \ref{Graph LSTM}.
The literature \cite{267} develops a general framework, named structure-evolving LSTM, for learning interpretable data representations via the graph LSTM.
It progressively evolves the multi-level node representations by stochastically merging two adjacent nodes with high compatibilities estimated by the adaptive forget gate of the graph LSTM.
As a result, the new graph is produced with a Metropolis-Hastings sampling method.
The Gated Graph Sequence Neural Networks (GGS-NNs) \cite{122} employs the Gated Recurrent Unit (GRU) \cite{115} to modify the vanilla GCNNs so that it can be extended to process the sequential data.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.8\textwidth]{Graph-lstm.pdf}
  \caption{Computational framework of the Graph LSTM.}
  \label{Graph LSTM}
\end{figure}

\subsubsection{GRNNs for Dynamic Graphs}
A dynamic graph is the one whose structure (i.e. adding a node, removing a node, adding an edge, removing an edge) or features on nodes/edges evolve over time.
The GRNNs are a straightforward approach to tackling dynamic graphs. Below, we introduce some studies on the GRNNs for dynamic graphs.

The literature \cite{107} proposes a Dynamic Graph Neural Network (DGNN) concerning the dynamic graph only with  nodes or edges adding.
More specifically, the DGNN is composed of two key components: an update component and a propagation component. Suppose an edge $(v_s,v_g,t)$ is added to the input dynamic graph at time $t$.
Let $t-$ denotes a time before the time $t$. The update component consists of three sequential units: the interacting unit, the S- or G-update unit and the merging unit.
The interacting unit takes the source and target representations before the time $t$ as input, and outputs the joint representation of the interaction, i.e.
$x_e^{(t)}=\rho\left(W_sx_s^{(t-)}+W_gx_g^{(t-)}+b_e\right)$. The S- and G-update units employ the LSTM \cite{160} to respectively update the cell states and hidden states of
the source and target, i.e.
\begin{displaymath}
\begin{array}{l}
\left(C_{v_s}^{(t)},h_{v_s}^{(t)}\right)=\text{LSTM}_s\left(C_{v_s}^{(t-)},h_{v_s}^{(t-)},\Delta t_s\right),\quad \left(C_{v_g}^{(t)},h_{v_g}^{(t)}\right)=\text{LSTM}_g\left(C_{v_g}^{(t-)},h_{v_g}^{(t-)},\Delta t_g\right).
\end{array}
\end{displaymath}
The merging unit adopts the similar functions to the interacting unit to respectively merge $h_{v_s}^{(t)}$ and $h_{v_s}^{t-}$, and $h_{v_g}^{(t)}$ and $h_{v_g}^{t-}$.
The propagation component can propagate information from two interacting nodes ($v_s$ and $v_g$) to influenced nodes (i.e. their neighbors). It also consists of three units: the interacting unit,
the propagation unit and the merge unit, which are defined similarly to the update component except that they have different learnable parameters.
The literature \cite{105} addresses the vertex- and graph-focused prediction tasks on dynamic graphs with a fixed node set by combining GCNs, LSTMs and fully connected layers.
The Variational Graph Recurrent Neural Networks (VGRNNs) \cite{113} is essentially a variational graph auto-encoder whose encoder integrates the GCN and RNN into a graph RNN (GRNN) framework
and decoder is a joint probability distribution of a multi-variate Gaussian distribution and Bernoulli Distribution. The semi-implicit variational inference is employed to approximate the posterior
so as to generate the node embedding.

\subsubsection{GRNNs based on Vanilla RNNs}
The GRNNs based on vanilla RNNs firstly employ random walk techniques or traversal methods, e.g. Breadth-First Search (BFS) and Depth-First Search (DFS),
to obtain a collection of node sequences, and then leverage a RNN, e.g. LSTM and GRU, to capture long short-term dependencies.
The literature \cite{116} performs joint random walks on attributed networks, and utilizes them to boost the deep node representation learning.
The proposed GraphRNA in \cite{116} consists of two key components, namely a collaborative walking mechanism AttriWalk and a tailored deep embedding architecture for joint random walks GRN.
Suppose $\mathcal{A}_G$ denotes the node-attribute matrix of size $N\times M$. The AtriWalk admits the transition matrix of size $\mathbb{R}_+^{(N+M)\times(N+M)}$ which is written as
\begin{displaymath}
\mathcal{T}=
\left[
\begin{array}{cc}
\alpha A_G & (1-\alpha)\mathcal{A}_G \\
(1-\alpha)\mathcal{A}_G^T & 0
\end{array}
\right].
\end{displaymath}
After obtaining the sequences via the collaborative random walk, the bi-directional GRU \cite{115} and pooling operator are employed to learn the global representations of sequences.
The literature \cite{111} leverages the BFS node ordering and truncation strategy to obtain a collection of node representation sequences,
and then uses the GRU model and variational auto-regression regularization to perform the graph classification.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Extensions and Applications}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The aforementioned architectures essentially provide ingredients of constructing the GNNs for us. Below, we investigate the extensions of the GNNs from the next 8 aspects:
GCNNs on spectral graphs, capability and interpretability, deep graph representation learning, deep graph generative models, combinations of the PI and GNNs,
adversarial attacks for the GNNs, graph neural architecture search and graph reinforcement learning, and briefly summarize the applications of the GNNs at last.

\subsection{GCNNs on Special Graphs}

The vanilla GCNNs aims at learning the representations of input graphs (directed or undirected, weighted or unweighted).
The real-world graphs may have more additional characteristics, e.g. spatial-temporal graphs, heterogeneous graphs, hyper-graphs, signed graphs and so all.
The GCNN for signed graphs \cite{28} leverage the balance theory to aggregate and propagate information through positive and negative links.

\subsubsection{Heterogeneous Graphs.}
Heterogeneous Graphs are composed of nodes and edges of different types, and each type of edges is called a relation between two types of nodes.
For example, a bibliographic information network contains at least 4 types of nodes, namely Author, Paper, Venue and Term, and at least 3 types of edges, namely Author-Paper, Term-Paper and Venue-Paper \cite{395}.
The heterogeneity and rich semantic information brings great challenges for designing heterogeneous graph convolutional neural networks. In general, a heterogeneous graph can be denoted as $H=(V,E,\nu,\zeta)$,
where $\nu(v)$ denotes the type of node $v\in V$ and $\zeta(e)$ denotes the type of edge $e\in E$. Let $\mathcal{T}^v$ and $\mathcal{T}^e$ respectively denote the set of node types and edge types.
Below, we summarize the vanilla heterogeneous GCNNs, namely HetGNNs \cite{61}.

\textbf{Vanilla Heterogeneous GCNNs.}
The Heterogeneous Graph Neural Networks (HetGNNs) \cite{61} aims to resolve the issue of jointly considering heterogeneous structural information
as well as heterogeneous content information of nodes. It firstly samples a fixed size of strongly correlated heterogeneous neighbors for each node via a Random Walk with Restart (RWR)
and groups them into different node types. Then, it aggregates feature information of those sampled neighboring nodes via a bi-directional Long Short Term Memory (LSTM) and attention mechanism.
Running RWR with a restart probability $p$ from node $v$ will yield a collection of a fixed number of nodes, denoted as $\text{RWR}(v)$. For each node type $t$,
the $t\text{-type}$ neighbors $N_G^t(v)$ of node $v$ denotes the set of $\text{top-}k_t$ nodes from $\text{RWR}(v)$ with regard to frequency.
Let $\mathcal{C}_v$ denote the heterogeneous contents of node $v$, which can be encoded as a fixed size embedding via a function $f_1(v)$, i.e.
\begin{displaymath}
f_1(v)=\dfrac{\sum_{j\in\mathcal{C}_v}\left[\overrightarrow{\text{LSTM}}\left(\mathcal{FC}_{\theta_x}(x_j)\right)\bowtie\overleftarrow{\text{LSTM}}\left(\mathcal{FC}_{\theta_x}(x_j)\right)\right]}{|\mathcal{C}_v|},
\end{displaymath}
where $\mathcal{FC}_{\theta_x}(\cdot)$ denotes feature transformer, e.g. identity or fully connected neural networks with parameter $\theta_x$, and
$\overrightarrow{\text{LSTM}}\left(\cdot\right)$ and $\overleftarrow{\text{LSTM}}\left(.\right)$ is defined by the Eq. (\ref{LSTM Formulation}).
The content embedding of the $t\text{-type}$ neighbors of node $v$ can be aggregated as follows,
\begin{displaymath}
\begin{array}{lcl}
f_2^t(v) & = & \text{\textsc{Aggregate}}^T\left(\left\{f_1(v'):v'\in N_G^t(v)\right\}\right) \\[2mm]
         & = & \dfrac{\sum_{v'\in N_G^t(v)}\left[\overrightarrow{\text{LSTM}}(f_1(v'))\bowtie\overleftarrow{\text{LSTM}}(f_1(v'))\right]}{|N_G^t(v)|}.
\end{array}
\end{displaymath}
Let $\mathcal{F}(v)=\{f_1(v)\}\cup\{f_2^t(v):t\in\mathcal{T}^v\}$.
As a result, the output embedding of node $v$ can be obtained via the attention mechanism, i.e. $\mathcal{E}_v=\sum_{f_j(v)\in\mathcal{F}(v)}\omega_{v,j}f_j(v)$,
where the attention weights is computed by
$\omega_{v,j}=\dfrac{\exp\left(\rho\left(u^t\left[f_j(v)\bowtie f_1(v)\right]\right)\right)}{\sum_{f_j(v)\in\mathcal{F}(v)}\exp\left(\rho\left(u^t\left[f_j(v)\bowtie f_1(v)\right]\right)\right)}$.
In addition, the GraphInception \cite{397} can be employed to learn the hierarchical relational features on heterogeneous graphs
by converting the input graph into a multi-channel graph (each meta path as a channel) \cite{62}.

\textbf{Heterogeneous Graph Attention Mechanism.} The literature \cite{75} firstly proposes a hierarchical attention based
heterogeneous GCNNs consisting of node-level and semantic-level attentions. The node-level attention aims to learn the attention weights of a node and its meta-path-based neighbors,
and the semantic-level attention aims to learn the importance of different meta-paths. More specifically, given a meta path $\Phi$, the node-level attention weight of a node $v_i$
and its meta-path-based neighbors $v_j\in N_G^{\Phi}(v_i)$ is defined to be
\begin{displaymath}
\omega_{j,k}^{\Phi}=\dfrac{\exp\left(\rho\left(a_{\Phi}^T(M_{\nu(v_j)}x_j\bowtie M_{\nu(v_k)}x_k)\right)\right)}{\sum_{v_k\in N_G^{\Phi}(v_j)}\exp\left(\rho\left(a_{\Phi}^T(M_{\nu(v_j)}x_j\bowtie M_{\nu(v_k)}x_k)\right)\right)},
\end{displaymath}
where $M_{\nu(v_j)}$ transforms the feature vectors of nodes of type $\nu(v_j)$ in different vector spaces into a unified vector space.
The embedding of node $v_i$ under the meta path $\Phi$ can be computed by
\begin{displaymath}
x_j^{\Phi,l+1}=\bowtie_{k=1}^K\rho\left(\sum_{k\in N_G^{\Phi}(v_j)}\omega_{j,k}^{\Phi}M_{\nu(v_k)}x_k^{\Phi,l}\right).
\end{displaymath}
Given a meta-path set $\left\{\Phi_0,\cdots,\Phi_P\right\}$, performing the node-level attention layers under each meta path will yield a set of semantic-specific node representations,
namely $\left\{X^{\Phi_0},\cdots,X^{\Phi_P}\right\}$. The semantic-level attention weight of the meta path $\Phi_j$ is defined as
\begin{displaymath}
\beta^{\Phi_j}=\dfrac{\exp\left(\omega^{\Phi_j}\right)}{\sum_{p=1}^P\exp\left(\omega^{\Phi_p}\right)},
\end{displaymath}
where $\omega^{\Phi_p}=\frac{1}{|V|}\sum_{v_k\in V}q^T\tanh\left(Wx_k^{\Phi_p}+b\right)$. As a result, the embedding matrix $X=\sum_{p=1}^P\beta^{\Phi_p}X^{\Phi_p}$.
In addition, there are some available studies on the GCNNs for multi-relational graphs \cite{36,68,361} and the transformer for dynamic heterogeneous graphs \cite{60,114}.

\subsubsection{Spatio-Temporal Graphs.}
Spatio-temporal graphs can be used to model traffic networks \cite{59,71} and skeleton networks \cite{55,57}.
In general, a spatio-temporal graph is denoted as $G_{ST}=(V_{ST},E_{ST})$ where $V_{ST}=\{v_{t,j}:t=1,\cdots,T,j=1,\cdots,N_{ST}\}$.
The edge set $E_{ST}$ is composed of two types of edges, namely spatial edges and temporal edges. All spatial edges $(v_{t,j},v_{t,k})\in E_{ST}$ are collected in the intra-frame edge set $E_S$,
and all temporal edges $(v_{t,j},v_{t+1,j})\in E_{ST}$ are collected in the inter-frame edge set $E_T$.
The literature \cite{59} proposes a novel deep learning framework, namely Spatio-Temporal Graph Convolutional Networks (STGCN), to tackle the traffic forecasting problem.
Specifically, STGCN consists of several layers of spatio-temporal convolutional blocks, each of which has a "sandwich" structure with two temporal gated convolution layers (abbreviated as Temporal Gated-Conv)
and a spatial graph convolution layer in between (abbreviated as Spatial Graph-Conv). The Spatial Graph-Conv exploits the conventional GCNNs to extract the spatial features,
whereas the Temporal Gated-Conv the temporal gated convolution operator to extract temporal features. Suppose that the input of the temporal gated convolution for each node is a $\text{length-}M$ sequence
with $C_{\text{in}}$ channels, i.e. $X\in\mathbb{R}^{M\times C_{\text{in}}}$. The temporal gated convolution kernel $\Gamma\in\mathbb{R}^{K\times C_{\text{in}}\times 2C_{\text{out}}}$ is used to
filter the input $Y$, i.e.
\begin{displaymath}
\Gamma\ast_T Y=P\circledast\sigma(Q)\in\mathbb{R}^{(M-K+1)\times C_{\text{out}}},
\end{displaymath}
to yield an output $P\bowtie Q\in\mathbb{R}^{(M-K+1)\times(2C_{\text{out}})}$.
The Spatial Graph-Conv takes a tensor $\underline{X}^{(l)}\in\mathbb{R}^{M\times N_{ST}\times C^{(l)}}$ as input, and outputs a tensor $\underline{X}^{(l+1)}\in\mathbb{R}^{(M-2(K-1))\times N_{ST}\times C^{(l+1)}}$, i.e.
\begin{displaymath}
\underline{X}^{(l+1)}=\Gamma_1^{(l)}\ast_T\rho\left(\Theta^{(l)}\ast_G\left(\Gamma_0^{(l)}\ast_T X^{(l)}\right)\right),
\end{displaymath}
where $\Gamma_{0}^{(l)},\Gamma_1^{(l)}$ are the upper and lower temporal kernel and $\Theta^{(l)}$ is the spectral kernel of the graph convolution.
In addition, some other studies pay attention to the GCNNs on the spatio-temporal graphs from other perspectives, e.g. Structural-RNN \cite{56} via a factor graph representation of the spatio-temporal graph
and GCRNN \cite{108,109} combining the vanilla GCNN  and RNN.

\subsubsection{Hypergraphs.}
The aforementioned GCNN architectures are concerned with the conventional graphs consisting of pairwise connectivity between two nodes.
However, there could be even more complicated connections between nodes beyond the pairwise connectivity, e.g. co-authorship networks. Under such circumstances, a hypergraph, as a generalization to the convectional graph,
provides a flexible and elegant modeling tools to represent these complicated connections between nodes. A hypergraph is usually denoted as $\mathcal{G}=(\mathcal{V},\mathcal{E},\omega)$,
where $\mathcal{V}=\{v_1,\cdots,v_N\}$ like the conventional graph, $\mathcal{E}=\{e_1,\cdots,e_M\}$ is a set of $M$ hyperedges.
$\omega(e_k)$ denote weights of hyperedges $e_k\in\mathcal{E}$. A non-trivial hyperedge is a subset of $\mathcal{V}$ with at least 2 nodes.
In particular, a trivial hyperedge, called a self-loop, is composed of a single node. The hypergraph $\mathcal{G}$ can also be denoted by an incidence matrix $\mathcal{H}_{\mathcal{G}}\in\mathbb{R}^{N\times M}$,
i.e.
\begin{displaymath}
\mathcal{H}_{\mathcal{G}}[j,k]=
\left\{
\begin{array}{ll}
0, & v_j\notin e_k \\
1, & v_j\in e_k.
\end{array}
\right.
\end{displaymath}
For a node $v_j\in\mathcal{V}$, its degree $\text{deg}_{\mathcal{V}}(v_j)=\sum_{e_k\in\mathcal{E}}\omega(e_k)\mathcal{H}_{\mathcal{G}}[j,k]$.
For a hyperedge $e_k\in\mathcal{E}$, its degree $\text{deg}_{\mathcal{E}}(e_k)=\sum_{v_j\in e_k}\mathcal{H}_{\mathcal{G}}[j,k]$.
Let $\mathcal{D}_{\mathcal{V}}=\text{diag}\left(\text{deg}_{\mathcal{V}}(v_1),\cdots,\text{deg}_{\mathcal{V}}(v_N)\right)$,
$\mathcal{D}_{\mathcal{E}}=\text{diag}\left(\text{deg}_{\mathcal{E}}(e_1),\cdots,\text{deg}_{\mathcal{E}}(e_M)\right)$, and
$\mathcal{W}_{\mathcal{G}}=\text{diag}\left(\omega(e_1),\cdots,\omega(e_M)\right)$.
The hypergraph Laplacian \cite{65} $\mathcal{L}_{\mathcal{G}}$ of $\mathcal{G}$ is defined to be
\begin{displaymath}
\mathcal{L}_{\mathcal{G}}=\mathcal{I}_N-\mathcal{D}_{\mathcal{V}}^{-\frac{1}{2}}\mathcal{H}_{\mathcal{G}}\mathcal{W}_{\mathcal{G}}\mathcal{D}_{\mathcal{E}}^{-1}\mathcal{H}_{\mathcal{G}}^T\mathcal{D}_{\mathcal{V}}^{-\frac{1}{2}}.
\end{displaymath}
It can also be factorized by the eigendecomposition, i.e. $\mathcal{L}_{\mathcal{G}}=\mathcal{U}\Lambda\mathcal{U}^T$.
The spectral hypergraph convolution operator, the Chebyshev hypergraph convolutional neural network and the hypergraph convolutional network
can be defined in analogy to the Eqs (\ref{Spectral Graph Convolution},\ref{Chebyshev Spectral Filter},\ref{GCN-Layer}).
The HyperGraph Neural Network (HGNN) architecture proposed in the literature \cite{65} is composed of multiple layers of the hyperedge convolution, and each layer is defined as
\begin{displaymath}
X^{(l+1)}=\rho\left(\mathcal{D}_{\mathcal{V}}^{-\frac{1}{2}}\mathcal{H}_{\mathcal{G}}\mathcal{W}_{\mathcal{G}}\mathcal{D}_{\mathcal{E}}^{-1}\mathcal{H}_{\mathcal{G}}^T\mathcal{D}_{\mathcal{V}}^{-\frac{1}{2}}X^{(l)}\Theta^{(l)}\right).
\end{displaymath}
In essence, the HGNN essentially views each hyperedge as a complete graph so that the hypergraph is converted into a conventional graph.
Treating each hyperedge as a complete graph obviously incurs expensive computational cost. Hence, some studies \cite{63,64} propose various approaches to approximate the hyperedges.
The HNHN \cite{174} interleaves updating the node representations with the hyperedge representations by the following formulas,
\begin{displaymath}
\mathcal{X}_{\mathcal{V}}^{(l+1)}=\rho\left(\mathcal{D}_{\mathcal{V}}^{-1}\mathcal{H}_{\mathcal{G}}\mathcal{X}_{\mathcal{E}}^{(l)}\Theta_{\mathcal{V}}^{(l)}\right),\quad
\mathcal{X}_{\mathcal{E}}^{(l+1)}=\rho\left(\mathcal{D}_{\mathcal{E}}^{-1}\mathcal{H}_{\mathcal{G}}^{T}\mathcal{X}_{\mathcal{V}}^{(l)}\Theta_{\mathcal{E}}^{(l)}\right).
\end{displaymath}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Capability and Interpretability}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The GCNNs have achieved tremendous empirical successes over the supervised, semi-supervised and unsupervised learning on graphs. Recently,
many studies start to put their eyes on the capability and interpretability of the GCNNs.

\subsubsection{Capability}
The capability of the GCNNs refers to their expressive power. If two graphs are isomorphic, they will obviously output the same representations of nodes/edges/graph.
Otherwise, they should output different representations. However, two non-isomorphic graphs maybe output the same representations in practice.
This is the theoretical limitations of the GCNNs.
As described in the literatures \cite{83,88,360}, The $1\text{-hop}$ spatial GCNNs (1-GCNNs) have the same expressive power as the $1\text{-dimensional}$ Weisfeiler-Leman (1-WL)
graph isomorphism test in terms of distinguishing non-isomorphic graphs.
The 1-WL iteratively update the colors of nodes according to the following formula
\begin{displaymath}
C_{l}^{(l+1)}(v)=\text{\textsc{Hash}}\left(C_{l}^{(l)}(v),\left\{\hspace{-1.5mm}\left\{C_{l}^{(l)}(u):u\in N_G(v)\right\}\hspace{-1.5mm}\right\}\right).
\end{displaymath}
According to the literatures \cite{83,88}, we have that the 1-GCNN architectures do not have more power in terms of distinguishing two non-isomorphic graphs than the 1-WL heuristic. Nevertheless,
they have equivalent power if the aggregation and update functions are injective. In order to overcome
the theoretical limitations of the GCNNs, the literature \cite{83} proposes a Graph Isomorphism Network (GIN) architecture, i.e.
\begin{displaymath}
\begin{array}{rl}
A_v^{(l+1)}& =\text{\textsc{Aggregate}}^{(l+1)}\left(\left\{\left\{X^{(l)}[u,:]:u\in N_G(v)\right\}\right\}\right) \\[2mm]
           & \triangleq\sum_{u\in N_G(v)}X^{(l)}[u,:] \\[2mm]
X^{(l+1)}[v,:] & =\text{\textsc{Update}}^{(l+1)}\left(X^{(l)}[v,:],A_v^{(l+1)}\right) \\ [2mm]
               & \triangleq\text{MLP}\left((1+\epsilon^{(l+1)})X^{(l)}[v,:]+A_v^{(l+1)}\right),
\end{array}
\end{displaymath}
where $\epsilon^{(l)}$ is a scalar parameter. The literature \cite{163} studies the expressive power of the spatial GCNNs, and presents two results:
(1) The spatial GCNNs are shown to be a universal approximator under sufficient conditions on their depth, width, initial node features and layer expressiveness;
(2) The power of the spatial GCNNs is limited when their depth and width is restricted.
In addition, there are some other studies on the capability of the GCNNs from different perspectives, e.g. the first order logic \cite{164},
$p\text{-order}$ graph moments \cite{167}, algorithmic alignment with the dynamic programming \cite{401}, generalization and representational limits of the GNNs \cite{407}.

\subsubsection{Interpretability}
Interpretability plays a vital role in constructing a reliable and intelligent learning systems.
Although some studies have started to explore the interpretability of the conventional deep learning models, few of studies put their eyes on the interpretability of the GNs \cite{85}.
The literature \cite{165} bridges the gap between the empirical success of the GNs and lack of theoretical interpretations. More specifically, it considers two classes of techniques:
(1) gradient based explanations, e.g. sensitivity analysis and guided back-propagation; (2) decomposition based explanations, e.g. layer-wise relevance propagation and Taylor decomposition.
The GNNExplainer \cite{166} is a general and model-agnostic approach for providing interpretable explanations for any spatial GCNN based model in terms of graph machine learning tasks.
Given a trained spatial GCNN model $\Phi$ and a set of predictions, the GNNExplainer will generate a single-instance explanation by identifying a subgraph
of the computation graph and a subset of initial node features, which are the most vital for the prediction of the model $\Phi$. In general, the GNNExplainer can be formulated as an optimization problem
\begin{equation}\label{GNNExplainer optimization framework}
\max_{G_S,X_S^F} I\left(Y,(G_S,X_S^F)\right)=H\left(Y\right)-H\left(Y|G=G_S,X=X_S^F\right),
\end{equation}
where $I(\cdot,\cdot)$ denotes the mutual information of two random variables,
$G_S$ is a small subgraph of the computation graph and $X_S^F$ is a small subset of node features $\left\{X^F[j,:]:v_j\in G_S\right\}$.
The entropy term $H(Y)$ is constant because the spatial GCNN model $\Phi$ is fixed.
In order to improve the tractability and computational efficiency of the GNNExplainer, the final optimization framework is reformulated as
\begin{displaymath}
\min_{M,F} -\sum_{c=1}^C\mathbb{I}[y=c]\log P_{\Phi}\left(Y=y|G=A_G\circledast\sigma(M),X=X_S^F\right).
\end{displaymath}
In addition, the GNNExplainer also provides multi-instances explanations based on graph alignments and prototypes so as to answer questions like "How did a GCNN predict that a given set of nodes all have label $c$?".

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Deep Graph Representation Learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Graph representation learning (or called network embedding) is a paradigm of unsupervised learning on graphs. It gains a large amount of popularity since the DeepWalk \cite{154}.
Subsequently, many studies exploit deep learning techniques to learn low-dimensional representations of nodes \cite{139}. In general, the network embedding via the vanilla deep learning
techniques learn low-dimensional feature vectors of nodes by utilizing either stacked auto-encoders to reconstruct the adjacent or positive point-wise mutual information features \cite{124,132,133,136,141,142}
or RNNs to capture long and short-term dependencies of node sequences yielded by random walks \cite{134,135}. In the following, we introduce the network embedding approaches based on GNNs.

\subsubsection{Network Embedding based on GNNs}
In essence, the GNNs provides an elegant and powerful framework for learning node/edge/graph representations. The majority of the GCNNs and GRNNs are concerned with
semi-supervised learning (i.e. node-focused tasks) or supervised learning (i.e. graph-focused) tasks. Here, we review the GNN based unsupervised learning on graphs.
In general, the network embedding based on GNNs firstly utilize the GCNNs and variational auto-encoder to generate gaussian-distributed hidden states of nodes, and then reconstruct the adjacency matrix
and/or the feature matrix of the input graph \cite{87,127,129,131,138,148}. A representative approach among these ones is the Variational Graph Auto-Encoder (VGAE) \cite{87}
consisting of a GCN based encoder and an inner product decoder.
The GCN based encoder is defined to be
\begin{displaymath}
q(Z|X,A_G)=\prod_{j=1}^Nq(Z[j,:]|X,A_G)=\prod_{j=1}^N\mathcal{N}\left(Z[j,:]|\mu_j,\text{diag}(\sigma_j^2)\right),
\end{displaymath}
where $\mu_j=\text{GCN}_{\mu}\left(X,A_G\right)$ and $\log\sigma_j=\text{GCN}_{\sigma}(X,A_G)$.
The inner product decoder is defined to be

\begin{displaymath}
p(A_G|Z)=\prod_{j=1}^N\prod_{k=1}^Np\left(A_G[j,k]|Z[j,:],Z[k,:]\right)=\prod_{j=1}^N\prod_{k=1}^N\sigma\left(Z[j,:]Z[k,:]^T\right).
\end{displaymath}
They adopt the evidence lower bound \cite{151} as their objective function.
The adversarially regularized (variational) graph autoencoder \cite{129} extends the VGAE by adding an adversarial regularization term to the evidence lower bound.
The literature \cite{128} proposes a symmetric graph convolutional autoencoder which produces a low-dimensional latent nodes representations. Its encoder employs
the Laplacian smoothing \cite{16} to jointly encode the structural and attributed information,
and its decoder is designed based on Laplacian sharpening as the counterpart of the Laplacian smoothing of the encoder. The Laplacian sharpening is defined to be
$X^{(l+1)}=(1+\gamma)X^{(l)}-\gamma D^{-1}AX^{(l)}=X^{(l)}+\gamma(\mathbb{I}_N-D^{-1}A)X^{(l)}$,
which allows utilizing the graph structure in the whole processes of the proposed autoencoder architecture.
In addition, there are some other methods to perform the unsupervised learning on graphs, which do not rely on the reconstruction of the adjacency and/or feature matrix,
e.g. the graph auto-encoder on directed acyclic graphs \cite{79}, pre-training GNNs via context prediction and attribute masking strategies \cite{130},
and deep graph Infomax using a noise-contrastive type objective with a standard binary cross-entropy loss between positive examples and negative examples \cite{140}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Deep Graph Generative Models}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The aforementioned work concentrates on embedding an input graph into a low-dimensional vector space so as to perform semi-supervised/supervised/unsupervised learning tasks on graphs.
This subsection introduces deep graph generative models aiming to mimic real-world complex graphs.
Generating complex graphs from latent representations is confronted with great challenges due to high nonlinearity and arbitrary connectivity of graphs.
Note that graph translation \cite{145} is akin to graph generation. However, their difference lies in that the former takes two graphs, i.e. input graph and target graph, as input,
and the latter only takes a single graph as input. The NetGAN \cite{154} utilizes the generative adversarial network \cite{149} to mimic the input real-world graphs.
More specifically, it is composed of two components, i.e. a generator $G$ and a discriminator $D$, as well. The discriminator $D$ is modeled as a LSTM
in order to distinguish real node sequences, which are yielded by the second-order random walks scheme node2vec \cite{155}, from faked ones.
The generator $G$ aims to generate faked node sequences via another LSTM, whose generating process is as follows.
\begin{displaymath}
\begin{array}{l}
v_0=0, \quad z\sim N_m(0,\mathbb{I}_m), \quad (C_0,h_0)=g_{\theta'}(z), \\ [2mm]
(C_j,h_j,o_j)=\text{LSTM}_G(C_{j-1},h_{j-1},v_{j-1}),\quad v_j\sim\text{Cat}\left(\text{Softmax}(o_j)\right).
\end{array}
\end{displaymath}
The motif-targeted GAN \cite{147} generalizes random walk based architecture of the NetGAN to characterize mesoscopic context of nodes.
Different from \cite{147,154}, the GraphVAE \cite{146} adopts a probabilistic graph decoder to generate a probabilistic fully-connected graph, and then employs approximate graph matching to reconstruct the input graph.
Its reconstruction loss is the cross entropy between the input and reconstructed graphs.
The literature \cite{150} defines a sequential decision-making process to add a node/edge via the graph network \cite{85}, readout operator and softmax function.
The GraphRNN \cite{152} is a deep autoregressive model, which generates graphs by training on a representative set of graphs and decomposes the graph generation process into
a sequence of node and edge formations conditioned on the current generated graph.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Combinations of the PI and GNNs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The GNNs and PI are two different learning paradigms for complicated real-world data.
The former specializes in learning hierarchical representations based on local and global structural information, and the latter learning the dependencies between random variables.
This subsection provides a summarization of studies of combining these two paradigms.

\subsubsection{Conditional Random Field Layer Preserving Similarities between Nodes}
The literature \cite{162} proposes a CRF layer for the GCNNs to enforce hidden layers to preserve similarities between nodes.
Specifically, the input $X^{(l)}$ to $(l+1)\text{-th}$ layer is a random vector around the output $B^{(l)}=\text{GCNN}(X^{(l-1)},A_G, X)$ of the $(l-1)\text{-th}$ layer.
The objective function for the GCNN with a CRF layer can be reformulated as
\begin{displaymath}
\begin{array}{l}
J\left(W;X,A_G,Y\right) =\mathcal{L}\left(Y;B^{(L)}\right)+\displaystyle\sum_{l=1}^{L-1}\left(\frac{\gamma}{2}\|X^{(l)}-B^{(l)}\|_F^2+\mathcal{R}(X^{(l)})\right),
\end{array}
\end{displaymath}
where the first term after "=" is the conventional loss function for semi-supervised node classification problem, and
the last term is a regularization one implementing similarity constraint. The similarity constraint $\mathcal{R}(X^{(l)})$ is modeled as a CRF, i.e.
$p\left(X^{(l)}|B^{(l)}\right)=\frac{1}{Z(B^{(l)})}\exp\left(-E\left(X^{(l)}|B^{(l)}\right)\right)$
where the energy function $E\left(X^{(l)}|B^{(l)}\right)$ is formulated as
\begin{displaymath}
\begin{array}{ll}
 & E\left(X^{(l)}|B^{(l)}\right) \\
 = & \displaystyle\sum_{v\in V}\varphi_v(X^{(l)}[v,:],B^{(l)}[v,:])+\displaystyle\sum_{(u,v)\in E}\varphi_{u,v}\left(X^{(l)}[u,:],X^{(l)}[v,:],B^{(l)}[u,:],B^{(l)}[v,:]\right).
\end{array}
\end{displaymath}
Let $s_{u,v}$ denote the similarity between $u$ and $v$. The unary energy component $\varphi_v\left(\cdot,\cdot\right)$ and pairwise energy component
$\varphi_{u,v}\left(\cdot,\cdot,\cdot,\cdot\right)$ for implementing the similarity constraint are respectively formulated as
\begin{displaymath}
\begin{array}{l}
\varphi_v\left(X^{(l)}[v,:],B^{(l)}[v,:]\right)=\|X^{(l)}[v,:]-B^{(l)}[v,:]\|_2^2, \\[1.5mm]
\varphi_{u,v}\left(X^{(l)}[u,:],X^{(l)}[v,:],B^{(l)}[u,:],B^{(l)}[v,:]\right)=s_{u,v}\|X^{(l)}[u,:]-X^{(l)}[v,:]\|_2^2.
\end{array}
\end{displaymath}
The mean-field variational Bayesian inference is employed to approximate the posterior $p(B^{(l)}|X^{(l)})$. Consequently, the CRF layer is defined as
\begin{displaymath}
\left(X^{(l)}[v,:]\right)^{(k+1)}=\frac{\alpha B^{(l)}[v,:]+\beta\sum_{u\in N_G(v)}s_{u,v}\left(X^{(l)}[u,:]\right)^{(k)}}{\alpha+\beta\sum_{u\in N_G(v)}s_{u,v}}.
\end{displaymath}

\subsubsection{Conditional GCNNs for Semi-supervised Node Classification}
The conditional GCNNs incorporate the Conditional Random Field (CRF) into the conventional GCNNs so that the semi-supervised node classification can be enhanced by
both the powerful node representations and the dependencies of node labels.
The GMNN \cite{80} performs the semi-supervised node classification by incorporating the GCNN into the Statistical Relational Learning (SRL).
Specifically, the SRL usually models the conditional probability $p(Y_V|X_V)$ with the CRF, i.e.
$p(Y_V|X_V)=\frac{1}{Z(X_V)}\prod_{(i,j)\in E}\varphi_{i,j}\left(y_i,y_j,X_V\right)$,
where $y_*=Y_V[*,:], *=i,j$. Note that $Y_V$ is composed of the observed node labels $Y_L$ and hidden node labels $Y_U$.
The variational Bayesian inference is employed to estimate the posterior $p(Y_U|Y_L,X_V)$.
The objective function is defined as
\begin{displaymath}
\text{ELBO}\left(q_{\theta_v}(Y_U|X_V)\right)=\mathbb{E}_{q_{\theta_v}(Y_U|X_V)}\left[\log\left(p_{\theta_l}(Y_L,Y_U|X_V)\right)-\log\left(q_{\theta_v}(Y_U|X_V)\right)\right].
\end{displaymath}
This objective can be optimized by the variational Bayesian EM algorithm \cite{161}, which iteratively updates the variational distribution $q_{\theta_v}(Y_U|X_V)$ and the likelihood $p_{\theta_l}(Y_U|Y_L,X_V)$.
In the VBE stage, $q_{\theta_v}(Y_U|X_V)=\prod_{v\in U}q_{\theta_v}(y_v|X_V)$, and $q_{\theta_v}(y_v|X_V)$ is approximated by a GCNN.
In the VBM stage, the pseudo-likelihood is employed to approximate
\begin{displaymath}
\mathbb{E}_{q_{\theta_v}(Y_U|X_V)}\left[\sum_{v\in V}\log p_{\theta_l}\left(y_n|Y_{V\backslash v},X_V\right)\right]
=\mathbb{E}_{q_{\theta_v}(Y_U|X_V)}\left[\sum_{v\in V}\log p_{\theta_l}\left(y_n|Y_{N_G(v)},X_V\right)\right],
\end{displaymath}
and $p_{\theta_l}\left(y_n|Y_{N_G(v)},X_V\right)$ is approximated by another GCNN. The literature \cite{159} adopts the similar idea to the GMNN.
Its posterior is modeled as a CRF with unary energy components and pairwise energy components whose condition is the outputs of the prescribed GCNN.
The maximum likelihood estimation employed to estimate the model parameters.

\subsubsection{GCNN-based Gaussian Process Inference}
A Gaussian Process (GP) defines a distribution over a function space and assumes any finite collection of marginal distributions follows a multivariate Gaussian distribution.
A function $f:\mathbb{R}^d\rightarrow\mathbb{R}$ follows a Gaussian Process $\text{GP}(m(\cdot),\kappa(\cdot,\cdot))$ iff $\left(f(X_1),\cdots,f(X_N)\right)^T$ for any $N$ $d\text{-dimensional}$ random vectors.
follows a $N\text{-dimensional}$ Gaussian distribution $\mathcal{N}_N\left(\mu,\Sigma\right)$, where $\mu=\left(m(X_1),\cdots,m(X_N)\right)^T$ and $\Sigma=\left[\kappa(X_j,X_k)\right]_{N\times N}$,
For two $d\text{-dimensional}$ random vectors $X$ and $X'$, we have $\mathbb{E}\left[f(X)\right]=m(X)$ and $\text{Cov}(f(X),f(X'))=\kappa(X,X')$.
Given a collection of $N$ samples $\mathcal{D}=\left\{(X_j,y_j):j=1,\cdots N\right\}$, the GP inference aims to calculate the probability $p(y|X)$ for predictions, i.e.
\begin{displaymath}
f\sim\text{GP}(0(\cdot),\kappa(\cdot,\cdot)), y_j\sim\text{DIST}(\lambda(f(X_j))),
\end{displaymath}
where $\lambda(\cdot)$ is a link function and $\text{DIST}(\cdot)$ denotes an arbitrary feasible noise distribution. To this end, the posterior $p(f|\mathcal{D})$ needs to be calculated out firstly.
The literature \cite{156} employs amortized variational Bayesian inference to approximate $p(f|\mathcal{D})$, i.e. $f=\mu+L\epsilon$, and the GCNNs to estimate $\mu$ and $L$.

\subsubsection{Other GCNN-based Probabilistic Inference}
The literature \cite{157} combines the GCNNs and variational Bayesian inference to infer the input graph structure.
The literature \cite{158} infers marginal probabilities in probabilistic graphical models by incorporating the GCNNs to the conventional message-passing inference algorithm.
The literature \cite{168} approximates the posterior in Markov logic networks with the GCNN-enhanced variational Bayesian inference.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Adversarial Attacks for the GNNs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In many circumstances where classifiers are deployed, adversaries deliberately contaminate data in order to fake the classifiers \cite{149,364}.
This is the so-called adversarial attacks for the classification problems. As stated previously, the GNNs can solve semi-supervised node classification problems and supervised graph classification tasks.
Therefore, it is inevitable to study the adversarial attacks for GNNs and defense \cite{365}.

\subsubsection{Adversarial Attacks on Graphs}
The literature \cite{182} firstly proposes a reinforcement learning based attack method, which can learn a generalizable attack policy, on graphs.
This paper provides a definition for a graph adversarial attacker.
Given a sample $(G,c,y)\in\{(G_j,c_j,y_j):j=1,\cdots,M\}$ and a classifier $f\in\mathcal{F}$,
the graph adversarial attacker $g:\mathcal{F}\times\mathcal{G}\rightarrow\mathcal{G}$ attempts to modify a graph $G=(V,E)$ into $\widetilde{G}=(\widetilde{V},\widetilde{E})$, such that
\begin{displaymath}
\begin{array}{ll}
\max_{g} & \mathbb{I}(f(\widetilde{G},c)\neq y) \\ [1.5mm]
\text{s.t.}          & \widetilde{G}=g(f,(G,c,y)), \\ [1.5mm]
                     & \mathcal{I}(G,\widetilde{G},c)=1,
\end{array}
\end{displaymath}
where $\mathcal{I}: \mathcal{G}\times\mathcal{G}\times V\rightarrow\{0,1\}$, named an equivalence indicator,
checks whether two graph $G$ and $\widetilde{G}$ are equivalent under the classification semantics.
The equivalence indicator are usually defined in two fashions, namely explicit semantics and small modifications.
The explicit semantics are defined as $\mathcal{I}\left(G,\widetilde{G},c\right)=\mathbb{I}\left(f^*(G,c)=f^*(\widetilde{G},c)\right)$ where $f^*$ is a gold standard classifier,
and the small modifications
are defined as
$\mathcal{I}\left(G,\widetilde{G},c\right)=\mathbb{I}\left(|(E-\widetilde{E})\cup(\widetilde{E}-E)|<m\right)\cdot\mathbb{I}\left(\widetilde{E}\subseteq N(G,b)\right)$
where $N(G,b)=\left\{(u,v):u,v\in V,d_G(u,v)<=b\right\}$. In order to learn an attack policy,
the attack procedure is modeled as a finite horizon Markov Decision Process (MDP) $\mathcal{M}_m(f,G,c,y)$ and is therefore optimized by Q-learning with a hierarchical Q-function.
For the MDP $\mathcal{M}_m(f,G,c,y)$, its action $a_t\in\mathcal{A}\subseteq V\times V$ at time step $t$ is defined to add or delete edges in the graph,
its state $(\widehat{G}_t,c)$ at time step $t$ is a partially modified graph with some of the edges added/deleted from $G$,
and the reward function is defined as
\begin{displaymath}
R\left(\widetilde{G},c\right)=\left\{
\begin{array}{ll}
1 & f(\widetilde{G},c)\neq y \\ [2mm]
-1 & f(\widetilde{G},c)=y.
\end{array}
\right.
\end{displaymath}
Note that the GCNNs are employed to parameterize the Q-function.
The \textsc{Nettack} \cite{181} considers attacker nodes in $\mathcal{A}$ to satisfy a feature attack constraint $X'_{u,j}\neq X_{u,j}^{(0)}\Longrightarrow u\in\mathcal{A}$,
a structure attack constraint $A'_{u,v}\neq A_{u,v}^{(0)}\Longrightarrow u\in\mathcal{A}\vee v\in\mathcal{A}$ and an equivalence indicator constraint
$\sum_{u}\sum_{j}|X_{u,j}^{(0)}-X'_{u,j}|+\sum_{u<v}|A_{u,v}^{(0)}-A'_{u,v}|\leq\Delta$, where $G'$ is derived by perturbing $G^{(0)}$.
Let $\mathcal{P}_{\Delta,\mathcal{A}}^{G0}$ denote the set of all perturbed graphs $G'$ satisfying these three constraints.
The goal is to find a perturbed graph $G'=(A',X')$ that classifies a target node $v_0$ as $c_{\text{new}}$
and maximizes the log-probability/logit to $c_{\text{old}}$, i.e.
$\max_{(A',X')\in\mathcal{P}_{\delta,\mathcal{A}}^{G0}}\max_{c_{\text{new}}\neq c_{\text{old}}}\log Z_{v_0,c_{\text{new}}}^*-\log Z_{v_0,c_{\text{old}}}^*$
where $Z^*=f_{\theta^*}(A',X')$ with $\theta^*=\arg\min_{\theta}\mathcal{L}(\theta;A',X')$. The \textsc{Nettack} employs the GCNNs to model the classifier.
The literature \cite{183} adopts the similar equivalence indicator, and poisoning attacks are mathematically formulated as a bilevel optimization problem, i.e.
\begin{equation}\label{bilevel optimization}
\begin{array}{ll}
\min_{\widetilde{G}} & \mathcal{L}_{\text{attack}}\left(f_{\theta^*}\left(\widetilde{G}\right)\right) \\ [2mm]
s.t.                 & \widetilde{G}\in\mathcal{P}_{\Delta,\mathcal{A}}^{G0} \\ [2mm]
                     & \theta^*=\arg\min_{\theta}\mathcal{L}_{\text{train}}\left(f_{\theta}\left(\widetilde{G}\right)\right).
\end{array}
\end{equation}
This bilevel optimization problem in formula (\ref{bilevel optimization}) is then tackled using meta-gradients,
whose core idea is to treat the adjacency matrix of the input graph as a hyperparameter.

\subsubsection{Defense against the Adversarial Attacks} A robust GCNN requires that it is invulnerable to perturbations of the input graph.
The robust GCN (RGCN) \cite{180} can fortify the GCNs against adversarial attacks. More specifically, it adopts Gaussian
distributions as the hidden representations of nodes, i.e.
\begin{displaymath}
X^{(l+1)}[j,:]\sim N\left(\mu_j^{(l+1)},\text{diag}(\sigma_j^{(l+1)})\right),
\end{displaymath}
in each graph convolution layer so that the effects of adversarial attacks can be absorbed into the variances of the Gaussian distributions.
The Gaussian based graph convolution is defined as
\begin{displaymath}
\mu_j^{(l+1)}=\rho\left(\displaystyle\sum_{v_k\in N_G(v_j)}\frac{\mu_k^{(l)}\circledast\alpha_k^{(l)}}{\sqrt{\widetilde{D}_{j,j}\widetilde{D}_{k,k}}}W_{\mu}^{(l)}\right),
\sigma_j^{(l+1)}=\rho\left(\displaystyle\sum_{v_k\in N_G(v_j)}\frac{\sigma_k^{(l)}\circledast\alpha_k^{(l)}\circledast\alpha_k^{(l)}}{\widetilde{D}_{j,j}\widetilde{D}_{k,k}}W_{\sigma}^{(l)}\right),
\end{displaymath}
where $\alpha_j^{(k)}$ are attention weights. Finally, the overall loss function is defined as regularized cross-entropy.
The literature \cite{176} presents a batch virtual adversarial training method
which appends a novel regularization term to the conventional objective function of the GCNNs, i.e.
\begin{displaymath}
\mathcal{L}=\mathcal{L}_0+\alpha\cdot\frac{1}{N}\sum_{u\in V}E(p(y|X_u,W))+\beta\cdot\mathcal{R}_{vadv}(V,W),
\end{displaymath}
where $\mathcal{L}_0$ is an average cross-entropy loss of all labelled nodes, $E(\cdot)$ is the conditional entropy of a distribution,
and $\mathcal{R}_{\text{vadv}}(V,W)$ is the average Local Distributional Smoothness (LDS) loss for all nodes. Specifically,
$\mathcal{R}_{\text{vadv}}(V,W)=\frac{1}{N}\sum_{u\in V}\text{LDS}(X[u,:],W,r_{\text{vadv},u})$ where $\text{LDS}(x,w,r_{\text{vadv}})=D_{KL}\left(p(y|x,\widehat{W})||p(y|x+r_{\text{vadv}},W)\right)$
and $r_{\text{vadv}}$ is the virtual adversarial perturbation. Additionally, there are some other studies aiming at verifying certifiable (non-)robustness to structure and feature perturbations for the GCNNs
and developing robust training algorithm \cite{177,178}. The literature \cite{180} proposes to improve GCN generalization by minimizing the expected loss
under small perturbations of the input graph.
Its basic assumption is that the adjacency matrix $A_G$ is perturbed by some random noises. Under this assumption, the objective function is defined as
$\min_{W}\int q(\epsilon|\alpha)\mathcal{L}(X,Y,A_G(\epsilon),W)d\epsilon$, where $A_G(\epsilon)$ denotes the perturbed adjacency matrix of $G$ and $q(\epsilon|\alpha)$ is a zero-centered density of the noise $\epsilon$
so that the learned GCN is robust to these noises and generalizes well.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Graph Neural Architecture Search}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Neural Architecture Search (NAS) \cite{171} has achieved tremendous success in discovering
the optimal neural network architecture for image and language learning tasks. However, existing NAS algorithms cannot be
directly generalized to find the optimal GNN architecture. Fortunately, there have been some studies to bridge this gap.
The graph neural architecture search \cite{172,173} aims to search for an optimal GNN architecture within a designed search space.
It usually exploits a reinforcement learning based controller, which is a RNN, to greedily validate the generated architecture, and then the validation results are fed back to the controller.
The literature \cite{175} proposes a Graph HyperNetwork (GHN) to amortize the search cost of training thousands of different networks,
which is trained to minimize the training loss of the sampled network with the weights generated by a GCNN.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Graph Reinforcement Learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The GNNs can also be combined with the reinforcement learning so as to solve sequential decision-making problems on graphs.
The literature \cite{188} learns to walk over a graph from a source node towards a target node for a given query via reinforcement learning.
The proposed agent M-Walk is composed of a deep RNN and Monte Carlo Tree Search (MCTS). The former maps a hidden vector representation $h_t$,
yielded by a special RNN encoding the state $s_t$ at time $t$, to a policy and Q-values, and the latter is employed to generate trajectories yielding more positive rewards.
The NerveNet \cite{187} propagates information over the underlying graph of an agent via a GCNN, and then predicts actions for different parts of the agent.
The literature \cite{186} combines the GNNs and Deep Reinforcement Learning (DRL), named DRL+GNN, to learn, operate and generalize over arbitrary network topologies.
The DRL+GNN agent employs a GCNN to model the Q-value function.

\subsection{Applications}

%\begin{table}[H]
%  \centering
%  \caption{The applications of the GNNs.}
%  \begin{tabular}{|c|m{5.3cm}|}
%    \hline
%     \textbf{Application Fields} & \textbf{References} \\
%    \hline
%     Complex Network Analysis & \cite{228,229,230,231,232} \\ %,233,234,235,236,237,238} \\
%    \hline
%     Combinatorial Optimization & \cite{214,215,216,217,218} \\ %,219,220,221,222,223,224,225,226,227} \\
%    \hline
%     Knowledge Graph & \cite{272,273,274,285,286} \\ %,287,288,289,290,291,292,293,294,295,404} \\
%    \hline
%     Chemistry and Biology & \cite{195,196,197,198,199} \\ %,200,201,202,203,204,205,206,207,208,209,210,211,212} \\
%    \hline
%     Physical System & \cite{332,333,334,335,336} \\ %,337,338,339} \\
%    \hline
%     Source Code Analysis & \cite{340,341,342} \\
%    \hline
%     Intelligent Traffic & \cite{353,354,355,356,357} \\ %,358,359,403} \\
%    \hline
%     Recommender Systems & \cite{112,343,344,345,346} \\ %,347,348,349,350,351,352} \\
%    \hline
%     Computer Vision & \cite{249,241,259,261,266,275} \\ %,250,251,252} \\
%    %\cline{2-3}
%%       & Image Classification & \cite{241,248,253,254,255,256,257,258,363} \\
%%    \cline{2-3}
%%       & Point Cloud & \cite{259,260,265} \\
%%    \cline{2-3}
%%       & Scene Graph Generation & \cite{261,262,263,264,284} \\
%%    \cline{2-3}
%%       & Semantic Segmentation & \cite{266,268,269,405} \\
%%    \cline{2-3}
%%       & Visual Question Answering & \cite{275,276,277,278,279,280,281,282} \\
%%    \cline{2-3}
%%       & Miscellaneous & \cite{239,240,242,243,244,245,246,247,270,283,368,369,370,371} \\
%    \hline
%     Natural Language Porcessing & \cite{303,308,312,318,326} \\ %,304,305,306,307} \\
%     %\cline{2-3}
%%       & Reading Comprehension & \cite{308,309,310,311} \\
%%     \cline{2-3}
%%       & Relation Extraction & \cite{312,313,314,315,316,317,406} \\
%%     \cline{2-3}
%%       & Text Classification & \cite{318,319,320,321,322,323,324,325} \\
%%     \cline{2-3}
%%       & Text Generation & \cite{326,327,328,329,330} \\
%%     \cline{2-3}
%%       & Miscellaneous & \cite{296,297,298,299,300,301,302,331} \\
%    \hline
%  \end{tabular}
%  \label{application_summarization}
%\end{table}

In this subsection, we introduce the applications of the GNNs.
Due to the space limitation, we only list the application fields, including complex network analysis \cite{233,234,238}, combinatorial optimization \cite{215,216,223,224},
knowledge graph \cite{272,273,289}, bioinformatics \cite{201,210,211}, chemistry \cite{197,202,203}, brain network analysis \cite{205,206},
physical system \cite{102,338,339}, source code analysis \cite{340,341,342}, intelligent traffic \cite{353,355,403}, recommender systems \cite{343,344,349},
computer vision \cite{112,343,344,345,346} and natural language processing \cite{303,308,312,318,326}.

%\begin{figure}[htb]
%  \centering
%  \includegraphics[width=1.0\textwidth]{ApplicationSummarization.pdf}
%  \caption{The GNN Application Summarizations}
%  \label{application summarization}
%\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Benchmarks and Evaluation Pitfalls}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we briefly introduce benchmarks and evaluation Pitfalls. The benchmarks provide ground truth for various GNN architectures so that different GNNs can be compared fairly,
the evaluation pitfalls empirically show that the existing evaluation criterion have potential pitfalls.

\subsubsection{Benchmarks}
Graph neural networks have become a powerful toolkit for mining complex graphs.
It becomes more and more critical to evaluate the effectiveness of new GNN architectures and
compare different GNN models under a standardized benchmark with consistent experimental settings and large datasets.
A feasible benchmark for the GNNs should include appropriate graph datasets, robust coding interfaces and experimental settings
so that different GNN architectures can be compared in the same settings. The literature \cite{169} makes a pioneering effort to
construct a reproducible GNN benchmarking framework in order to facilitate researchers to gauge the effectiveness of different GNN architectures.
Specifically, it releases an open-source benchmark infrastructures for GNNs, hosted on GitHub based on PyTorch and DGL libraries \cite{402},
introduces medium-scale graph datasets with 12k-70k graphs of variable sizes 9-500 nodes, and identifies important building blocks of GNNs
(graph convolutions, anistropic diffusion, residual connections and normalization layers) with the proposed benchmark infrastructures.
The literature \cite{372} presents an Open Graph Benchmark (OGB) including challenging, real-world and large-scale benchmark graph datasets, encompassing
multiple important graph deep learning tasks ranging from social and information networks to biological networks, molecular graphs and knowledge graphs,
to facilitate scalable, robust and reproducible deep learning research on graphs. The OGB datasets, which provide a unified evaluation protocol
using application-specific train/validation/test dataset splits and evaluation metrics,
releases an end-to-end graph processing pipeline including graph data loading, experimental settings and model evaluations.

\subsubsection{Evaluation Pitfalls}
The literature \cite{170} compares four typical GCNN architectures: GCN \cite{6}, MoNet \cite{18},
GAT \cite{66} and GraphSAGE using three aggregation strategies \cite{104}
against 4 baseline models: logistic regression, multi-layer perceptron, label propagation and normalized laplacian label propagation,
and uses a standardized training and hyper-parameter tuning procedure for all these models so as to perform a more fair comparison.
The experimental results show that different train/validation/test splits of datasets lead to dramatically different rankings of models.
In addition, their findings also demonstrate that simpler GCNN architectures can outperform more sophisticated ones
only if the hyper-parameters and training procedures are tuned fairly for all models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Research Directions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Although the GNNs have achieved tremendous success in many fields, there still exists some open problems.
This section summarizes the future research directions of the GNNs.

\subsubsection{Highly Scalable GNNs}
The real-world graphs usually contain hundreds of millions of nodes and edges, and have dynamically evolving characteristics.
It turns out that it is difficult for the existing GNN architectures to scale up to the huge real-world graphs.
This motivates us to design highly scalable GNN architectures which can efficiently and effectively learn node/edge/graph representations
for the huge dynamically-evolving graphs.

\subsubsection{Robust GNNs}
The existing GNN architectures are vulnerable to adversarial attacks.
That is, the performance of the GNN models will sharply drop once the structure and/or initial features of the input graph are attacked by adversaries.
Therefore, we should incorporate the attack-and-defense mechanism into the GNN architectures, i.e. constructing robust GNN architecture,
so as to reinforce them against adversarial attacks.

\subsubsection{GNNs Going Beyond WL Test}
The capabilities of the spatial GCNNs are limited by the 1-WL test,
and the higher-order WL test is computationally expensive. Consequently, two non-isomorphic graphs
will produce the same node/edge/graph representations under appropriate conditions.
This motivates us to develop a novel GNN framework going beyond WL test, or design an elegant higher-order GNN architectures
corresponding to the higher-order WL test.

\subsubsection{Interpretable GNNs}
The existing GNNs work in a black box. We do not understand why they achieve state-of-the-art performance in terms of
the node classification task, graph classification task and graph embedding task etc. Interpretability has become a major obstacle to
apply the GNNs to real-world issues. Although there have been some studies to interpret some specific GNN models,
they cannot interpret general GNN models. This motivates us to construct a unified interpretable framework for the GNNs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This paper aims to provide a taxonomy, advances and trends for the GNNs. We expand the content of this paper from 4 dimensions: architectures, extensions and applications, benchmarks and evaluation pitfalls,
and future research directions.
The GNN architectures are expanded from 4 perspectives: graph convolutional neural networks, graph pooling operators, graph attention mechanisms and graph recurrent neural networks.
The extensions are expanded from 8 perspectives: GCNNS on special graphs, capability and interpretability, deep graph representation learning, deep graph generative models, combinations of the PI and GNNs
adversarial attachks for the GNNs, graph neural architecture search and graph reinforcement. In the future directions, we propose 4 prospective topics on the GNNs: highly scalable GNNs, robust GNNs,
GNNs going beyond WL test and interpretable GNNs. We expect that the relevant scholars can understand the computational principles of the GNNs,
consolidate the foundations of the GNNs and apply them to more and more real-world issues, through reading this review.

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
This work was supported by National Natural Science Foundation of China (Grant No. 62002255, 62076177, 61972273).
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

%%
%% If your work has an appendix, this is the place to put it.
%\appendix
%\section{Research Methods}
%\subsection{Part One}
%xxx
%\subsection{Part Two}
%xxx
%\section{Online Resources}
%xxx

\end{document}
\endinput
%%
%% End of file `sample-acmsmall.tex'.
