\section{Introduction}
%
Reinforcement learning is one of the most studied domains in machine learning thanks to its drastic improvement over the past few years. Reinforcement learning has led to recent breakthroughs of machine learning applications in games, prominently in Atari games \cite{atari}, and more recently, the Chinese game of Go \cite{Silver_2016}. What differs reinforcement learning from traditional machine learning is that there is no training dataset or labels, and agents are to learn how to behave in an environment by performing actions and observing rewards that it receives, where the goal of the agent is to find a set of actions that will maximize the total reward. Essentially, the idea of reinforcement learning is that agents are to learn based on their experience just like how humans learn. Since reinforcement learning enables an agent to learn autonomously from its own experience, it is a great framework for learning behaviors of agents in applications such as soccer games, in particular, where a vast amount of possibilities can occur.

Efforts to use reinforcement learning in various games have been made
\cite{StarCraft}\cite{Dota}\cite{ViZDoom}. The AI World Cup, a set of AI competitions based on the game of soccer, was even established in 2017 and the official international AI World Cup has been held in 2018 and 2019 and the AI Masters competition was held in 2019 as a part of the World Cyber Games \cite{aiwc}. Some organizations also have held soccer simulation tournaments \cite{fira}\cite{robocup}. In addition, various strategies and methods for obtaining good results in the tournaments were presented \cite{inbook}\cite{Fukushima}. Among them, we used the AI Soccer environment proposed by \cite{aiwc} (available at \cite{git}) for the experiment. AI Soccer is a 5:5 robot soccer game where each team is composed of one goalkeeper, two defenders, and two forwards. However, the role does not affect the robots’ capabilities: the only difference between each player is its specifications and initial position. The game is comprised of 5 minutes first half and 5 minutes second half. At the beginning of each half and after a team makes a goal, kick-off happens where only the second forward player of the ball owner’s team can move. 

In AI Soccer, there is a situation where the robots fail to track the soccer ball correctly, preventing the game to proceed further. In order to avoid and handle such situations effectively, three different deadlock rules are implemented. Deadlock here is referred to as a situation where the soccer ball moves at a speed that is slower than 0.4 m/s for 4 seconds. First, deadlock can occur in one of the four corner areas. In such a case, the game proceeds to a corner kick. A corner kick can also be initiated when the ball leaves the soccer field, and no robots except the second forward player in the ball owner’s team can move. Secondly, deadlock can happen in one of the two penalty areas, then the game proceeds either to a penalty kick if the ball owner is on the opposite side of their goalpost or a goal kick, otherwise. In such a situation, if the foul is made by the defense team, the defending team can only have three robots inside the penalty area at the same time, while if the foul is made by the offense team, the attacking team can only have two robots inside the penalty area at the same time. Lastly, deadlock can occur in other regions which will lead to a ball relocation. There are four ball relocation positions in total, and during ball relocation, the ball will be relocated to a position closest to the current ball. Each region of the soccer field is shown in Fig. \ref{areas}.

\begin{figure}
    \centerline{\includegraphics[width=10cm]{Figures/Fig1.png}}
    \caption[Different areas on the soccer field]{Different areas in the soccer field. The figure is taken from \cite{aiworldcup}.
    } \label{areas}
\end{figure}

There is a situation where a robot falls and fails to recover for more than 3 seconds. If such a case happens, the robot is moved out of the soccer field and stays inactive for 5 seconds. After 5 seconds, the robot is moved back to the designated position and orientation on the soccer field. 

Reinforcement learning can be explained using numerous factors such as an agent, actions, environment, states, and rewards. An agent is the one that takes an action, where the action is the set of all possible behavior that the agent can take. The environment is the world that the agent is navigating through, and the state is a situation in which the agent finds itself. The Reward is the feedback that the agent receives indicating how good or bad the action was. The strategy that the agent utilizes to determine the next action is called policy, $\pi$. A value function, \textit{$V_{\pi}(s)$}, estimates how good it is for the agent to be in a given state, and it is expressed as follows:
\begin{eqnarray}
V_{\pi}(s)=E_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=s\right]
\label{eqn1}
\end{eqnarray}

Similarly, the long-term return of the current state, $s$, taking action, $a$, under the policy, $\pi$, is referred to as the action-value function, $Q_\pi(s, a)$, expressed as follows:
\begin{eqnarray}
Q_{\pi}(s, a)=E_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=s, A_{t}=a\right].
\label{eqn2}
\end{eqnarray}

Many reinforcement learning algorithms follow Markov Decision Process (MDP) to approximate the probability distribution of reward over state-action pairs and the idea of estimating the action-value function by using the Bellman equation. MDP is a mathematical framework to describe an environment in reinforcement learning and almost all RL problems can be formalized using MDPs. An MDP consists of a set of states $S$, a set of possible actions $A(s)$ in each state, a reward function $R(s)$, and a transition model $P(s\prime, s | a)$. The above two value equations obey the Bellman equation, described in equation \ref{eqn3}, of which the fundamental idea is that if the optimal value of the sequence at the next time step is known for all possible actions, then the optimal strategy is to select the action which maximizes the expected value of return \cite{atari} as follows:
\begin{eqnarray}
Q(s, a)=r(s, a)+\gamma \max _{a} Q\left(s^{\prime}, a\right).
\label{eqn3}
\end{eqnarray}

Reinforcement learning algorithms repeat actions that lead to reward, which is called exploitation. However, this will stop agents to take exploration, which could eventually lead to a better reward, and lead to a narrow number of paths agents can take. In order to prevent agents from only exploiting, Epsilon Greedy Policy is employed, where agents are to take a known path with epsilon probability and explore the rest.
Many researchers have proposed different algorithms for reinforcement learning such as Deep Q-Network (DQN), Proximal Policy Optimization (PPO) \cite{ppo}, Deep Deterministic Policy Gradient (DDPG), and Soft Actor-Critic (SAC) \cite{sac}, etc. In order to train our agents for the AI Soccer game, we employed the DQN algorithm.
