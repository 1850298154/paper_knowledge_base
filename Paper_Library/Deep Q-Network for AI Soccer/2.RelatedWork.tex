\section{Related Work}

In this section, we review some of the most well-known reinforcement learning algorithms that we attempted to use for AI Soccer to train our agents.

\subsection{Deep Q-Network (DQN)}
Deep Q-Network \cite{atari} was first proposed in 2013 demonstrating its superior performance on seven Atari 2600 games. DQN is the first deep learning model to successfully learn control policies directly from high-dimensional input using reinforcement learning. The model is a convolutional neural network whose input is raw pixels and output is a value function estimating future rewards. The network is trained with a variant of the Q-learning algorithm with stochastic gradient descent to update the weights. 

In reinforcement learning, both the input and the target change constantly during the training process and make training unstable. To resolve instability, DQN employs two networks, one for retrieving Q-values, while the other one includes all updates in the training. After a set number of updates, the two networks are synchronized to fix the parameters of the target function and replace them with the latest network. In addition, DQN adapts the experience replay mechanism which randomly samples previous transitions and forms an input dataset with enough stability for training. To alleviate the high correlation between data and to make data independent of each other, data are then randomly sampled from the replay buffer. Lastly, to obtain the optimal action-value function, the following equation is used:
\begin{eqnarray}
Q^{*}(s, a)=E_{s^{\prime} \sim \varepsilon}\left[r+\gamma \max _{a} Q^{*}\left(s^{\prime}, a^{\prime}\right) \mid s, a\right].
\label{eqn4}
\end{eqnarray}

\iffalse
\begin{figure}
    \centerline{\includegraphics[width=10cm]{Figures/Fig2.png}}
    \caption[Algorithm of Deep Q-Network]{Algorithm of Deep Q-Network. The figure is taken from \cite{atari}
    } \label{dqn}
\end{figure}
\fi 
While DQN solves problems with high-dimensional observation spaces, it can only handle discrete and low-dimensional action spaces. Hence, to address the limitations of DQN, Deep Deterministic Policy Gradient (DDPG) was introduced.


\subsection{Deep Deterministic Policy Gradient (DDPG)}
Deep Deterministic Policy Gradient (DDPG) \cite{ddpg} was first introduced in 2015 to address the limitations of DQN. DDPG is an actor-critic, model-free algorithm that can operate over continuous action spaces. The algorithm can solve more than 20 simulated tasks such as cart-pole, dexterous manipulation, legged locomotion, and car driving.
DDPG is an algorithm that concurrently learns a Q-function and a policy. It uses off-policy data and the Bellman equation to learn the Q-function and uses the Q-function to learn the policy. DDPG utilizes Q-learning, of which fundamental idea is that if you know the optimal action-value function, then in any given state, the optimal action can be found by solving the following equation:
\begin{eqnarray}
a^{*}(s)=\arg \max _{a} Q^{*}(s, a).
\label{eqn5}
\end{eqnarray}

The optimal action-value function can be found using the same equation \ref{eqn4} used in DQN. 

DDPG is comprised of two networks: the actor and the critic network. The actor function, $\mu(s | \theta_{\mu})$, in the actor network specifies action given the current state of the environment. The critic value function, $Q(s, a | \theta_{Q})$, in the critic network then calculates the TD error to criticize the actions made by the actor. Similar to DQN, DDPG ensures exploration by introducing noise to the action, and it also uses the same experience replay mechanism adopted in DQN to have a stable behavior. \iffalse Fig. \ref{ddpg} is the algorithm of DDPG.

\begin{figure}
    \centerline{\includegraphics[width=10cm]{Figures/Fig3.png}}
    \caption[Algorithm of Deep Deterministic Policy Gradient]{Algorithm of Deep Deterministic Policy Gradient. Figure is taken from \cite{ddpg}
    } \label{ddpg}
\end{figure} \fi 