%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


%以下部分均为新增--22.10.31
\usepackage{amsmath,amsfonts}
%\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage[T1]{fontenc}

%新增的编写表格文档
\usepackage{booktabs}
\usepackage{multirow}

%新增的编写伪代码的引用文档
\usepackage{CJK}
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}
%\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
%\usepackage{amsmath}

%此处用来改算法的头栏目注释：algorithm 1，此处加1，则现实算法1，2，3的序号
\floatname{algorithm 1}{算法}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

%使用插入网址/超链接的包
\usepackage{hyperref}

\hypersetup{hidelinks,
	colorlinks=true,
	allcolors=black,
	pdfstartview=Fit,
	breaklinks=true}

%百家提供的模板
%\usepackage[pdftex]{graphicx}
%\graphicspath{{graphs/}}
%\else
%\fi
%
%\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%\usepackage{algorithm}
%\usepackage{algorithmic}
%\usepackage{algpseudocode}
%\renewcommand{\algorithmicrequire}{\textbf{Input:}} 
%\renewcommand{\algorithmicensure}{\textbf{Output:}} 
\makeatletter
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}
\makeatother

%\ifCLASSOPTIONcompsoc
%\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%\usepackage[caption=false,font=footnotesize]{subfig}
%\fi



\usepackage{stfloats}


\usepackage{bm}

\usepackage{multirow}
\usepackage{color}
\usepackage{footnote}
\usepackage{threeparttable}

\newtheorem{problem}{\it Problem}
\newtheorem{condition}{Condition}
\newtheorem{assumption}{Assumption}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{\bf Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{notation}{Notation}
\newtheorem{definition}{\bf Definition}
\newtheorem{remark}{Remark}

%新增加可选颜色--22.11.04
\usepackage{xcolor}

%原本就有
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}







\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{ORCA-DRL: A Socially Aware Mapless Navigation Policy in Complex Dynamic Scenes}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Jianmin~Qin,
        Jiahu~Qin,
        Qingchen~Liu,
        Huijun~Gao,~\IEEEmembership{Fellow,~IEEE}
        and~Sandra~Hirche,~\IEEEmembership{Fellow,~IEEE}% <-this % stops a space
\thanks{Jianmin Qin, Jiahu Qin and Qingchen Liu are with the Department of Automation, University of Science and Technology of China, Hefei 230027, China. Huijun Gao is with the Research Institute of Intelligent Control and Systems, Harbin Institute of Technology, Harbin 150006, China. Sandra Hirche is with the Chair of Information-Oriented Control, Technical University of Munich, Munich, Germany.
}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
For real-world navigation tasks, it is challenging to endow robots capabilities to navigate safely and efficiently in a complex environment with dynamic and non-convex static obstacles. However, achieving path-finding in non-convex complex environments as well as enabling multiple robots to follow social rules for obstacle avoidance remains challenging problems. In this article, we propose a socially aware robot mapless navigation algorithm with ORCA-Deep Reinforcement Learning. ORCA-DRL combines the advantages of Reinforcement Learning and optimal reciprocal collision avoidance(ORCA) while considering the social norms. Our framework introduces traffic norms of human society to improve social comfort and achieve cooperative avoidance following human social customs  (e.g. passing on the right, overtaking on the left and low-priority vehicles avoiding high-priority ones). The simulation results show that ORCA-DRL learns strategies to obey specific traffic rules. Compared to DRL, ORCA-DRL shows a significant improvement in navigation success rate in complex mixed scenarios (from 58.0\% to 90.7\% in Scenario-3 and from 84.0\% to 90.0\% in Scenario-4) with the application of the same training network. ORCA-DRL is able to cope with non-convex obstacle environments without falling into local extremes and has a 14.1\% improvement in path quality (i.e., average time to target) comparing to ORCA.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Mapless navigation, Collision avoidance, Deep reinforcement learning (DRL), Social norms.
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.

% \IEEEPARstart{I}{n} real autonomous navigation tasks, robots often encounter complex scenarios and varying numbers of dynamic obstacles. Since accurate maps are often lacking in many unknown environments, it is crucial for robots to achieve safe and efficient mapless navigation based only on the environmental information collected by their on-board sensors. Yet, finding collision-free, time efficient paths around other agents remains challenging due to dynamic obstacle environments and complex coupled interactions between agents [6].


\IEEEPARstart{T}{he} {\color{blue}mapless navigation is an important task for various practical applications in industry, transportation, and public services, such as smart warehousing and autonomous driving. One essential problem of mapless navigation is to design safe and efficient control algorithms with limited observations of the dynamic environment. In conventional mapless navigation, the control policy is designed for a single robot where the other robots are modeled as dynamic obstacles in the environment. This approach neglects both the similarities of the motions of the robots and the interactions among them, thus complicated the design of the control algorithms. Nowadays, it is popular to use a multi-robot system to model a mapless navigation task. Among them, there are two major types of control design methods for mapless navigation, reinforcement learning based methods and model based methods.}

In recent years, the development of Deep Reinforcement Learning (DRL) has achieved remarkable results in a variety of fields [1]-[3]. In the field of mapless navigation, DRL is used to train an end-to-end navigation planner for the robot [17]. The robot utilizes sensor information (e.g. data collected by LIDAR or cameras) as input to the DRL algorithm. DRL continuously interacts with the environment to learn the mapping from states to actions and obtains a better navigation strategy by maximizing the reward. However, in complex dynamic obstacle scenarios, DRL based algorithms is difficult to ensure its success rate of collision-free obstacle avoidance, meanwhile the stability and interprebility cannot be provided [17][18].

{\color{green} Traditional model-based navigation algorithms are usually a combination of global planning algorithms (e.g., A* [12], D* [13]) and local collision avoidance algorithms (e.g., Artificial Potential Field (APF) [7], Dynamic Window Approach (DWA) [8], Velocity Obstacles (VO) [9], BUG [10]). However, for scenarios lacking maps, global planning algorithms (e.g., A* [12], D* [13]) usually do not work accurately and only local algorithms work efficiently, which can easily let the robot fall into local extremes and fail in navigation. For example,} Berg [11] proposed optimal reciprocal collision avoidance (ORCA) framework developed from VO, which is popular in multi-agent systems and crowd simulation. ORCA derived sufficient conditions {\color{red}why say conditions?} for collision-free motion by solving a low-dimensional linear programming problem, and guaranteed {\color{red} collision-free motion Q: what is local collison-free motion?} all time for a large number of robots in a cluttered workspace. Nevertheless, it is difficult for ORCA to effectively achieve navigation in complex non-convex static obstacle scenarios without a map or global path planning algorithm,such as A* [12],D* [13]. {\color{red}Moreover, ORCA can not obtain better or shorter trajectory or take human society rules into account in navigation, leading to unnatural behavior of robots in crowded environments [14], such as trajectory oscillations and traffic habit violations.}

{\color{blue}
We note that the recent advances in robotics technology are gradually enabling robots to work in shared spaces with humans. This is especially common in places such as hospitals, airports, and shopping malls. Therefore, understanding and complying with social rules to cooperate in avoiding collisions and maintaining a safe and comfortable social distance from each other is also an important aspect when designing navigation algorithms [14].
}

In order to combine the advantages of RL and traditional collaborative collision avoidance method while considering the social rules, we propose a socially aware mapless navigation algorithm with ORCA-Deep Reinforcement Learning. 
Compared with the existing works, the main contributions and novelties of this article are {summarised} as follows:

\begin{itemize}
	\item{{\color{red} We propose a novel method}, i.e., ORCA-DRL, is proposed to the problem of mapless navigation in this article. The DRL and model-based collision avoidance method (ORCA) are integrated into a new navigation algorithm, which improves the success rate of dynamic obstacle avoidance and shows better performance in motion safety of robots.}
	\item{ORCA-DRL framework introduced traffic norms of human society to enhance social comfort and achieve cooperative avoidance that follows human social (such as passing on the right, overtaking on the left and low-priority vehicles avoiding high-priority ones).}
	\item{ORCA-DRL overcomes the problem that ORCA cannot handle non-convex static obstacles and significantly improves navigation success in complex mixed scenes with dynamic and non-convex static obstacles.}
\end{itemize}


\section{RELATED WORK}

{\color{red}For mapless navigation tasks in complex dynamic environments, three parts of work (as in Fig.1) need to be executed, they are: finding  feasible paths to destinations based on sensor information (Task-1); obeying common social rules (e.g., traffic rules) among the agents (Task-2); and achieving dynamic and static obstacle avoidance (Task-3). Q: What is the relationship between this paragraph and related work?}{\color{green} R: we add the related work of the combined navigation method of model-based and learn-based, such as RL-RVO. } Existing studies on socially compliant multi-agent obstacle avoidance and cooperative navigation can be broadly classified into two categories: learning-based and model-based. 

\begin{figure}[!t]
	\centering
		\vspace{-0.1cm}
	\includegraphics[width=3.0in]{fig1}
	\caption{For mapless navigation tasks in complex dynamic environments, three parts of work need to be executed, they are: finding feasible paths to destinations based on sensor information (Task-1); obeying common social rules (e.g., traffic rules) among the agents (Task-2); and achieving dynamic and static obstacle avoidance (Task-3).}
	\label{fig1}
\end{figure}

\subsection{Learning-based Navigation approachs}

Learning-based method is an end-to-end approach to achieve Tasks 1-3. Learning-based {\color{blue}navigation} technique has recently gained much attention, which is extensively studied because of its greater performance and robustness {\color{blue}over} traditional approaches [15]. {\color{blue}The work in} [16] trained a target-oriented end to-end motion planning model {\color{red}module?} {\color{blue} based on} expert demonstrations generated in simulation with an existing motion planner. However, the performance of the learning-based algorithms is severely limited by the quality of the labeled training set. To overcome this deficiency, {\color{red}M.Everett and Y.Chen [17], [18] proposed a decentralized multi-agent collision avoidance algorithm based on a novel application of DRL and LSTM, which showed more than $26\% $ improvement in paths quality (i.e., the time to reach the destination) compared with ORCA. Q:why compare to ORCA?????}{\color{green}We designed a novel Imitation Learning training scheme based on Dataset Aggregation (DAgger) to obtain a mapless navigation policy [40].}{\color{green} R. Han [38] proposed a navigation approach RL-RVO which combines the concept of RVO and DRL to solve the reciprocal collision avoidance problem under limited information. But the experimental scenario of RL-RVO only contains dynamic obstacles, and the RL-RVO was not tested in scenes with a mixture of dynamic and static obstacles.} Nonetheless, due to the relatively simple and structured test environment of RL, it is difficult for the learned planner to guarantee absolute safety and collision-free success rate  (Task-3) in complex dynamic environment [17]-[20].

\subsection{Model-based approaches}

{\color{green} Model-based navigation algorithms are usually a combination of global planning algorithms and local collision avoidance algorithms. However, global planning algorithms (e.g., A* [12], D* [13]) usually do not work accurately for scenarios lacking maps and only local algorithms work effectively.
The Optimal Reciprocal Collision Avoidance (ORCA) framework [11] is widely used in multi-agent systems and crowd simulation in limited scenarios without maps.} {\color{red}J. Berg et al. [11], [21], [23], [24] has conducted a series of researches in the field of multi-agent collision avoidance and proposed various efficient algorithms, such as RVO [21], ORCA [11], AVO [22], LQR [23] and GRCA [24]. To deploy ORCA on the most common differential drive robots, several modified algorithms (ORCA-DD [25], NH-ORCA [26]) have been proposed to deal with the difficulty of non-holonomic robot kinematics. Non-holonomic optimal reciprocal collision avoidance (NH-ORCA) builds on the concepts introduced in RVO [21], but makes a differential drive robot tracking a holonomic speed vector with a certain tracking error $\varepsilon _{H} $, as shown in Fig.2. It guarantees smooth and collision-free motions under non-holonomic constraints and is widely popular in multi-robot navigation. 

Nonetheless, ORCA's series of algorithms cannot perform motion planning to obtain optimal trajectories (e.g., the shortest trajectory) [17], nor can they complete pathfinding (Task-1) in a non-convex static obstacle environment and get stalled in local extremum regions [11] (as shown in Fig.14b).}


\subsection{Inducing Social Norms in Navagation}

{\color{red}In human society, people tend to cooperate and adopt traffic rules in order to avoid collisions while ensuring comfort and safety (Task-2).} {\color{blue}This motivated the use of social norms in navigation algorithms enable robots to interact with humans in a socially compliant way.} {\color{green} In model-based approaches, the work in [27]-[29] introduced additional parameters to interpret social interactions and establish social comfort criteria, extending the traditional multi-agent collision avoidance algorithm.} {\color{red}Q: unclear} Since they usually correspond to intuitive geometric relations, model-based approaches are usually computationally efficient, but they have been observed to potentially lead to oscillatory paths [17], [31]. In contrast, learning-based methods recover the expert policy directly through supervised learning (e.g., behavior cloning) [33], or aim to use reinforcement learning (RL) or inverse reinforcement learning [34] to model the complex interactions and cooperation among agents [14] [32]. Chen et al. [14] designed a complex negative reward to train the agent to develop a time-efficient navigation policy that accords with common social norms (e.g., passing by the right/left side). 

In this work, we combine the advantages of model-based approach (ORCA) and reinforcement learning to propose a fused novel algorithm ORCA-DRL for navigation in dynamic environments. The fusion method improves pathfinding and dynamic obstacle avoidance in complex scenes while considering social norms.


\begin{figure}[!t]
	\centering
		\vspace{-0.3cm}
	\includegraphics[width=3.0in]{fig2}
	\caption{The dynamics model of the differential wheel robot [26]. To guarantee the robot's motion safety under the non-holonomic kinematic constraints, a non-holonomic tracking error $\varepsilon _{H} $ is added to the robot. ICR is the instantaneous center of rotation, $v_{t} $ is the current velocity, $v_{t+1} $, $w_{t+1} $ are the velocity and angular velocity of the action $a_{t+1} $ at the next moment, $a_{t+1}=\left ( v_{t+1},w_{t+1} \right ) $.	$V_{H} $ represents the complete velocity and $\theta_{H} $ represents the rotation angle.}
	\label{fig2}
\end{figure}



% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.


\section{ PROBLEM FORMULATION}

In this section, we formulate multi-robot non-communicating mapless navigation as a partially observable sequential decision problem. Similar to the scenarios of ORCA, our algorithm defines agents with linear equations of motion to achieve safe navigation in complex environments filled with static obstacles and other decision-making agents. {\color{green}All the agents are decentralized to adopt policy $\pi_{\theta }^{orca-drl}$  obtained by ORCA-DRL to select new velocities and angular velocities to achieve obstacle avoidance and navigation without communication.}{\color{red}Q:need discussion} choosing new velocities and angular velocities to achieve reciprocal obstacle avoidance without communication.{\color{green}Therefore, we can use multi-agent reinforcement learning to manipulate multi-agent mapless navigation.} {\color{red}Q:Find a place to explain how you make it a multi-agent reinforcement learning problem.}



In our algorithm ORCA-DRL, both the DRL and ORCA parts can be uniformly specified as a Markov decision process (MDP). 
{\color{green}Hence, ORCA and DRL algorithms can be integrated in the framework of safe reinforcement learning [39]. In the specific problem of mapless navigation, ORCA can be used as external knowledge that provides safety advice to DRL to avoid taking unsafe actions (e.g. collisions). For the MDP process of ORCA-DRL, we can define} a 5-tuple $(S,A,P,R,\gamma)$ , where $\gamma $ is the discount factor in (0,1). There are a total of $M $ moving agents in the scene. For the $i $-th agent  $\left ( 1\leq i\leq M \right )$ , there are $k $ agents within its observable range that are equivalent to dynamic obstacles.

\begin{itemize}
	\item{\textbf{State space: } At each timestep $t $, the state vector of agent $i $ can be divided into three parts, it is $s_{t}=\left [ s_{t}^{in},\tilde{s_{t}},s_{t}^{env} \right ] $ , where $s_{t}^{in} $ denotes all the information the agent has obtained about its internal state, $\tilde{s_{t}}=\left [ \tilde{s}_{t}^{1},\tilde{s}_{t}^{2},\cdots ,\tilde{s}_{t}^{k} \right ] $ denotes the state of other $k $ dynamic obstacles observed by the agent $i $ ,, and $s_{t}^{env} $ denotes the data of static obstacles in the vicinity measured through sensors (e.g., LIDAR), $ s_{t}^{env}\in \mathbb{R}^{8}$. 
	
	In this paper, let $d_{g} $ represent the destination of agent; let the position and velocity vectors of agent be denoted by $p_{x},p_{y} $ and $v_{x},v_{y} $, respectively; let $v_{pref} $ be agent’s preferred speed [11], at which the agent moves when it is not hindered by other obstacles; let $\psi $ represent the directional angle of the agent moving forward; let $r_{safe} $ represent the safety radius of the agent; let $pr $ be the driving priority level, and low priority agents should actively avoid high priority ones. Therefore, the state space of it's internal information is $s_{t}^{in}=\left [ d_{g},p_{x},p_{y},v_{x},v_{y},v_{pref},\psi,r_{safe},pr \right ]\in \mathbb{R}^{9} $. The state of $k $-th dynamic obstacle is  the observed position, speed, safety radius and priority level of other agent, $\tilde{s}_{t}^{k}=\left [ \tilde{p_{x}},\tilde{p_{y}},\tilde{v_{x}},\tilde{v_{y}},\tilde{r}_{safe},\tilde{\psi},\tilde{pr} \right ]\in \mathbb{R}^{7} $,  while their destination $\tilde{d}_{g} $ and preferred speed $\tilde{v}_{pref} $ are hidden states cannot be observed.
	}

	\item{\textbf{Action space: } In this work, we implement the method on a simulated differential drive robot (as {\color{blue} shown} in Fig. 2). The action of the robot is a set of allowed velocities in continuous space, including translational velocity $ v_{t} $ and rotational velocity $ w_{t} $, i.e., $a_{t}=\left [ v_{t} ,w_{t} \right ]\in \mathbb{R}^{2} $. To simplify the motion scenario, the differential robot can perform continuous actions directly, but is not allowed to move $ \left ( v_{t}> 0 \right ) $ backward.
	}
	
	\item{	\textbf{Reward function:} {\color{blue}The rewards at} a specified time step $t$  are divided into internal curiosity reward $R_{t}^{C} $, navigation rewards $R_{t}^{nav} $ and social norm rewards $R_{t}^{norm} $, {\color{blue}respectively}. In particular, $R_{t}^{nav} $ is to reward agents for safely approaching or reaching a goal, and $R_{t}^{norm} $ induces agents to learn to follow social norms through slight penalties. Note that only the DRL part of the policy $\pi _{\theta }^{RL} $ uses rewards, while the ORCA part of the policy $\pi ^{ORCA} $ does not need rewards but calculates the result directly, as described in \textbf{Algorithm 1}. The design of the reward function will be discussed in detail in section \uppercase\expandafter{\romannumeral4}-B.
	}
	
	\item{	\textbf{State transition model:} The probabilistic state transfer model  $P\left ( s_{t+1},s_{t+1}| a_{t} \right ) $ is jointly determined by the kinematics of the subject defined in (2) and the movement  policies of other dynamic obstacles. The action choices of other dynamic obstacles depend on their policies and hidden intentions (e.g., destinations), which cannot be observed externally. Therefore, the state transition model of the system is unknown.
	}
\end{itemize}

We define the mapless navigation problem as follows: Find a shared strategy that avoids collisions during navigation while minimizing the average arrival time of all agents. At each time step $t$, the  $i$-th agent has access to a state  $s_{t}^{i}$ and independently computes a collision-free motion command  $a_{t}^{i} $, sampled from a random policy $\pi _{\theta }$ shared by all agents, i.e.: 

\begin{equation} \mathbf{a}_{t}\sim\pi_{\theta}(\mathbf{a}_{t}\vert \mathbf{s}_{t}),  
\end{equation} where $\theta $ is the policy parameter. Action $a_{t}^{i} $ drives the agent to safely approach the destination $d_{g}^{i}$ from the current position $p_{t}^{i}$ while avoiding collisions with other agents and the obstacle $O_{k}\left ( 0\leq k\leq N \right )$.
After a travel time $t_{d}^{i} $ , the agent forms a trajectory $l_{i} $ from its starting position, $p_{t=0}^{i} $ to its desired goal  $p_{t=t_{d}^{i}}^{i}=d_{g}^{i}$.


{\color{green}To guarantee that agents do not collide, }
{\color{red}Q: what do you mean by modeling a condition?} we define $\mathbb{L} $ as the set of trajectories of all agents that are subject to the kinematic constraints of the robot, i.e.:

\begin{equation}
	\begin{aligned} 
		\mathbb{L}=& \{l_{i}, i=1, \ldots, M\vert \\ 		        &\mathbf{v}_{t}^{i}\sim\pi_{\theta}(\mathbf{a}_{t}^{i}\vert \mathbf{s}_{t}^{i}),\\ &\mathbf{p}_{t}^{i}=\mathbf{p}_{t-1}^{i}+\Delta t\cdot \mathbf{v}_{t}^{i},\\ &\forall j\in[1,\ M], j\neq i:\Vert \mathbf{p}_{t}^{i}-\mathbf{p}_{t}^{j}\Vert >  \mathbf{r}^{i}_{safe}+\mathbf{r}^{j}_{safe}\\
		&\wedge\forall k\in[1, N]:\Vert \mathbf{p}_{t}^{i}-\mathbf{O}_{k}\Vert > \mathbf{r}^{i}_{safe}\\ &\wedge\Vert \mathbf{v}_{t}^{i}\Vert\leq v_{\max}^{i}\}
	\end{aligned}
\end{equation}
where ${r}^{i}_{safe} $ is the agent safety radius and $ v_{\max}^{i}$ is the agent maximum speed.
We adopt an approach that minimizes the expectation of the average arrival time $t_{d}^{i} $ of all agents in the same scenario so as to develop the optimal policy shared by all agents, which is defined as:

\begin{equation} \mathop{\mathrm{argmin}}_{\pi_{\theta}}\ \mathbb{E}[\frac{1}{N}\sum_{i=1}^{N}t_{d}^{i}\vert \pi_{\theta}], 
\end{equation}




\begin{figure}[!t]
	\centering
	\includegraphics[width=3.5in]{fig3}
	\caption{An overview of our approach. In ORCA-DRL framework, two sets of algorithms, Priority-ORCA and DRL, operate in parallel. According to state information $s_{t} $ , DRL uses common policy $\pi_{\theta }^{RL} $ to output action $a_{t}^{RL} $ that follows social norms, and Priority-ORCA calculates the collision avoidance action interval $ORCA_{t}^{\tau -prior} $ . Based on $a_{t}^{RL} $ and $ORCA_{t}^{\tau -prior} $ , we classify and calculate the safe ultimate action $a_{t}^{orca-drl} $. In particular, DRL adopts curiosity-driven PPO algorithm, $\theta ^{V} $ is value network, $\theta ^{P} $ is policy network, $\theta ^{A} $ is action prediction network, and $\theta ^{S} $ is state prediction network. During training, $R_{t}^{C} $ is curiosity reward, $R_{t}^{nav} $ is navigation reward, and $R_{t}^{norm} $ is norm reward.	}
	\label{fig3}
\end{figure}

\section{APPROACH}

In this paper, we design a {\color{blue}socially-aware} ORCA-DRL navigation algorithm by adopting the fusion framework of model-based method (ORCA) + DRL (shown in Fig. 3). In complex environments mixed with dynamic and static obstacles, ORCA-DRL can better achieve the three tasks in mapless navigation (Fig. 1), improving trajectory quality (e.g., average arrival time) and success rate of obstacle avoidance, while learning to follow social norms. In particular, pathfinding (Task-1) is implemented by DRL, while obeying social rules (Task-2) and collision avoidance (Task-3) are jointly implemented by DRL and ORCA. We adopt a centralized training (shown in \textbf{Algorithm 1}) and distributed execution approach (shown in \textbf{Algorithm 2}). During the execution, the agents use the common policy and do not communicate without each other, using only their own sensors to collect dynamic obstacle information and environmental information. Next, ORCA-DRL is presented in two parts, part-A introduces the multi-agent reinforcement learning part, and part-B introduces the Priority-ORCA and algorithmic fusion methods.


\subsection{Multi-Agent Deep Reinforcement Learning Part}

In the DRL-part, we use Proximal Policy Optimization (PPO) [36] to train our navigation policy to produce an action $a_{t}^{RL} $ that follows the social norms. The policy $\pi _{\theta}^{RL }:\left ( s_{t}^{in },\tilde{s_{t}},s_{t}^{env } \right )\rightarrow a_{t}^{RL} $ (the training algorithm for ORCA-DRL, as outlined in \textbf{Algorithm 1}) obtained by training performs more robustly and effectively  in environments mixed with dynamic and non-convex static obstacles, and it is "smarter" to cope with maze-like scenarios [37]. Compared to traditional A*[12] or D*[13] algorithms that require global maps, DRL can navigate in complex non-convex obstacle environments without global maps at low sensor levels (e.g., sparse LiDAR only). According to the multi-agents reinforcement learning approach, our training process uses a centralized learning, decentralized execution paradigm. The policy $\pi _{\theta}^{RL } $ is trained based on the experience collected by all agents simultaneously. During the execution of ORCA-DRL (as outlined in \textbf{Algorithm 2}), each agent $i $ receives state data $\left ( s_{t}^{in },\tilde{s_{t}},s_{t}^{env } \right ) $ at each time step $t $ and outputs an action  $a_{i}^{RL} $ generated by the shared strategy $\pi _{\theta}^{RL } $, which is further computed by the ORCA-part to obtain $a _{i}^{orca-drl } $  for movement.


\textbf{(1) Neural network architecture with curiosity module:}
In many real-world navigation environments (e.g., mazes and corridors), it is difficult to design a reward function to satisfy all situations, and sometimes the external rewards for agents are very sparse. Thus reinforcement learning methods often do not  achieve effective exploration and navigation policies. We hope that better navigation strategies can still be obtained with simple and sparse external reward functions. Therefore, we use a curiosity-driven approach based on self-supervised prediction to enhance agent exploration. The prediction error in the learning feature space (the difference between the predicted states and true states, i.e., curiosity) is used as an intrinsic reward signal that enables the agent to actively explore the surroundings and learn policies that may be effective in subsequent navigation.

The convolutional network architecture for robot navigation in ORCA-DRL is shown in Fig.3, and we adopt PPO-clip [36] as the reinforcement learning training algorithm. Meanwhile, we used curiosity networks [35] to generate internal rewards to strengthen exploration under sparse rewards in complex non-convex static environments. The input of this network $s_{t} $ consists of its own state $s_{t}^{in} $, dynamic obstacles information $\tilde{s}_{t} $ and static obstacle data $s_{t}^{env} $ . 
The input layer is employed to efficiently process the input information  $s_{t}  $ and the ReLU nonlinearity is applied as the hidden layer activation function. We designed two 2-hidden layer neural networks (policy network $\theta ^{P} $ and value network $\theta ^{V} $) as nonlinear function approximators of policy $\pi _{\theta } $ and value function $V _{\theta } $. Each hidden layer is a fully connected layer with 128 rectifier units. We applied two XX fully connected neural networks (action estimation network $\theta ^{A} $ and state estimation network $\theta ^{S} $) for prediction of action $\hat{a_{t}} $ and state $\hat{s}_{t+1} $, respectively. 


The training process of ORCA-DRL is as follows (shown in \textbf{Algorithm 1}), first we initialize the 4 networks $(\theta ^{P},\theta ^{V},\theta ^{A},\theta ^{S}) $ and the actions of the agent-$i $ $\left ( 1\leq i\leq M \right ) $ should be contained in the allowed holonomic velocity set $S_{AHV_{i}}^{RL} $ (shown in Fig.7) [26]. Each agent takes action under current policy $\pi _{\theta } $ and generates trajectories, then updates the policy $\pi _{\theta } $ based on the sampled data of the trajectories.


For network $\theta ^{A} $, we input state $s_{t} $ encoded as feature vector $\lambda \left ( s_{t} \right ) $, and then predict that the agent takes the action $\hat{a}_{t} $ to transfer from state $s_{t} $ to $s_{t+1} $  based on the input feature vectors $\lambda \left ( s_{t} \right ) $ and $\lambda \left ( s_{t+1} \right ) $. From the inverse dynamics function $f\left ( \cdot  \right ) $, we can obtain the predicted action $\hat{a}_{t}=f\left (s_{t},s_{t+1}:\theta ^{A} \right ) $. According to the interaction data with the environment using the current policy $\pi_{\theta}^{RL} \left ( s_{t} \right ) $ during training, we optimize the neural network parameter  $\theta ^{A} $ to reduce the loss function  $L_{A} $ that measures the difference between predicted actions and actual actions:


\begin{small}
	\begin{equation}
		\underset{\theta^{A}}{\min} \: L_{A} \left \{ \left ( \hat{a}_{t},a_{t}\right )|\hat{a}_{t}=f\left ( s_{t},s_{t+1}:\theta ^{A} \right )  \right \}.
	\end{equation}
\end{small}


For network $\theta^{S} $, we input $a_{t} $ and  $\lambda \left (s_{t}  \right ) $ to train neural network $\theta^{S} $ to predict the feature vector  $\hat{\lambda} \left (s_{t+1}  \right ) $ of the state on time step $t+1 $, which gives the forward dynamics function  $h\left ( \cdot  \right ) $ as:  $\hat{\lambda} \left (s_{t+1}  \right )=h\left ( \lambda \left (s_{t}  \right ),a_{t}:\theta ^{S} \right ) $. During the training process, the neural network parameter $\theta^{S} $ is optimized by minimizing the difference between the predicted feature vector $\hat{\lambda} \left (s_{t+1}  \right ) $ and the actual  $\lambda \left (s_{t+1} \right ) $. The minimization loss function  $L_{S} $ is,

\begin{small}
	\begin{equation}
         \underset{\theta^{S}}{\min} \: L_{S} \left \{ \left ( \lambda \left (s_{t+1} \right ),\hat{\lambda} \left (s_{t+1}  \right ) \right )|\hat{\lambda} \left (s_{t+1}  \right )=h\left ( \lambda \left (s_{t}  \right ),a_{t}:\theta ^{S} \right ) \right \}.
	\end{equation}
\end{small}

When agents explore new environments, the predicted feature vector $\hat{\lambda} \left (s_{t+1}  \right ) $ is significantly different from the real $\lambda \left (s_{t+1} \right ) $. Exploring new environments gives agents a "curiosity" reward, which is necessary for agents to enhance exploration to seek future rewards. Following this principle, we can design the internal curiosity reward as,

\begin{small}
	\begin{equation}
		R_{t}^{C}=\frac{\delta }{2}\left \| \hat{\lambda} \left (s_{t+1}  \right )- \lambda \left (s_{t+1} \right ) \right \|_{2}^{2},
	\end{equation}
\end{small}
\noindent where $\delta $ is the curiosity strength factor. Curiosity is a new mode of learning, and when extrinsic rewards are sparse, intrinsic rewards are important to motivate agents to explore new environments and discover novel states in order to obtain better learning policies. In the experiments of this paper, we employ PPO-clip[36] for policy learning.
As shown in \textbf{Algorithm 1, line 16}, the overall optimization problem solved by the agent is the joint optimization of PPO's policy update formulation [36] and loss functions (5) and (6), which can be defined as follows,

\begin{small}
	\begin{equation}
		\underset{\theta ^{P},\theta ^{A},\theta ^{S}}{\min}\left [ -\alpha \underset{s,a\sim \pi _{\theta_{k}^{P} }}{\mathbb{E}}\left [ L\left ( s,a,\theta_{k}^{P},\theta ^{P} \right ) \right ]+\left ( 1-\beta  \right )L_{A}+\beta L_{S} \right ]
	\end{equation}
\end{small}

where $\alpha > 0 $ is a scaling factor to evaluate the importance of the strategy gradient, $1\geq \beta  \geq  0 $ is a factor to weight the predicted action loss $L_{A} $ and the predicted state feature vector loss $L_{S} $. The training of the value network $\theta^{V} $ follows the PPO-clip approach and still fits the value function by regression on mean square error [36].




\begin{figure}[!htb]

	\removelatexerror
	\begin{algorithm}[H]
		\caption{The training algorithm of ORCA-DRL}
		\begin{algorithmic}[1]
			\Require Initial network parameters $\theta_{0}^{P} $, $\theta_{0}^{V} $, $\theta_{0}^{A} $, $\theta_{0}^{S} $. Group of agents $i\in \left [ 1,M \right ] $ provide with: allowed holonomic velocities $S_{AHV_{i}}^{RL} $, priority level $pr_{i} $.
			\For{iteration = 1,2,...,}
			\State //Collect data in parallel
			\For{agent $i\in \left [ 1,M \right ] $}
			\State Run policy $\pi _{\theta }^{RL} $ for $T_{i} $ timesteps.
			\If{ORCA-DRL1}
			\State Obtain action $a_{i}^{RL}=\vec{v}_{i}^{RL} $ based on policy $\pi _{\theta }^{RL}  $.
			
			\ElsIf{ORCA-DRL2}
			\State Obtain action $a_{i}^{RL}=\vec{v}_{i}^{RL} $ by Algorithm-2.
			\EndIf
			
			\State Compute curiosity rewards $R_{t}^{C} $ by by Eq.(6).
			
			\State Collect $\left \{ s_{i}^{t},r_{i}^{t},a_{i}^{t} \right \}$, where $t\in \left [ 0, T_{i} \right ] $.
			
			\State Compute advantage estimates as in PPO-Clip.
			
			\EndFor
			\State $\pi _{old}^{RL}\leftarrow \pi _{\theta }^{RL} $.
			\State //Update policy and estimates
			\State Joint optimal policy $\theta^{P} $,   and estimation $\theta^{A} $, $\theta^{S} $ as in Eq.(7).
			\State //Update value function
			\State Update $\theta^{V} $ as in PPO-Clip.
			\EndFor
			
		\end{algorithmic}
		\label{alg:Training}
	\end{algorithm}
\end{figure}



\begin{figure}[!t]
	\centering
	\vspace{-0.3cm}
	\includegraphics[width=3.0in]{fig4}
	\caption{The light blue area is the asymmetric social norm penalty area $area_{norm}^{i} $. When an agent's own penalty area $area_{norm}^{i} $ overlaps with another agent's penalty area $area_{norm}^{j} $ ($i\neq j,1\leq i\leq M,1\leq j\leq M $ ), both of them will receive a slight penalty reward $R_{t}^{norm} $.}
	\label{fig4}
\end{figure}

\textbf{(2) Reward design with social norms:} 
The external reward function is divided into two parts navigation reward  $R_{t}^{nav} $ and  norm reward $R_{t}^{norm} $ , where $R_{t}^{nav} $  is to guide agents to explore the path and reach the goal without collision, and $R_{t}^{norm} $  induces agents to learn the behavior of following social rules by introducing small biases:	
\begin{equation} R_{t}^{ex}=R_{t}^{nav}+R_{t}^{norm}. \end{equation}

For the navigation reward $R_{t}^{nav}  $, our objective is to avoid collisions during navigation and to minimize the average arrival time of all agents, and we specify the reward $R_{t}^{nav}  $ at time step $t $ as:

\begin{equation} R_{t}^{nav}=R_{t}^{mf}+R_{t}^{dir}+R_{t}^{col-s}+R_{t}^{col-d}+R_{t}^{tim}+R_{t}^{goal},  
\end{equation}
where $R_{t}^{mf} $ is a reward for moving forward;  $R_{t}^{dir} $ represents a reward for the angle between the agent's movement direction and the target's movement direction, with a reward for the correct direction and a penalty for the opposite:

\begin{small}
	\begin{equation}
		R_{t}^{mf}=3\cdot \left ( \left \| d_{g}^{i}-p_{t=0}^{i} \right \|  - \left \| d_{g}^{i}-p_{t=t_{now}^{i}}^{i} \right \|\right )
	\end{equation}
\end{small}

\begin{small}
\begin{equation} 
	\begin{aligned}
	&R_{t}^{dir}=\pi -2\cdot \left | arccos\left ( \frac{\vec{v_{t}^{i}}\cdot \vec{goal_{t}^{i}}}{\left \| \vec{v_{t}^{i}} \right \|\cdot \left \| \vec{goal_{t}^{i}} \right \|} \right ) \right |
	\end{aligned}
 \end{equation}
\end{small}


we set $\vec{goal_{t}^{i}}=d_{g}^{i}-p_{t}^{i} $ goal in (11).
 $R_{t}^{col-s} $ and $R_{t}^{col-d} $ represent penalties for collisions with static and dynamic obstacles, respectively:
\begin{small}
\begin{equation}
	R_t^{col-s} = \left \{
	\begin{aligned}
		&-40, &&\text{if } \forall k\in \left [ 1,N \right ]:\left \| p_{t}^{i}-O_{k} \right \|\leq r_{safe}^{i}\\
		&0, &&\text{otherwise}
	\end{aligned}
\right.
\end{equation}
\end{small}



\begin{scriptsize}
	\begin{equation}
		R_t^{col-d} = \left \{
		\begin{aligned}
			&-15,&&\text{if } \forall j\in \left [ 1,M \right ],j\neq i:\left \| p_{t}^{i}-p_{t}^{j} \right \|\leq r_{safe}^{i}+r_{safe}^{j}\\
			&0,&&\text{otherwise}
		\end{aligned}
		\right.
	\end{equation}
\end{scriptsize}	



$R_{t}^{tim} $ is a small time penalty to facilitate reaching the target in a shorter time; $R_{t}^{goal} $ represents a reward for reaching the target, i.e.:
\begin{small}
\begin{equation}
	R_t^{tim} = \left \{
	\begin{aligned}
		&-0.25, &&\text{if time step t is over } \\
		&0, &&\text{otherwise}
	\end{aligned}
	\right.
\end{equation}
\end{small}

\begin{small}
	\begin{equation}
		R_t^{goal} = \left \{
		\begin{aligned}
			&80, &&\text{if } \left \| p_{t}^{i}-d_{g}^{i} \right \|\leq 0.12m \\
			&0, &&\text{otherwise}
		\end{aligned}
		\right.
	\end{equation}
\end{small}



In the training procedure,we set time step $t=0.2s $. For the norm reward $R_{t}^{norm} $ , we induce agents to learn to follow social norms during training by designing a reasonably small penalty region. As demonstrated in [14], similar to the principle that humans tend to follow simple navigation norms to avoid collisions, agents can be successfully induced to follow social norms by designing a deviation area. In our algorithm, our penalty region describes the extent to which the agent violates social rules. When other dynamic obstacles enter this region, the agent receives a negative reward that favors the behavior of following social rules.



\begin{figure}[htbp]
	\centering
	\vspace{-0.3cm}
	\subfloat[]{\includegraphics[scale=0.22]{fig5a.png}}\quad
	\vspace{-0.3cm}
	\label{fig5a}
	
	\subfloat[]{\includegraphics[scale=0.22]{fig5b.png}}\\	
	\label{fig5b}
	\caption{(a) the case of agents meeting head-on; (b) the case of agents overtaking. In (a), the width of the right region of the agent is 1.5 times that of the left region, which ensures that agents will develop a preference to pass on the right (\textbf{norm-1}) in order to avoid negative penalties. In (b), the agent carries a prominent "tail" area behind the right border. When a rear agent overtakes from the right side(\textbf{norm-2}), it receives a negative reward. Therefore, this induces a preference of overtaking on the left side.}
	\label{fig5}
\end{figure}


In particular, we will consider several common human \textbf{social traffic norms:} \textbf{1.overtake on the left.}\textbf{ 2. pass on the right.} (1, 2 are applicable in some countries, such as China, USA, etc.) \textbf{ 3. low priority vehicles avoid high priority ones }(e.g., Fire engines and ambulances have priority over other traffic).
Because of the urgency and importance of avoiding high-priority vehicles, we enforce \textbf{norm-3 } in the ORCA-part, while inducing agents to follow \textbf{norm-1, norm-2} in the DRL-part. As shown in Fig.4, for a differential wheeled robots with safety radius $r_{safe}^{i}=0.105 \, m $, we design an asymmetric social norm penalty area $area_{norm}^{i} $. For a time step $t $ , norm reward  $R_t^{norm} $ is as follows:

\begin{small}
	\begin{equation}
		R_t^{norm} = \left \{
		\begin{aligned}
			&-2, &&\text{if } \exists j\in \left [ 1,N \right ],j\neq i:p_{t}^{j}\in area_{norm}^{i} \\
			&0, &&\text{otherwise}
		\end{aligned}
		\right.
	\end{equation}
\end{small}

First, we discuss the situation in which the robots meet head-on \textbf{(norm-1)}. With the geometric center of the agent as the midline, the width of the right region is 1.5 times that of the left region, while the front tip is in front of the right boundary of the agent, which ensures that the agents will form a preference for passing on the right when meeting head-on, as shown in Fig.5a.

In the case of overtaking \textbf{(norm-2)}, the agent carries a prominent "tail" area behind the right border (as in Fig.5). When a following vehicle overtakes from the right side, the vehicle invades this area and generates a negative reward (Fig.5b). Hence, a agent overtaking from the left side at a safe distance gains a greater reward.

In the case of crossing situation (e.g., at the crossroads), the two borders on the left side of the agent form a $44^{\circ} $ angle  (as in Fig.5), and the safety radius of the agent passing anti-clockwise on the right is smaller (clockwise radius $R_{1}= 0.44 \,m $, anti-clockwise radius $R_{2}= 0.31\,m $, as in Fig.6a,b). That induces the robot to form cooperative behaviors (e.g., circling in a circular loop, as in Fig.13a) enabling a safer crossing.



\begin{figure}[htbp]
	\centering
	\vspace{-0.3cm}
	\subfloat[]{\includegraphics[scale=0.22]{fig6a.png}}\quad
	\vspace{-0.3cm}
	\label{fig6a}
	
	\subfloat[]{\includegraphics[scale=0.22]{fig6b.png}}\\	
	\label{fig6b}
	\caption{(a) clockwise; (b) anti-clockwise. In the case of crossing situation (e.g., at the crossroads), the two borders on the left side of the agent form a $44^{\circ} $ angle  (as in Fig.5), and the safety radius of the agent passing anti-clockwise on the right is smaller (  Fig.6 (a) clockwise radius $R_{1}= 0.44 \,m $; Fig.6 (b) anti-clockwise radius $R_{2}= 0.31\,m $). That induces the agent to form cooperative behaviors (e.g., circling in a circular loop, as in Fig.13a) enabling a safer crossing. }
	\label{fig6}
\end{figure}

\begin{figure}[!t]
	\centering
	\vspace{-0.4cm}
	\includegraphics[width=3.0in]{fig7}
	\caption{The velocity obstacle diagram of Priority-ORCA. $ORCA_{i|j}^{\tau-prior } $ (red region) has an avoidance offset in the direction of  $\vec{n} $ with respect to $ORCA_{i|j}^{\tau } $ (purple region). According to different driving priority levels $pr_{i} $ and  $pr_{j} $, the amount of avoidance variation $\vec{u} $  is allocated among robots in proportion to their priorities, so that vehicles with higher priorities take less responsibility for avoidance to obtain higher speeds and smoother trajectories. }
	\label{fig7}
\end{figure}




\subsection{Priority-ORCA and Algorithmic Fusion Methods}

In the previous section, we can obtain action policy $\pi _{\theta }^{RL} $ 
that follow social rules (Task-2) and make agents better achieve pathfinding (Task-1) in non-convex static obstacle scenes by reinforcement learning with norm reward. At the same time, the policy has a certain obstacle avoidance effect on dynamic obstacles (Task-3) in the scene (e.g., the success rate of DRL is $\bar{\chi }^{drl}= 58.0\% $, in Scenario-3 of section \uppercase\expandafter{\romannumeral5}-B). However, the DRL-based navigation strategy is difficult to achieve a high success rate in complex multi-dynamic obstacle scenarios (Task-3), while the interpretability and stability of this learning strategy is insufficient. The safety of learning-based navigation method $\pi _{\theta }^{RL} $ needs to be improved. In the ORCA-part, based on the learning-based policy $\pi _{\theta }^{RL} $, we will combine the Priority-ORCA method and propose a novel fusion algorithm $\pi _{\theta }^{orca-drl} $ to further improve the success rate and stability of dynamic obstacle avoidance.

ORCA provides a sufficient condition for multiple robots to avoid collision with each other without communication. This algorithm can be easily extended to handle large-scale systems with a large number of robots (up to several hundred robots) [11]. From the code of ORCA\footnote{https://gamma.cs.unc.edu/RVO2/}, we can know that ORCA guarantees obstacle avoidance for dynamic obstacles, but the algorithm assumes that all static obstacles in the scene are convex. For non-convex static obstacle scenes (especially mazes and corridors), the agent in ORCA tends to fall into local extrema. The ORCA algorithm cannot implement pathfinding (Task-1) properly. Meanwhile, when multiple agents meet in navigation, each agent plans its own path without cooperation, resulting in chaotic trajectories (as in Fig.13b). Obeying social norms (Task-2) is also unachievable. In contrast, the strategy $\pi _{\theta }^{RL} $ obtained by learning is already able to handle non-convex scenarios and achieve cooperation better without global maps.

In summary, if the advantages of DRL and ORCA can be combined, the fusion algorithm will overcome their respective shortcomings and significantly improve the performance of the new algorithm. Next, we will describe the fusion algorithm ORCA-DRL in detail (shown in \textbf{Algorithm 2}). 

\textbf{(1)Priority-ORCA:} Based on ORCA, we introduce \textbf{norm-3} into the strategy to achieve an absolute rule of low-priority vehicles avoiding high-priority vehicles, and the improved algorithm is called Priority-ORCA. In human traffic scenarios, some vehicles executing emergency tasks (e.g., fire engines, ambulances) should have priority to pass, and integrating \textbf{norm-3} into the traditional algorithm can guarantee absolute avoidance actions.


For two agents $i $ and $j $, the velocity obstacle $VO_{i|j}^{\tau} $ for agent $i $ is the set of all relative velocities of $i $ (the safety radius is $r_{i}+\varepsilon_{i} $ at $p_{i} $, ,$r_{i} $ is the physical radius, $\varepsilon_{i} $ is the tracking error, as in fig.2) with respect to agent $j $ (the safety radius is $r_{j}+\varepsilon_{j} $ at $p_{j} $), which will result in a collision between agent $i $ and $j $ at some moment before time $\tau $ [11]. As shown in the velocity obstacle diagram (Fig.7), $VO_{i|j}^{\tau} $ is formally defined as follows:

\begin{small}
	\begin{equation}
	VO_{i|j}^{\tau} =\left \{ v|\exists t\in \left [ 0,\tau  \right ],t\cdot \vec{v}\in D\left ( p_{j}- p_{i},r_{i}+\varepsilon_{i} +r_{j}+\varepsilon_{j} \right ) \right \}
	\end{equation}
\end{small}

with $D\left ( \textbf{p},r \right )=\left \{ \textbf{q}|\left \| \textbf{q}-\textbf{p} \right \|<r \right \} $ the open ball of radius $r = r_{i}+\varepsilon_{i} +r_{j}+\varepsilon_{j} $. Next, the set of collision avoidance velocities $CA_{i|j}^{\tau}\left ( V_{j} \right ) $ for agent $i $ (given that robot $j $ selects its velocity from  $V_{j} $) can be obtained,  $CA_{i|j}^{\tau}\left ( V_{j} \right )=\left \{ \vec{v}|\vec{v}\notin VO_{i|j}^{\tau }\bigoplus V_{j} \right \} $. Then we can calculate the minimum variation  $\vec{u} $ (the vector from $\left ( \vec{v}_{i}^{opt}-\vec{v}_{j}^{opt} \right ) $ to the closest point on the boundary of the velocity obstacle): $\vec{u}=\left ( \underset{\vec{v}\in{\partial }VO_{i|j}^{\tau }}{argmin}\left \| \vec{v}-\left ( \vec{v}_{i}^{opt}-\vec{v}_{j}^{opt} \right ) \right \| \right )-\left ( \vec{v}_{i}^{opt}-\vec{v}_{j}^{opt} \right ) $, where $\vec{v}_{i}^{opt} $ is the optimization velocity and  $\vec{n} $ denotes the outward normal of the boundary of  $VO_{i|j}^{\tau } $ at point  $\left ( \vec{v}_{i}^{opt}-\vec{v}_{j}^{opt} \right ) +\vec{u} $.

Unlike the original algorithm of ORCA which shares the avoidance amount equally, the algorithm of Priority-ORCA considers the social rule of avoiding high priority vehicles (\textbf{norm-3}). According to different driving priority levels $pr_{i} $ and  $pr_{j} $, the amount of avoidance variation $\vec{u} $ is allocated among agents in proportion to their priorities, so that vehicles with higher priorities take less responsibility for avoidance to obtain higher speeds and smoother trajectories for urgent tasks (e.g., fire fighting and patients rescue). As shown in Fig.7, the set of collision-free velocities  $ORCA_{i|j}^{\tau-prior } $ for agent $i $ relative to robot $j $ can geometrically be constructed from  $VO_{i|j}^{\tau} $,

\begin{small}
	\begin{equation}
	ORCA_{i|j}^{\tau-prior }=\left \{ \vec{v}|\vec{v}-\left ( \vec{v_{i}^{opt}} +\frac{pr_{j}}{pr_{i}+pr_{j}} \cdot \vec{u}\right )\cdot \vec{n} \geq  0\right \}
	\end{equation}
\end{small}

where $ORCA_{i|j}^{\tau-prior } $ (red region in Fig.7) has an avoidance offset in the direction of  $\vec{n} $ with respect to $ORCA_{i|j}^{\tau } $ (purple region in Fig.7).
Unlike the DRL part of the algorithm that generates rule preferences through reward induction (norm-1,norm-2), the Priority-ORCA (norm-3) enforces priority avoidance rules, which is theoretically interpretable and stable. This improves the avoidance safety of the ORCA-DRL algorithm.

For scenarios with multiple dynamic obstacles, we apply Priority-ORCA algorithm to perform $n $-body collision avoidance between mobile agents. 
The set of collision-free velocities $ORCA_{i}^{\tau-prior } $ relative to all agents can be given as

\begin{small}
	\begin{equation}
		ORCA_{i}^{\tau-prior }=S_{AHV_{i}}^{RL}\cap \bigcap_{j\neq i}ORCA_{i|j}^{\tau-prior }
	\end{equation}
\end{small}

Fig.7 shows the set $ORCA_{i}^{\tau-prior } $ of configurationa with multiple agents, and the set of allowed holonomic velocities $S_{AHV_{i}}^{RL} $ is selected by the DRL-part under the kinematic constraints of agent $i $. If the kinematic constraints of the agent are holonomic, $S_{AHV_{i}}^{RL}=D\left ( 0,V_{H_{i}}^{max} \right ) $.



\begin{figure}[!htb]
	\label{alg:Execution}
	\removelatexerror
	\begin{algorithm}[H]
		\caption{The execution algorithm of ORCA-DRL}
		\begin{algorithmic}[1]
			\Require Policy $\pi _{\theta }^{RL} $. Group of agents $i\in \left [ 1,M \right ] $ provide with: allowed holonomic velocities $S_{AHV_{i}}^{RL} $, state information $s_{i}^{i}=\left [s_{i}^{own},\tilde{s_{i}},s_{i}^{env} \right ]$.
			\Ensure  Actions $a_{i}^{orca-drl}=\vec{v}_{i}^{orca-drl}=\left [ v_{i},\omega_{i} \right ] $.
			\Loop
			\For{$i\in \left \{ 1,...,M \right \} $}
			\State Rotate $S_{AHV_{i}}^{RL} $ to match orientation $\psi _{i} $.
			\State Obtain action $\vec{v}_{i}^{RL} $ based on policy $\pi _{\theta }^{RL}  $.
			\For{$j\in \left \{ 1,...,M \right \},i\neq j $}
			\State Compute $VO_{i|j }^{\tau} $ for agents at $p_{i} $ and $p_{j} $.
			\State Compute $ORCA_{i|j }^{\tau-prior} $.
			\EndFor
			\State Construct $ ORCA_{i}^{\tau-prior}$ following Eq.(19).
			
			\If{$ORCA_{i}^{\tau-prior } \neq \emptyset, \vec{v}_{i}^{RL}\in ORCA_{i}^{\tau-prior } $}
			\State \textbf{Case-1:} $\vec{v}_{i}^{orca-drl}=\vec{v}_{i}^{RL} $.
			
			\ElsIf{$ORCA_{i}^{\tau-prior } \neq \emptyset $,      and        
				$ \vec{v}_{i}^{RL}\notin ORCA_{i}^{\tau-prior } $}
			\State \textbf{Case-2:} Compute $\vec{v}_{i}^{orca-drl} $ by Eq.(20).
			
			\ElsIf{$ORCA_{i}^{\tau-prior } = \emptyset, \vec{v}_{i}^{RL}\in S_{AHV_{i}}^{RL} $}
			\State \textbf{Case-3:} Compute $\vec{v}_{i}^{orca-drl} $ by Eq.(21).
			\EndIf
			
			\State \textbf{Output:} Actions  $\vec{v}_{i}^{orca-drl} $ for agent $i $.
			\EndFor
			\EndLoop
			
		\end{algorithmic}
	\end{algorithm}
\end{figure}




\textbf{(2)Algorithmic Fusion of DRL and ORCA:} 
In the previous sections, we obtained the navigation policy $\pi _{\theta }^{RL} $ adapted to the complex non-convex scenarios (with social norm-1,2) and the priority permitted speed set $ORCA_{i}^{\tau-prior } $ (with norm-3), respectively. Policy $\pi _{\theta }^{RL} $ can achieve better results in purely static scenarios, but due to the high dimensionality and complexity of the dynamic obstacle avoidance problem, it is difficult to achieve a high success rate in mixed scenarios with lots of dynamic obstacles accompanied by static obstacles. 


Then we will incorporate DRL-part and OCRA-part in the same framework to obtain a better fusion algorithm ORCA-DRL. In this work, two sets of algorithms operate in parallel (as in Fig.3) and DRL implements pathfinding (Task-1), overcoming the problem that ORCA cannot cope with non-convex obstacles and easily falls into the extreme region. DRL provides interactions that obey the norms (\textbf{norm-1, 2}) and Priority-ORCA provides  \textbf{norm-3}, which avoids uncooperative congestion and chaotic trajectories in ORCA (Task-2). And by adopting the principle of ORCA to optimize policy $\pi _{\theta }^{RL} $ again, it can improve the shortcomings of dynamic obstacle avoidance for policy $\pi _{\theta }^{RL} $ and guarantee the safety of dynamic and static collision avoidance (Task-3). In this way, all of Task-1,2,3 can be better implemented, and the fusion algorithm will achieve stronger performance compared to DRL or ORCA.


The execution process of ORCA-DRL is shown in \textbf{Algorithm 2}. According to the definition of ORCA[11], we select the optimal speed for agent $i $ as $\vec{v}_{i}^{opt}=\vec{v}_{i}^{RL}$ . Velocity $\vec{v}_{i}^{RL} $ is the current action $a_{i}^{RL}=\vec{v}_{i}^{RL} $  generated by policy $\pi _{\theta }^{RL} $ for agent $i $. Here the optimal speed  $\vec{v}_{i}^{opt} $ is a time-varying speed that adapts automatically to the scene. In the absence of a global map, the agent's learned policy $\pi _{\theta }^{RL} $ can react based on current sensor information $s_{t}=\left [ s_{t}^{in},\tilde{s_{t}},s_{t}^{env} \right ] $, enabling navigation. When dealing with non-convex static areas, policy $\pi _{\theta }^{RL} $ can bypass non-convex obstacle areas rather than easily falling into areas of local extremes (Fig.14a,b). When agents encounter dynamic obstacles mixed in the scene, policy $\pi _{\theta }^{RL} $ follows social rules and is less likely to form crowded trajectories at high agent densities compared to algorithms such as A* [12] and D* [13] (requiring accurate global maps), improving the average speed, as shown in Fig.13a,b.

In the next step, since the velocity of policy $\pi _{\theta }^{RL} $ does not perfectly cope with dynamic obstacle avoidance, we will further optimize velocity  $\vec{v}_{i}^{opt}=\vec{v}_{i}^{RL}$  to ensure the success rate of dynamic obstacle avoidance. Based on speed $\vec{v}_{i}^{RL} $ and the priority permitted speed set $ORCA_{i}^{\tau-prior } $, agent $i $ selects a new speed $\vec{v}_{i}^{new}=\vec{v}_{i}^{orca-drl} $ that is closest to its preferred speed. Velocity $\vec{v}_{i}^{orca-drl} $ is the speed of the next step that agent $i $ eventually executes. Here, we discuss three scenarios:


\begin{itemize}
\item{Case-1: }$ORCA_{i}^{\tau-prior } \neq \emptyset, \vec{v}_{i}^{RL}\in ORCA_{i}^{\tau-prior } $

This case indicates that speed $\vec{v}_{i}^{RL} $ is the safe speed that fulfills the priority rule and speed $\vec{v}_{i}^{RL} $ will guarantee not to collide with any other agents in time $\tau $. We can let $\vec{v}_{i}^{orca-drl}=\vec{v}_{i}^{RL} $, as in Fig.8a. Equation (20) can also be used to calculate the new velocity $\vec{v}_{i}^{orca-drl} $.
	\item{Case-2: }$ORCA_{i}^{\tau-prior } \neq \emptyset, \vec{v}_{i}^{RL}\notin ORCA_{i}^{\tau-prior } $

Velocity $\vec{v}_{i}^{RL} $ cannot guarantee the priority rule or no collision. Since the set $ORCA_{i}^{\tau-prior } $ is a convex region, we can use a two-dimensional linear optimization function to choose a new velocity $\vec{v}_{i}^{orca-drl} $ in the set $ORCA_{i}^{\tau-prior } $ that is closest to the velocity $\vec{v}_{i}^{RL} $. As in Fig.8b, velocity $\vec{v}_{i}^{orca-drl} $ is given by:

\begin{small}
	\begin{equation}
	\begin{split}
		&\vec{v}_{i}^{orca-drl}=\argmin\left \| \vec{v}- \vec{v}_{i}^{RL}\right \|\\
		&s.t.\:\vec{v}\in ORCA_{i}^{\tau-prior }
	\end{split}
	\end{equation}
\end{small}


\item{Case-3: }$ORCA_{i}^{\tau-prior } = \emptyset, \vec{v}_{i}^{RL}\in S_{AHV_{i}}^{RL} $
	
	
This is the case with a high density of agents, therefore the velocity set $ORCA_{i}^{\tau-prior } $ is empty. For these scenarios with a large number of dynamic obstacles, the linear programming in Equation (20) is not feasible. To ensure that there are no collisions between agents and they continue to move without local deadlock, we still use the 3D linear programming of the original ORCA algorithm within the set $S_{AHV_{i}}^{RL} $, as in Equation (21). Let $d_{i|j}\left ( \vec{v} \right ) $ denote the signed (Euclidean) distance of velocity $\vec{v} $ to the edge of the half-plane $ORCA_{i|j}^{\tau-prior } $. As in Fig.8c, We select the velocity $\vec{v}_{i}^{orca-drl} $ that minimizes the maximum $d_{i|j}\left ( \vec{v} \right ) $ induced by the other agents:

\begin{small}
	\begin{equation}
		\begin{split}
		&\vec{v}_{i}^{orca-drl}=\argmin\:  \underset{i\neq j}{\max}\:  d_{i|j}\left ( \vec{v} \right )\\
		&s.t.\:\vec{v}\in S_{AHV_{i}}^{RL}
		\end{split}
	\end{equation}
\end{small}

During the learning process in the DRL-part with a safe social distance penalty (Equation (16) ), policy $\pi _{\theta }^{RL} $ will keep the agents at a safe distance from each other so that high-density situation (Case-3) are less likely to occur. Thereby they are less prone to slow down due to crowding (as in Fig.13a,b). Once the agents have crossed the crowded area (Case-3), policy $\pi _{\theta }^{RL} $ will continue to navigate the agents towards their targets.

\end{itemize}

The fusion algorithm ORCA-DRL can be subdivided into two algorithms, ORCA-DRL1 and ORCA-DRL2, according to  whether the ORCA-part is involved in the training process, as in \textbf{Algorithm 1}.
ORCA-DRL1 is trained purely with reinforcement learning to obtain policy $\pi _{\theta }^{RL-1} $. When the algorithm is operating, policy $\pi _{\theta }^{RL-1} $ is optimized with the ORCA-part and outputs action $\vec{v}_{i}^{orca-drl} $.
ORCA-DRL2 performs the ORCA-part during reinforcement learning training, producing the fusion policy $\pi _{\theta }^{RL-2} $. During the operation, policy $\pi _{\theta }^{RL-2} $ is optimized with the ORCA-part and outputs action $\vec{v}_{i}^{orca-drl} $.
The two algorithms use different reward function hyperparameters, and the effects and advantages are discussed in section-\uppercase\expandafter{\romannumeral5}.


\begin{figure*}[!t]
	\centering
	\vspace{-0.5cm}
	\includegraphics[width=6.5in]{fig8}
	\caption{Discussed in four scenarios. }
	\label{fig8}
\end{figure*}



\begin{figure}[htbp]
	\centering
	\vspace{-0.3cm}
	\subfloat[]{\includegraphics[scale=0.22]{fig9a.png}}\quad
	\vspace{-0.3cm}
		\label{fig9a}
		
	\subfloat[]{\includegraphics[scale=0.24]{fig9b.png}}\\	
		\label{fig9b}
	\caption{(a) shows the structure of training scenes for Scenarios 1-4; (b) presents the simulation training environment in Unity. }
	\label{fig9}
\end{figure}



\section{RESULTS}

In this section, we will quantitatively evaluate the performance of our ORCA-DRL navigation algorithm in various simulated/realistic scenarios and compare it with other methods. The navigation policy ORCA-DRL is trained through simulated scenes in Unity\footnote{https://unity.com/} and Gazebo\footnote{https://gazebosim.org/}, and implemented with tensorflow (Python 3.7).  The training process is executed on a computer with CPU i9-11900K and GPU Nvidia GTX 3070Ti. We test the algorithms in four challenging and complex simulation environments and compare the navigation performance of three algorithms, ORCA-DRL, ORCA, and DRL, in these scenarios. Fig.9a shows the structure of the four scenarios for training ( described in detail in Section \uppercase\expandafter{\romannumeral5}-B), and Fig.9b presents the simulation training environment in Unity. We introduce three metrics to evaluate the performance of the navigation algorithm as follows: 

\begin{itemize}
	\item{Success rate ($\bar{\chi } $): }The percentage of episodes in which robots successfully reach their target without collisions.
	\item{Average cycle time ($\bar{t}$): }The average cycle time spent by all arrived robots in successful episodes. Fewer cycles indicate more efficient navigation.
	\item{Social rules success rate ($\bar{\chi_{s} }$): }The proportion of successful episodes in which the robot follows social rules. 
\end{itemize}


We also deploy the trained model to a set of differentially driven robots Turtlebot3 to validate the performance of ORCA-DRL in the real world. Experiments show that our algorithm guarantees a high success rate and follows social rules in mixed scenarios of dynamic and static non-convex obstacles, while the arrival time/average speed is improved. The video of the simulation and real experiments can be found at \href{}{https://blog.csdn.net/Strive For Future/article/details/122169097-XXXXXXXXXX}. 


\subsection{Performance of Reinforcement Learning Policy $\pi _{\theta}^{RL} $}

We simulate the navigation of a group of 8 robots with LIDAR in the Unity simulator in four different social scenarios. Based on the hyperparameters listed in Section \uppercase\expandafter{\romannumeral4}-A, we train the navigation policy $\pi _{\theta}^{RL }$ of the differentially driven robot following the curiosity-driven PPO algorithm. The offline training of the algorithm takes 8 million steps (no more than 3500 steps per episode). In particular, the curiosity strength $\delta $ of the state prediction network $\theta ^{S} $ is set to $\delta =0.01 $. In our experiments, we implement continuous actions for the robot, we set $ v_{t}\in \left ( 0.01,0.20 \right ) m/s $ and $ w_{t}\in \left ( -2.5,2.5 \right ) rad/s $.

We compare the navigation performance of the curiosity-driven policy $\pi _{\theta }^{RL-cd} $ and the curiosity-free policy $\pi _{\theta }^{RL-cf} $ under Scenario-4 in Section \uppercase\expandafter{\romannumeral5}-B. Here, Scenario-4 is a complex non-convex static scene containing dynamic obstacles. Fig.10a shows the external environment curves under sparse rewards (moving forward reward $R _{t }^{mf}=0$ and direction reward $R _{t }^{dir}=0$), Fig.10c shows the external reward curves under normal rewards (moving forward reward $R _{t }^{mf}$ and direction reward $R _{t }^{dir}$ set as in section \uppercase\expandafter{\romannumeral4}-A(2) ), and Fig.10b, Fig.10d shows the curiosity rewards corresponding to Fig.10a, Fig.10c.


\begin{figure}[htbp]
	\centering
	\vspace{-0.5cm}
	\subfloat[]{\includegraphics[scale=0.22]{fig10a.png}}\quad
	\vspace{-0.3cm}
	\label{fig10a}
	
	\subfloat[]{\includegraphics[scale=0.22]{fig10b.png}}\\
	\vspace{-0.3cm}
	\label{fig10b}
	
	\subfloat[]{\includegraphics[scale=0.22]{fig10c.png}}\quad
	\vspace{-0.3cm}
	\label{fig10c}
	
	\subfloat[]{\includegraphics[scale=0.22]{fig10d.png}}\\
	\label{fig10d}
	
	\caption{Environmental reward curve of ORCA-DRL algorithm in Scenario-4 (8-agents in non-convex intersection) in Section \uppercase\expandafter{\romannumeral5}-B. (a) shows the external environment curves under sparse rewards (moving forward reward $R _{t }^{mf}=0$ and direction reward $R _{t }^{dir}=0$), (c) shows the external reward curves under normal rewards (moving forward reward $R _{t }^{mf}$ and direction reward $R _{t }^{dir}$ set as in section \uppercase\expandafter{\romannumeral4}-A(2) ), and (b), (d) shows the curiosity rewards corresponding to (a), (c).}
	\label{fig10}
\end{figure}

\textbf{(1)Sparse rewards:} From Fig.10a we can see that the reward of policy $\pi _{\theta }^{RL-cd} $ (blue line) is smaller than that of policy $\pi _{\theta }^{RL-cf} $ (pink line) when the number of training steps is less than 2.7 million under the sparse reward condition. 
At this time, the curiosity reward (shown in Fig.10b) is larger and the agents of policy $\pi _{\theta }^{RL-cd} $ are more actively exploring the environment, thus the environmental reward of policy $\pi _{\theta }^{RL-cd} $ is smaller than that of policy $\pi _{\theta }^{RL-cf} $. After the training steps exceed 2.7 million, the curiosity reward gradually decreases as most of the environment is explored. 
Meanwhile, curiosity driven policy $\pi _{\theta }^{RL-cd} $ obtains better results ($\bar{\chi }^{drl-cd}=75.0\%$ for policy $\pi _{\theta }^{RL-cd} $ and $\bar{\chi }^{drl-cf}=37.2\%$  for policy $\pi _{\theta }^{RL-cf} $) and greater external rewards due to deeper exploration of the environment and more efficient utilization of environmental information as in Fig.10a. 

\textbf{(2)Normal rewards:} Since external rewards are more intensive (as in section \uppercase\expandafter{\romannumeral4}-A(2)), the agents explore the environment faster than that of the sparse reward situation under the combined effect of external and curiosity rewards. Consequently, curiosity driven policy $\pi _{\theta }^{RL-cd} $ (blue line) outperforms curiosity-free policy $\pi _{\theta }^{RL-cf} $ (orange line) at 0.9 million training steps in Fig.10c.
As the number of training steps increases from 0.9 to 5 million, the curiosity-driven policy $\pi _{\theta }^{RL-cd} $ is more fully optimized and eventually navigates better ($\bar{\chi }^{drl-cd}=84.0\%$ and $\bar{\chi }^{drl-cf}=75.2\%$) with greater external rewards, as in Fig.10c,d and \textbf{Table \uppercase\expandafter{\romannumeral2}}.

The above experimental results demonstrate that curiosity-driven reinforcement learning has acquired more effective navigation policies and improved the speed of training.





\begin{figure}[htbp]
	\centering
	\vspace{-0.5cm}
	\subfloat[]{\includegraphics[scale=0.33]{fig11a.png}}\quad
	\vspace{-0.3cm}
	\label{fig11a}
	
	\subfloat[]{\includegraphics[scale=0.33]{fig11b.png}}\\
	\vspace{-0.3cm}
	\label{fig11b}
	
	\subfloat[]{\includegraphics[scale=0.33]{fig11c.png}}\quad
	\vspace{-0.3cm}
	\label{fig11c}
	
	\subfloat[]{\includegraphics[scale=0.33]{fig11d.png}}\\
	\label{fig11d}
	
	\caption{Scenario-1: 2 robots are randomly born from the central red areas (0.5$\times $0.56m) and travel head-on to each other. The targets (indicated by the pentagrams)  are randomly born in the red areas (0.5$\times $0.56m) at both edges. (a) indicates the robot avoidance trajectory using ORCA-DRL; (b)  indicates the robot avoidance trajectory using ORCA; (c) indicates the robot avoidance trajectory using DRL; and (d) indicates the robot avoidance trajectory using ORCA-DRL with priority (the blue robot has priority level $pr_{1}=1 $ and the green robot has priority level $pr_{2}=10 $). }
	\label{fig11}
\end{figure}



\begin{figure}[htbp]
	\centering
	\vspace{-0.5cm}
	\subfloat[]{\includegraphics[scale=0.2]{fig12a.png}}\quad
	\vspace{-0.3cm}
	\label{fig12a}
	
	\subfloat[]{\includegraphics[scale=0.2]{fig12b.png}}\\
	\vspace{-0.3cm}
	\label{fig12b}
	
	\subfloat[]{\includegraphics[scale=0.2]{fig12c.png}}\quad
	\vspace{-0.3cm}
	\label{fig12c}
	
	\caption{Scenario-2: The red robot will overtake the yellow robot, and then both of them will reach their destinations in the corridor. (a) shows the overtaking trajectory of the robots when using the ORCA-DRL. The red robot overtakes the previous yellow robot from the left side (norm-2) and maintains a proper overtaking distance. (b) shows the overtaking trajectory of the robots when using the ORCA. The red robot squeezes the yellow robot during overtaking, causing trajectory oscillation for both robots. (c) shows the overtaking trajectory of the robots when using the DRL.}
	\label{fig12}
\end{figure}




\begin{table*}
	\centering
	\caption{Simulation results of Scenario-1 and Scenario-2\label{tab:table2}}
	\resizebox{\linewidth}{!}{
	\begin{tabular}{cccccccccc} 
	\hline
	Number of robots   & Scenario                      & Method   & Success Rate(\%) & Soclial rules success rate(\%) & Collision With robots(\%) & Collision with obstacles(\%) & Time Out(\%) & Rotate in place(\%) & Average Time(s)  \\ 
	\hline
	\multirow{3}{*}{2} & \multirow{3}{*}{1-Passing}    & DRL-cd   & 96.4~            & 92.8                           & 2.4~                      & 1.2~                         & 0.0~         & 0.0~                & 19.6             \\
	&                               & ORCA     & 55.7~            & 48.4~                          & 0.0~                      & 0.0~                         & 44.3~        & 0.0~                & fail             \\
	&                               & ORCA-DRL & 97.3~            & 94.6~                          & 0.0~                      & 2.7~                         & 0.0~         & 0.0~                & 20.6             \\ 
	\hline
	\multirow{3}{*}{2} & \multirow{3}{*}{2-Overtaking} & DRL-cd   & 100.0~           & 100.0~                         & 0.0~                      & 0.0~                         & 0.0~         & 0.0~                & 25.4             \\
	&                               & ORCA     & 100.0~           & 49.8~                          & 0.0~                      & 0.0~                         & 0.0~         & 0.0~                & 24.4             \\
	&                               & ORCA-DRL & 100.0~           & 100.0~                         & 0.0~                      & 0.0~                         & 0.0~         & 0.0~                & 23.4             \\
	\hline
	\end{tabular}
	}
\end{table*}

\begin{table*}[]
	\centering
	\caption{Simulation results of Scenario-3 and Scenario-4\label{tab:table2}}
	\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccccc} 
	\hline
	Number of robots   & Scenario                                                                                      & Method   & Success Rate(\%) & Collision With robots(\%) & Collision with obstacles(\%) & Time Out(\%) & Rotate in place(\%) & Average Time(s)  \\ 
	\hline
	\multirow{4}{*}{8} & \multirow{4}{*}{3-Crossroad}                                                                  & DRL-cf   & 54.7~            & 38.0~                     & 5.3~                         & 1.3~         & 0.7~                & fail             \\
	&                                                                                               & DRL-cd   & 58.0~            & 38.7~                     & 1.3~                         & 2.0~         & 0.0~                & fail             \\
	&                                                                                               & ORCA     & 100.0~           & 0.0~                      & 0.0~                         & 0.0~         & 0.0~                & 31.90~           \\
	&                                                                                               & ORCA-DRL & 90.7~            & 2.7~                      & 3.3~                         & 1.3~         & 2.0~                & 27.95~           \\ 
	\hline
	\multirow{4}{*}{8} & \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}4-Non-convex \\Obstacle Intersection\end{tabular}} & DRL-cf   & 75.2~            & 21.5~                     & 3.3~                         & 0.0~         & 0.0~                & fail             \\
	&                                                                                               & DRL-cd   & 84.0~            & 10.7~                     & 5.3~                         & 0.0~         & 0.0~                & fail             \\
	&                                                                                               & ORCA     & 25.0~            & 0.0~                      & 0.0~                         & 75.0~        & 0.0~                & fail             \\
	&                                                                                               & ORCA-DRL & 90.0~            & 0.0~                      & 4.7~                         & 2.0~         & 3.3~                & 33.65            \\
	\hline
\end{tabular}
	}
\end{table*}



\subsection{Experiments on Simulation Scenarios}

In this chapter, we test the navigation performance of ORCA-DRL, ORCA and DRL algorithms in challenging and complex simulation environments (Scenarios 1-4). In the following four scenarios, both ORCA-DRL and DRL use the same trained neural network as the controller to fully compare the performance of the algorithms.

\textbf{Scenario-1: }In the corridor, 2 robots are randomly born from the central red areas and travel head-on to each other. They need to avoid some non-convex/convex static wall obstacles and then reach the targets (indicated by the pentagrams in Fig.11) that are randomly born in the red areas at both edges. Fig.11a shows the avoidance trajectory of the ORCA-DRL algorithm, Fig.11d shows the avoidance trajectory with priority of the ORCA-DRL algorithm (the blue robot has priority level $pr_{1}=1 $ and the green robot has priority level $pr_{2}=10 $), Fig.11b shows the avoidance trajectory of the ORCA algorithm, and Fig.11c shows the avoidance trajectory of the DRL algorithm. 

Comparing Fig.11a and Fig.11b, we can witness that the robot trajectory generated by ORCA-DRL follows the rule of passing on the right side (\textbf{norm-1}) and maintains an adequate safety distance, while  the trajectory generated by ORCA follows the rule of passing on the right or left side is completely random. Meanwhile, since ORCA cannot deal with non-convex obstacles, the green robot in Fig.11b is stuck in local extremes and cannot reach the destination. From \textbf{Table \uppercase\expandafter{\romannumeral1}}, it can be concluded that the ORCA-DRL algorithm achieves a higher success rate than the DRL algorithm ($\bar{\chi }_{s}^{orca-drl}= 94.6\%$) and is more disciplined compared to the ORCA or DRL algorithm ($\bar{\chi }_{s}^{orca}= 48.4\%$, and $\bar{\chi }_{s}^{drl-cd}= 92.8\%$). When the two robots meet ($t= 6.4s $) as in Fig.9d, the blue robot urgently avoids the green robot, because the green robot has priority (\textbf{norm-3}). As a result, the green robot obtains a higher speed and a smoother trajectory.

In summary, ORCA-DRL implements \textbf{norm-1} and \textbf{norm-3}. However, since the dynamic obstacles are sparse (only two robots), ORCA-DRL and DRL have similar success rates using the same network ($\bar{\chi }^{orca-drl}= 97.3\%$, and $\bar{\chi }^{drl-cd}= 96.4\%$). We will continue to examine this issue in Scenario-3.



\textbf{Scenario-2: }The red robot will overtake the yellow robot, and then both of them will reach their destinations in the corridor. Here, we do not put any static obstacles in the scenario to avoid disturbing the social rules shown when overtaking. Fig.12a shows the overtaking trajectory of the robots when using the ORCA-DRL; Fig.12b shows the overtaking trajectory of the robots when using the ORCA; and Fig.12c shows the overtaking trajectory of the robots when using the DRL. To achieve overtaking, the speed range for the yellow robot was set to $60\%$ of that for the red robot ($ v_{i=1}\in \left ( 0.01,0.20 \right ) m/s,v_{i=2}\in \left ( 0.01,0.12 \right ) m/s $), while they had the same angular speed range($ w_{i=1},w_{i=2} \in \left ( -2.5,2.5 \right ) rad/s $).


In Fig.12a,b, the trajectory generated by ORCA-DRL shows that the red robot overtakes the previous yellow robot from the left side (\textbf{norm-2}) and maintains a proper overtaking distance, while the trajectory of ORCA still randomly overtakes from the left or right side. As in Fig.12b, the safety distance of the ORCA algorithm is small, and the red robot squeezes the yellow robot during overtaking, causing trajectory oscillation for both robots. Similarly, \textbf{Table \uppercase\expandafter{\romannumeral1}} indicates that the ORCA-DRL algorithm and the DRL algorithm have a higher social rules success rate than the ORCA algorithm ($\bar{\chi }_{s}^{orca-drl}= 100.0\%, \bar{\chi }_{s}^{orca}=48.4\%, \bar{\chi }_{s}^{drl-cd}= 100.0\%$) in the overtaking scenario. ORCA-DRL and DRL have similar success rates using the same network ($\bar{\chi }^{orca-drl}= 100.0\%, \bar{\chi }^{drl-cd}= 100.0\%$) in the Scenario-2. 

In summary, ORCA-DRL implements \textbf{norm-2}. Simulation results for Scenario-1 and Scenario-2 demonstrate that ORCA-DRL achieves the task of obeying specific social norms (Task-2).


\begin{figure}[htbp]
	\centering
	\vspace{-0.6cm}
	\subfloat[]{\includegraphics[scale=0.28]{fig13a.png}}\quad
	\vspace{-0.4cm}
	\label{fig13a}
	
	\subfloat[]{\includegraphics[scale=0.28]{fig13b.png}}\\	
	\vspace{-0.4cm}
	\label{fig13b}
	
	\subfloat[]{\includegraphics[scale=0.28]{fig13c.png}}\\	
	\label{fig13c}
	
	\caption{Scenario-3: 8 robots each start from 4 different directions and cross the intersection to reach their destinations. (a) shows that ORCA-DRL can coordinate robots well to pass on the right and avoid oncoming vehicles. (b) shows that ORCA produces a chaotic interlaced trajectory in the intersection area. (c) shows that only 6 of the robots using DRL reach their targets safely, while the other 2 robots collide with each other.}
	\label{fig13}
\end{figure}


\textbf{Scenario-3: }At the intersection, 8 robots each start from 4 different directions and cross the intersection to reach their destinations. 

In Fig.13a, it can be illustrated that the trajectory generated by ORCA-DRL can coordinate robots well to pass on the right and avoid oncoming vehicles. At the intersection multiple robots spontaneously form a loop, thus avoiding collisions without reducing the speed too much. 

In contrast, the trajectory generated by ORCA is not applicable to the social rules and the algorithm produces a chaotic interlaced trajectory in the intersection area (shown in Fig13.b). At the same time, the speed of each robot is very low in order to achieve dynamic obstacle avoidance. Thus, \textbf{Table \uppercase\expandafter{\romannumeral2}} shows that ORCA-DRL arrives $14.1\% $ earlier than ORCA and has a better global trajectory. Compared to ORCA, ORCA-DRL effectively increases the average speed by enabling cooperation and social norms. This results in a faster and smoother flow of moving robots in scenarios with high robot density.

From Fig.13c, although there was cooperation among the robots using the DRL algorithm, only 6 robots reached their destinations safely, while 2 robots collided with each other. In scenarios with high robot density, the success rate of obstacle avoidance by the DRL algorithm decreases significantly. The behavior of collision avoidance requires very precise and robust actions of the controller, yet the neural network trained by DRL has poor generalization ability to achieve sufficiently safe collision avoidance. From \textbf{Table \uppercase\expandafter{\romannumeral2}}, it can be seen that ORCA-DRL substantially improves the success rate compared to DRL using the same network ($\bar{\chi }^{orca-drl}=90.7\%,  \bar{\chi }^{drl-cd}= 58.0\%$), where the dynamic collision ratio is reduced from $38.7\% $ to $2.7\% $. This $2.7\% $ of collisions is due to the fact that ORCA-DRL requires more computing power, and the lack of real-time computation during operation can lead to a small number of collision avoidance failures.

In the simulation video we provide, as the 8 robots cyclically birth from the starting point and cross the intersection in Scenario-3, the number and relative positions of the robots that meet at the intersection change all the time. In this complex condition, ORCA-DRL still guarantees a high success rate, which indicates that the controller of ORCA-DRL has stronger robustness than the controller of DRL.


In summary, ORCA-DRL achieves cooperative behavior among robots compared to ORCA, obtaining smoother trajectories and faster average speeds ($14.1\% $ faster than ORCA). Moreover, ORCA-DRL theoretically overcomes the shortcomings of DRL in terms of collision avoidance (Task-3). 



\begin{figure}[htbp]
	\centering
	\vspace{-0.5cm}
	\subfloat[]{\includegraphics[scale=0.26]{fig14a.png}}%
	\quad
	\label{fig14a}
	\vspace{-0.4cm}
	
	\subfloat[]{\includegraphics[scale=0.26]{fig14b.png}}%
	\quad
	\label{fig14b}
	\vspace{-0.4cm}
	
	\subfloat[]{\includegraphics[scale=0.26]{fig14c.png}}%
	\quad
	\label{fig14c}
	\caption{Scenario-4: 8 robots each start to cross the obstacle area ahead and then go through the intersection to reach the destination (6 straight ahead, 2 turn right). (a) shows that ORCA-DRL drives the robots to bypass the non-convex region and navigate smoothly. (b) shows that 6 robots are trapped in front of convex obstacles and only 2 robots with a convex obstacle in front of them can move normally when using ORCA. (c) shows that though the robots using DRL were able to avoid the non-convex region, 2 robots still collided at the intersection, and only 6 robots reached their destinations.}
	\label{fig14}
\end{figure}



\textbf{Scenario-4: }8 robots each start to cross the obstacle area ahead (6 non-convex obstacles, 2 convex obstacles) and then go through the intersection to reach the destination (6 straight ahead, 2 turn right). This is a complex mixed scenario of dynamic and static non-convex obstacles. 

In Fig.14a, the policy $\pi _{\theta }^{orca-drl} $ obtained by learning in ORCA-DRL drives the robots to bypass the non-convex region and navigate smoothly ($\bar{\chi }^{orca-drl}=90.0\%$). At the intersection, the encounter robots also demonstrated full cooperation, driving on the right side and avoiding the meeting vehicles. As shown in Fig.14b, since the ORCA algorithm cannot deal with non-convex static obstacles, 6 robots get stuck in the local extreme area and freeze in front of the convex obstacles, and only 2 agents with convex obstacles in front of them can move normally ($\bar{\chi }^{orca}=25.0\%$). Comparing Fig.14a and Fig.14b, ORCA-DRL achieves the highest success rate ($\bar{\chi }^{orca-drl}=90.0\%$) in this scenario than ORCA algorithm and DRL algorithm ($\bar{\chi }^{drl-cd}=84.0\%$), as shown in \textbf{Table \uppercase\expandafter{\romannumeral2}}. The experimental results of both Scenario-1 and Scenario-4 adequately demonstrate that the ORCA-DRL algorithm is more "intelligent" to handle complex non-convex mixed scenes without maps. 

Fig.14c shows that though the robots using DRL were able to avoid the non-convex region, two robots still collided at the intersection where multiple robots met, and only six robots reached their destinations. Similar to Scenario-3, this indicates that the collision avoidance capability and security of the DRL algorithm are insufficient.

In summary, the ORCA-DRL algorithm has better pathfinding performance (Task-1) in complex non-convex environments. Under Scenario-4, ORCA-DRL achieves the highest success rate $90.0\% $ compared to ORCA and DRL.



In the above four different scenarios, ORCA-DRL accomplished the combined tasks of pathfinding (\textbf{Task-1}), obeying social norms (\textbf{Task-2}), and collision avoidance (\textbf{Task-3}) better in mapless navigation. Furthermore, the above scenarios all demonstrate that ORCA-DRL successfully follows specific social norms (\textbf{norm-1, 2, 3}). In dynamic complex environments, ORCA-DRL achieves the best navigation success rate ($90.7\% $ in Scenario-3, $90\% $ in Scenario-4). Compared to the DRL algorithm, ORCA-DRL improves the success rate of obstacle avoidance; compared to the ORCA algorithm, ORCA-DRL copes with non-convex obstacles better and improves the average speed. However, due to the fact that the policy $\pi _{\theta }^{orca-drl} $ learned in ORCA-DRL is not optimal and very stable, the success rate in the above scenarios cannot reach 100 percent. We still need further research on safe and efficient multi-agent cooperation and navigation algorithms. 



\section{CONCLUSION}

We propose a socially aware robot mapless navigation algorithm with ORCA-Deep Reinforcement Learning. ORCA-DRL combines the advantages of RL and traditional collaborative collision avoidance method while considering the social norms. We integrate RL and model-based collision avoidance methods (ORCA) into a new navigation framework that better achieves the composite tasks of mapless navigation, including pathfinding (Task-1), following social norms (Task-2), and collision avoidance (Task-3). ORCA-DRL overcomes the shortcomings of obstacle avoidance in DRL and improves the success rate of dynamic obstacle avoidance $(90.7\%)$, it shows better performance in robot motion safety. ORCA-DRL framework introduces traffic norms of human society to enhance social comfort and achieve cooperative avoidance that follows human social customs (e.g., passing on the right, overtaking on the left and low-priority vehicles avoiding high-priority ones). Finally, compared to ORCA, ORCA-DRL achieves a significant improvement in navigation success (90.0\%) and better trajectory quality (14.1\% faster than ORCA) in complex mixed scenarios with dynamic and non-convex static obstacles. 

Our future work will further investigate the social norms and cooperation among agents in hybrid scenarios to improve the security of navigation algorithms.





% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


\appendices
\section{Proof of the First Zonklar Equation}
Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
\section{}
Appendix two text goes here.


% use section* for acknowledgment
\section*{Acknowledgment}


The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}
\bibliographystyle{IEEEtran}

\bibitem{ref1}
D.Silver, J.Schrittwieser, K.Simonyan, L.Antonoglou, A.Huang,A.Guez, T.Hubert, L.Baker, M.Lai, A.Bolton, Y.Chen, T.Lillicrap, F.Hui, L.Sifre, G.Van den Driessche, T.Graepel, and D.Hassabis, "Mastering the game of Go without human knowledge," {\it{Nature}}, vol.550, pp. 354-359, 2017.

\bibitem{ref2}
A. Puigdomenech Badia, B. Piot, S. Kapturowski, P. Sprechmann, A. Vitvitskyi, D. Guo, and C. Blundell, "Agent57: Outperforming the Atari Human Benchmark," in  {\it{Proceedings of the 37th International Conference on Machine Learning (PMLR)}}, 2020, pp. 507-517.

\bibitem{ref3}
D. Ye, Z. Liu, M. Sun, B. Shi, P. Zhao, H. Wu, H. Yu, S. Yang, X. Wu, Q. Guo, Q. Chen, Y. Yin, H. Zhang, T. Shi, L. Wang, Q. Fu, W. Yang, and L. Huang, "Mastering Complex Control in MOBA Games with Deep Reinforcement Learning", in {\it{Proceedings of the AAAI Conference on Artificial Intelligence}}, 2020, vol. 34, pp. 6672-6679.

\bibitem{ref4}
L. Tai, and M. Liu, "Towards Cognitive Exploration through Deep Reinforcement Learning for Mobile Robots," {\it{arXiv:1610.01733}}, 2016.

\bibitem{ref5}
E. Marchesini, D. Corsi, A. Benfatti, A. Farinelli, and P. Fiorini, "Double Deep Q-Network for Trajectory Generation of a Commercial 7DOF Redundant Manipulator," in  {\it{2019 Third IEEE International Conference on Robotic Computing (IRC)}} , pp. 421-422, 2019. 

\bibitem{ref6}
S. Choi, E. Kim, and S. Oh, "Real-time navigation in crowded dynamic environments using Gaussian process motion control," in  \textit{IEEE International Conference on Robotics and Automation (ICRA)}, 2014, pp. 3221–3226.

\bibitem{ref7}
O.Khatib, "Real-Time Obstacle Avoidance for Manipulators and Mobile Robots," in  {\it{Proceedings. 1985 IEEE International Conference on Robotics and Automation}}, March 1985, pp. 396–404.

\bibitem{ref8}
D. Fox, W. Burgard, and S. Thrun, "The dynamic window approach to collision avoidance," in {\it{IEEE Robotics \& Automation Magazine}} , March 1997, vol. 4, pp. 23–33. 

\bibitem{ref9}
P. Fiorini and Z. Shillery, "Motion planning in dynamic environments using velocity obstacles," {\it{The International Journal of Robotics Research (IJRR)}}, vol. 17, July 1998, pp. 760-772.

\bibitem{ref10}
V. Lumelsky, and A. Stepanov, "Dynamic path planning for a mobile automaton with limited information on the environment," {\it{IEEE Transactions on Automatic Control}}, vol.31, 1986, pp. 1058-1063.

\bibitem{ref11}
J. Van den Berg, S. J. Guy, M. Lin, and D. Manocha, "Reciprocal n-body collision avoidance," in  {\it{Robotics Research}}, Springer, 2011, pp. 3-19. 

\bibitem{ref12}
D. Dolgov, S. Thrun, M. Montemerlo, and J. Diebel, "Practical Search Techniques in Path Planning for Autonomous Driving," in {\it{American Association for Artificial Intelligence (AAAI)}}, 2008, pp. 32-37.

\bibitem{ref13}
Anthony Stentz, "Optimal and efficient path planning for partially known environments," in {\it{IEEE International Conference on Robotics and Automation (ICRA)}}, May 1994, pp. 3310-3317.

\bibitem{ref14}
Y. F. Chen, M. Everett, M. Liu, and J. P. How, "Socially aware motion planning with deep reinforcement learning," in {\it{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}}, 2017, pp. 1343-1350.

\bibitem{ref15}
H.-T. L. Chiang, A. Faust, M. Fiser, and A. Francis, "Learning navigation behaviors end-to-end with Au-toRL,"{\it{IEEE Robotics and Automation Letters}}, vol. 4, 2019, pp. 2007-2014.

\bibitem{ref16}
M. Pfeiffer, M. Schaeuble, J. Nieto, R. Siegwart, and C. Cadena, "From perception to decision: A data-driven approach to end-toend motion planning for autonomous ground robots," in {\it{IEEE International Conference on Robotics and Automation (ICRA)}}, 2017, pp. 1527-1533.

\bibitem{ref17}
Y. Chen, M. Liu, M. Everett, and J. P. How, "Decentralized, non-communicating multiagent collision avoidance with deep reinforcement learning," in {\it{IEEE International Conference on Robotics and Automation (ICRA)}}, 2017,pp. 285-292.

\bibitem{ref18}
M. Everett, Y. Chen, and J. P. How, "Motion Planning Among Dynamic, Decision-Making Agents with Deep Reinforcement Learning ," in {\it{IEEE International Conference on Robotics and Automation (ICRA)}}, 2018, pp. 3052-3059.

\bibitem{ref19}
P. Long, T. Fan, X. Liao, W. Liu, H. Zhang, and J. Pan, "Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep Reinforcement Learning ," in {\it{IEEE International Conference on Robotics and Automation (ICRA)}}, 2018, pp. 6252-6259.

\bibitem{ref20}
T. Fan , P. Long , W. Liu, and J. Pan, "Distributed multi-robot collision avoidance via deep reinforcement learning for navigation in complex scenarios ," {\it{The International Journal of Robotics Research (IJRR)}}, vol. 39, 2020, pp. 856-892.

\bibitem{ref21}
J. van den Berg, M. Lin, and D. Manocha, "Reciprocal Velocity Obstacles for real-time multi-agent navigation," in {\it{IEEE International Conference on Robotics and Automation (ICRA)}}, 2008, pp. 1928-1935.

\bibitem{ref22}
J. van den Berg, S. J. Guy, M. Lin, and D. Manocha, "Reciprocal collision avoidance with acceleration-velocity obstacles," in {\it{IEEE International Conference on Robotics and Automation (ICRA)}}, 2011, pp.3475-3482.

\bibitem{ref23}
D. Bareiss, and J. van den Berg, "Reciprocal collision avoidance for robots with linear dynamics using LQR-obstacles," in {\it{IEEE International Conference on Robotics and Automation (ICRA)}}, 2013, pp.3847-3853.

\bibitem{ref24}
D. Bareiss, and J. van den Berg, "Generalized reciprocal collision avoidance," {\it{The International Journal of Robotics Research (IJRR)}}, vol. 34, 2015, pp. 1501-1514.

\bibitem{ref25}
J. Snape, J. Van Den Berg, S. J. Guy, and D. Manocha, "Smooth and collision-free navigation for multiple robots under differential-drive constraints," in {\it{International Conference on Intelligent Robots and Systems}}, 2010, pp. 4584-4589. 

\bibitem{ref26}
J. Alonso-Mora, A. Breitenmoser, M. Ruflfli, P. Beardsley, and R. Siegwart, "Optimal reciprocal collision avoidance for multiple non-holonomic robots," in {\it{Distributed Autonomous Robotic Systems}}. Springer, 2013, pp. 203-216.

\bibitem{ref27}
S. Kim, S. J. Guy, W. Liu, D. Wilkie, R. W. Lau, M. C. Lin, and D. Manocha, "BRVO: Predicting pedestrian trajectories using velocityspace reasoning," {\it{International Journal of Robotics Research (IJRR)}}, vol. 34, 2015, pp. 201-217.

\bibitem{ref28}
G. Ferrer, A. Garrell, and A. Sanfeliu, "Robot companion: A socialforce based approach with human awareness-navigation in crowded environments," in {\it{IEEE/RSJ International Conference on Intelligent Robots and Systems}}, 2013, pp. 1688-1694.

\bibitem{ref29}
G. Ferrer, and A. Sanfeliu, "Behavior estimation for a complete framework for human motion prediction in crowded environments," in {\it{IEEE International Conference on Robotics and Automation (ICRA)}}, 2014, pp. 5940-5945. 

\bibitem{ref30}
D. Mehta, G. Ferrer, and E. Olson, "Autonomous navigation in dynamic social environments using multi-policy decision making," in {\it{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}}, 2016, pp. 1190-1197. 

\bibitem{ref31}
H. Kretzschmar, M. Spies, C. Sprunk, and W. Burgard, "Socially compliant mobile robot navigation via inverse reinforcement learning," {\it{The International Journal of Robotics Research}}, 2016, pp. 1289-1307. 

\bibitem{ref32}
J. Choi, C. Dance, J. Kim, K. Park, J. Han, J. Seo, M. Kim, "Fast Adaptation of Deep Reinforcement Learning-Based Navigation Skills to Human Preference," in {\it{IEEE International Conference on Robotics and Automation (ICRA)}}, 2020, pp. 3363-3370.

\bibitem{ref33}
B. Kim, and J. Pineau, "Socially adaptive path planning in human environments using inverse reinforcement learning," {\it{International Journal of Social Robotics}}, vol. 8, 2015, pp. 51-66.

\bibitem{ref34}
P. Abbeel, and A. Y. Ng, "Apprenticeship learning via inverse reinforcement learning," in {\it{Proceedings of the 21st International Conference on Machine Learning(ICML)}}, 2004.

\bibitem{ref35}
D. Pathak, P. Agrawal, A.A. Efros, and T.Darrell, "Curiosity-driven Exploration by Self-supervised Prediction," in {\it{Proceedings of the 34th International Conference on Machine Learning (ICML)}}, 2017.

\bibitem{ref36}
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal policy optimization algorithms," {\it{arXiv:1707.06347}}, 2017.

\bibitem{ref37}
P. Mirowski,R. Pascanu,F. Viola,H. Soyer,A. J. Ballard,A. Banino,M. Denil,R. Goroshin,L. Sifre,K. Kavukcuoglu,D. Kumaran,R. Hadsell, "Learning to navigate in complex environments," {\it{arXiv:1611.03673}}, 2016.

\bibitem{ref38}
R. Han et al., "Reinforcement Learned Distributed Multi-Robot Navigation With Reciprocal Velocity Obstacle Shaped Rewards," in IEEE Robotics and Automation Letters, vol. 7, no. 3, pp. 5896-5903, July 2022.

\bibitem{ref39}
G. Javier, and F. Fernández. "A comprehensive survey on safe reinforcement learning." Journal of Machine Learning Research 16.1(2015), pp. 1437-1480.

\bibitem{ref40}
C. Yan, J. Qin, Q. Liu, Q. Ma and Y. Kang, "Mapless Navigation with Safety-Enhanced Imitation Learning," in IEEE Transactions on Industrial Electronics, 2022.

\bibitem{ref41}
M. Richard, and J. Shavlik. "Creating advice-taking reinforcement learners." Machine Learning 22.1 (1996), pp. 251-281.

\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}{Michael Shell}
Biography text here.
\end{IEEEbiography}

% if you will not have a photo at all:
\begin{IEEEbiographynophoto}{John Doe}
Biography text here.
\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiographynophoto}{Jane Doe}
Biography text here.
\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


