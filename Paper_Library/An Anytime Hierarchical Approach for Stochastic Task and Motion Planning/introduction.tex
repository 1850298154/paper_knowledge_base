\section{Introduction}

A long-standing goal in robotics is to develop robots that can operate autonomously in real-world environments and solve complex tasks such as cleaning a room or organizing a table. Recent developments in sampling-based motion planning algorithms~\cite{kavraki1996probabilistic,Lavalle98rrt,sucan2012open} have enabled robots to efficiently plan in configuration spaces that have infinite states and a large branching factor. However, these sampling-based motion planners are not designed for to plan over long horizons and changing configuration spaces for solving complex real-world problems. The problem becomes even more challenging when the robot's actions and/or its environment are stochastic, as the agent has to not only deal with a long horizon but also needs to compute a contingent solution that deals with all possible situations that might arise while acting in the real world. E.g., consider a household robot that is arranging a dining table. The robot may to pick up objects or may drop objects while carrying them from one location to another. What should robot do if this happens?



\begin{figure}[t!]
  \centering
  \includegraphics[height=1.5in]{./3pi.eps}\hspace{1cm}
  \includegraphics[height=1.5in]{./fetch_1.png}
  \caption{Left: YuMi robot uses the algorithm developed in this paper to build a $3\pi$ structure using Keva planks despite stochasticity in their initial locations. Right:
    A stochastic variant of the cluttered table domain where robot
    is instructed to pick up the black can, but pickups may fail and crush the cans requiring them to be disposed.}
  \label{fig:domainFig}
\end{figure}

A na\"ive approach for overcoming such a problem would be to first compute a symbolic high-level policy using an abstract model of the domain defined using a symbolic language such as Probabilistic Planning Domain Definition Language (PPDDL)~\cite{younes2004ppddl1} or Relational Dynamic Influence Diagram (RDDL)~\cite{sanner10_rddl} and then refining each possible scenario in the policy by computing low-level motion plans for every action in it. This approach is na\"ive in the sense that 
% it neither prioritizes more likely outcomes ahead of outcomes that are highly unlikely (but still possible) nor it guarantees completeness as every action might not admit a low-level motion plan as 
\begin{enumerate*}[a)] \item abstracted models are lossy and may lose important geometric information about the problem. Policies resulting from such approaches might not have any feasible motion planning refinements for some of their actions ~\cite{cambon09_asymov,kaelbling11_hierarchical,srivastava14_tmp} and 
  \item the number of actions in a policy grows exponentially with the horizon. Computing motion planning refinements for the entire policy may be expensive, and it may require large amount of time. 
\end{enumerate*}

Therefore, most solutions approaches to this problem focus on most likely scenarios. They compute task and motion plan for these most likely actions and outcomes and execute it until the robot achieves the goal or reaches a state for which it has not yet planned an action. In the latter situation, they replan from that state and compute a new plan that reaches the goal from it. While this approach may achieve the goal, it may not be safe or sufficient as on-the-fly replanning is prone to errors and may result in unwanted situations or dead ends. E.g., a vacuum cleaner robot may end up in a water puddle and damage itself if such a determinization~\cite{determinization} based approach is employed. 

To the best of our knowledge, this paper presents the first \emph{probabilistically complete} and \emph{anytime} approach for computing integrated task and motion policies for stochastic environments where each action has a discrete set of possible outcomes using off-the-shelf symbolic planners and motion planners.
% Our approach interleaves the computation of refinements with updating of abstractions to compute truly feasible task and motion policies for stochastic environments where each action in the environment has a discrete set of possible outcomes. 
Our approach computes the likelihood of each possible outcome of the policy and weighs it against the estimated cost of computing solution for that outcome and use it prioritize outcomes for refining.
The approach is anytime in the sense that it continually improves the quality of the solution while ensuring that more likely situations are resolved earlier by the approach. It also provides a running estimate of the probability mass of likely executions covered in the current policy. This estimate can be used to start execution based on the level of risk acceptable in a given application, allowing one to trade-off pre-computation time for likelihood of outcomes covered.

This way our approach generalizes the methods of computing solutions for most likely outcomes during execution~\cite{hadfield15_modular,determinization,teichteil2008rff} to the problem of integrated task and motion planning by using the anytime approaches in \emph{AI} planning~\cite{dean88_anytime,zilberstein93_anytime,dean95_anytime}. 
% The presented approach is the first \emph{probabilistically complete} approach that uses sound abstractions to solve combined task and motion planning problems in stochastic environments.
%  It works with arbitrary off-the-shelf symbolic solvers and off-the-shelf motion planners which allows it to scale automatically with improvements in either of these active areas of planning research and can also be used in deterministic settings as a special case.

In contrast to the closest related work (a conference paper by the team)~\cite{shah2020anytime}, this paper includes a new, more general, and rigorous problem formulation with an algorithmic paradigm that provides guarantees of probabilistic completeness, an extensive empirical evaluation on a broader class of test domains including experiments with a new generalized SSP solver, and a thorough analysis of the results.

The rest of the paper is structured as follows: Sec.~\ref{sec:background} provides the formal background; Sec.~\ref{sec:related} discusses recent work on the topic; Sec.~\ref{sec:formal} provides our formal framework and defines the stochastic task and motion planning problem; Sec.~\ref{sec:algo} discusses our overall algorithm and Sec. \ref{sec:empirical} provides the empirical evaluation for our approach. 


