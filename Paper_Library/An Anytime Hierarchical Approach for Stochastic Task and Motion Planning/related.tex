\section{Related Work}
\label{sec:related}
% As integrated task and motion planning rests on the intersection of task planning and motion planning, developments in either area affect it. So, now we discuss some related work in each of these areas developed in recent times. 


\paragraph{\textbf{Stochastic Task Planning}} 
Many approaches have been developed for classical planning efficiently in recent years~\cite{blum1997fast,bonet2001planning,hoffmann2001ff}.
%  \cite{bonet2001planning} introduce a way to synthesize \emph{domain-independent} heuristics by relaxing the problem through ignoring predicates in the \emph{delete} list of each action.  The relaxed problem is easier to solve and the solution can be used to estimate heuristics for the states encountered in the solution. GraphPlan~\cite{blum1997fast} generates planning graph for a given planning problem which can be used to automatically synthesize heuristics such as $h_{add}$ and $h_{max}$. \cite{hoffmann2001ff} uses planning graphs to generate tighter heuristic than $h_{add}$ and $h_{max}$  known as $h_{\text{ff}}$ by avoiding re-counting of actions that achieve similar predicates. 
% Most classical planners use a relational language known as \emph{Planning Domain Definition Language (PDDL)}~\cite{McDermott1998PDDL}.
Similarly, numerous approaches have been developed to solve stochastic shortest path problems. Dynamic
programming algorithms such as value iteration and policy iteration can be
used to compute policies for SSPs. 
% But these approaches require computing optimal cost action for each state in the state space to converge which can take a huge amount of time for large state spaces.
 \emph{Real-time dynamic programming} (RTDP)~\cite{barto1993rtdp} generalizes \emph{Korf's Learning-Real-Time-A*} algorithm to a trial-based dynamic programming method that ignores a large part of the state-space by only expanding states encountered in trials to solve $SSPs$ faster. \emph{LAO*}~\cite{hansen2001lao} uses heuristics to expand the partial policy tree along with local value iteration to compute policies for SSPs. \emph{Labeled RTDP}~\cite{bonet2003labeled} extends \emph{RTDP} by labeling states that have converged greedy policy to reduce the number of states considered for expansion to decrease policy computation time. \cite{Muise2012ImprovedNP} use state relevance to guide the search to reduce the time to compute the policy. \cite{larach2019sspdead} provide a method that decomposes an $SSP$ into multiple smaller $SSPs$ and combines the solution to handle \emph{dead ends}. 


\paragraph{\textbf{Hierarchical Planning}} Hierarchical approaches~\cite{sacerdoti1974planning,knoblock1990learning,erol1995semantics,seipp2018counterexample} use abstractions to generate different hierarchies of relaxed planning problems in order to compute a solution for a complex planning problem. State abstraction generates hierarchies by removing certain predicates (in relational domains) or variables (in factored domains) from the domain vocabulary. ABSTRIPS \cite{sacerdoti1974planning} is one of the earliest hierarchical planning approaches which assigns a rank to each literal using a predefined order and complexity of achieving that literal in the STRIPS planning process. Abstraction hierarchy is generated by dropping literals from the precondition of actions in the domain in the order specified by the rank of literals. The planning hierarchy generated using ABSTRIPS is common for all problems in the given domain and not catered to independent problems. 

ALPINE \cite{knoblock1990learning} uses \emph{ordered monotonicity} to overcome this issue by generating abstraction hierarchies tailored to each problem for the given domain.  
% While this approach makes strong assumptions of ordered monotonicity and downward refinement property~\cite{bacchus1991downward}, our approach does not require such critical assumptions.
\cite{seipp2013counterexample,seipp2018counterexample} use counter-example guided abstraction refinement (CEGAR) to solve a complex planning problem hierarchically using Cartesian abstraction -{}- a variant of predicate abstraction. This CEGAR-based approach starts with a na\"ive abstraction for the problem and computes an optimal plan for the abstract model. It tries to execute this plan in the original model. If it fails to execute the plan successfully, it computes a flaw in the current plan and uses it to refine the current abstract model. This approach requires a pre-image of each \emph{grounded operator} and a bounded branching factor for the search tree. Such approaches are not conducive to task and motion planning setups because they require discrete action and state spaces while task and motion planning operates in continuous states and action spaces. 


Temporal abstractions generate high-level actions that are compositions of multiple low-level actions. Some hierarchical planning approaches employ temporal abstraction to create relaxed problems. Multiple approaches~\cite{kambhampati1998hybrid,bacchus2000using,bercher2014hybrid} have used hierarchical task networks (HTNs)~\cite{erol1995semantics} to compute plans efficiently for complex tasks. HTNs use temporal abstractions to define tasks over primitive actions. The goal is to compute a final plan which is a composition of the high-level tasks that are achieved through the partial order planning of the primitive actions. \citet{marthi2007hla} compute hierarchical domain descriptions based on angelic semantics using temporal abstractions. They use a top-down forward search algorithm to refine the high-level actions into a sequence of primitive actions. While this approach and HTN-based approaches efficiently perform top-down planning using temporal abstraction, they fail to compute accurate plans in the models that do not fulfill downward refinement property. Additionally, they do not handle stochasticity. 
% On the other hand, our approach handles such domains using an interleaved approach that refines actions while continually improving the abstract model.

Several approaches utilize abstraction for solving MDPs (\cite{hostetler14_state,bai16_markovian,li06_abstractMDP,singh95_abstractRL}). However,
these approaches assume that the full, unabstracted MDP can be
efficiently expressed as a discrete MDP. \cite{marecki06_cmdp} consider continuous-time MDPs with finite
sets of states and actions. In contrast, our focus is on MDPs with
high-dimensional and uncountable state and action spaces. Recent work on
deep reinforcement learning (e.g.,
\cite{hausknecht16_iclr,mnih15_drl}) presents approaches for using
deep neural networks in conjunction with reinforcement learning to
solve short-horizon MDPs with continuous state spaces. These
approaches can be used as primitives in a complementary fashion with
task and motion planning algorithms, as illustrated in recent
promising work by \cite{wang18_active}.

  
Task planning efficiently computes solutions for complex goals. But, it can not handle manipulation problems with continuous domains that have an infinite branching factor. Though \emph{PDDL 2.1}~\cite{maria2003pddl2} allows using continuous variables, it still struggles to handle infinite branching factor. 




\paragraph{\textbf{Motion Planning}} Recent research resulted in significant improvements in sampling-based motion planners. Probabilistic roadmaps (PRM)~\cite{kavraki1996probabilistic}  randomly sample from the C-space to generate a roadmap that can be lazily used to generate motion plans. Rapidly-exploring random trees (RRT)~\cite{Lavalle98rrt} computes a collision-free path from an initial robot configuration to the target configuration by connecting randomly sampled robot configurations from the C-space. Bi-directional RRT (BiRRT)~\cite{kuffner2000birrt} updates existing RRT to initiate search trees from the initial and goal configurations to boost the speed of motion planning. Constrained BiRRT (CBiRRT)~\cite{bernson2009cbirrt} extends the BiRRT technique constraining the search space by using projection techniques to explore configurations spaces and finds bridges between them. 

% While motion planning efficiently computes plans in high-dimensional continuous spaces, it lacks the capabilities to reason over a long horizon and to compute plans for complex tasks such as arranging a dining table. Our approach does not alter existing motion planning techniques and works with any off-the-shelf motion planner.

\paragraph{\textbf{Integrated Task and Motion Planning}}
\label{sec:itmp}
% Integrated task and motion planning (TMP) overcomes the limitations of task planning and motion planning by combining them. TMP utilizes the power of solving long horizon complex tasks and capabilities of motion planning to deal with possibly infinite state space. 

Most of the prior work in the field of integrated task and motion planning has focused on solving deterministic task and motion planning problems. Most of these approaches can be classified into three categories: \emph{1)} approaches that use symbols to guide the low-level motion planning, \emph{2)} approaches that extend high-level representations to simultaneously search high-level plans along with continuous parameters, and \emph{3)} approaches that use interleaved search for valid high-level plans with low-level refinements for its actions. Our approach falls under the last category. \citet{garrett2021integrated} present an exhaustive survey of these approaches; we discuss only the most closely related approaches here. 


\paragraph{Approaches that use symbols to guide the motion planning:}
\cite{cambon09_asymov} introduced one of the earliest approaches named aSyMov. ASyMov uses symbolic knowledge to guide planning in geometric space using location references. \cite{plaku10_sampling} use a similar approach to allow combined task and motion planning for robots with constrained manipulators. Such approaches employ task planning as a heuristic for planning in the C-space, which may not always be efficient due to a lack of knowledge of geometric constraints at the task-planning level. In order to overcome this limitation, we interleave the process of computing motion plans and updating the high-level specification.

 

\paragraph{Approaches that extend high-level representations:} Another class of approaches ~\cite{dornhege12_semantic,garrett15_ffrob,garrett2020pddlstream} extends the high-level representation to allow the high-level planner to validate preconditions of the high-level actions in the geometric space while computing the high-level plan. \cite{dornhege12_semantic} do so by developing \emph{semantic attachments} for \emph{PDDL} representation that check the validity of each high-level action using a motion planner in the low level. \emph{FFRob}~\cite{garrett15_ffrob} uses pre-sampled robot configurations to discretize the problem and build a \emph{roadmap} to evaluate the preconditions of the high-level action. \emph{PDDLStream} \cite{garrett2020pddlstream} uses optimistic samplers to sample continuous arguments in the PDDL descriptions. Their optimistic samplers are analogous to ``generators'' used by our approach (explained later in Sec.~\ref{sec:entity}) that are used to instantiate abstract actions and serves the same purpose. Our approach and PDDLStream use these samplers to sample concrete values for symbolic abstract arguments.

% While these approaches require a carefully crafted high-level domain description including an explicit specification of the geometric constraints for all high-level actions, our approach does not require such explicit constraint specification for each action. These approaches are computationally expensive compared to our approach as the motion planner is invoked much more frequently as part of high-level planning.}
 

 \paragraph{Approaches that perform an interleaved search: } The last group of approaches performs an interleaved search to find a high-level solution that also has valid motion planning refinements in the low level. These approaches incrementally update the high-level models using the feedback from the low level while searching for the refinements. \cite{srivastava14_tmp} implement a modular approach that uses a planner-independent interface layer to allow communication between a task planner and a motion planner. 
%  The interface layer is used to compute refinements for high-level actions as well as update high-level models with the feedback received from the low-level environment while computing these refinements.
 \cite{dantam2018incremental} develop a constraint-based approach that incrementally adds constraints to the high-level specification of the problem discovered while trying to refine a high-level plan generated using an \emph{SMT}-based planner. Because these approaches commit to a single high-level model, it is not clear how they would be able to avoid dead ends. Additionally, all these approaches work only for deterministic problems and do not handle stochastic settings.
%  On the other hand, our approach maintains multiple abstract models and a defined strategy to explore them. This provides more thorough guarantees of probabilistic completeness.


To the best of our knowledge, the only approaches designed to handle stochastic task and motion planning problems were presented by \cite{kaelbling2011hierarchical}, \cite{hadfield15_modular}, and \citet{garrett2020online}. These approaches consider a partially observable formulation of the problem. \citet{kaelbling2011hierarchical} utilize regression modules on belief fluents to develop a regression-based solution algorithm. 
% While they address the more general class of partially observable problems, their approach follows a process of online, incremental discretization and does not address the computation of branching policies, which is the focus of this paper. 
\citet{hadfield15_modular} extend the work on deterministic task and motion planning by \citet{srivastava14_tmp} for partially observable settings. They use maximum likelihood observations~\cite{platt2010belief} to obtain a determinized high-level representation. 
% Our approach, in contrast to it, our approach uses abstraction to derive a high-level representation that is stochastic and requires a contingent solution. 
\citet{garrett2020online} develop an online algorithm that uses observational actions to gather belief about partially-observable environments and performs task and motion planning using discretized actions. These approaches address a more general class of partially observable problems. However, they do not address the computation of branching policies, which is the key focus of this paper.
 
