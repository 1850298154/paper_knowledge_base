\section{Introduction}
\vspace{-5pt}
Motivated by the reasonable prices and the good quality of cloud services, several enterprises and governments are deploying their applications in the cloud. Hadoop~\cite{HadoopOnline} has become enormously popular for processing data-intensive applications in the cloud. A core constituent of Hadoop is the scheduler. The Hadoop scheduler is responsible for the assignment of tasks belonging to applications across available computing resources. Scheduling data-intensive applications' tasks is a crucial problem, especially in the case of real-time systems, where several constraints should be satisfied.
Indeed, an effective scheduler must assign tasks to meet applications' specified deadlines and to ensure their successful completions. %using the assigned resources.
In the cloud, failures are the norm rather than the exception and they often impact the performance of tasks running in Hadoop. Consequently, the Hadoop scheduler may fail to achieve its typical goal for different reasons such as resources-deadlock, task starvation, deadline non-satisfaction and data loss~\cite{WOHA2014}~\cite{RAFT2011}.
This results in extra delays that can be added and propagated to the overall completion time of a task, which could lead to resources wastage, and hence significantly decreasing the performance of the applications and increasing failure rates.
%\Foutse{i would removed the following paragraph or rephrased it a bit ....we are making too much emphasis on the failures in the cloud but not much on the failures of the tasks which may be due to poor scheduling...}
For instance, Dinu and Ng~\cite{Dinu:2012} analyzed the performance of the Hadoop framework under different types of failures. They reported that the Hadoop scheduler experiences several failures due to the lack of information sharing about its environment (\eg{} scheduler constraints, resources availability), which can lead to poor scheduling decisions.
For instance, they claim that the Hadoop scheduler experiences several failures because of prioritized executions of some long tasks before small ones, long delays of the struggling tasks or unexpected resources contentions~\cite{Dinu:2012}~\cite{SOUALHIA2017}. 
%\Foutse{in the paragraph below you sound like if the verification is only important to make hadoop scheduler more reliable and you don't emphasize enough the fact that your framework will allow people to simulate the behavior of their clusters and the fact hat they will be able to prevent certain failures from occurring in the field....because they understood the circumstances that would caused them!}
The widespread use of the Hadoop framework in safety and critical applications, such as healthcare~\cite{Real-Time2017} and aeronautics~\cite{NASA}, poses a real challenge to software engineers for the designing and testing of such framework to meet its typical goal and avoid poor scheduling decisions.
Although several fault-tolerance mechanisms are integrated in Hadoop to overcome and recover from these failures, several failures still occur when scheduling and executing tasks~\cite{SOUALHIA2017}.
%\Foutse{which failures? those from the environment or those stemming from poor scheduling?}
Indeed, these failures can occur because of poor scheduling decisions (\eg{} resources deadlock, task starvation) or constraints related to the environment where they are executed (\eg{} data loss, network congestion).
As such, verifying the design of Hadoop scheduler is an important and open challenge to identify the circumstances leading to task failures and performance degradation %Also, it can help check the main issues that can occur while scheduling tasks and their impact on tasks' executions.
%\Foutse{do you want to say that they can anticipate on the potential issues?}
so that software engineers studying Hadoop can anticipate these potential issues and propose solutions to overcome them. This is to improve the performance of the Hadoop framework.\\
\indent Different approaches have been adopted by the research community to verify and validate the behavior of Hadoop with respect to scheduling requirements.
Traditionally, simulation and analytical modeling have been widely used for this purpose. However, with many Hadoop-nodes being deployed to accommodate the increasing number of demands, they are inadequate because they are not efficient 
%\Foutse{do you mean that they do not scale?} \red{I do not like the work scale :(}
in exploring large clusters.
In addition, given the highly dynamic nature of Hadoop systems and the complexity of its scheduler, they cannot provide a clear understanding and exhaustive coverage of the Hadoop system especially when failures occur. %\Foutse{why? can you explain more?}.
Particularly, they are not able to ascertain a complete verification of the Hadoop scheduler because of the wide range of constraints and unpredictable aspects involved in the Hadoop model and its environment (\eg{} diversity of running loads, availability of resources and dependencies between tasks).
%\Foutse{can we be more specific?}
%On the other hand,
Formal methods have recently been used to model and verify Hadoop. However, to the best of our knowledge %Despite many successful results~\cite{Su2009,Towards-Reddy2013,OnoCoq2011} have been published in this direction,
very few efforts %, to the best of our knowledge,
have been invested in applying formal methods to verify the performance of Hadoop (\eg{ data locality, read and write operations, correctness of running application on Hadoop})~\cite{Su2009}~\cite{Towards-Reddy2013}~\cite{OnoCoq2011}.
%\Foutse{what kind of verification did they do then...which is unrelated to behavior...you are trying to say that they didn't verified the behavior of the scheduler?} 
In addition, there is no work that addresses the formal verification of Hadoop schedulers. %so far.
Considering the above facts, we believe that it is important to apply formal methods to verify the behavior of Hadoop schedulers. This is because formal methods are more able to model complex systems and thoroughly analyze their behavior than empirical testing.
Our aim is to formally analyze the impact of the scheduling decisions on the failures rate and avoid simulation cost in terms of execution time and hardware cost (especially for large clusters). This allows to early identify circumstances leading to potential failures, in a shorter time compared to simulation, and prevent their occurrences.
In contrast to simulation and analytical modeling, knowing these circumstances upfront would help practitioners better select the cluster settings (\eg{} number of available resources, type of scheduler) and adjust the scheduler design (\eg{} number of queues, priority handling strategies, failures recovery mechanisms) to prevent poor scheduling decisions in Hadoop.  \\
%\Foutse{why? its not because it havent been done that it should be done...we should provide a better motivation, explaining why it should work intuitively...}
\indent \Mbarka{In this paper,
%we investigate the impact of the Hadoop scheduler performance \Foutse{what do you mean by performance exactly? is it really what you are verifying?} on the failures rate and possible strategies to overcome them. Specifically,
we present a novel methodology that combines simulation and model checking techniques to perform a formal analysis of Hadoop schedulers. Particularly, we study the feasibility of integrating model checking techniques and simulation to help in formally verifying some of the functional scheduling properties
%\Foutse{you said before that you were verifying performance..which is a very wide concept....please be precise and use a consistent terminology!!!}
in Hadoop including schedulability, resources-deadlock freeness, and fairness~\cite{Cheng2002}.}
To this aim, we first present a formal model to construct and analyze the three mentioned properties within the Hadoop scheduler. 
We use the Communicating Sequential Processes (CSP) language~\cite{CSP} to model the existing Hadoop schedulers (FIFO, Fair and Capacity schedulers~\cite{SOUALHIA2017}), and use the Process Analysis Toolkit (PAT) model checker~\cite{PAT} to verify the schedulers' properties.
Indeed, the CSP language has been successfully used to model the behavior of synchronous and parallel components in different distributed systems~\cite{CSP}. Furthermore, PAT is a CSP-based model checker that has been widely used to simulate and verify concurrent, real-time systems, etc.~\cite{PAT}. 
\Mbarka{Based on the generated verification results in PAT, we explore the relation between the scheduling decisions and the failures rate while simulating different load scenarios running on the Hadoop cluster. This is in order to propose possible scheduling strategies to reduce the number of failed tasks and improve the overall cluster performance (resources utilization, total completion time, etc).} 
In order to illustrate the usability and benefits of our work to identify failures that could have been prevented using our formal analysis methodology, we apply our approach on the scheduler of OpenCloud, a Hadoop-based cluster that simulates the real Hadoop load~\cite{OpenCloud}.
Using our proposed methodology, developers would % analysis allows to early
identify up to 78\% of tasks failures early on before the deployment of the application. Based on the performed failures analysis, our methodology could provide potential strategies to overcome these failures. \Mbarka{For instance, our solution could propose new scheduling strategies to adjust the Hadoop cluster settings (\eg{} size of the queue, allocated resources to the queues in the scheduler, etc.) and reduce the failures rate when compared to the real-execution simulation results.}
% of (up to 78\%) when compared to the real-execution simulation traces.
%\Foutse{really?how? I thought the result would help practitioner better assign resources to mitigate the failures...to propose novel scheduling strategies we should propose an extension of existing schedulers like we did for  ATLAS, but is it what we do in this paper?}
To the best of our knowledge, the present work is different from existing research in applying and integrating both formal methods and simulation for the analysis of Hadoop schedulers and formally analyzing the impact of the scheduling decisions on the failures rate in Hadoop. Furthermore, our proposed methodology to integrate model checking and simulation to verify Hadoop schedulers could be also applied to formally analyze other schedulers, than Hadoop, such as Spark~\cite{SparkOnline}, which has become one of the key cluster-computing framework that can be running on Hadoop.\\
%One can adapt the proposed methodology according to the architecture of Spark. \\
%\Foutse{in which aspect? be precise please....}.
\indent The rest of the paper is organized as follows:
Section~\ref{preliminaries} describes basics of Hadoop architecture, CSP language, and the tool PAT.
Section~\ref{framework} presents the proposed methodology for the formal analysis of the Hadoop scheduler.
We describe the analysis of the scheduler of the OpenCloud a Hadoop cluster in Section~\ref{casestudy}.
Section~\ref{relatedwork} summarizes the most relevant related work.
Finally, Section~\ref{conclusion} concludes the paper and highlights some future directions. 



