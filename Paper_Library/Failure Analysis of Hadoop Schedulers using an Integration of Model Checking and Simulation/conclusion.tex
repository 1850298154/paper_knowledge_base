\section{Conclusion and Future Work}
\label{conclusion}
\vspace{-5pt}
Given the dynamic behavior of the cloud environment, the verification of the Hadoop scheduler has become an open challenge especially with many Hadoop-nodes being deployed to accommodate the increasing number of demands.
Existing approaches such as performance simulation and analytical modeling are not able to ascertain a complete verification of the Hadoop scheduler because of the wide range of constraints involved in Hadoop.
\Mbarka{In this paper, we presented a novel methodology that combines and integrates simulation and model checking techniques to perform a formal analysis of Hadoop schedulers. Particularly, we studied the feasibility of integrating model checking techniques and simulation to help in formally verifying some of the functional scheduling properties. So, we formally verified some of the most important scheduling properties on Hadoop including the schedulability, resources-deadlock freeness, and fairness properties, and analyzed their impact on the failure rates.
The ultimate goal of this work is to be able to propose possible scheduling strategies to reduce the number of failed tasks and improve the overall cluster performance and practitioner better assign and configure resources to mitigate the failures that a Hadoop scheduler may experience while simulating the Hadoop workload.}
We used CSP to model Hadoop schedulers, and the PAT model checker to verify the mentioned properties.
To show the practical usefulness of our work, we applied our approach on the scheduler of OpenCloud, a real Hadoop-based cluster.
%Our aim is to verify the three selected properties and check their impact on the tasks' failures.
The obtained results show that our approach is able to formally verify the selected properties and that such analysis can help identify up to 78\% of failures before they occur in the field.\\ %the failures when compared to the real-execution simulation traces of the OpenCloud scheduler.\\
\indent An important direction of future work is to verify other properties that can impact the failures rate in Hadoop, like the resource assignment, load balancing, and modeling the internal recovery mechanisms of Hadoop. Another direction can be the use of our work to automatically generate scheduling guidelines to improve the performance of the Hadoop framework and reduce the failures rate.
Finally, the failure analysis approach and findings of this paper can be extended and evaluated on Spark~\cite{SparkOnline}, which has become one of the key
cluster-computing framework that can be running on Hadoop. In fact, one can easily adapt the proposed methodology according to the architecture of Spark.