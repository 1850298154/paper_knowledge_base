\section{Related Work}
\label{relatedwork}
In this section, we discuss the most relevant work that apply formal methods in the context of Hadoop framework.
In~\cite{Performance2015} and~\cite{Petrinets2015}, the authors analyzed the behavior of MapReduce using Stochastic Petri Nets and Timed Colored Petri Nets, respectively. They modeled the mean delay in each time transition of the scheduled tasks as formulas, and the Hadoop jobs were simulated based on the used Petri Nets. The proposed approaches could evaluate the correctness of the system and analyze the performance of MapReduce. But, they lack several constraints about the scheduling of the jobs and cannot cover larger Hadoop clusters.
Su \textit{et al.}~\cite{Su2009} used the CSP language to formalize four key components in MapReduce. Specifically, they formally modeled the master, mapper, reducer and file system while considering the basic operations in Hadoop (\eg{} task state storing, error handling, progress controlling). However, none of the properties of the Hadoop framework is verified using the formalized components.
Xie \textit{et al.}~\cite{Xie2016} address the formal verification of the HDFS reading and writing operations using CSP and the PAT model checker.
For instance, they formally modeled the reading and writing operations for the HDFS based on the formalized components proposed in~\cite{Su2009}.
Moreover, they verified some of the HDFS properties including the deadlock-freeness, minimal distance scheme, mutual exclusion, write-once scheme and robustness. While this approach allows to detect unexpected traces generating errors and verify data consistency in the HDFS, a limitation of this work is that it only models the reading and writing operations for just one file system and requires to investigate the validity of these operations for distributed files as in HDFS.
In \cite{Towards-Reddy2013}, Reddy \emph{et al.} propose an approach to model Hadoop's system using the PAT model checker. They used CSP to model Hadoop software components including the ``NameNode", ``DataNode", task scheduler and cluster setup. They identified the benefits of some properties like data locality, deadlock-freeness and non-termination among others and proved the correctness of these properties. However, their proposed model is evaluated on a small workload, and none of the properties is verified to check the performance of the Hadoop scheduler.
%Although theorem provers are widely used to check the correctness and reliability of several distributed systems,
%To the best of our knowledge, we only find one theorem prover-based study that verifies the actual running code of MapReduce applications.
Kosuke \emph{et al.}~\cite{OnoCoq2011} used the proof assistant Coq to write an abstract computation model of MapReduce. They modeled the mapper and reducer as Coq functions and proved that the implementation of the mapper and reducer satisfies the specification of some applications such as WordCount~\cite{Wordcount2015}.
The authors present an abstracted model of MapReduce, where many details are not included
%hidden\Foutse{are you saying that they didn't provided all details?} 
such as task assignment or resources allocation. These issues can affect the performance of applications running on Hadoop.
\vspace{-10pt} 