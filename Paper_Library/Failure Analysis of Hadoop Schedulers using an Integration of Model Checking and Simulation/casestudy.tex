\section{Case Study: Formal Analysis of OpenCloud Scheduler}
\label{casestudy}
%\vspace{-10pt}
\Mbarka{In this section, we illustrate the practical usability and benefits of our work by formally analyzing the scheduler of OpenCloud, a Hadoop-based cluster~\cite{OpenCloud} that simulates the Hadoop load.}
\vspace{-5pt}
\subsection{Case Study Description}
%\vspace{-10pt}
To apply our formal approach on a case study, we investigated existing Hadoop case studies in the open literature.
Overall, we found three main public case studies including: Google~\cite{Google}, Facebook~\cite{Facebook}, and OpenCloud~\cite{OpenCloud} traces. %, that we checked to identify whether they contain enough inputs data of our methodology.
We found out that the Google traces do not provide data about the cluster settings, which is an important factor affecting the analysis results in our approach. For the Facebook traces, we noticed that they do not provide data about the cluster settings, capacity of nodes, failures rate, etc., which are essential for our approach.
Whereas, we found that the OpenCloud traces provide the required inputs of our verification approach (\eg{} \# nodes, capacity of nodes, workload). Therefore, we choose to formally analyze the scheduler of this cluster because it provides public traces of real-executed Hadoop workload for more than 20 months.
OpenCloud~\cite{OpenCloud} is an open research cluster managed by Carnegie Mellon University. It is used by several groups in different areas such as machine learning, astrophysics, biology, cloud computing, etc. \\
%\Foutse{what is the type of this scheduler? FIFO? capacity?...}
%Indeed, we use these traces to apply and validate our proposed methodology, comparing \red{the obtained verification results} with the ones of the real execution of the workload in terms of performance and failures rate. Our aim here is to check whether our proposed methodology could allow for an early identification of the failures that occurred in the field (i.e., failures that are reported in execution traces).\\
\indent Based on the modeled system of the OpenCloud's scheduler and the specified properties, we start the verification by parsing the trace files to extract the input information needed in our methodology.
%\Foutse{i though you parsed the trace files to extract the information needed by your framework?}.
Specifically, we use the description of the workload included in the first month traces, and the first six months traces together. This allows us to evaluate the scalability of our methodology, in terms of number of visited states and execution time, using different traces. % and its scalability.
Although these traces provide the required inputs for our methodology, we did not find any information describing the type of scheduler used in the cluster. Since the type of the scheduler is an important factor that impacts the performance of the cluster, we evaluate the performance of the modeled cluster for the three existing schedulers of Hadoop (FIFO, Fair and Capacity).
%\Foutse{how exactly? explain your assumptions...and dont forget to mention this in threats to validity!}.
%%%% ==> I guess that adding a section about threats will not be appreciated by NFM reviewers ;-)%%%
\Mbarka{We vary the property requirements to assess the performance of the used scheduler under different rates and evaluate their impact on the failures rate in the cluster while executing the same Hadoop workload.}
For the search engines in PAT, we use the ``First Witness Trace using Depth First Search" in order to perform an analysis without reduction, and the ``First Witness Trace with Zone Abstraction" for the analysis with symmetry reduction. The testbed is a workstation with Intel i7-6700HQ (2.60GHz*8) CPU and 16 GB of RAM. 
\vspace{-5pt}
\subsection{Verification and Evaluation}
\vspace{-5pt}
In this section, we present the obtained scalability results of our methodology along with the results of the formal quantitative and qualitative analyses of failures in Hadoop schedulers. 
\vspace{-10pt}
%\Foutse{have at least a sentence before diving into sub-sections...}
\subsubsection{Properties Verification and Scalability Analysis}
\label{perfanalysis}
Tables~\ref{Results-File1} and~\ref{Results-File3} summarize the verification results of the first month trace, and the first six  months traces together, respectively. The first trace provides information about 1,772,144 scheduled tasks, whereas the six files describing the executed workload for six months contain information about 4,006,512 scheduled tasks.
In the sequel, we discuss the results of the performed analysis. \\
\indent The results presented in Table~\ref{Results-File1} show that only the Fair scheduler satisfies the schedulability property (for the two given rates), meaning that up to 80\% are scheduled and executed within their expected deadlines. The Capacity scheduler does not meet the schedulability rate of 80\%.
However, the FIFO does not satisfy the schedulability properties for the two input rates, meaning that more than 50\% of scheduled tasks are exceeding their expected deadlines. Hence, these tasks are using their assigned resources more than expected, which can affect the overall performance of the cluster.  \\
\indent For the fairness property, only the Fair scheduler satisfies the property of fairness with a rate of 50\% and 80\%, meaning that at most 80\% of tasks get served and executed on time. However, both the FIFO and the Capacity schedulers violate the fairness property for the two given values. Therefore, we can claim that more than 80\% of the submitted tasks are waiting longer than expected in the queue before getting executed. This may lead to a problem of task starvation.
 %, especially when tasks are waiting in the queue for long time.

\input{table1}
\indent For the resources-deadlock, we can report that the Capacity scheduler is characterized by more tasks that experience a resources-deadlock compared to the FIFO and the Fair schedulers. This is because it satisfies the property of resources-deadlock for the two given rates (\eg{} 10\% and 30\%). This can be explained by the fact that the Capacity scheduler suffers from the problem of miscalculation of resources (headroom calculation).
The FIFO scheduler shows that only 10\% of the scheduled tasks experience the problem of resources-deadlock.
The Fair scheduler does not satisfy the resources-deadlock property for two given rates (10\% and 30\%) for the first trace, which means that less than 10\% of tasks may experience an issue of resources-deadlock.
Hence, we can conclude that the Fair scheduler generates less scheduling decisions leading to resources-deadlock situations for the OpenCloud cluster.\\
\indent Overall, our proposed methodology is able to verify the given three properties for the three existing Hadoop schedulers, while exploring on average 11086 K states in 3724 seconds without symmetry reduction, and 742 K states in 1619 seconds with symmetry reduction, as shown in Table~\ref{Results-File1}.
%\input{table2}

%When applied to the second traces of the OpenCloud cluster, our proposed methodology could formally analyze the three properties for the three schedulers using raw data of 476,034 tasks.
%We observe that the FIFO, Fair and Capacity schedulers verify achieve a schedulability rate of 30\% and a fairness rate of 20\%, whereas they all do not meet a rate of 90\% of schedulability and fairness, as shown in Table~\ref{Results-File2}.
%\red{For the resources-deadlock, we notice that at least 35\% of tasks can experience resource-deadlock when scheduled with the Capacity scheduler.} %is characterized by 35\% of scheduled tasks that experienced resource-deadlock situation,
%%, while it does not meet a resource-deadlock \Foutse{what does this mean???? meeting resource-deadlock??} of 50\%.\\
%In a comparison with the first trace, our approach explores 2856 K states in (up to) 2186 seconds without symmetry reduction, and 581 K states in (up to) 1246 seconds when applying symmetry reduction. This is expected due to the huge difference in the size between the two input traces.\\
%Overall, the formal analysis of the second trace using our framework explored less states

\indent In order to evaluate the scalability of our methodology, we perform the formal analysis of a cumulative workload. This is done by incrementally adding the Hadoop workload of each month to the previous trace(s) to be analyzed. We start the evaluation by analyzing the trace of the first month, and add the traces up to the trace where the tool cannot perform the analysis. This is in order to check the bounds of the analysis performed in terms of explored states.
The obtained results, presented in Table~\ref{Results-File3}, show that our approach could formally analyze the first six traces (combined) describing the workload (about 4,006,512 tasks) for the first six months. It could analyze the first five traces together with and without symmetry reduction, however, it could analyze the six traces together
only with symmetry reduction by exploring on average 17,692 K states in 4346 seconds.
\input{table3}
%\vspace{-5pt}
\subsubsection{Quantitative Failures Analysis}
\label{QFA}
We use the traces generated by the PAT model checker during the verification of the three properties described in Section~\ref{perfanalysis}, to explore states where the scheduler did not satisfy a given property value.
This step is important to map these states to the failed tasks, if found, and examine the relationship between the verified property and the task failure. We apply this step on the first trace file because it contains an important number of scheduled tasks (i.e., 1,772,144 tasks).
To do so, we first identify the tasks that did not satisfy a given property, for example schedulability = 80\% or resources-deadlock
= 10\%. Then, we check whether these tasks were failed when they were executed in the real cluster.
Next, we classify the observations into four main categories: \textit{True Positive (TP)}, \textit{True Negative (TN)}, \textit{False Positive (FP)}, and  \textit{False Negative (FN)}.
\textit{TP} represents the successfully executed tasks, based on the simulation traces, that are identified as finished tasks using our approach.
%\Foutse{you never mentioned prediction before....please use consistent term...in your case you need a term adequate for verification studies!}
Likewise, \textit{TN} represents the failed tasks, based on the simulation traces, that are identified as failed tasks.
While \textit{FP} denotes the amount of identified finished tasks that failed during the real simulation. Finally, \textit{FN} denotes the number of identified failed tasks that are finished in reality.
These four metrics are calculated by comparing the status of the tasks in the generated traces using our methodology, specifically from PAT, and the simulation traces across the total number of scheduled tasks in the input trace (1,772,144 tasks). \\
\indent Overall, we observe that our formal analysis approach could identify up to 61.49\% of the finished tasks (\textit{TP}, Fair, schedulability = 50\%), and up to 4.62\% of the failed tasks (\textit{TN}, Fair, schedulability = 80\%) without symmetry reduction. Here, we notice that the \textit{TN} values are small compared to the \textit{TP} ones. This can be explained by the fact that the first trace contains more than 94\% of successful tasks. For this reason, we computed another metric that we call the \textit{Detected Failures (DF)}.
The \textit{DF} is calculated by mapping the \textit{TN} rate over the total number of failed tasks in the input trace.
\Mbarka{Our aim here is to quantify the amount of detected failures using our formal verification approach when compared to the given OpenCloud traces (obtained from real-execution simulation). This is to answer the question of how many failures could be identified by our formal verification approach before the application is deployed in the field?}\\ %our formal approach early identify failures when compared to the real-execution traces.
\indent At this level, we can claim that our approach is able to catch between 42\% and 78.57\% of the total failed tasks without symmetry reduction. While it can catch between 42\% and 78.91\% of the failures when using the symmetry reduction.
On the other hand, we notice that the \textit{FN} is in the order of 40\%, which means that more than 40\% of tasks are finished, in the simulation traces, but our methodology indicates their failures. This can be explained by the internal mechanisms implemented in Hadoop internally to recover these tasks in case of a failure. For example, the mechanism of pausing and resuming running tasks allows higher priority tasks to be executed without killing the lower priority ones~\cite{RAFT2011}. Furthermore, we notice that the false positive rate varies from 1.26\% and 3.41\% and indicates failed tasks that are identified as finished using our methodology. A possible explanation for these false positive results can be the lack of some real-world constraints that affect the execution of tasks (\eg{} network congestion, lack of resources). In addition, failed tasks identified as finished can generate more false positive results due to the tight dependency between the map and the reduce tasks (the reduce tasks will be launched when all the map tasks are successfully executed). 
%\vspace{-30pt}
%\Foutse{can you give an example of such recovery mechanisms?}\Foutse{say that you will investigate ways to include these recovery mechanisms in your methodology in the future!} \\
\input{table6}
%\vspace{-10pt}
%\vspace{-10pt}

\indent Next, we check the states of the failed tasks and analyze the factors that may cause the failure of these tasks.
We find that the scheduler of OpenCloud experiences several failures, up to 32\%, because of long delays that exceed the maximum timeout for a task to be finished (property ``mapred.task.timeout": defining the maximum timeout for a task to be finished).
%These tasks do not meet the schedulability requirement.
We carefully checked the sates of these tasks and found that these delays are mainly caused by data locality constraints~\cite{SOUALHIA2017} and that they can reach 10 minutes (for small and medium tasks). Moreover, we found out that about 40\% of these straggling tasks cause the failure of the job to which they belong, resulting in a waste of resources.
Moreover, we noticed that many failures are cascaded from one job to another, especially in the long Hadoop chains. A Hadoop chain is a connection between more than one Hadoop job to execute a specific workload. This may result in a degradation in performance and many failures. We can claim that the Hadoop scheduler lacks mechanisms to share information about these failures between its components to avoid their occurrence.
On the other hand, we found out that 26\% of tasks failed because they exceeded the maximum number of allowed speculative execution (\eg{} property ``mapred.map.tasks.speculative.execution": defining the maximum number of allowed speculative execution, they have likely more chances to fail).
When checking the tasks waiting for a long time in the queue before being served, we observed that this is due to the fact that the long tasks are scheduled first and they occupy their assigned resources for a long time (\eg{} a large input file to be processed, a job composed of more than 1000 tasks). Consequently, we can conclude that knowing these factors and issues, one can adapt the scheduler system of the Hadoop framework to overcome these failures and improve its performance.\\

% \Foutse{how concretely? elaborate...and possibly give some examples!}.
\vspace{-15pt}
\subsubsection{Qualitative Failures Analysis}
\Mbarka{To show the benefits of our formal analysis approach, we propose to integrate and simulate some guidelines or strategies to adapt the scheduling decisions of the created Hadoop cluster and evaluate their impact on the failures rate.} This is based on the generated traces and the performed failures analysis to check whether our work can help early identify the occurrence of failures and then propose appropriate mechanisms to overcome them.
For instance, one possible strategy could be to change the cluster settings by adding more resources on its nodes or adding the number of nodes on it. This could solve for example the fairness and resources-deadlock issues. Another scheduling strategy could be to change the value of timeout of scheduled task, which represents the number of milliseconds before terminating a task. Another strategy could be to adjust the type of the scheduler used in the cluster (\eg{} FIFO, Fair, Capacity, etc.).
%\Foutse{this can have downsides since tasks that will fail anyway will consume more resources that they should...so we should discuss these aspects!...}.
\Mbarka{In this paper, we evaluate the impact of the strategy to change the cluster settings by adding more resources on the OpenCloud cluster where we change the number of nodes from 64 to 100 and simulate the same Hadoop workload}. Figure~\ref{Figure:Results} gives an overview of the impact of adding more resources in the cluster for the three schedulers considering a failures' rate of 5.88\%; the identified failure rate from the first trace file.
% \Foutse{what do you mean by "such that the initial failures rate is"?}.
Overall, we noticed that adding more nodes in the cluster could reduce the failure rates by up to 2.34\% (Fair scheduler), which means a reduction rate of 39.79\%.
%\Foutse{this is the reduction rate or the new failure rate after addition of resources?}  
This was expected since when we analyzed the traces we found out that several tasks are straggling for more than 10 minutes, waiting for other resources to be released. \\ 
\Mbarka{\indent Given the obtained findings, we can claim that our solution could identify new scheduling strategies to adjust the Hadoop cluster to reduce failures rates by combining simulation and model checking techniques and formally verifying the functional scheduling properties.}

%\vspace{-30pt}
\begin{figure}[ht]
\centering
\includegraphics[scale=0.8]{Figures/Cluster100.pdf}
\vspace{-10pt}
\caption{Impact of Resources Adding on Failures Rate}
%Trace for the First Month (1,772,144 Tasks)}
\label{Figure:Results}
\end{figure}
%\vspace{-20pt}
