\documentclass[letterpaper, 10 pt, conference]{ieeeconf}
%\documentclass[conference]{IEEEtran}

\IEEEoverridecommandlockouts

\overrideIEEEmargins
\usepackage{}
\usepackage{cite}
\let\proof\relax
\let\endproof\relax
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts,amsmath}
\usepackage{mathtools}
%\mathtoolsset{showonlyrefs}
\usepackage[font=footnotesize]{caption}
\usepackage{subcaption}
%\usepackage{multicol}
%\usepackage{subfigure}
\usepackage{xcolor}
\usepackage{graphics}

\usepackage{graphicx}
\usepackage{tikz}
\usepackage{bm}
\newlength\imagewidth
\newlength\imagescale
\usepackage{booktabs}
\usepackage{color, soul}
\usepackage{url}
\let\labelindent\relax % to avoid conflict of enumitem with IEEE
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{wrapfig}

\usepackage{mdframed}

\usepackage{setspace} 
\setstretch{0.96}




% Patch the bibliography item command to allow color changes
\makeatletter 
\pretocmd\@bibitem{\color{black}\csname keycolor#1\endcsname}{}{\fail}

% Define a command to highlight specific references in the bibliography
\newcommand\citecolor[1]{\@namedef{keycolor#1}{\color{black}}}

\makeatother

% Mark specific citations to appear in blue in the bibliography
\citecolor{cai2021reinforcement}  % Change "CiteKey2" to any reference key you want to highlight
\citecolor{fuggitti-ltlf2dfa}

%\usepackage{lmodern} % for textit??? DON`T USE THIS - CHANGES THE FONT

\usepackage{mathrsfs}
\newtheorem{definition}{Definition}

%\usepackage{lineno}
%\usepackage{ulem} % for underlining and strike through
%\normalem % reset emph to normal
%\usepackage{setspace} % for line spacing - e.g. 1.5, 2
%\usepackage[usenames,x11names,table]{xcolor}
\usepackage{xspace} % for inserting a space in TeX commands if needed
\usepackage[bottom, multiple]{footmisc}
%\renewcommand{\thefootnote}{\dag}

\usepackage{caption}

%\usepackage{cleveref}
\usepackage{accents}
\usepackage{lipsum}
%\renewcommand{\thesubfigure}{\relax} % no subfig counters
%\let\chapter\section % algorithm2e natbib compatibility
%\usepackage{accents}

%% Packages
%\usepackage[margin=1in]{geometry}
%\usepackage[abs]{overpic}
\let\listofalgorithms\relax
\usepackage[linesnumbered,vlined,ruled]{algorithm2e}
\usepackage{multirow} % for cell tables spanning multiple rows
\usepackage{array,tabularx} % for lines in tables

\usepackage{romannum}
\let\listofalgorithms\relax
\usepackage{algorithm}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
%\usepackage[linesnumbered]{algorithm2e}

\usepackage{amssymb}
\usepackage{algpseudocode}

\newtheorem{prop}{Proposition}
\newtheorem{ass}{Assumption}
\newtheorem{problem}{Problem}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem*{excont}{Example \continuation}
\newcommand{\continuation}{??}
\newenvironment{continueexample}[1]
 {\renewcommand{\continuation}{\ref{#1}}\excont[\textit{Cont'd}]}
 {\endexcont}
\newcommand{\xs}{\boldsymbol{\mathrm{x}}}
\newcommand{\ys}{\boldsymbol{\mathrm{y}}}
\newcommand{\argmin}{\arg\min}
%\newcommand{\argmax}{\arg\max}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\Ss}{\boldsymbol{S}}


\usepackage{float}
%\mathtoolsset{showonlyrefs}
\setlength{\textfloatsep}{0pt}% Remove \textfloatsep




% paper title
\title{\LARGE \bf  Motion Planning Under Temporal Logic Specifications In Semantically Unknown Environments}

\author{Azizollah Taheri and Derya Aksaray
\thanks{A. Taheri and D. Aksaray are with the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, 02115.}
\thanks{This work was partially supported by ARL contract W911NF2420026.}
}%<-this % stops a space



%\renewcommand{\arraystretch}{0.95}
\IEEEoverridecommandlockouts
\begin{document}
% \bibliographystyle{IEEEtran}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}
This paper addresses a motion planning problem to achieve spatio-temporal-logical tasks, expressed by syntactically co-safe linear temporal logic specifications {($\text{scLTL}_{\setminus{\emph{next}}}$)}, in uncertain environments. Here, the uncertainty is modeled as some probabilistic knowledge on the semantic labels of the environment. For example, the task is ``first go to region 1, then go to region 2"; however, the exact locations of regions 1 and 2 are not known a priori, instead a probabilistic belief is available. We propose a novel automata-theoretic approach, where a special product automaton is constructed to capture the uncertainty related to semantic labels, and a reward function is designed for each edge of this product automaton. The proposed algorithm utilizes value iteration for online replanning. We show some theoretical results and present some simulations/experiments to demonstrate the efficacy of the proposed approach. 

%investigates motion planning in environments with uncertain semantic information. The tasks are defined by syntactically co-safe linear temporal logic specifications. Our goal is to design control policies that enable a robot to accomplish the specified task in such environments. The robot is assumed to have deterministic motion and precise sensing capabilities; however, it does not have the complete information about the labels in the environment. By using an automata-theoretic approach, we construct a special product automaton that can capture the uncertainty related to labels. Furthermore, we propose an algorithm that utilizes value iteration with online replanning and exploration to adapt strategies based on newly available environmental data. Simulations and real-world experiments validate our approach, demonstrating its effectiveness even in scenarios where the initial information is misleading. We further support our algorithm with some theoretical results.
\end{abstract}

\color{black}

\section{Introduction}
Autonomous navigation is essential for a wide range of applications from domestic robots to search and rescue missions\cite{lavalle2006planning}. Traditional motion planning focuses on generating robot trajectories that navigate from an initial state to a desired goal while avoiding obstacles. However, future's applications demand solutions to more complex tasks beyond simple point-to-point navigation. These tasks, such as sequencing, coverage, response, and persistent surveillance, can be expressed using temporal logic (TL), 
%such as Linear Temporal Logic (LTL) \cite{plaku2012planning} and Computation Tree Logic (CTL) \cite{lahijanian2011temporal}
which provides a structured and compact way to define high-level mission requirements (e.g., \cite{plaku2012planning,lahijanian2011temporal}). The integration of TL in motion planning has led to the development of verifiable control synthesis methods that enable robots to satisfy desired TL constraints (e.g., \cite{fainekos2005hybrid}).

Existing planning algorithms under TL constraints often assume complete knowledge about the environment, which allow for the design of correct-by-construction controllers (e.g.,\cite{kress2007s,vasile2013sampling}). However, in real-world scenarios, robots often operate in partially or fully unknown environments, which requires the ability to adapt and replan as new information becomes available. For example, Fig. \ref{fig:exp} illustrates a scenario where a drone aims to first \color{black} visit \color{black} Region 1 then Region 2 while it doesn't exactly know where these regions are. To address these challenges, recent studies have focused on incorporating semantic uncertainties into the planning process (e.g., \cite{guo2013revising, kantaros2020reactive,haesaert2018temporal,hashimoto2022collaborative,kantaros2022perception, kloetzer2012ltl, 8272366}). This involves considering label uncertainties in the robot's environment, e.g., unknown regions and obstacles. 

In this paper, we introduce an automata-theoretic framework that addresses motion planning problems for a sub-class of Linear Temporal Logic (LTL) specifications in uncertain environments. The proposed framework leverages value iteration algorithm to compute the control policy based on the current probabilistic belief of the labels in the environment. We show that the desired co-safe LTL specification is satisfiable as long as there exists a way to satisfy it based on the initial belief.


%One approach is to model these uncertainties probabilistically, allowing the robot to update its knowledge and adapt its plan as it gathers new information from the environment. This dynamic replanning capability is crucial for ensuring that the robot can continue to meet its mission objectives even when initial assumptions about the environment are incorrect. In this paper, we address the problem of motion planning under a family of temporal logic specifications in uncertain semantic maps.
\begin{figure}[t!]
    \centering
    %\vspace{3mm}
    \begin{tabular}{cc}
        \begin{subfigure}[t]{0.4\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{drone_experiment.JPG}
            \caption{}  
        \end{subfigure} &
        \begin{subfigure}[t]{0.4\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{1_b_2.png}
            \caption{}  
        \end{subfigure}
    \end{tabular}
    \caption{\scriptsize (a) A drone in an environment aims to \color{black} visit \color{black} Region 1 first, and then Region 2, without prior knowledge of their exact locations. (b) The drone's initial belief about the possible locations of regions 1 and 2. \textcolor{black}{ The values represent independent probabilities for the cells, indicating the likelihood of a cell having that label (or no label otherwise).}}
    \label{fig:exp}
\end{figure}

\section{Related Work}
The works that are related to our paper are \cite{guo2013revising, kantaros2020reactive,haesaert2018temporal,hashimoto2022collaborative,kantaros2022perception, kloetzer2012ltl, 8272366}, where they also consider label uncertainty in the environment. However, the studies in\cite{guo2013revising} and\cite{kantaros2020reactive} do not consider probabilities over the semantic map. Instead, the authors in \cite{guo2013revising} construct an initial product graph (incorporating the physical state and task progress), and they revise it (pruning/adding edges) after obtaining new information about the unknown map. Alternatively, the authors in \cite{kantaros2020reactive} consider known regions of interest and unknown but static obstacles, whereas we consider a probability distribution for the labels associated with regions of interest. 

\color{black}
Similar to our paper, the works  \cite{haesaert2018temporal,hashimoto2022collaborative,kantaros2022perception,kloetzer2012ltl,8272366} consider probabilities over the labels in the environment. In \cite{haesaert2018temporal}, a planning problem is formulated as a partially observable Markov decision process (POMDP), where the robot is uncertain about both its current position and the location of target regions. In contrast, we assume that the robot has perfect knowledge of its current position and receives accurate observations at that location.
%This problem is modeled as a partially observable Markov decision process (POMDP), which suffers from the curse of dimensionality and history dependence. This makes planning computationally intensive. In our work, the robot has perfect knowledge of its current position and receives accurate observations at that location. Hence, our problem does not fall within the POMDP framework. 
In \cite{hashimoto2022collaborative}, the authors address planning in an uncertain semantic map using a team consisting of a copter and a rover. The rover is responsible for satisfying an scLTL mission specification, while the copter assists by first exploring the environment and reducing uncertainty. In this setup, the copter can move freely without risking task violation, and the rover's safety relies on the information gathered by the copter. In contrast, we consider a single robot which individually ensures both safety and task satisfaction guarantees. In \cite{kantaros2022perception}, the authors propose a sampling-based approach, where probabilistic labels are transformed from probabilistic to certain using some user-defined thresholds. Here threshold-sensitive label assignments ignore states below the threshold and treat those above it as identical, which causes planning inefficiencies.
%; low thresholds may equate states with low and high label probabilities, while high thresholds may even exclude states with high label probabilities, potentially leading to inefficient planning. 
In contrast, our method considers all label uncertainties collectively during planning. In \cite{kloetzer2012ltl} and \cite{8272366}, the authors present an automata-theoretic method, which solves the planning problem by incorporating all probabilistic labels in the environment. Nonetheless, both \cite{kloetzer2012ltl} and \cite{8272366} consider a model for the probabilistic labels, which appear and disappear in some known states with some unknown frequency. %These models have the underlying assumption that a label is always present in some known states, but when it is active is unknown. %(e.g., a state is sometimes region $A$ and sometimes not). 
As a result, their solutions rely on the fact that the robot stays in those states until the desired label appears. However, this approach can lead to a deadlock, where the robot waits indefinitely under the false assumption that a label exists in a state, even though it does not exist in reality. In contrast, our paper can address such scenarios where the robot may hold incorrect beliefs about the labels of the states. Moreover, our paper proposes an online approach which enables the robot to replan whenever new information is discovered whereas \cite{8272366} presents an offline approach where a linear program, that takes into account all probabilistic labels, is solved in one shot before the mission. Hence, \cite{8272366} may get stuck due to incorrect initial beliefs (as discussed in our benchmark analysis in Sec. VI) whereas our paper facilitates resilient planning in semantically uncertain environments. %approach that can find a solution if the task is satisfiable in the environment, whereas .    


%Therefore, using their methods in cases where labels are static but their locations are unknown will create a deadlock (waiting indefinitely) if a label is not present in those states. In contrast, our paper addresses scenarios where labels are static but their locations are unknown. Moreover, our setup can even capture cases where the system believes that a state has a label as region $A$ whereas it is not in reality. 

%Authors in \cite{kloetzer2012ltl}, solve a shortest path over their product automaton to maximize the probability of task satisfaction while the goal of our paper and \cite{8272366} is to find a policy that minimizes some cost. 

%Finally, the authors in \cite{8272366} construct a representation (i.e., product automaton), which incorporates all label possibilities by augmenting the state-space and they solve a linear programming problem over this representation in one shot. On the other hand, we incorporate all the label possibilities in a different manner (i.e., as the edges of the product automaton), which does not lead to an increase in state-space size and we use value iteration over our product automaton to find our plan. Moreover, their offline method lacks guarantees for mission satisfaction, making it unsuitable for scenarios with incorrect initial beliefs. In contrast, our online approach ensures mission satisfaction by allowing belief updates about labels based on newly discovered information, enabling the robot to replan dynamically until the mission is successfully accomplished.
\color{black}
\vspace{-1mm}

\section{Preliminaries}
\subsection{Notation and Graph Theory}
\color{black}
Let $O$ be a set of Boolean statements. The power set (set of all subsets) of $O$ is denoted by $2^O$. The set of infinite sequences (words) defined over $O$ and $2^O$ are denoted by $O^\omega$ and $(2^O)^\omega$, respectively. The set of positive real numbers is denoted by $\mathbb{R}^{>0}$.

A directed graph is defined as a tuple $\mathcal{G} = (X, \Delta)$, where X represents the nodes and $\Delta \subset X \times X$ is a set of directed edges connecting these nodes. A node $x_j$ is considered an out-neighbor of another node $x_i$ if $(x_i, x_j) \in \Delta$. We use $N_{x_i}$ to represent the set of out-neighbors of $x_i$. In this paper, for brevity, we use the term ``neighbor" instead of ``out-neighbor". $N_{x_i}^h$ denotes the set of all nodes that can be reached from $x_i$ within at most $h$-hops.
%{%
%\renewcommand{\labelitemi}{}
%\begin{itemize}
%    \item $\emptyset,\{\}$: Empty set.
%    \item $2^O $: Power set (set of all subsets) of a set $O$.
%    \item $\bm{l} =   l(0)  l(1)  l(2)\dots, $l(i)\in 2^O $: Word over set $2^O$.
%    \item $(2^O)^\omega$: Set of infinite words $over $2^O$.
%\end{itemize}
%}
\color{black}

%\subsection{Graph theory}

\subsection{Probabilistically Labeled DMDP}
We define a Probabilistically Labeled Deterministic Markov Decision Process (PL-DMDP) as in \cite{8272366} but more compact, to model semantic uncertainty in an environment.
%\vspace{-2mm}
\begin{definition}[PL-DMDP]
\textit{ A probabilistically labeled deterministic MDP (PL-DMDP) is a tuple $\mathcal{M} = (X, \Sigma, \delta, O, L,l_{true}, p_L, c)$, where:
\begin{itemize}
    \item   $X$ is the finite set of states,
    \item   $\Sigma$ is a finite set of actions,
    \item   $\delta: X \times \Sigma \rightarrow X$ is a deterministic transition function,
    \item $O$ is the set of observations,
    \item $L : X \rightarrow 2^{2^O}$ is the labeling function,
    \item $l_{true}: X \rightarrow 2^O$ is the true label function,
    \item $p_L: X \times 2^O \rightarrow [0,1] $ is a mapping such that $p_L(x, l)$ indicates  the probability \textcolor{black}{ of seeing the set of observations $  l \in 2^O$ in state $x\in X$.} Note that for any $x\in X$,  $\sum_{  l\in L(x)}p_L(x,   l) = 1$ and if $  l\notin L(x)$ then $p_L(x,  l) = 0$,
    \textcolor{black}{\item $c: X \times X \rightarrow  \mathbb{R}^{>0}$ is the cost function.}
\end{itemize}
}
\end{definition}

An example of a PL-DMDP, where the knowledge about the label of each state is uncertain, is shown in \textcolor{black}{Fig. \ref{fig:figure2}}. Note that the standard labeled deterministic MDP (e.g.,\cite{9636598,10342259}) $\mathcal{M}_l = (X, \Sigma, \delta, O, L^*, c)$ is a special case of PL-DMDP, where $L^*: X \rightarrow 2^O$ is the labeling function and there is no non-determinism in labeling the states. When the label of a state is revealed, the $p_L$ function may change as new information is discovered, which makes both $p_L$ and the PL-DMDP time-varying. For simplicity, we denote them as $p_L$ and $\mathcal{M}$.

\color{black}
Given a PL-DMDP $\mathcal{M}$,  a finite \textbf{action sequence} is   $\color{black}\bm{\sigma{\scriptstyle[0:n]}} = \sigma(0)\sigma(1)\dots\sigma(n)$, where $\sigma(i)\in \Sigma$ for all $i \in\{0,1,\dots,n\}$;
%Here, $n \in \mathbb{Z}^{\geq0}$ is the length of the action sequence and is not a fixed number. 
\color{black} a finite \textbf{trajectory} generated by  $\bm{\sigma{\scriptstyle[0:n]}}$ is  $\color{black}\bm{x^{\sigma{\tiny[0:n]}}}=x(0)x(1)\dots x(n+1)$, where $x(i) \in X$; and the corresponding \textbf{word} is $\bm{  l({\color{black}\color{black}\bm{x^{\sigma{\tiny[0:n]}}}  })} =   l(0)  l(1) \dots   l(n+1)$, where $  l(i) = l_{true}(x(i))$. We define the cost of the  trajectory $\bm{x^{\sigma{\tiny[0:n]}}}$  as $C(\color{black}\color{black}\bm{x^{\sigma{\tiny[0:n]}}}  ) = \sum_{i=0}^{n}  c(x(i),x(i+1)) $. \textcolor{black}{In this paper, we consider a uniform cost function 
$c$, which can be interpreted as the time required to complete the mission under the assumption that each transition takes equal time.}
\color{black}
%as the finite trajectory generated by $\color{black} \color{black} \color{black} \bm{\sigma{\scriptstyle[0:n]}} \color{black} \color{black} \color{black}$ over $\mathcal{M}$, where $x(i) \in X$. The finite trajectory $\color{black}\color{black}\bm{x^{\sigma{\tiny[0:n]}}} \color{black} \color{black}$ generates a finite word $\bm{  l({\color{black}\color{black}\bm{x^{\sigma{\tiny[0:n]}}} \color{black} \color{black}})} =   l(0)  l(1) \dots   l(n+1)$ over $\mathcal{M}$, where $  l(i) = l_{true}(x(i))$. We define the cost of the  trajectory $\bm{x^{\sigma{\tiny[0:n]}}}$  as $C(\color{black}\color{black}\bm{x^{\sigma{\tiny[0:n]}}} \color{black} \color{black}) = \sum_{i=0}^{n}  c(x(i),x(i+1)) $.


\subsection{Temporal logic}
\begin{wrapfigure}{r}{0.48\columnwidth}
    \centering
     \vspace{-10mm}
\centerline{\includegraphics[width=0.48\columnwidth]{PLDMDP_bigger_1.PNG}}
\caption{\scriptsize An illustration of PL-DMDP $\mathcal{M}_1 = (X, \Sigma, \delta, O, L, l_{true}, p_L, c)$ where $X=\{x_0, x_1, x_2, x_3\}$, $\Sigma=\{Up,Right,Down,Left,Stay\}$, and $O=\{r_1,r_2\}$. Some examples of $\delta$, $L$, and $p_L$ are $\delta(x_0, Right)=x_1$, $L(x_0) = \{{\{r_1\},\{r_2\},\{r_1,r_2\},\{\}}\}$, $p_L(x_1, \{r_1\}) = 0.2$, respectively.}
 \vspace{-5mm}
\label{fig:figure2}
\end{wrapfigure}
Temporal logic (TL) is a formal language used to define the temporal characteristics of a dynamical system. Linear Temporal Logic (LTL) is a type of TL that can handle words of infinite length $\bm{l} =   l(0)  l(1)  l(2)\dots$ where $  l(i)\in 2^O$ for all $i\geq0 $. LTL is widely employed in diverse domains (e.g.,\cite{4459804,fu2016optimal,guo2013revising,buyukkoccak2023state }) and can be used for verification and control synthesis in complex missions. \textcolor{black}{In this work, we focus on a specific fragment of LTL known as scLTL}.
%\vspace{-2mm}
\begin{definition}[$\text{scLTL}_{\setminus{\emph{next}}}$]   
\textit{A syntactically co-safe linear temporal logic ($\text{scLTL}_{\setminus{\emph{next}}}$) formula $\phi$ over a set of observations \textit{O} is recursively defined
as:
\begin{equation*}
\phi = \top\mid o\mid \neg o\mid \phi_1\lor\phi_2\mid\phi_1\land\phi_2\mid\phi_1\mathcal{U}\phi_2\mid\color{black}\lozenge\phi_1\color{black}
\end{equation*}
where $o\in O$ is an observation and $\phi$, $\phi_1$ and $\phi_2$ are $\text{scLTL}_{\setminus{\emph{next}}}$ formulae.
$\top$ (true), $\neg$ (negation), $\lor$ (disjunction) and $\land$ (conjunction) are Boolean operators, and $\mathcal{U}$ (until) and $\lozenge$ (eventually) are temporal operators.}
\end{definition}
The \emph{globally} operator cannot be represented in this language, since $\text{scLTL}_{\setminus{\emph{next}}}$ only allows for the negation of the observations. This means that expressions like $\neg \lozenge \neg \phi$ are not part of the $\text{scLTL}_{\setminus{\emph{next}}}$ fragment. \color{black}
We exclude the \emph{next} operator from the syntax (e.g., \cite{kantaros2020reactive}) because requiring tasks, like finding an object in $n$ steps when its location is unknown, is overly restrictive in uncertain environments. In the remainder of the paper, we will refer to $\text{scLTL}_{\setminus{\emph{next}}}$ as scLTL.
 
 \color{black}
 The semantics of scLTL formulae are interpreted over infinite words in $2^O$. We define \color{black}the \color{black} language of an scLTL formula $\phi$ as the set of infinite words satisfying $\phi$  and denote it as $\mathscr{L}_\phi
$. Even though scLTL formulae are interpreted over infinite words (i.e., over $({2^O})^\omega$), their satisfaction is guaranteed in finite time.
Any infinite word $\bm{  l} =   l(0)  l(1)  l(2)\dots$ that satisfies formula $\phi$ contains a finite “good”
prefix $  l(0))  l(1) \dots   l(n)$ such that any infinite word that contains the prefix, i.e.,
$  l(0)  l(1) \dots   l(n)  \bm{l^\prime} ,   \bm{l^\prime} \in (2^O)^\omega$
, satisfies $\phi$ \textcolor{black}{\cite{belta2017formal}}. We denote the language of
finite good prefixes of $\phi$ by $\mathscr{L}_{pref,\phi}$.
A deterministic finite state automaton (DFA) can be constructed from any scLTL formula\color{black}\cite{fuggitti-ltlf2dfa} \color{black} that compactly represents all the satisfactory words and defined as follows:
%\vspace{-2mm}

\begin{definition}[DFA] 
\textit{A deterministic finite state automaton (DFA) is a tuple $\mathcal{A} = (S, s_0, 2^O, \delta_{\color{black}a\color{black}}, F_a)$, where:
\begin{itemize}
\item $S$ is a finite set of states,
\item $s_0 \in S$ is the initial state,
\item $2^O$ is the alphabet,
\item $\delta_{\color{black}a\color{black}}: S \times 2^O \rightarrow S$  is a transition function,
\item $F_a \subseteq S$  is the set of accepting states.
\end{itemize}
}
\label{DFA}
\end{definition}

 
The semantics of a DFA are defined over finite input words in $2^O$. A run of DFA $\mathcal{A}$ over a word $\bm{  l} =  l(0)  l(1) \dots   l(n)$ is represented by a sequence $\bm{s} = s(0)s(1)\dots s(n+1)$ where $s(i)\in S$, $s(0) = s_0$ and $s(i+1) = \delta_{\color{black}a\color{black}}(s(i),  l(i))$ for all $i \geq 0$. If the corresponding run of the word $\bm{  l}$ ends in an accepting state, i.e., $s(n+1) \in F_a$, then we say the word is accepted. The language accepted by $\mathcal{A}$ is the set of all words accepted by $\mathcal{A}$ and is denoted by $\mathscr{L}_{\color{black}\mathcal{A}\color{black}}$.
An scLTL formula $\phi$ over a set $O$ can always be translated into a DFA $\mathcal{A}_\phi$ with alphabet $2^O$ that accepts all and only good prefixes of $\phi$ (i.e., $\mathscr{L}_{\mathcal{A}_\phi}
= \mathscr{L}_{pref,\phi}$ ) \cite{belta2017formal}. Note that the DFA described above only
encodes accepting words. However, one can construct a total
DFA in order to track the violation cases as well.

%\vspace{-2mm}

\begin{definition}[Total DFA]
\textit{A DFA is called total if for all $s \in S$ and any $  l \in 2^O$, the transition $\delta_{\color{black}a\color{black}}(s,   l )\neq \emptyset$ \cite{lin2015hybrid}.}
\end{definition}

For any given DFA $\mathcal{A}$, % = (S, s_0, 2^O, \delta_{\color{black}a\color{black}}, F_a)$, 
one can always create a language-equivalent \color{black}(also defined over finite input words) \color{black} total DFA by adding a trash state, referred to as $s_t$ and introducing a transition $\delta_{\color{black}a\color{black}}(s, l)= s_t$ if and only if  $\delta_{\color{black}a\color{black}}(s, l)= \emptyset$. %Figure \ref{fig:figure3} shows an example of the DFA and the total DFA corresponding to the scLTL specification $\phi = \neg r_1  \mathcal{U}  r_2$.

% \vspace{-3mm}
% \begin{figure}[h!]
%     \centering
%     \begin{tabular}{cc}  
%         \begin{subfigure}[t]{0.30\columnwidth}  
%             \centering
%             \includegraphics[width=\textwidth]{dfa_fixed.JPG}
%             \caption{}
%         \end{subfigure} &
%         \begin{subfigure}[t]{0.30\columnwidth} 
%             \centering
%             \includegraphics[width=\textwidth]{dfa_with_trash_fixed.JPG}
%             \caption{}
%         \end{subfigure}
%     \end{tabular}
%     \caption{\scriptsize (a) The DFA of the specification $\phi = \neg r_1\mathcal{U}r_2$. (b) The total DFA $\mathcal{A}_1 = (S, s_0, 2^O, \delta_{\color{black}a\color{black}}, F_a, F_t)$ of the same specification, where $S = \{s_0,s_1,s_t\}$, $2^O = \{\{r_1\},\{r_2\},\{r_1,r_2\},\{\}\}$, $\delta_{\color{black}a\color{black}}(s_0,\{\}) = s_0$, $\delta_{\color{black}a\color{black}}(s_0,\{r_2\}) = s_1$, $\delta_{\color{black}a\color{black}}(s_0,\{r_1\}) = s_t$, $\delta_{\color{black}a\color{black}}(s_0,\{r_1,r_2\})=\color{black}s_1\color{black}$, $\delta_{\color{black}a\color{black}}(s_1,\top) = s_1$, $\delta_{\color{black}a\color{black}}(s_t,\top) = s_t$, $F_a=\{s_1\}$ and $F_t=\{s_t\}$. }
%     \label{fig:figure3}
% \end{figure}
%\vspace{-6mm}



% \section{Problem Formulation}\label{sec:problem_formulation}
% We consider a robot whose goal is to achieve an scLTL task in an environment where the semantic labels are static but initially unknown. We model the robot's decision-making as a PL-DMDP with unknown $l_{true}$. In this problem, we assume $c$ to be a uniform cost function. This means that for any $x, x^\prime \in X$, if there exists a $\sigma \in \Sigma$ such that $x^\prime = \delta(x,\sigma)$, then $c(x,x^\prime) = \beta$, where $\beta > 0$.
% Note that the challenge of this problem lies in the fact that the true labeling function is not available to the robot. Hence, finding a sequence of actions that can satisfy the $\phi$ with minimum cost is not possible. Therefore, our goal in this paper is to find an action sequence $\bm{\sigma}$ resulting in the satisfaction of $\phi$ ($\bm{  l({\bm{x^{\sigma}}})}\in \mathscr{L}_{pref,\phi}$) whenever it is feasible while minimizing the expected cost of achieving this. 




\section{Problem Formulation}\label{sec:problem_formulation}
We consider a robot whose goal is to achieve an scLTL task $\phi$ in an environment where the semantic labels are static but initially unknown. \textcolor{black}{Accordingly, we model the robot's decision-making as a PL-DMDP $\mathcal{M} = (X, \Sigma, \delta, O, L,l_{true}, p_L, c)$ with unknown $l_{true}$} and a uniform cost function $c$, which has a value of $\beta > 0$.
%to be a uniform cost function (i.e., for any $x, x^\prime \in X$, if there exists a $\sigma \in \Sigma$ such that $x^\prime = \delta(x,\sigma)$, then $c(x,x^\prime) = \beta$, where $\beta > 0$). 
\color{black} A standard way of formulating such scLTL planning problems (e.g., \cite{kantaros2020reactive}, \cite{kantaros2022perception}) is as follows:\color{black}
    
 % \textit{Given a PL-DMDP $\mathcal{M} = (X, \Sigma, \delta, O, L,l_{true}, p_L,c)$, with unknown $l_{true}$, an initial state $x_0 \in X$ and an scLTL formula $\phi$, find the solution to the following optimization problem:}

 
% \begin{equation}
% C_{mission}^{\pi_m}=\sum_{i=0}^{n}\gamma^i(1-Pr^{\pi_m} [\bm{  l({\bm{x^{\sigma_i}}})}\in \mathscr{L}_{pref,\phi}])c(x(i),x(i+1))\\
% \end{equation}

% \begin{equation}
% C_{mission}^{\pi_m} =\sum_{i=0}^{n}\gamma^i(1-Pr^{\pi_m} [s(i)\in F_a]) \cdot c(x(i),x(i+1)) \\
% \end{equation}


% \begin{equation}
% \min_{\pi_m}\biggl[\sum_{i=0}^{n}\gamma^i(1-Pr^{\pi_m} [s(i)\in F_a]) c(x(i),x(i+1))\biggl] \\
% \end{equation}
 
% \begin{equation}
% C^{\pi_m}(x) =\mathbb{E}^{\pi_m}\left[\sum_{i=0}^{\infty}\gamma^i c(x(i),x(i+1)) \bigg| x(0)=x\right].
% \end{equation}
\vspace{-3mm}
\begin{subequations}
 \begin{align}
    & \hspace{-5mm}\min_{n,\color{black} \color{black} \color{black} \color{black} \bm{\sigma{\scriptstyle[0:n]}} \color{black} \color{black} \color{black} \color{black}}[ C(\color{black}\color{black}\bm{x^{\sigma{\tiny[0:n]}}} \color{black} \color{black})] \\
    &  x(0) = x_0, \sigma(i) \in \Sigma, \\
    & x(i+1) = \delta(x(i),\sigma(i)), \\
    & \bm{  l({\color{black}\color{black}\bm{x^{\sigma{\tiny[0:n]}}} \color{black} \color{black}})}\in \mathscr{L}_{pref,\phi},
\end{align}
\end{subequations}
 \noindent where the constraints (1b) and (1c) hold for all $i \in\{0,1,\dots,n\}$.
 Note that when $l_{true}$ is unknown (as in our case), the constraint (1d) $\bm{  l({\color{black}\color{black}\color{black}\bm{x^{\sigma{\tiny[0:n]}}} \color{black} \color{black} \color{black}})}$ cannot be evaluated. This requires reformulating the problem to handle initially unknown semantic labels. Hence, the next section introduces a higher resolution representation (product automaton), our problem defined over it, and our proposed solution. 



 %our problem is different from the standard TL motion planning problems due to the constraint (1d) where the evaluation of  requires the true labeling function that is not available to the robot in our problem. Hence, in the following section, we discuss how we formulate our problem and propose our solution to achieve this goal.

\section{Solution Approach}\label{sec:solution_approach}

% Our proposed solution to the mentioned problem is an automata-theoretic approach that involves an algorithm with offline and online parts. The proposed algorithm is based on value iteration and exploration to find the desired action sequence. We also prove that this algorithm is complete, regardless of the accuracy of the initial belief about the labels.

We propose an automata-theoretic approach, with offline and online parts, to \color{black} find a sequence of actions $\bm{\sigma{\scriptstyle[0:n]}}$ such that $\bm{  l({\bm{x^{\sigma{\tiny[0:n]}}} })}\in \mathscr{L}_{pref,\phi}$. 
%\color{black} is an automata-theoretic approach that involves an algorithm with offline and online parts. \color{blue} The proposed algorithm employs value iteration to find the desired action sequence with formal guarantees.

\color{black}
%need to write something here
\subsection{Product Automaton}

We construct a special  product automaton that can model the uncertainty associated with the labels.
\vspace{-2mm}
\begin{definition}[Product automaton] \textit{ Given a PL-DMDP $\mathcal{M} = (X, \Sigma, \delta, O, L, l_{true}, p_L, c)$ and a total DFA $\mathcal{A} = (S, s_0, 2^O, \delta_{\color{black}a\color{black}}, F_a, \{s_t\})$, the product automaton is a tuple $\mathcal{P}=\mathcal{M}\times \mathcal{A}=(S_p, S_{p_0},\Sigma, \delta_p, p_p, \mathcal{F}_a, \mathcal{F}_t)$, where:
\begin{itemize}
  \item $S_p = X \times S$ is the set of states,
  \item $S_{p_0} = X \times \{s_0\} \subseteq S_p$ is the set of initial states,
  \item  $\Sigma$ is the finite set of actions,
  \item $\delta_p \subseteq S_p \times S_p$ is a transition relation such that for any $(x, s)$ and $(x^\prime,s^\prime)\in S_p$, we have $((x, s), (x^\prime,s^\prime)) \in \delta_p$ if and only if $\exists \sigma \in \Sigma$ such that $\delta(x,\sigma) = x^\prime$ and $\exists   l \in L(x^\prime) \text{ such that } p_L(x^\prime,  l)\geq 0\text{ and } \delta_{\color{black}a\color{black}}(s,  l) = s^\prime $,
  \item $p_p: S_p \times \Sigma \times S_p \rightarrow [0,1]$ assigns a probability to each edge in the product automaton $\mathcal{P}$ based on the information of $\mathcal{M}$ such that $\forall ((x, s), (x^\prime,s^\prime)) \in \delta_p$ and $\sigma \in \Sigma$, $p_p((x, s),\sigma,  (x^\prime,s^\prime)) = \sum_{  l\in L_{s\rightarrow s^\prime}}p_L(x^\prime,  l)$ if $\delta(x,\sigma) = x^\prime$; otherwise $p_p((x, s),\sigma,  (x^\prime,s^\prime)) = 0$. Here, $L_{s\rightarrow s^\prime} = \{  l\in 2^O \mid \delta_{\color{black}a\color{black}}(s,  l) = s^\prime\}$,
  \item $\mathcal{F}_a = X \times F_a$ is the set of accepting states,
  \item $\mathcal{F}_t = X \times \{s_t\}$ is the set of trash states.
\end{itemize}
}
\label{def:P}
\end{definition}


The product automaton encodes both sets of physical states and the total DFA states of the robot. Reaching an accepting state in the product automaton guarantees the satisfaction of constraint (1d). Our goal will be to find a policy over this product automaton that has a non-zero probability of reaching an accepting state while minimizing the expected cost. To this end, we define the following reward function. 

\subsection{Reward Design And Value Iteration}
For any two product automaton states $s_p = (x,s)$ and $s_p^\prime=(x^\prime,s^\prime) \in S_p$, we define our reward function as follows:




\begin{equation}\label{eq:reward}
    \scriptstyle
    r(s_p,\sigma, s_p^\prime)
    =
    \small
    \begin{Bmatrix}
     \frac{-\beta}{1-\gamma}\color{black}& \text{      if } s_p \notin \mathcal{F}_t, s_p^\prime \in \mathcal{F}_t \text{ and } \delta(x,\sigma) = x^\prime  \\
     0 & \text{      if } s_p \in \{\mathcal{F}_a\cup \mathcal{F}_t\}   \\
     -\beta& \text{otherwise}
    \end{Bmatrix}.
\end{equation}

 The first expression indicates a reward of $ \frac{-\beta}{1-\gamma}$ when transitioning from a non-trash state to a trash state, which assigns a large penalty for violating the specification. Here, $\gamma\in [0,1)$ is a discount factor, % taking
 %\footnote{$\gamma$ will act as the discount factor in equation (\ref{eq:return}).} is 
 %a value within the interval $[0,1)$, 
 \textcolor{black}{and its value determines the importance given to future rewards when computing the policy as in \eqref{eq:return}.} The second expression indicates $0$ reward to the transitions that start from an accepting or a trash state. This implies that we disregard events occurring after the robot enters an accepting or a trash state. In the third expression, a uniform negative reward ($-\beta$) is applied to all other transitions, which penalizes the robot for not completing the mission. This reward shaping strategy encourages the robot to reach the set of accepting states in $\mathcal{F}_a$ while avoiding the trash states in $\mathcal{F}_t$ and minimizing the cost. An example of the defined reward function over a portion of the product automaton is illustrated in \textcolor{black}{Fig. \ref{fig:figure4}}.

 



\begin{wrapfigure}{r}{0.2\textwidth}
    \centering
    \includegraphics[width=0.2\textwidth]{partialpalarge.JPG}
    \caption{\scriptsize A portion of the product automaton 
    %constructed from $\mathcal{M}_1$ (Fig.\ref{fig:figure2}) and $\mathcal{A}_1$ (Fig.\ref{fig:figure3}), 
    showing only the outgoing edges from $(x_0, s_0)$ under action $Right$, along with their associated probabilities and rewards.}
    \label{fig:figure4}
\end{wrapfigure}
A deterministic policy $\pi_p$ is a function that maps each product automaton state
to an action, i.e., $\pi_p: S_p \rightarrow \Sigma$. A trajectory over $\mathcal{P}$ generated by $\pi_p$ is an infinite sequence $ \bm{s_p^{\pi_p}}= s_p(0)s_p(1) \dots$, where $s_p(i)\in S_p$ for all $i\geq 0$. Due to the uncertainty associated with the labels, multiple trajectories can be generated over $\mathcal{P}$ under a policy $\pi_p$. We define the expected return of any state $s_{p}\in S_p$ under policy $\pi_p$ as follows: 


\begin{equation}\label{eq:return}
\scriptsize
U^{\pi_p}(s_p) =\mathbb{E}^{\pi_p}\left[\sum_{i=0}^{\infty}\gamma^i r\big(s_{p}(i),\pi_p\big(s_p(i)\big), s_p(i+1)\big) \bigg| s_p(0)=s_p\right].
\end{equation}

\color{black} 

As the robot collects new information about the environment by revealing true labels, the value of the expected return in (\ref{eq:return}) changes accordingly. Therefore, we define the following problem, which is solved iteratively.

\color{black}
   
\color{black}\begin{problem}


 \textit{Given a product automaton $\mathcal{P}$ as in Def. \ref{def:P}, find a policy $\pi_p^*$ that maximizes the expected return $U^{\pi_p}(s_p)$ for all the states $s_p\in S_p$:}\color{black}

 \begin{equation}\label{eq:policy}
   \pi_p^*(s_p) =  \argmax_{\pi_p\in \Pi_p}[U^{\pi_p}(s_p)], \forall s_p \in S_p,
\end{equation}
\noindent where $\Pi_p$ is the set of all deterministic policies over $S_p$. 

 \vspace{-2mm}
\end{problem}\color{black}
\begin{ass} \textit{If the robot is in a state $x \in X$, it has access to \textcolor{black}{ $l_{true}(x)$ and $l_{true}(x')$ for all $x'\in N_{x}^h$.}  Here $h\geq1$ depends on the range of the robot's sensors.}
\label{sensor}
\end{ass}
% \vspace{-1mm}

Assumption \ref{sensor} is not restrictive, as most modern sensors offer high accuracy in capturing environmental details, which enables the robot to reliably determine the labels of the states. To update the belief based on sensor data, we update $p_L$ for each state $x \in N_{x_{\text{current}}}^h$ as follows:





\begin{equation}\label{eq:map_update}
    \small
    p_L(x,l)
    =
    \begin{Bmatrix}
    1& \text{      if }  l = l_{true}(  x)    \\
     0 & \text{    otherwise } 
    \end{Bmatrix}.
\end{equation}


% \begin{definition}[Update-Map] The $\textbf{Update-Map}$ is a function that takes the $h$-hop neighbors of the current state $x_{\text{current}}$ and $\mathcal{M}$ as inputs. The output of this function is the updated PL-DMDP with the true labels of states within $ N_{x_\text{current}}^h$. In detail, for each state $x \in N_{x_\text{current}}^h$, it updates $p_L(x,l_{true}(x)) = 1$ and $p_L(x,l) = 0$ for all $l \in L(x)\setminus l_{true}(x).$
% \end{definition}
% \begin{definition}[Uncertain states]
% We define the set of uncertain states as $\mathcal{X} \subseteq X$, where there is uncertainty regarding their true label: $\mathcal{X} = \{ x \in X \mid \exists\, l \in 2^O \text{ such that } p_L(x, l) \in (0,1) \}$. That is, $\mathcal{X}$ contains all states $x$ for which there exists at least one label $l$ whose associated probability $p_L(x, l)$ is strictly between 0 and 1.
% \end{definition}

\begin{definition}[Uncertain States]
The set of uncertain states is defined as $\mathcal{X} \subseteq X$, where $\mathcal{X}$ includes all states $x \in X$ for which the true label $l_{\text{true}}(x)$ is not known.
\end{definition}




% \begin{ass}
%      At the beginning of the mission, based on the initial belief $p_L$, we assume that for any $x \in \mathcal{X}$ such that $l_{\text{true}}(x) = l$, the initial belief assigns a non-zero probability, i.e., $p_L(x, l) > 0$.
% \end{ass}}
% }
% \textcolor{black}{
% \begin{ass}
% \textit{In the actual product automaton, where $l_{true}$ is known, every accepting state $s_p' \in \mathcal{F}_a$ is reachable from any state $s_p \in S_p \setminus  \mathcal{F}_t$.}
% \label{belief}
% \end{ass}
% }
% \vspace{-1mm}
% \color{black}Assumption 2 indicates the feasibility of $\phi$ over a PL-DMDP. Moreover, \textcolor{black}{the reachability of any accepting state from any product automaton state $s_p \in S_p \setminus  \mathcal{F}_t$} indicates that any progress towards the satisfaction cannot lead to irreversible situations. For example, consider a task $\phi =
% \big((\neg B \hspace{3pt} \mathcal{U} \hspace{3pt} A) \land \lozenge B \big) \lor \big( \neg A \hspace{3pt} \mathcal{U} \hspace{3pt} C \big)$, which means ``avoid 
% $B$ until visiting $A$ and eventually visit $B$; or avoid $A$ until visiting $C$". If the robot initially believes $A$ and $B$ exist and plans to find $A$ first, discovering $C$ but not $B$ after reaching $A$ renders the task unsatisfiable. Assumption 2 prevents such scenarios. 

%(\neg r_2 \hspace{3pt} \mathcal{U} \hspace{3pt} r_1)) \lor ((\neg r_1 \hspace{3pt} \mathcal{U} \hspace{3pt} r_4) \land (\neg r_4 \hspace{3pt} \mathcal{U} \hspace{3pt} r_3))$



%is necessary to handle specifications where progressing in one direction may prevent satisfaction through alternative paths. For example, the task $\phi'=
%((\neg r_3 \hspace{3pt} \mathcal{U} \hspace{3pt} r_2) \land (\neg r_2 \hspace{3pt} \mathcal{U} \hspace{3pt} r_1)) \lor ((\neg r_1 \hspace{3pt} \mathcal{U} \hspace{3pt} r_4) \land (\neg r_4 \hspace{3pt} \mathcal{U} \hspace{3pt} r_3))
% is an example of such tasks that require verifying the existence of certain labels to avoid irreversible decisions. This assumption indicates the feasibility of $\phi$ over a PL-DMDP. \color{black} In other words, if an observation exists in the observation set $O$ of the PL-DMDP, there must be a state with the associated label. Since all observations are present in the PL-DMDP, all good prefixes in %$\mathscr{L}_{\mathcal{A}_\phi} = 
%$\mathscr{L}_{\text{pref}, \phi}$ can be generated. Accordingly, the existence of a path from any state $s_p \in S_p \setminus (\mathcal{F}_a \cup \mathcal{F}_t)$ to an accepting state is guaranteed.
 % Given $\mathcal{P}$, we compute the following policy $\pi_p^*$ which maximizes (3) for all the states $s_p\in S_p$, i.e.,
\color{black}
\begin{definition}[Non-zero probability satisfying policy]
    \textit{A policy $\pi_p$ is a non-zero probability satisfying policy if, starting from any initial state $s_p(0) \in \mathcal{S}_p \setminus \mathcal{F}_t$, among all possible trajectories that can be generated by $\pi_p$ over $\mathcal{P}$, there exists a trajectory $\bm{s_p^{\pi_p}} = s_p(0)s_p(1) \dots$, in which there is a state $s_p(i) \in \mathcal{F}_a$ for $i \geq 0$.
}\end{definition}


% Our solution to the Problem 1 is the solution to the problem of finding the policy $\pi_p^*$ that minimizes the expected cost of satisfying the task $\phi$, which, as discussed above, is the policy that maximizes the expected return (3) for all the states $s_p\in S_p$.

% Next, we formally define the problem that we are going to solve which is finding a non-zero probability satisfying policy $\pi_p^*$ that minimizes the expected cost of satisfying the task $\phi$, which, as discussed above, is the policy that maximizes the expected return (3) for all the states $s_p\in S_p$.

\color{black}By following a non-zero probability satisfying policy, there is always a chance of satisfying the specification.

\begin{ass}
\label{non-zerp}
    At any time during the mission, there exists a non-zero probability satisfying policy over $\mathcal{P}$.
\end{ass}
Assumption \ref{non-zerp} is a mild assumption indicating the feasibility of $\phi$ over the PL-DMDP. It states that the initial belief must contain sufficient information to ensure the existence of a non-zero probability satisfying policy. According to this assumption, when $l_{\text{true}}$ is known for all states, the existence of a trajectory from any state $s_p \in S_p \setminus (\mathcal{F}_a \cup \mathcal{F}_t)$ to an accepting state is guaranteed.


\color{black}

\vspace{-1mm}


\begin{lemma}
\label{threshold}
\textit{
A policy $\pi_p$ is a non-zero probability satisfying policy if and only if it satisfies the condition $U^{\pi_p}(s_p) > \frac{-\beta}{1 - \gamma}$ for all $s_p \in \mathcal{S}_p \setminus (\mathcal{F}_a \cup \mathcal{F}_t)$.
}
\end{lemma}
\begin{proof}
   For any policy $\pi_p$, all possible trajectories that can be generated over the product automaton $\mathcal{P}$, starting from a state $s_p \in \mathcal{S}_p \setminus (\mathcal{F}_a \cup \mathcal{F}_t)$, can be categorized into three distinct groups:






    \textbf{Group 1:} Trajectories that never reach an accepting state or a trash state. For any such trajectory, the return is:
    \begin{equation}
        U_1 = \sum_{i=0}^{\infty} \gamma^i (-\beta) = \frac{-\beta}{1 - \gamma}.
    \end{equation}

    \textbf{Group 2:} Trajectories that reach a trash state. Suppose a trajectory reaches a trash state after $n$ time steps. Then, the return is:
    \begin{equation}
        U_2 = -\beta - \gamma\beta  - \dots - \gamma^{n-1}\beta - \gamma^n \frac{\beta}{1 - \gamma}
        = \frac{-\beta}{1 - \gamma}.
    \end{equation}

    \textbf{Group 3:} Trajectories that reach an accepting state. Suppose the trajectory reaches an accepting state after $n$ time steps. Then, the return is:
    \begin{equation}
        U_3 = -\beta - \gamma\beta - \dots - \gamma^{n-1}\beta + \gamma^n \cdot 0
        > \frac{-\beta}{1 - \gamma}.
    \end{equation}

    For a policy $\pi_p$ and any state $s_p \in \mathcal{S}_p \setminus (\mathcal{F}_a \cup \mathcal{F}_t)$, the expected return $U^{\pi_p}(s_p)$ is a weighted sum over the three groups:

    \begin{equation}
        \label{weights}
        U^{\pi_p}(s_p) = w_1 U_1 + w_2 U_2 + w_3 U_3,
    \end{equation}
    where $w_1$, $w_2$, and $w_3$ are the probabilities of generating trajectories in each group, and $w_1 + w_2 + w_3 = 1$.

    Now assume that $\pi_p$ is a non-zero probability satisfying policy. Then, by definition, there exists at least one trajectory that reaches an accepting state, which implies $w_3 > 0$. Since $U_3 > \frac{-\beta}{1 - \gamma}$, and $U_1 = U_2 = \frac{-\beta}{1 - \gamma}$, it follows from \eqref{weights} that:
    \[
    U^{\pi_p}(s_p) > \frac{-\beta}{1 - \gamma}.
    \]

    Conversely, suppose that for all $s_p \in \mathcal{S}_p \setminus (\mathcal{F}_a \cup \mathcal{F}_t)$, we have $U^{\pi_p}(s_p) > \frac{-\beta}{1 - \gamma}$. From \eqref{weights}, this inequality can only hold if $w_3 > 0$, since both $U_1$ and $U_2$ are equal to $\frac{-\beta}{1 - \gamma}$. Thus, at least one trajectory generated by $\pi_p$ must reach an accepting state, which implies that $\pi_p$ is a non-zero probability satisfying policy.
\end{proof}


\begin{corollary}
\label{corollary}
Let Assumption~\ref{non-zerp} hold. Then, the policy $\pi_p^*$ computed in~\eqref{eq:policy} satisfies $U^{\pi_p^*}(s_p) > \frac{-\beta}{1 - \gamma}$ for all $s_p \in \mathcal{S}_p \setminus (\mathcal{F}_a \cup \mathcal{F}_t)$. Therefore, $\pi_p^*$ is a non-zero probability satisfying policy.
\end{corollary}

\color{black}

% \begin{lemma}
% \label{threshold}
%     \textit{A policy $\pi_p$ that holds the following property: $U^{\pi_p}(s_p) > -\frac{\beta}{1-\gamma}$ for all $s_p\in S_p\setminus {(\mathcal{F}_a\cup \mathcal{F}_t)}$ is a non-zero probability satisfying policy.}
% \end{lemma}
% \begin{proof}
%  In order to prove Lemma \eqref{threshold}, we utilize a proof by contradiction.
%   Let us assume that policy $\pi_p$ holds the property $U^{\pi_p}(s_p) > -\frac{\beta}{1-\gamma}$ for all $s_p\in S_p\setminus {(\mathcal{F}_a\cup \mathcal{F}_t)}$ but is not a non-zero probability satisfying policy. Then, none of the trajectories under $\pi_p$ will contain an accepting state from $F_a$. According to the definition of the reward function in (\ref{eq:reward}), the highest expected return for any state $s_p\in S_p\setminus {(\mathcal{F}_a\cup \mathcal{F}_t)}$ under policy $\pi_p$ is always less than or equal to always receiving a reward of $-\beta$. Thus, for any state $s_p\in S_p\setminus {(\mathcal{F}_a\cup \mathcal{F}_t)}$, we have:
% \begin{equation}      \small
%   U^{\pi_p}(s_p) \leq  \sum_{i=0}^{\infty} \gamma^i (-\beta)=-\frac{\beta}{1-\gamma}.
% \end{equation}
% This contradicts the fact that $U^{\pi_p}(s_p) > -\frac{\beta}{1-\gamma}$, which means that $\pi_p$ is a non-zero probability satisfying policy.\end{proof}
% Note that, in the case of a fully known environment in PL-DMDP $\mathcal{M}$, the true label function $l_{true}$ is no longer unknown. As a result, all the values of the $p_L$ and $p_p$ functions are either $0$ or $1$. In this case, a non-zero probability satisfying policy is a satisfying policy, meaning that it will lead the robot to an accepting state in $\mathcal{F}_a$.

 \vspace{-1mm}

\begin{lemma}
\label{known}
\textit{Let $\mathcal{P}=(S_p, S_{p_0}, \Sigma, \delta_p, p_p, \mathcal{F}_a, \mathcal{F}_t)$ be the product of PL-DMDP $\mathcal{M} = (X, \Sigma, \delta, O, L, l_{true}, p_L, c)$ with known $l_{true}$ \textcolor{black}{ (i.e., set of uncertain states $\mathcal{X}=\emptyset$)} and a total DFA $\mathcal{A}$ of the desired scLTL $\phi$. If Assumption \ref{non-zerp} holds, then the policy in  (\ref{eq:policy}) which maximizes the return in (\ref{eq:return}) will lead the robot to an accepting state in $\mathcal{F}_a$.}
\end{lemma}

\begin{proof}
Let us assume that the policy $\pi_p^*$ in~\eqref{eq:policy}, that maximizes the return in~\eqref{eq:return}, does not lead the robot to an accepting state. Since $l_{\text{true}}$ is known, the transitions in $\mathcal{P}$ are deterministic, and therefore $\pi_p^*$ generates a unique trajectory that does not reach any accepting state in $\mathcal{F}_a$.
This contradicts Corollary~\ref{corollary}, which states that if Assumption \ref{non-zerp} holds, $\pi_p^*$ is a non-zero probability satisfying policy, which implies the existence of at least one trajectory that reaches an accepting state. Therefore, the assumption is contradicted, and $\pi_p^*$ must lead the robot to an accepting state in $\mathcal{F}_a$.
\end{proof}

\color{black}
The policy $\pi_p^*$ can be computed by the value iteration algorithm \cite{bellman1957markovian} using the reward function in (\ref{eq:reward}). This algorithm has a computational time complexity of $\mathcal{O}(|S_p|^2 \times |\Sigma|)$. Note that lines 4-9 of Alg. 1 represent the standard iterative Bellman equation over the product automaton states. In line 8, $r$ and $p_p$ are the reward function and the probability of the edges of the product automaton, respectively. Finally, lines 10-12 find the optimal policy $\pi_p^*$ as in (\ref{eq:policy}). 


\color{black}
\vspace{-1mm}
\floatname{algorithm}{Alg.}
\begin{mdframed}
\vspace*{-5mm}
\begin{algorithm}[H] 
\scriptsize
\caption{ Value iteration over product automaton}\label{Algo}
\setcounter{AlgoLine}{0}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{product automaton $\mathcal{P}$, reward function $r$, discount factor $\gamma$, convergence threshold $\epsilon_0$}
\Output{Optimal policy $\pi_p^*$}

\For{\text{All} $s_p \in S_p$}{
      $v(s_p) = 0$\;
    }
    
    $\epsilon = \infty$
    
    \While{$\epsilon > \epsilon_0$}{

       $\epsilon = 0$
       
      \For{\text{each} $s_p \in S_p$}{

        \hspace{-2mm}$\text{old-value} = v(s_p)$
        
        \scriptsize$\hspace{-2mm}v(s_p) = \hspace{-.5mm}\max\limits_{\substack{\sigma}}\hspace{-2mm} \sum\limits_{\substack{{s_p^\prime \in S_p}}}\hspace{-1mm}p_{p}(s_p,\sigma,s_p^\prime)(r(s_p,\sigma, s_p^\prime)\hspace{-0.5mm}+\hspace{-1mm}\gamma v(s_p^\prime))$\;
        
        \hspace{-2mm}$\epsilon = \max(\epsilon, |v(s_p) - \text{old-value}|)$
      }
      
    }\For{\text{each} $s_p \in S_p$}{
        $\hspace{-2mm}v^*(s_p) = v(s_p)$\;
        
        \scriptsize$\hspace{-2mm}\pi_p^*(s_p)\hspace{-0.25mm} = \argmax\limits_{\substack{\sigma}}\hspace{-2mm}\sum\limits_{\substack{{s_p^\prime \in S_p}}}\hspace{-1mm}p_{p}(s_p,\sigma,s_p^\prime)(r(s_p,\sigma,s_p^\prime)\hspace{-0.5mm}+\hspace{-0.5mm}\gamma v^*(s_p^\prime))$\;
      }

\end{algorithm}
\vspace{-5mm}
\end{mdframed}



\vspace{-1mm}
To integrate new information gathered by the robot during the mission, Alg. 1 should be executed iteratively, for example, after each action. However, updating the policy at every step can be computationally expensive and often unnecessary. To determine when an update is necessary, we first need to define the information matrix \textcolor{black}{$I(N_{x_i}^h, \mathcal{M}) \in \mathbb{R}^{m \times |2^O|}$, where $\mathcal{M}$ is the PL-DMDP, $N_{x_i}^h \subseteq X$ is the set of $h$-hop neighbor states of the current state $x_i \in X$, $m$ \textcolor{black}{is} the cardinality of $N_{x_i}^h$, and each element is defined as follows:}  

%$$ This matrix has dimensions $m \times |2^O|$, where $\mathcal{M}$ is the PL-DMDP, \textcolor{black}{$N_{x_i}^h \subseteq X$} is the set of $h$-hop neighbor states of the current state $x_i \in X$. The information matrix is defined as follows:
\begin{equation}
\scriptsize
I(N_{x_i}^h, \mathcal{M}) = \begin{pmatrix}
I_{11} & I_{12} & \cdots & I_{1|2^O|} \\
I_{21} & I_{22} & \cdots & I_{2|2^O|} \\
\vdots & \vdots & \ddots & \vdots \\
I_{m1} & I_{m2} & \cdots & I_{m|2^O|}
\end{pmatrix},
\end{equation}
\noindent where $I_{jk}$ represents the difference between the true probability of $ l_k$ in state $x_j$ and the prior belief as per $\mathcal{M}$, i.e.,
\begin{equation}\label{eq:information_matrix}
    \small
    I_{jk}
    =
    \begin{Bmatrix}
    |1- p_L(  x_j,  l_k)|& \text{      if }  l_{true}(  x_j) =   l_k   \\
     |0 - p_L(  x_j,  l_k)|& \text{    otherwise } 
    \end{Bmatrix}.
\end{equation}


Note that $p_L$ here refers to its value at the step prior to entering the current state $x_i$.
\color{black}
%\textcolor{black}{can you please write a small example here, which illustrates I matrix? for example, suppose that $x_i$ has 2 neighbors, and  \color{black} Let us \color{black} say $2^O$ has a dimension of 2. Then, I will be a 3x2 matrix. And then illustrate equation (7).}
\vspace{-1mm}
\begin{definition}[Update-trigger function]
    \textit{ The update-trigger function calculates the infinity norm of the information matrix $I(N_{x_i}^h,\mathcal{M})$, which is $\|I(N_{x_i}^h,\mathcal{M})\|_{\infty}$.} %expressed as:
%\begin{equation}\small
%\|I(N_{x_i}^h,\mathcal{M})\|_{\infty}.
%\end{equation}}
\label{update_trigger}
\end{definition}

According to \eqref{eq:information_matrix}, a non-zero value of $\|I(N_{x_i}^h,\mathcal{M})\|_{\infty}$ indicates a difference between the robot's previous belief and the true environment, which necessitates an update.


\subsection{\color{black} Main Algorithm\color{black}}  
The main algorithm (Alg. 2) begins by constructing the total DFA $\mathcal{A}$ and the product automaton  $\mathcal{P}$ (lines 1 and 2). Lines 3 to 5 set the initial values of the states. In line 4, $\mathcal{X}$ is the set of uncertain states (i.e., their $l_{true}$ have not been revealed). %Elements of $\mathcal{X}$ are states whose . 
The main loop starts at line 6. Line 7 checks if an update is necessary and ensures the execution of value iteration to find an initial policy at $t=0$. Line 8 updates edge probabilities in $\mathcal{P}$ to incorporate any new information about $l_{true}$ into the policy. Note that the map is updated at each step of the mission (\textcolor{black}{line 10}). The $\textbf{Update-Map}$ is a function that takes the $h$-hop neighbors of the current state $x_{\text{current}}$ and $\mathcal{M}$ as inputs. The output of this function is the PL-DMDP with an updated $p_L$ function, in which the true labels of states within $N_{x_{\text{current}}}^h$ are incorporated as in~\eqref{eq:map_update}. \color{black} \textcolor{black}{Lines 11 to 17} involve executing $\pi_p^*$, updating the current states and the set of uncertain states. Overall, the main algorithm returns a sequence of actions that satisfies the given specification in the PL-DMDP\textcolor{black}{, as shown in Fig. \ref{fig:figure5}.}
% Overall, the main algorithm returns a sequence of actions that can satisfy the given specification in a PL-DMDP is shown in Fig. \ref{fig:figure5}.
\vspace{-1mm}

%Line 22 executes Alg. 2, where the aim is to gather useful information to satisfy the specification. If all states have been visited and the specification cannot be satisfied, line 21 returns ``Task Cannot Be Satisfied". The last line returns the sequence of actions taken by the robot to satisfy the specification.} The proposed framework of how to generate 
\begin{mdframed}
\vspace*{-5mm}
\begin{algorithm}[H]
\scriptsize
\caption{Main Algorithm \color{black}
(Online)\color{black}}\label{Algo}
\setcounter{AlgoLine}{0}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{$\mathcal{M} = (X, \Sigma, \delta, O, L,l_{true}, p_L, c)$, $\phi$, Initial\_state ($x_0$)}
\Output{Sequence of actions $\color{black} \bm{\sigma{\scriptstyle[0:n]}} \color{black}$}
\text{From $\phi$ construct $\mathcal{A} = (S, s_0, 2^O, \delta_{\color{black}a\color{black}}, F_a, \{s_t\})$}\;

\text{From $\mathcal{M}$ and $\mathcal{A}$ construct $\mathcal{P}=(S_p, S_{p_0}, \Sigma, \delta_p, p_p, \mathcal{F}_a, \mathcal{F}_t)$ }\;


$\color{black}\bm{\sigma{\scriptstyle[0:n]}}\color{black} = [\ ] $, $x_\text{current} = x_0$, $s_\text{current}= s_0, {s_{p}}_\text{current} =(x_0,s_0)$ \;

$\mathcal{X} = X \setminus N_{x_\text{current}}^h$

$t = 0$




\While{${s_p}_\text{current} \notin \mathcal{F}_a$}{
  
  \If{$\|I(N_{x_\text{current}}^h,\mathcal{M})\|_{\infty} > 0$ \text{or} $t \hspace{-1mm}=\hspace{-0.5mm} 0$}{
  
  
  
  \text{Update the product automaton based on the most recent $p_L$ }

  \text{Calculate} $\pi_p^*$ \text{ from Alg. 1} \color{black} (Problem 1) \color{black}
  \color{black}
  
   

  
    }


 

    $\mathcal{M}= \textbf{Update-map}(N_{x_\text{current}}^h,\mathcal{M})$
    
   
    
      \text{Execute $\pi_p^*({s_p}_\text{current})$}

          $ \bm{\sigma} [t] = \pi_p^*({s_p}_\text{current})$


      \color{black}$t = t+1$\color{black}
      
      

      $x_\text{current} = \delta(x_\text{current},\pi_p^*({s_p}_\text{current}))$\;
      
      $s_\text{current}= \delta_{\color{black}a\color{black}}(s_\text{current}, l_{true}(x_\text{current}))$
      
      ${s_p}_\text{current} = (x_\text{current}, s_\text{current})$

      
      $\mathcal{X} = \mathcal{X}\setminus N_{x_\text{current}}^h$

      
    

    
     }
    
    \text{return} $\color{black} \bm{\sigma{\scriptstyle[0:n]}} \color{black}$


\end{algorithm}
\vspace{-5mm}
\end{mdframed}



% \begin{ass}
%  \textit{Given a PL-DMDP and an scLTL specification, there exists a policy $\pi_p$ that does not lead the robot to a trash state.}
% \end{ass}

% Assumption 2 is not restrictive since the existence of a policy that does not lead the robot to a trash state is often true for robotics application. For example, the robot can always choose the action ``stay" and prevent itself to go to a state from which the task can never be satisfiable. 
\begin{lemma}
\label{safety}
\textit{Let Assumptions 1 and 2 hold, and let $x_{current} \in X$ denote the current state. Then, the optimal policy $\pi_p^*$ in  (\ref{eq:policy}) never leads the robot to enter a trash state.}
\end{lemma}
\begin{proof} 

Assumption \ref{sensor} implies that the robot is aware of the true labels $l_{true}$ of its neighbors within $N_{x_\text{current}}^h$ for some $h \geq 1$. 
Consider a non-accepting and non-trash state $s_p\in S_p\setminus {(\mathcal{F}_a\cup \mathcal{F}_t)}$ where there exists an action $\sigma \in \Sigma$ that results in a transition from $s_p$ to a trash state $s_p' \in \mathcal{F}_t$, i.e., $p_p(s_p,\sigma,s_p') = 1$ which is available to the robot due to Assumption \ref{sensor}. Assume that the optimal policy $\pi_p^*$ \color{black} selects action $\sigma$ in state $s_p$, i.e., $\pi_p^*(s_p) = \sigma$, which results in a transition to $s_p' \in \mathcal{F}_t$. According to the reward definition in~\eqref{eq:reward}, the expected return from $s_p$ under $\pi_p^*$ is:
$
U^{\pi_p^*}(s_p) =
\frac{-\beta}{1 - \gamma}.
$
However, Corollary~\ref{corollary} guarantees that for any $s_p \in \mathcal{S}_p \setminus (\mathcal{F}_a \cup \mathcal{F}_t)$, it holds that $U^{\pi_p^*}(s_p) > \frac{-\beta}{1 - \gamma}$. This contradicts the outcome $U^{\pi_p^*}(s_p) = \frac{-\beta}{1 - \gamma}$.
Therefore, $\pi_p^*$ cannot lead the robot into a trash state.
\end{proof}

\color{black}
%In the following, we demonstrate that Algorithm 3 is complete.
% \color{black}
% \vspace{-4mm}
% \begin{figure}[h]
%     %\vspace{3mm}
%   \centering
%   \includegraphics[width=0.48\textwidth,height=0.25\textwidth]{outline_l.JPG}
%   \caption{\scriptsize Outline of the proposed framework.}
%   \label{fig:figure5}
% \end{figure}

%\vspace{-4mm}
\begin{figure}[!t]
    %\vspace{3mm}
  \centering
  \includegraphics[width=0.48\textwidth,height=0.25\textwidth]{onlyvalueoutline.PNG}
  \caption{\scriptsize Outline of the proposed framework.}
  \label{fig:figure5}
\end{figure}

\begin{lemma}
\label{lemfortheorem}
\textit{Let Assumptions 1 and 2 hold. Suppose that there exist some uncertain states in the environment, i.e., $\mathcal{X} \neq \emptyset$. Following the policy $\pi_p^*$ results in at least one of the following:}
    \begin{itemize}
        \item the size of the set of uncertain states $\mathcal{X}$ decreases;
        \item the robot reaches an accepting state in the set $\mathcal{F}_a$.
    \end{itemize} 
\end{lemma}
\begin{proof}
Since Assumptions \ref{sensor} and \ref{non-zerp} hold, Corollary~\ref{corollary} implies that $\pi_p^*$ is a non-zero probability satisfying policy. This means that among all the possible trajectories that $\pi_p^*$ can generate, there exists at least one trajectory that reaches an accepting state. \color{black}Additionally, Lemma~\ref{safety} indicates that the robot cannot end up in a trash state by following $\pi_p^*$. Now, let $ \bm{s_p^{\pi_p^*}}= s_p(0)s_p(1) \dots$ be the product automaton trajectory generated under $\pi_p^*$ where each $s_p(i)\in S_p$  for all $i\geq 0$. If for any state $s_p(i) = (x_i,s_i)$ we have $x_i\notin \mathcal{X}$, then $l_{true}(x_i)$ is known for all $i\geq 0$. This implies that $\pi_p^*$ can generate only one possible trajectory $\bm{s_p^{\pi_p^*}}$ over the product automaton. Thus, there must exist a state $s_p(j)$ in $\bm{s_p^{\pi_p^*}}$, where $j\geq 0$, such that $s_p(j)\in \mathcal{F}_a$. On the other hand, if there exists a state $s_p(i) = (x_i,s_i)$ in $\bm{s_p^{\pi_p^*}}$ where $x_i\in \mathcal{X}$, then the robot eventually reveals $l_{true}(x_i)$ by following $\pi_p^*$. This means the size of $\mathcal{X}$ decreases. 
%Therefore, by following $\pi_p$, if the robot does not enter an accepting state then size of $\mathcal{X}$ is decreasing.% 
As a result, at least one of the outcomes described in Lemma \ref{lemfortheorem} is attained.\end{proof} 



%; otherwise, it can be removed from the set of observations.
\vspace{-2mm}
\color{black}
\begin{theorem}
\textit{If Assumptions 1 and 2 hold, then Alg. \ref{Algo} will find a sequence of actions that satisfies the scLTL task $\phi$.}\end{theorem}

\begin{proof}
 In Alg. \ref{Algo}, the robot follows the policy $\pi_p^*$. Since Assumptions~\ref{sensor} and~\ref{non-zerp} hold, Lemma~\ref{threshold} indicates that $\pi_p^*$ is a non-zero probability satisfying policy. \color{black}
 Here, there are two possibilities to consider: $\mathcal{X}= \emptyset$ or $\mathcal{X}\neq \emptyset$. 
 \vspace{1mm}
 \paragraph{$\mathcal{X} = \emptyset$} $l_{true}(x)$ for all $x\in X$ is known, which results in no discrepancies between the robot's belief and the true environment. So $\|I(N_{x_\text{current}}^h,\mathcal{M})\|_{\infty}$ is always zero, i.e, line 7 is not triggered and $\pi_p^*$ does not change. Consequently, $\pi_p^*$ will lead the robot to an accepting state (Lemma \ref{known}).
 \vspace{1mm}
 \paragraph{$\mathcal{X} \neq \emptyset$}
 If $|I(N_{x_\text{current}}^h, \mathcal{M})|_{\infty}$ is always zero during the execution of $\pi_p^*$, then the policy $\pi_p^*$ remains unchanged. Since Assumptions 1 and 2 hold, Lemma \ref{lemfortheorem} indicates that following $\pi_p^*$ results in either a decrease in the size of $\mathcal{X}$ or reaching an accepting state. However, if  $\|I(N_{x_\text{current}}^h,\mathcal{M})\|_{\infty} \neq 0$, Def. \ref{update_trigger} indicates that there is a difference between the robot's belief and the true environment. This suggests that the robot has revealed the true labels of some states $x \in N_{x_{\text{current}}}^h$ for which $l_{\text{true}}(x)$ was previously unknown. These states are then removed from $\mathcal{X}$ \textcolor{black}{(line 17)}. 
 
Overall, the robot either enters an accepting state or the size of $\mathcal{X}$ decreases.
Since the PL-DMDP $\mathcal{M}$ consists of a finite number of states, the set of uncertain states will eventually become empty (i.e., $\mathcal{X} = \emptyset$). Accordingly, when $\mathcal{X} = \emptyset$,  Lemma \ref{known} indicates that Alg. \ref{Algo} which computes the policy according to \eqref{eq:policy} leads the robot to an accepting state in $\mathcal{F}_a$. According to Def. \ref{DFA}, reaching a state in $\mathcal{F}_a$ implies the generation of a “good prefix". Therefore, if $\color{black} \color{black} \color{black} \color{black} \bm{\sigma{\scriptstyle[0:n]}} \color{black} \color{black} \color{black} \color{black}$ is the output of Alg. \ref{Algo} and $\color{black}\color{black}\color{black}\bm{x^{\sigma{\tiny[0:n]}}} \color{black} \color{black} \color{black}$ is the trajectory produced by it, then we have $\bm{  l({\color{black}\color{black}\color{black}\bm{x^{\sigma{\tiny[0:n]}}} \color{black} \color{black} \color{black}})}\in \mathscr{L}_{pref,\phi}$. This means the task is satisfied.
%\textcolor{black}{When $\mathcal{X}=\emptyset$, $l_{true}$ is no longer unknown and the transitions are deterministic. As a result, each policy $\pi_p$ generates a single, unique trajectory over $\mathcal{P}$. Assumption 2 indicates that from any state $s_p\in S_p\setminus {(\mathcal{F}_a\cup \mathcal{F}_t)}$ there exists a way to reach to an accepting state. Thus, there exists a non-zero probability satisfying policy $\pi_p^*$ where according to \eqref{cor1} $U^{\pi_p^*}(s_p) > -\frac{\beta}{1-\gamma}$. This corresponds to \textit{case 1(a)} in Theorem 1, where the robot eventually reaches an accepting state. We demonstrated that under the Assumptions 1, 2 and 3, by following Algorithm 3, the robot reaches an accepting state.} According to Def. 3, reaching a state in the set of accepting states implies the generation of a “good prefix.” Therefore, if $\bm{\sigma}$ is the output of Alg. 3 and $\bm{x^\sigma}$ is the trajectory produced by it, then we have $\bm{  l({\bm{x^\sigma}})}\in \mathscr{L}_{pref,\phi}$.
 \end{proof}
\color{black}
%\begin{remark}
%    If $\bm{\sigma}$ is the sequence of actions generated by Alg. 3 and $\bm{x^\sigma}$ is its corresponding trajectory, then $\bm{  l({\bm{x^\sigma}})}\in \mathscr{L}_{pref,\phi}$. 
%\end{remark}
\color{black}

\vspace{-3mm}
\section{Case Studies}\label{sec:case_studies}

\noindent \textbf{Simulation:} \textcolor{black}{ In the following case study, a robot operates over a $5 \times 5$ grid (Fig. \ref{fig:case1}, with each cell being $2\times 2$ meters)} using an action set $\Sigma = \{Up, Right, Down, Left, Stay\}$. These actions allow the robot to move to the corresponding feasible adjacent state in the four cardinal directions or remain in its current position. %Note that in all these scenarios, we assume deterministic transitions, meaning the robot follows the selected action with a probability of 1. 
In our simulations, we also consider that the robot's sensor range $h=1$ and the discount factor $\gamma = 0.99$. \color{black} The computations are carried out on a laptop with an Intel Core i7 processor (2.3 GHz) and 16 GB of RAM.\color{black}


% \vspace{1mm}
% \subsubsection{Pick up and delivery}
% In this scenario, the robot performs a pick-up and delivery task, which is expressed by the scLTL formula $\phi_1 = \lozenge(Pick up \land \lozenge (Delivery))$, which means ``eventually reach a $Pick up$ state and subsequently enter a $Delivery$ state''. The robot is not aware of the exact locations for pick-up and delivery but has access to some information regarding potential locations, as shown in Fig. \ref{fig:case1}. As observed, the robot moves through the environment, following its policy while updating its map knowledge. Ultimately, the robot successfully finds the real pick-up and delivery locations and completes the mission. This scenario is illustrated in Fig. \ref{fig:case1}, where the cells marked in yellow show the potential $Pick up$ locations, and those in blue indicate the possible $Delivery$ locations.

% \vspace{1mm}

% \subsubsection{Reach and avoid}

% The robot is tasked with reaching the $Goal$ state while avoiding any $Danger$ states. This mission is captured by the scLTL formula $\phi_2 = \neg Danger\hspace{3pt} \mathcal{U} \hspace{3pt} Goal$, which means ``avoid the $Danger$ states until reaching a $Goal$ state''. Similar to the previous scenario, the robot starts with some initial information. Figure \ref{fig:case2} illustrates that the robot successfully locates the $Goal$ state and enters it while avoiding the $Danger$ states. In this visualization, the yellow cells represent the possible $Goal$ locations, and the red cells indicate the potential $Danger$ locations.


% \begin{figure}[htb!]
%     \centering
%     \begin{tabular}{ccc} 
%         \begin{subfigure}[t]{0.255\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{case 11.png}
%             \caption{}
%         \end{subfigure} &
%         \begin{subfigure}[t]{0.25\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{case 12.png}
%             \caption{}
%         \end{subfigure} &
%         \begin{subfigure}[t]{0.25\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{case 13.png}
%             \caption{}
%         \end{subfigure} \\
%         \begin{subfigure}[t]{0.25\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{case 14.png}
%             \caption{}
%         \end{subfigure} &
%         \begin{subfigure}[t]{0.25\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{case 15.png}
%             \caption{}
%         \end{subfigure} &
%         \begin{subfigure}[t]{0.25\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{case 16.png}
%             \caption{}
%         \end{subfigure}
%     \end{tabular}
%     \caption{\scriptsize The illustrations of (a) the planned trajectory in \emph{red} based on the initial belief, (b,c,d,e) the re-planned trajectories in \emph{red} based on the current belief and the realized trajectory in \emph{black}, (f) the final trajectory accomplishing the mission.}%    Various snapshots of the robot within the environment, each illustrating a moment when the policy gets updated based on newly acquired information. These snapshots are sequenced from the initial stage (a) to the final stage (f), where the mission is accomplished. States marked with probabilities indicate the likelihood of those labels being accurate in each state. Solid yellow and blue colors denote the actual $Pickup$ and $Delivery$ locations, respectively. Red arrows show the potential trajectory if the robot follows the current policy, while black arrows depict the trajectory that the robot actually traversed.}
%     \label{fig:case1}
% \end{figure}


% \begin{figure}[htb!]
%     \centering
%     \begin{tabular}{cccc}  % 
%         \begin{subfigure}[t]{0.20\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{case2 1.png}
%             \caption{}
%         \end{subfigure} &
%         \begin{subfigure}[t]{0.20\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{case2 2.png}
%             \caption{}
%         \end{subfigure} &
%         \begin{subfigure}[t]{0.20\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{case2 3.png}
%             \caption{}
%         \end{subfigure} &
%         \begin{subfigure}[t]{0.20\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{case2 4.png}
%             \caption{}
%         \end{subfigure} \\
%         \begin{subfigure}[t]{0.20\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{case2 5.png}
%             \caption{}
%         \end{subfigure} &
%         \begin{subfigure}[t]{0.20\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{case2 6.png}
%             \caption{}
%         \end{subfigure} &
%         \begin{subfigure}[t]{0.20\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{case2 7.png}
%             \caption{}
%         \end{subfigure} &
%         \begin{subfigure}[t]{0.20\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{case2 8.png}
%             \caption{}
%         \end{subfigure}
%     \end{tabular}
%     \caption{\scriptsize A series of snapshots of the robot from the start of the mission in (a) to the end of it, where the robot reaches the $Goal$ state in (h). Solid yellow and red colors denote the actual $Goal$ and $Danger$ locations, respectively}
%     \label{fig:case2}
% \end{figure}


% \vspace{0mm}
 \color{black}
 \subsubsection{Ordering Reachability with Avoidance}

  In this scenario, the robot must reach the desired regions $A$, $B$, and $C$ while avoiding region $D$ throughout the mission. Visiting $A$ can happen at any time, but $B$ must be visited before $C$. This task is expressed by the scLTL formula $\phi_1 = (\neg C\hspace{3pt} \mathcal{U}\hspace{3pt} B)\land (\lozenge C) \land (\lozenge A) \land (\neg D \hspace{3pt}\mathcal{U} \hspace{3pt}A) \land (\neg D \hspace{3pt}\mathcal{U}\hspace{3pt} C)$. The robot is not aware of the exact locations for $A,B,C  \text{ and } D$ but has access to some information regarding potential locations, as shown in Fig. \ref{fig:case1}. As observed, the robot moves through the environment and follows its policy while updating its map knowledge. Ultimately, the robot successfully finds the real $A,B,C  \text{ and } D$ locations and completes the mission. This scenario is illustrated in Fig. \ref{fig:case1}, where yellow cells represent potential A locations, blue cells indicate potential B locations, green cells denote potential C locations, and red cells mark possible D locations.
  
  % This scenario is illustrated in Fig. \ref{fig:case1}, where the cells marked in yellow show the potential $A$ locations, cells marked in blue show the potential $B$, cells marked in green show the potential $C$ and those in red indicate the possible $D$ locations.



\begin{figure}[!htbp]
    \centering
    \begin{tabular}{ccc} 
        \begin{subfigure}[t]{0.25\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{paper_case_0.png}
            \caption{}
        \end{subfigure} &
        \begin{subfigure}[t]{0.25\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{paper_case_1.png}
            \caption{}
        \end{subfigure} &
        \begin{subfigure}[t]{0.25\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{paper_case_2.png}
            \caption{}
        \end{subfigure} \\
        \begin{subfigure}[t]{0.25\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{paper_case_3.png}
            \caption{}
        \end{subfigure} &
        \begin{subfigure}[t]{0.25\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{paper_case_4.png}
            \caption{}
        \end{subfigure} &
        \begin{subfigure}[t]{0.25\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{paper_case_5.png}
            \caption{}
        \end{subfigure}
        
    \end{tabular}
    \caption{\scriptsize The illustrations of (a) the planned trajectory in \emph{red} based on the initial belief, (b,c,d,e) the re-planned trajectories in \emph{red} based on the current belief and the realized trajectory in \emph{black}, (f) the final trajectory accomplishing the mission.}%    Various snapshots of the robot within the environment, each illustrating a moment when the policy gets updated based on newly acquired information. These snapshots are sequenced from the initial stage (a) to the final stage (f), where the mission is accomplished. States marked with probabilities indicate the likelihood of those labels being accurate in each state. Solid yellow and blue colors denote the actual $Pickup$ and $Delivery$ locations, respectively. Red arrows show the potential trajectory if the robot follows the current policy, while black arrows depict the trajectory that the robot actually traversed.}
\label{fig:case1}    
\end{figure}


%\vspace{-3mm}
 
\color{black}
\subsubsection{Scalability}
We evaluate the scalability of the proposed algorithm by computing a policy to satisfy the scLTL task $\phi_2 = \lozenge(A \land \lozenge (B \land \lozenge (C)))$ (which implies visiting $A$, $B$, and $C$ in order) across different size environments. \textcolor{black}{TABLE}~\ref{tab:split_column_example} presents the time required for the product automaton construction and the policy calculation across various sizes of PL-DMDPs. As PL-DMDP size increases, the computation time grows, but only the policy computation occurs online. 
        % $(25,105)$    & $(100, 1.05e3)$             & $0.01$ & $0.03$ \\ \hline
        
\begin{table}[!htbp]
\color{black}
\small
    \centering
    \resizebox{0.8\columnwidth}{!}{%
    \setlength{\tabcolsep}{4pt} % Adjust column spacing
    \renewcommand{\arraystretch}{1} % Adjust row spacing
    \begin{tabular}{|>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{1.3cm}|>{\centering\arraybackslash}p{1.3cm}|}
        \hline
        $\mathcal{M}$ & \multicolumn{2}{|c|}{$\mathcal{P}$} & $\pi_p$ \\ \hline
          Size        &  Size & Time [s]      &  Time [s]        \\ \hline

        $(100,460)$    & $(400, 4.6e3)$             & $0.12$ & $0.17$ \\ \hline
        $(400,1.92e3)$    & $(1.6e3, 1.92e4)$             & $1.76$  & $1.43$ \\ \hline
         $(900,4.38e3)$    & $(3.6e3, 4.38e4)$             & $7.22$  & $5.27$ \\ \hline
        $(1.6e3,7.84e3)$    & $(6.4e3, 7.84e4)$           & $30.04$ & $13.35$ \\ \hline
        $(2.5e3,1.23e4)$    & $(1e4, 1.23e5)$           & $67.69$ & $28.11$ \\ \hline
         
    \end{tabular}
    }
    \caption{\color{black}
Computation times for the product automaton construction and policy calculation under the same task $\phi_2$, evaluated for different sizes of $\mathcal{M}$ with $\gamma = 0.99$ and $\epsilon_0 = 0.01$.}
    \label{tab:split_column_example}
\end{table}
\color{black}
%\vspace{-3mm}
\subsubsection{Benchmark analysis}
\paragraph{Trajectory Performance}\color{black}
% \textcolor{black}{We compare the performance of our algorithm with three different strategies in terms of the trajectory lengths required to satisfy the desired task.} 
\textcolor{black}{We compare the performance of our algorithm against two other strategies in terms of the trajectory lengths required to satisfy the desired task $\phi_3 = \lozenge (\text{Pick-up} \land (\lozenge \text{Delivery}))$.}  \color{black}
\noindent \textbf{Map generation -} We conduct Monte Carlo simulations to generate 500 $6 \times 6$ true maps based on a fixed map with initial belief (as in Fig.~\ref{fig:monte_initial}). The process to generate a true map is as follows: a random number between $[0,1]$ is generated  to decide on the existence of a pick-up and/or delivery location according to the initial belief (yellow/blue locations and the corresponding \color{black}probability \color{black} thresholds). After this process, if there is not at least one pick-up and one delivery location, we randomly select one state from the set of possible pick-up locations and another from the set of possible delivery locations. These states are then assigned as pick-up and delivery, respectively, to satisfy Assumption \ref{non-zerp}. \color{black} 
\color{black}\noindent \textbf{Benchmark strategies -}
% The first strategy is an extreme case that does not trust initial information. It first explores the map using the shortest possible route to obtain the true map, then it generates a path to complete the mission.
The \textcolor{black}{first} strategy is the offline method in \cite{8272366}. The \textcolor{black}{second} strategy builds upon \cite{8272366} by incorporating belief updates and replanning, a feature not present in \cite{8272366}. To ensure consistency with our scenarios, we make minor modifications to their model, such as using the same action sets and deterministic transitions. Note that both our method and \cite{8272366} use the same initial belief (as in Fig.~\ref{fig:monte_initial}).
{\noindent \textbf{Results -} 
\textcolor{black}{As shown in TABLE \ref{tab:statistics}, 
our algorithm completes the mission with an average trajectory length of $9.75$. Although the method in \cite{8272366} achieves an average trajectory length of $6$, it satisfies the task in only $139$ maps due to deadlocks, as explained in Section \Romannum{2}. Even after incorporating belief updates into \cite{8272366}, the modified approach results in a higher average trajectory length of $10.67$. 
\paragraph{Runtime Performance}
 We also compare our method against the approach in~\cite{8272366} with belief update, in terms of product automaton construction time and policy calculation time. We denote by $\mathcal{B}$ the percentage of states in $\mathcal{M}$ that have uncertain labels in the initial belief. For each such state $x \in X$, the initial belief is assigned as follows: $p_L(x,\{A\}) = p_L(x,\{B\}) = p_L(x,\{C\}) = p_L(x,\{A,B\}) = p_L(x,\{A,C\}) = p_L(x,\{B,C\}) = p_L(x,\{A,B,C\})= 0.1$, and $p_L(x,\{\}) = 0.3$. We conduct this comparison across varying values of $\mathcal{B}$ and different sizes of $\mathcal{M}$, using the task $\phi_4 = (\lozenge A) \land (\lozenge B) \land (\lozenge C)$ under the same initial belief  for both methods. As shown in TABLE \ref{table-time}, our method outperforms \cite{8272366} with belief update by achieving faster policy computation in all cases and faster product automaton construction in most cases. While our computation times show little variation with increasing $\mathcal{B}$, the approach in~\cite{8272366} becomes substantially slower as $\mathcal{B}$ grows. Note that this difference in computation time becomes significant when planning in large unknown environments, where the number of replanning steps may be on the order of hundreds.
 \color{black}
Overall, our proposed method strategically utilizes time-varying knowledge about the environment and provides efficient solutions to motion planning problems in semantically uncertain environments. 
}


}\color{black}
%of each true map is \color{black}  For each of the 500 true maps, we iterate through every state (cell) in the grid map. For each state, we generate a random number between 0 and 1. We then determine the true label for the state by comparing the random number with the probabilities of the labels, as shown in Fig. \ref{fig:monte}. If the number is less than the label's probability, we assign that label to the state; otherwise, we assign empty. To account for scenarios where pick-up and delivery locations appear in unexpected states, we assign a probability of 0.02 to each empty state in the grid maps to potentially contain a pick-up or delivery label. If, for any map, there is not at least one pick-up and one delivery location, we randomly select two states and label them as pick-up and delivery to ensure consistency with Assumption 2. \color{black}
% \vspace{-2mm}





%Overall, we propose a method that strategically utilizes time-varying knowledge about the environment. We show that this method satisfies the desired specifications and also illustrate that it performs better than a method that does not incorporate uncertain knowledge about the environment into the planning.

% \begin{figure}[htb!]
%     \centering
%     \setlength{\tabcolsep}{2pt}
%     \begin{tabular}{cc}
%         \begin{subfigure}[t]{0.40\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{monte_initial.png}
%             \caption{}  
%         \end{subfigure} &
%         \begin{subfigure}[t]{0.57\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{new_bench.png}
%             \caption{}  
%         \end{subfigure}
%     \end{tabular}
%     \caption{\scriptsize (a) A 6 by 6 grid map with initial information about the possible location of labels. The Monte Carlo simulation results across 500 maps are shown in (b) for the benchmarks and for our algorithm.}
%     \label{fig:monte}
% \end{figure}
% %\subsection{Experiments}
% %\subsection{Experiments}
% %\vspace{-2mm}


% \begin{wrapfigure}{r}{0.35\columnwidth}
% \centering
%     \setlength{\tabcolsep}{2pt}
%     % \vspace{-4mm}
%     \includegraphics[width=0.35\columnwidth]{bench_bigger.png}
%     \caption{\scriptsize A 6 by 6 grid map with initial information about the possible location of labels.}
%     \label{fig:monte_initial}
%     % \vspace{-2mm}
% \end{wrapfigure}


\begin{table}[!htbp]
    \centering
    \resizebox{0.85\columnwidth}{!}{%
    \setlength{\tabcolsep}{4pt} % Adjust column spacing
    \renewcommand{\arraystretch}{1.1} % Adjust row spacing
    \begin{tabular}{|>{\centering\arraybackslash}p{3.5cm}|>{\centering\arraybackslash}p{1cm}|>{\centering\arraybackslash}p{1.1cm}|>{\centering\arraybackslash}p{1.1cm}|>{\centering\arraybackslash}p{1.1cm}|>{\centering\arraybackslash}p{1.5cm}|>{\centering\arraybackslash}p{1.1cm}|}
        \hline
        \textbf{Method} & \textbf{Mean} & \textbf{Median} & \textbf{SD} & \textbf{Success} \\ \hline
        Our Algorithm            & 9.75  & 9.0  & 3.73   & 500 \\ \hline

        {\cite{8272366}}  & 6  & 6  & 0    & 139 \\ \hline
        {\cite{8272366}} + belief update & 10.67 & 8.5 & 5.43 & 500 \\ \hline
    \end{tabular}
    }
    \caption{\color{black} Comparison of our algorithm,  \cite{8272366} and \cite{8272366} with belief update based on trajectory length statistics (mean, median, and standard deviation) and mission success count across 500 maps. }
    \label{tab:statistics}
\end{table}

% \begin{table}[h!]
% \color{blue}

%     \centering
%     \resizebox{0.95\columnwidth}{!}{%
%     \setlength{\tabcolsep}{4pt}
%     \renewcommand{\arraystretch}{1.2}
%     \begin{tabular}{|>{\centering\arraybackslash}p{3.5cm}|
%                     >{\centering\arraybackslash}p{2.5cm}|
%                     >{\centering\arraybackslash}p{2.5cm}|
%                     >{\centering\arraybackslash}p{2cm}|
%                     >{\centering\arraybackslash}p{2cm}|}
%         \hline
%         & $\mathcal{M}$ & \multicolumn{2}{c|}{$\mathcal{P}$} & $\pi_p$ \\ \hline
%         \text{Method} & Size & Size & Time [s] & Time [s] \\ \hline

%         \multirow{2}{*}{Our algorithm} 
%             & $(100,\ 460)$     & $(800,\ 1.242\text{e}4)$   & $0.27$ & $0.31$ \\ \cline{2-5}
%         & $(400,\ 1.92\text{e}3)$ & $(3.2\text{e}3,\ 5.184\text{e}4)$ & $3.84$ & $2.88$ \\ \hline

%         \multirow{2}{*}{\cite{8272366} + belief update} 
%             & $(100,\ 460)$     & $(6400,\ 2.35\text{e}5)$   & $2.41$ & $6.05$ \\ \cline{2-5}
%         & $(400,\ 1.92\text{e}3)$ & $(2.56e4, 9.83e5)$ & $10.21$ & $95.68$ \\ \hline

%     \end{tabular}
%     }
%     \caption{Computation time comparison between our algorithm and~\cite{8272366} with belief update for product automaton construction and policy calculation under the task $\phi_4$, across different sizes of $\mathcal{M}$.}

%     \label{tab:time-comparision}
% \end{table}




% \begin{table}[h!]
% \color{blue}

%     \centering
%     \resizebox{0.95\columnwidth}{!}{%
%     \setlength{\tabcolsep}{4pt}
%     \renewcommand{\arraystretch}{1.2}
%     \begin{tabular}{|>{\centering\arraybackslash}p{3.5cm}|
%                     >{\centering\arraybackslash}p{2.5cm}|
%                     >{\centering\arraybackslash}p{2.5cm}|
%                     >{\centering\arraybackslash}p{2cm}|
%                     >{\centering\arraybackslash}p{2cm}|}
%         \hline
%         & $\mathcal{M}$ & \multicolumn{2}{c|}{$\mathcal{P}$} & $\pi_p$ \\ \hline
%         \text{Method} & Size & Size & Time [s] & Time [s] \\ \hline

%         \multirow{2}{*}{Our algorithm} 
%             & $(100,\ 460)$     & $(800,\ 1.242\text{e}4)$   & $0.27$ & $0.31$ \\ \cline{2-5}
%         & $(400,\ 1.92\text{e}3)$ & $(3.2\text{e}3,\ 5.184\text{e}4)$ & $3.84$ & $2.88$ \\ \hline

%         \multirow{2}{*}{\cite{8272366} + belief update} 
%             & $(100,\ 460)$     & $(6400,\ 2.35\text{e}5)$   & $2.41$ & $6.05$ \\ \cline{2-5}
%         & $(400,\ 1.92\text{e}3)$ & $(2.56e4, 9.83e5)$ & $10.21$ & $95.68$ \\ \hline

%     \end{tabular}
%     }
%     \caption{Computation time comparison between our algorithm and~\cite{8272366} with belief update for product automaton construction and policy calculation under the task $\phi_4$, across different sizes of $\mathcal{M}$.}

%     \label{tab:time-comparision}
% \end{table}




\begin{table}[!htbp]
\centering
\captionsetup{justification=centering}
\begin{subtable}[t]{\linewidth}
\centering
\resizebox{0.98\columnwidth}{!}{%
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|>{\centering\arraybackslash}p{2.5cm}|
                >{\centering\arraybackslash}p{1.5cm}|
                >{\centering\arraybackslash}p{3cm}|
                >{\centering\arraybackslash}p{3cm}|
                >{\centering\arraybackslash}p{1.8cm}|
                >{\centering\arraybackslash}p{1.8cm}|}
\hline
$\boldsymbol{\mathcal{M}}$ (Size) & $\boldsymbol{\mathcal{B}}$ & \textbf{Method} & $\boldsymbol{\mathcal{P}}$ (Size) & $\mathcal{P}$ Time [s] & $\pi_p$ Time [s] \\ \hline

\multirow{8}{*}{(100,\ 460)} 
  & \multirow{2}{*}{25\%}  & Our algorithm           & $(800,\ 1.242\text{e}4)$   & $0.27$ & $0.24$ \\ \cline{3-6}
                           &                         & \cite{8272366} + belief update       & $(2.2\text{e}3,\ 3.689\text{e}4)$ & $0.39$ & $0.81$ \\ \cline{2-6}
  & \multirow{2}{*}{50\%}  & Our algorithm           & $(800,\ 1.242\text{e}4)$   & $0.27$ & $0.21$ \\ \cline{3-6}
                           &                         & \cite{8272366} + belief update       & $(3.6\text{e}3,\ 7.894\text{e}4)$ & $0.85$ & $1.77$ \\ \cline{2-6}
  & \multirow{2}{*}{75\%}  & Our algorithm           & $(800,\ 1.242\text{e}4)$   & $0.27$ & $0.19$ \\ \cline{3-6}
                           &                         & \cite{8272366} + belief update       & $(5\text{e}3,\ 1.507\text{e}5)$   & $1.65$ & $4.54$ \\ \cline{2-6}
  & \multirow{2}{*}{100\%} & Our algorithm           & $(800,\ 1.242\text{e}4)$   & $0.27$ & $0.18$ \\ \cline{3-6}
                           &                         & \cite{8272366} + belief update       & $(6.4\text{e}3,\ 2.355\text{e}5)$ & $2.47$ & $5.81$ \\ \hline
\end{tabular}
}
\caption{}
\end{subtable}

\vspace{0.3cm}

\begin{subtable}[!htp]{\linewidth}
\centering
\resizebox{0.98\columnwidth}{!}{%
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|>{\centering\arraybackslash}p{2.5cm}|
                >{\centering\arraybackslash}p{1.5cm}|
                >{\centering\arraybackslash}p{3cm}|
                >{\centering\arraybackslash}p{3cm}|
                >{\centering\arraybackslash}p{1.8cm}|
                >{\centering\arraybackslash}p{1.8cm}|}
\hline
$\boldsymbol{\mathcal{M}}$ (Size) & $\boldsymbol{\mathcal{B}}$ & \textbf{Method} & $\boldsymbol{\mathcal{P}}$ (Size) & $\mathcal{P}$ Time [s] & $\pi_p$ Time [s] \\ \hline

\multirow{8}{*}{(400,\ 1.92e3)} 
  & \multirow{2}{*}{25\%}  & Our algorithm           & $(3.2\text{e}3,\ 5.184\text{e}4)$   & $3.83$ & $1.05$ \\ \cline{3-6}
                           &                         & \cite{8272366} + belief update       & $(8.8\text{e}3,\ 1.433\text{e}5)$ & $1.57$ & $3.53$ \\ \cline{2-6}
  & \multirow{2}{*}{50\%}  & Our algorithm           & $(3.2\text{e}3,\ 5.184\text{e}4)$   & $3.83$ & $1.00$ \\ \cline{3-6}
                           &                         & \cite{8272366} + belief update       & $(1.44\text{e}4,\ 3.464\text{e}5)$ & $3.66$ & $8.10$ \\ \cline{2-6}
  & \multirow{2}{*}{75\%}  & Our algorithm           & $(3.2\text{e}3,\ 5.184\text{e}4)$   & $3.85$ & $0.98$ \\ \cline{3-6}
                           &                         & \cite{8272366} + belief update       & $(2\text{e}4,\ 6.290\text{e}5)$     & $6.66$ & $20.3$ \\ \cline{2-6}
  & \multirow{2}{*}{100\%} & Our algorithm           & $(3.2\text{e}3,\ 5.184\text{e}4)$   & $3.85$ & $0.83$ \\ \cline{3-6}
                           &                         & \cite{8272366} + belief update       & $(2.56\text{e}4,\ 9.830\text{e}5)$ & $10.47$ & $44.40$ \\ \hline
\end{tabular}
}
\caption{}
\end{subtable}

\caption{Computation time comparison between our algorithm and \cite{8272366} with belief update, which shows product automaton construction time and policy computation times under task $\phi_4$ for (a) $\mathcal{M} = (100,\ 460)$ and (b) $\mathcal{M} = (400,\ 1.92\text{e}3)$, across varying $\mathcal{B}$ values.}
\label{table-time}
\end{table}



% \begin{table}[h!]
% \color{blue}
% \centering
% \resizebox{0.98\columnwidth}{!}{%
% \setlength{\tabcolsep}{4pt}
% \renewcommand{\arraystretch}{1.2}
% \begin{tabular}{|>{\centering\arraybackslash}p{2.5cm}|
%                 >{\centering\arraybackslash}p{1.5cm}|
%                 >{\centering\arraybackslash}p{3cm}|
%                 >{\centering\arraybackslash}p{3cm}|
%                 >{\centering\arraybackslash}p{1.8cm}|
%                 >{\centering\arraybackslash}p{1.8cm}|}
% \hline
% $\boldsymbol{\mathcal{M}}$ (Size) & $\boldsymbol{\mathcal{B}}$ & \textbf{Method} & $\boldsymbol{\mathcal{P}}$ (Size) & $\mathcal{P}$ Time [s] & $\pi_p$ Time [s] \\ \hline

% \multirow{8}{*}{(100,\ 460)} 
%   & \multirow{2}{*}{25\%}  & Our algorithm           & $(800,\ 1.242\text{e}4)$   & $0.27$ & $0.24$ \\ \cline{3-6}
%                            &                         & \cite{8272366} + belief update       & $(2.2\text{e}3,\ 3.689\text{e}4)$ & $0.39$ & $0.81$ \\ \cline{2-6}

%   & \multirow{2}{*}{50\%}  & Our algorithm           & $(800,\ 1.242\text{e}4)$   & $0.27$ & $0.21$ \\ \cline{3-6}
%                            &                         & \cite{8272366} + belief update       & $(3.6\text{e}3,\ 7.894\text{e}4)$ & $0.85$ & $1.77$ \\ \cline{2-6}

%   & \multirow{2}{*}{75\%}  & Our algorithm           & $(800,\ 1.242\text{e}4)$   & $0.27$ & $0.19$ \\ \cline{3-6}
%                            &                         & \cite{8272366} + belief update       & $(5\text{e}3,\ 1.507\text{e}5)$   & $1.65$ & $4.54$ \\ \cline{2-6}

%   & \multirow{2}{*}{100\%} & Our algorithm           & $(800,\ 1.242\text{e}4)$   & $0.27$ & $0.18$ \\ \cline{3-6}
%                            &                         & \cite{8272366} + belief update       & $(6.4\text{e}3,\ 2.355\text{e}5)$ & $2.47$ & $5.81$ \\ \hline
% \end{tabular}
% }
% \caption{Computation time comparison between our algorithm and \cite{8272366} with belief update, showing product automaton construction time and policy computation times for task $\phi_4$ with $\mathcal{M} = (100, 460)$ across varying $\mathcal{B}$ values.}
% \label{tab:time-100}
% \end{table}











% \begin{table}[h!]
% \color{blue}
% \centering
% \resizebox{0.98\columnwidth}{!}{%
% \setlength{\tabcolsep}{4pt}
% \renewcommand{\arraystretch}{1.2}
% \begin{tabular}{|>{\centering\arraybackslash}p{2.5cm}|
%                 >{\centering\arraybackslash}p{1.5cm}|
%                 >{\centering\arraybackslash}p{3cm}|
%                 >{\centering\arraybackslash}p{3cm}|
%                 >{\centering\arraybackslash}p{1.8cm}|
%                 >{\centering\arraybackslash}p{1.8cm}|}
% \hline
% $\boldsymbol{\mathcal{M}}$ (Size) & $\boldsymbol{\mathcal{B}}$ & \textbf{Method} & $\boldsymbol{\mathcal{P}}$ (Size) & $\mathcal{P}$ Time [s] & $\pi_p$ Time [s] \\ \hline

% \multirow{8}{*}{(400,\ 1.92e3)} 
%   & \multirow{2}{*}{25\%}  & Our algorithm           & $(3.2\text{e}3,\ 5.184\text{e}4)$   & $3.83$ & $1.05$ \\ \cline{3-6}
%                            &                         & \cite{8272366} + belief update       & $(8.8\text{e}3,\ 1.433\text{e}5)$ & $1.57$ & $3.53$ \\ \cline{2-6}

%   & \multirow{2}{*}{50\%}  & Our algorithm           & $(3.2\text{e}3,\ 5.184\text{e}4)$   & $3.83$ & $1.00$ \\ \cline{3-6}
%                            &                         & \cite{8272366} + belief update       & $(1.44\text{e}4,\ 3.464\text{e}5)$ & $3.66$ & $8.10$ \\ \cline{2-6}

%   & \multirow{2}{*}{75\%}  & Our algorithm           & $(3.2\text{e}3,\ 5.184\text{e}4)$   & $3.85$ & $0.98$ \\ \cline{3-6}
%                            &                         & \cite{8272366} + belief update       & $(2\text{e}4,\ 6.290\text{e}5)$     & $6.66$ & $20.3$ \\ \cline{2-6}

%   & \multirow{2}{*}{100\%} & Our algorithm           & $(3.2\text{e}3,\ 5.184\text{e}4)$   & $3.85$ & $0.83$ \\ \cline{3-6}
%                            &                         & \cite{8272366} + belief update       & $(2.56\text{e}4,\ 9.830\text{e}5)$ & $10.47$ & $44.40$ \\ \hline
% \end{tabular}
% }
% \caption{Computation time comparison between our algorithm and \cite{8272366} with belief update, showing product automaton construction time and policy computation times for task $\phi_4$ with $\mathcal{M} = (400, 1.92\text{e}3)$ across varying $\mathcal{B}$ values.}
% \label{tab:time-400}
% \end{table}








% \begin{table}[h!]
% \color{blue}
%     \centering
%     \resizebox{0.98\columnwidth}{!}{%
%     \setlength{\tabcolsep}{4pt}
%     \renewcommand{\arraystretch}{1.2}
%     \begin{tabular}{|>{\centering\arraybackslash}p{3cm}|
%                     >{\centering\arraybackslash}p{1.2cm}|
%                     >{\centering\arraybackslash}p{2.2cm}|
%                     >{\centering\arraybackslash}p{2.5cm}|
%                     >{\centering\arraybackslash}p{1.8cm}|
%                     >{\centering\arraybackslash}p{1.8cm}|}
%         \hline
%         \textbf{Method} & $\boldsymbol{\mathcal{B}}$ & $\boldsymbol{\mathcal{M}}$ (Size) & $\boldsymbol{\mathcal{P}}$ (Size) & $\mathcal{P}$ Time [s] & $\pi_p$ Time [s] \\ \hline

%         \multirow{8}{*}{\centering Our algorithm}
%             & \multirow{2}{*}{$25\%$} & $(100,\ 460)$     & $(800,\ 1.242\text{e}4)$   & $0.27$ & $0.24$ \\ \cline{3-6}
%             &                         & $(400,\ 1.92\text{e}3)$ & $(3.2\text{e}3,\ 5.184\text{e}4)$ & $3.83$ & $1.05$ \\ \cline{2-6}
%             & \multirow{2}{*}{$50\%$} & $(100,\ 460)$     & $(800,\ 1.242\text{e}4)$   & $0.27$ & $0.21$ \\ \cline{3-6}
%             &                         & $(400,\ 1.92\text{e}3)$ & $(3.2\text{e}3,\ 5.184\text{e}4)$ & $3.83$ & $1.00$ \\ \cline{2-6}
%             & \multirow{2}{*}{$75\%$} & $(100,\ 460)$     & $(800,\ 1.242\text{e}4)$   & $0.27$ & $0.19$ \\ \cline{3-6}
%             &                         & $(400,\ 1.92\text{e}3)$ & $(3.2\text{e}3,\ 5.184\text{e}4)$ & $3.85$ & $0.98$ \\ \cline{2-6}
%             & \multirow{2}{*}{$100\%$} & $(100,\ 460)$     & $(800,\ 1.242\text{e}4)$   & $0.27$ & $0.18$ \\ \cline{3-6}
%             &                         & $(400,\ 1.92\text{e}3)$ & $(3.2\text{e}3,\ 5.184\text{e}4)$ & $3.85$ & $0.83$ \\ \hline

%         \multirow{8}{*}{\centering \cite{8272366} + belief update}
%             & \multirow{2}{*}{$25\%$} & $(100,\ 460)$     & $(2.2\text{e}3,\ 3.689\text{e}4)$   & $0.39$ & $0.81$ \\ \cline{3-6}
%             &                         & $(400,\ 1.92\text{e}3)$ & $(8.8\text{e}3,\ 1.433\text{e}5)$ & $1.57$ & $3.53$ \\ \cline{2-6}
%             & \multirow{2}{*}{$50\%$} & $(100,\ 460)$     & $(3.6\text{e}3,\ 7.894\text{e}4)$   & $0.85$ & $1.77$ \\ \cline{3-6}
%             &                         & $(400,\ 1.92\text{e}3)$ & $(1.44\text{e}4,\ 3.464\text{e}5)$ & $3.66$ & $8.10$ \\ \cline{2-6}
%             & \multirow{2}{*}{$75\%$} & $(100,\ 460)$     & $(5\text{e}3,\ 1.507\text{e}5)$   & $1.65$ & $4.54$ \\ \cline{3-6}
%             &                         & $(400,\ 1.92\text{e}3)$ & $(2\text{e}4,\ 6.290   \text{e}5)$ & $6.66$ & $20.3$ \\ \cline{2-6}
%             & \multirow{2}{*}{$100\%$} & $(100,\ 460)$     & $(6.4\text{e}3,\ 2.355\text{e}5)$   & $2.47$ & $5.81$ \\ \cline{3-6}
%             &                         & $(400,\ 1.92\text{e}3)$ & $(2.56\text{e}4,\ 9.830\text{e}5)$ & $10.47$ & $44.40$ \\ \hline
%     \end{tabular}
%     }
%     \caption{Computation time comparison between our algorithm and~\cite{8272366} with belief update, showing product automaton construction and policy calculation times for task $\phi_4$ across various $\mathcal{B}$ values and $\mathcal{M}$ sizes.}
%     \label{tab:time-comparision}
% \end{table}



\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.2\textwidth]{bench_bigger.png}
    \caption{\scriptsize  A 6 by 6 grid map with initial information about the possible location of labels.}
    \label{fig:monte_initial}
\end{figure}

\color{black}

\noindent \textbf{Experiments:} We test  our algorithm for real-time planning with a DJI Tello quadrotor in scenarios such as pick-up/delivery and reach/avoid. We use the DJITelloPy \cite{djitellopy2024} package for low-level controls. We utilize AprilTags to accurately detect different labels like $Pickup$, $Delivery$, $Goal$, and $Danger$. This enables the quadrotor to update its map effectively by recognizing these tags during operation. \color{black} Additionally, we conduct a Gazebo simulation with the Scout Mini robot, which tackles a more complex task in a larger environment. \color{black} More details about the experiments can be found at the following link: \color{black} (\url{https://youtu.be/CfB4e2B5P8Q})\color{black}.%\footnote{\href{https://youtu.be/mi-z6FF3SnE}{https://youtu.be/mi-z6FF3SnE}} 
\color{black}


\section{Conclusion}\label{sec:conclusion}
We introduce an automata-theoretic framework to solve motion planning problems under syntactically co-safe Linear Temporal Logic (scLTL) specifications in environments with uncertain semantic maps. Our method utilizes value iteration techniques to generate a sequence of actions that satisfies the desired scLTL task. \color{black}By updating beliefs dynamically based on new information, our approach ensures adaptive and accurate planning. Our key contributions include a compact representation of the product automaton and the ability to replan based on newly discovered information. %We demonstrate the effectiveness of our approach through simulations and real-world experiments, showcasing its ability to handle different scenarios. 
In future work, we aim to develop a scalable version of this approach to handle larger and more complex environments. \color{black} Additionally, we plan to investigate our problem in the reinforcement learning setting (e.g., \cite{cai2021reinforcement}) and focus on developing novel scalable methods that ensure task satisfaction even when the belief lacks sufficient information, by incorporating safe exploration techniques. \color{black}

%Additionally, we plan to explore Reinforcement Learning approaches in unknown environments \cite{cai2021reinforcement}, aiming to prevent specification violations during learning while enhancing the convergence rate. \color{black}%, enhancing its applicability in real-world scenarios.
%\bibliography{references.bib}

% \vspace{-6mm}
\bibliographystyle{IEEEtran}
\bibliography{references}



\end{document}