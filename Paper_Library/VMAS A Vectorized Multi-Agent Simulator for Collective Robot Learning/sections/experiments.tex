\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.5\linewidth}
        \resizebox{\linewidth}{!}{%
          \input{figures/experiments/transport}}%
           \caption{Transport}
           \label{fig:experiments_transport}
     \end{subfigure}%
    \begin{subfigure}[b]{0.5\linewidth}
            \resizebox{\linewidth}{!}{%
          \input{figures/experiments/wheel}}%
          \caption{Wheel}
          \label{fig:experiments_wheel}
     \end{subfigure}
      \begin{subfigure}[b]{0.5\linewidth}
             \resizebox{\linewidth}{!}{%
          \input{figures/experiments/balance}}%
         \caption{Balance}
          \label{fig:experiments_balance}
     \end{subfigure}%
     \begin{subfigure}[b]{0.5\linewidth}
        \resizebox{\linewidth}{!}{%
          \input{figures/experiments/give_way}}%
         \caption{Give Way}
          \label{fig:experiments_give_way}
     \end{subfigure}
    \caption{Benchmark performance of different PPO-based MARL algorithms in four VMAS scenarios. Experiments are run in RLlib~\cite{liang2018rllib}. Each training iteration is performed over 60,000 environment interactions. We plot the mean and standard deviation of the mean episode reward\footref{foot:reward} over 10 runs with different seeds.}
    \label{fig:experiments}
\end{figure}


% Heuristic wheel: -6.5933 +/- 1.6038 (60.000 steps)
% Heuristic balance: 62.7622 +/- 16.9914 (60.000 steps)
% Heuristic give way: 363.0147 +/- 0 (60.000 steps)

We run a set of training experiments to benchmark the performance of MARL algorithms on four VMAS scenarios. Thanks to VMAS's vectorization, we are able to perform a training iteration (comprised of 60,000 environment interactions and deep neural network training) in 25s on average. The runs reported in this section all took under 3 hours to complete. The models compared are all based on Proximal Policy Optimization~\cite{schulman2017proximal}, an actor-critic RL algorithm. The actor is a Deep Neural Network (DNN) which outputs actions given the observations and the critic is a DNN (used only during training) which, given the observations, outputs a value representing the goodness of the current state and action. We refer to the actor and critic as \textit{centralized} when they have access to all the agents' observations and output all the agent's actions/values and we call them \textit{decentralized} when they only map one agent's observations to its action/value. The models compared are:
\begin{itemize}
    \item \textbf{CPPO}: This model uses a centralized critic and actor. It treats the multi-agent problem as a single-agent problem with one super-agent.
    \item \textbf{MAPPO~\cite{yu2021surprising}}: This model uses a centralized critic and a decentralized actor. Therefore, the agents act independently, with local decentralized policies, but are trained with centralized information.
    \item \textbf{IPPO~\cite{de2020independent}}: This model uses a decentralized critic and actor. Every agent learns and acts independently. Model parameters are shared among agents so they can benefit from each other's experiences.
    \item \textbf{HetIPPO}: We customize IPPO to disable parameter sharing, making each agent's model unique.
    \item \textbf{Heuristic}: This is a hand-designed decentralized heuristic different for each task.
\end{itemize}

Experiments are run in RLlib~\cite{liang2018rllib} using the vectorized interface. We run all algorithms for 400 training iterations. Each training iteration is performed over 60,000 environment interactions. We plot the mean and standard deviation of the mean episode reward\footnote{The episode reward mean is the mean of the total rewards of episodes contained in the training iteration\label{foot:reward}} over 10 runs with different seeds. The model used for all critics and actors is a two layer Multi Layer Perceptron (MLP) with hyperbolic tangent activations. A video of the learned policies is available at this \href{https://youtu.be/aaDRYfiesAY}{link}\footref{foot:video}. In the following, we discuss the results for the trained scenarios. 
% In case the model is centralized it takes all agents' observations and outputs all agents' values/actions, in case it is decentralized it takes observation for one agent and outputs its value/action.
\vspace{3pt}

\noindent\textbf{Transport (\autoref{fig:experiments_transport})}. In the Transport environment, only IPPO is able to learn the optimal policy. This is because the other models, which have centralized components, have an input space consisting of the concatenation of all the agents' observations. Consequently, centralized architectures fail to generalize in environments requiring a high initial exploration like this one, where there is a high variance in possible joint states (and therefore there is a low probability that a similar state will be encountered).

\vspace{3pt}

\noindent\textbf{Wheel (\autoref{fig:experiments_wheel})}. The Wheel environment proved to be a hard task for MARL algorithms. Here, all models were not able to solve the task and performed worse than the heuristic. 

\vspace{3pt}

\noindent\textbf{Balance (\autoref{fig:experiments_balance})}. In Balance, all models were able to solve the task and outperform the heuristic. However, this is largely due to the use of a big observation space containing global information. The task can be made arbitrarily harder by removing part of the observation space and thus increasing partial observability.

\vspace{3pt}

\noindent\textbf{Give Way (\autoref{fig:experiments_give_way})}. In the Give Way scenario, it is shown that only algorithms able to develop heterogeneous agent behaviour can solve the environment. In fact, IPPO and MAPPO, which use parameter sharing and decentralized actors, fail this scenario. On the other hand, it is shown that the scenario can be solved either through a centralized actor (CPPO) or by disabling parameter sharing and allowing agent policies to be heterogeneous (HetIPPO).

The experimental results confirm that VMAS proposes a selection of scenarios which prove challenging in orthogonal ways for current state-of-the-art MARL algorithms. We show that there exists no one-fits-all solution and that our scenarios can provide a valuable benchmark for new MARL algorithms.  In addition, vectorization enables faster training, which is key to a wider adoption of multi-agent learning in the robotics community.