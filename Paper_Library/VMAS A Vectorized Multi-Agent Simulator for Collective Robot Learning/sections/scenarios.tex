Alongside VMAS, we introduce a set of 12 multi-robot scenarios. These scenarios contain various multi-robot problems, which require complex coordination---like leveraging heterogeneous behaviour and inter-agent communication---to be solved. While the ability to send communication actions is not used in these scenarios, communication can be used in the policy to improve performance. For example, Graph Neural Networks (GNNs) can be used to overcome partial observability through information sharing~\cite{blumenkamp2021framework}. 

Each scenario delimits the agents' input by defining the set of their observations. This set typically contains the minimum observation needed to solve the task (e.g., position, velocity, sensory input, goal position).
Scenarios can be made arbitrarily harder or easier by modifying these observations. For example, if the agents are trying to transport a package, the precise relative distance to the package can be removed from the agent inputs and replaced with LIDAR measurements. Removing global observations from a scenario is a good incentive for inter-agent communication.

All tasks contain numerous parametrizable components. 
Every scenario comes with a set of tests, which run a local heuristic on all agents. Furthermore, we vectorize and port all 9 scenarios from MPE~\cite{lowe2017multi} to VMAS. In this section, we give a brief overview of our new scenarios. For more details (e.g., observation space, reward, etc.) you can find in-depth descriptions in the \href{https://github.com/proroklab/VectorizedMultiAgentParticleSimulator}{VMAS repository}\footref{foot:vmas_url}. 

\vspace{8pt}

\noindent\textbf{Transport (\autoref{fig:transport})}. $N$ agents have to push $M$ packages to a goal. Packages have a customizable mass and shape. Single agents are not able to move a high-mass package by themselves. Cooperation with teammates is thus needed to solve the task.

\vspace{8pt}

\noindent\textbf{Wheel (\autoref{fig:wheel})}. $N$ agents have to collectively rotate a line. The line is anchored to the origin and has a parametrizable mass and length. The team's goal is to bring the line to a desired angular velocity. Lines with a high mass are impossible to push for single agents. Therefore, the team has to organize with agents on both sides to increase and reduce the line's velocity. 

\vspace{8pt}


\noindent\textbf{Balance (\autoref{fig:balance})}. $N$ agents are spawned at the bottom of a world with vertical gravity. A line is spawned on top of them. The agents have to transport a spherical package, positioned randomly on top of the line, to a given goal at the top. The package has a parametrizable mass and the line can rotate.

\vspace{8pt}

\noindent\textbf{Give Way (\autoref{fig:give_way})}. Two agents start in front of each other's goals in a symmetric environment. To solve the task, one agent has to give way to the other by using a narrow space in the middle of the environment.

\vspace{8pt}

\noindent\textbf{Football (\autoref{fig:football})}. A team of $N$ blue agents competes against a team of $M$ red agents to score a goal. By default, red agents are controlled by a heuristic AI, but self-play is also possible. Cooperation among teammates is required to coordinate attacking and defensive maneuvers. Agents need to communicate and assume different behavioural roles in order to solve the task.

\vspace{8pt}

\noindent\textbf{Passage (\autoref{fig:passage})}. 5 agents, starting in a cross formation, have to reproduce the same formation on the other side of a barrier. The barrier has $M$ passages ($M=1$ in the figure). Agents are penalized for colliding amongst each other and with the barrier. This scenario is a generalization of the one considered in~\cite{blumenkamp2021framework}.

\vspace{8pt}

\noindent\textbf{Reverse transport (\autoref{fig:reverse_transport})}. This task is the same as Transport, except only one package is present. Agents are spawned \textit{inside} of it and need to push it to the goal.

\vspace{8pt}

\noindent\textbf{Dispersion (\autoref{fig:dispersion})}. There are $N$ agents and $N$ food particles. Agents start in the same position and need to cooperatively eat all food. Most MARL algorithms cannot solve this task (without communication or observations from other agents) as they are constrained by behavioural homogeneity deriving from parameter sharing. Heterogeneous behaviour is thus needed for each agent to tackle a different food particle.

\vspace{8pt}

\noindent\textbf{Dropout (\autoref{fig:dropout})}. $N$ agents have to collectively reach one goal. To complete the task, it is enough for only one agent to reach the goal. The team receives an energy penalty proportional to the sum of all the agents' controls. Therefore, agents need to organize themselves to send only the closest robot to the goal, saving as much energy as possible. 

\vspace{8pt}

\noindent\textbf{Flocking (\autoref{fig:flocking})}. $N$ agents have to flock around a target without colliding among each other and $M$ obstacles. Flocking has been an important benchmark in multi-robot coordination for years, with first solutions simulating behaviour according to local rules~\cite{reynolds1987flocks}, and more recent work using learning-based approaches~\cite{tolstaya2020flocking}. In contrast to related work, our flocking environment contains static obstacles.

\vspace{8pt}

\noindent\textbf{Discovery (\autoref{fig:discovery})}. $N$ agents have to coordinate to cover $M$ targets as quickly as possible while avoiding collisions. A target is considered covered if $K$ agents have approached a target at a distance of at least $D$. After a target is covered, the $K$ covering agents each receive a reward and the target is re-spawned at a random position. This scenario is a variation of the Stick Pulling Experiment~\cite{ijspeert2001collaboration} and while it can be solved without communication, it has been shown that communication significantly improves performance for $N$ < $M$.

\vspace{8pt}

\noindent\textbf{Waterfall (\autoref{fig:waterfall})}. $N$ agents move from top to bottom through a series of obstacles. This is a testing scenario that can be used to discover VMAS's functionalities.

%VMAS contains 8 other scenarios, excluded for brevity. These scenarios range from adversarial and hierarchical tasks, such as football, to hard multi-robot problems such as flocking and the passage scenario introduced in~\cite{blumenkamp2021framework}. We invite the reader to refer to the \href{https://github.com/proroklab/VectorizedMultiAgentParticleSimulator}{VMAS codebase}\footref{foot:vmas_url} for a through description and visualization of all scenarios.