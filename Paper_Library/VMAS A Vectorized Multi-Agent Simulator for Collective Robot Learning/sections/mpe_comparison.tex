In this section, we compare the scalability of VMAS and MPE~\cite{lowe2017multi}. Given that we vectorize and port all the MPE scenarios in VMAS, we can compare the two simulators on the same MPE task. The task chosen is ``simple\_spread'', as it contains multiple collidable agents in the same environment. VMAS and MPE use two completely different execution paradigms: VMAS, being vectorized, leverages the Single Instruction Multiple Data (SIMD) paradigm, while MPE uses the Single Instruction Single Data (SISD) paradigm. Therefore, it is sufficient to report the benefits of this paradigm shift on only one task, as the benefits are task-independent.

In \autoref{fig:mpe_comparison}, we can see the growth in execution time with respect to the number of environments stepped in parallel for the two simulators. MPE runs only on the CPU, while VMAS, using PyTorch, runs both on the CPU and on the GPU. In this experiment, we compare the two simulators on an Intel(R) Xeon(R) Gold 6248R CPU @ 3.00GHz and we also run VMAS on an NVIDIA GeForce RTX 2080 Ti. The results show the impact of vectorization on simulation speed. On the CPU, VMAS is up to 5x faster than MPE. On the GPU, the simulation time for VMAS is independent of the number of environments, and runs up to 100$\times$ faster. The same results can be reproduced on different hardware. In the \href{https://github.com/proroklab/VectorizedMultiAgentParticleSimulator}{VMAS's repository}\footref{foot:vmas_url} we provide a script to repeat this experiment.

 
\begin{figure}[h!]
    \centering
    \scalebox{0.6}{%
    \input{figures/mpe_comparison/mpe_comparison}}
    \caption{Comparison of the scalability of VMAS and MPE~\cite{lowe2017multi} in the number of parallel environments. In this plot, we show the execution time of the ``simple\_spread'' scenario for 100 steps. MPE does not support vectorization and thus cannot be run on a GPU.}
    \label{fig:mpe_comparison}
\end{figure}
