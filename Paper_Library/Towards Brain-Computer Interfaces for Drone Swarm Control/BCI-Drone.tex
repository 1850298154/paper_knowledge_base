\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\usepackage{multirow, makecell}
\def\BigRoman{\uppercase\expandafter{\romannumeral\number\count 255 }}
\def\Romannumeral{\afterassignment\BigRoman\count255=}
\usepackage{tikz}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
  
\newcommand{\blu}{\color{blue}}
\newcommand{\blk}{\color{black}}
\newcommand{\qes}{\color{red}}

\makeatletter
\def\set@curr@file#1{%
  \begingroup
    \escapechar\m@ne
    \xdef\@curr@file{\expandafter\string\csname #1\endcsname}%
  \endgroup
}
\def\quote@name#1{"\quote@@name#1\@gobble""}
\def\quote@@name#1"{#1\quote@@name}
\def\unquote@name#1{\quote@@name#1\@gobble"}
\makeatother

\begin{document}

\title{Towards Brain-Computer Interfaces for Drone Swarm Control
\footnote{{\thanks{\hrule Research was funded by Institute of Information \& Communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government (No. 2017-0-00451, Development of BCI based Brain and Cognitive Computing Technology for Recognizing User’s Intentions using Deep Learning).}
}}
}

\author{\IEEEauthorblockN{Ji-Hoon Jeong$^1$, Dae-Hyeok Lee$^1$, Hyung-Ju Ahn$^1$, and Seong-Whan Lee$^2$}
\IEEEauthorblockA{$^1$Department of Brain and Cognitive Engineering, Korea University, Seoul, Republic of Korea}
\IEEEauthorblockA{$^2$Department of Artificial Intelligence, Korea University, Seoul, Republic of Korea}

{jh$\_$jeong@korea.ac.kr, lee$\_$dh@korea.ac.kr, hj\_ahn@korea.ac.kr, sw.lee@korea.ac.kr}
}



%\author{\IEEEauthorblockN{}
%\IEEEauthorblockA{{$^1$Department of Brain and Cognitive Engineering, Korea University, Seoul, Republic of Korea} \\
%{$^2$Department of Artificial Intelligence, Korea University, Seoul, Republic of Korea}\\
%}
%}

\maketitle

\begin{abstract}
Noninvasive brain-computer interface (BCI) decodes brain signals to understand user intention. Recent advances have been developed for the BCI-based drone control system as the demand for drone control increases. Especially, drone swarm control based on brain signals could provide various industries such as military service or industry disaster. This paper presents a prototype of a brain-swarm interface system for a variety of scenarios using a visual imagery paradigm. We designed the experimental environment that could acquire brain signals under a drone swarm control simulator environment. Through the system, we collected the electroencephalogram (EEG) signals with respect to four different scenarios. Seven subjects participated in our experiment and evaluated classification performances using the basic machine learning algorithm. The grand average classification accuracy is higher than the chance level accuracy. Hence, we could confirm the feasibility of the drone swarm control system based 
on EEG signals for performing high-level tasks.
\end{abstract}

\begin{small}
\textbf{\textit{Keywords-brain-computer interface; electroencephalogram; drone swarm control; visual imagery}}\\
\end{small}

\section{Introduction}
Brain-computer interface (BCI) analyzes brain signals to understand intention and status of human that can be used for controlling various machines. Since brain signals contain significant information about status of human, many BCI studies have attempted to understand brain signals \cite{C3,MRCP,B1,ECoG2}. In contrast, invasive methods such as electrocroticogram (ECoG) \cite{ECoG} place the electrodes on the brain directly to acquire high-quality brain signals. These methods can obtain the higher quality of brain signals compared with non-invasive methods such as electroencephalogram (EEG), functional near-infrared spectroscopy (fNIRS), and functional magnetic resonance imaging (fMRI), but they are riskier because they involve surgery to implant electrodes. EEG-based BCI has several paradigms for signal acquisition such as motor imagery (MI) \cite{A2,C2,kam}, event-related potential (ERP) \cite{EEG,A1}, and movement-related cortical potential (MRCP) \cite{jeong2020decoding,MRCP}. As applications of EEG-based BCI, a robotic arm \cite{C1,roboticarm,A2}, a speller \cite{speller,won2017motion,ECoG2,stawicki2017novel}, a wheelchair \cite{wheelchair}, and a drone \cite{wang2018wearable,lafleur2013quadcopter,karavas2015effect,karavas2017hybrid} were commonly used for communication between human and machines. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
\centerline{\includegraphics[width = \columnwidth]{figure1.pdf}}
\caption{The example of a brain-swarm interface for drone control}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Recently, a BCI-based drone swarm is one of the most interesting topics (Fig. 1). Drone swarm means a group of three or more drones. Before drone swarm studies became active, a BCI-based single-unit drone studies had begun first. Wang et al. \cite{wang2018wearable} designed a wearable BCI system based on the steady-state visual evoked potential (SSVEP), which enables three-dimensional navigation of quadcopter flight with visual feedback using a head-mounted device. They demonstrated the feasibility of using the head-mounted device and a proper control strategy to facilitate the portability and practicability of the SSVEP-based BCI system for its navigation utility. LaFleur et al. \cite{lafleur2013quadcopter} reported a novel experiment of BCI controlling a quadcopter in three-dimensional physical space using noninvasive scalp EEG signals in human subjects. They showed the ability to control a flying robot in three-dimensional physical space with EEG signals. Very few research groups have studied to control a drone swarm. Karavas et al. \cite{karavas2015effect} examined the perception and representation of collective behaviors of swarms at the brain level of human supervisors. They extracted event related potentials at EEG signals. Their study provided the first evidence of representation of swarm collective behaviors at the brain level which can lead to the design of a new generation of brain-swarm control and perception interfaces. Karavas et al. \cite{karavas2017hybrid} proposed a hybrid BCI system which combined EEG signals and joystick input. The purpose of applying this system was to show both the system’s capability for control of actual robotic platforms and the feasibility of controlling robotic swarm behaviors using EEG signals. They used event related desynchronization / synchronization phenomena. Their study allowed for continuous control variables extracted from the EEG signals.

In this study, we measured EEG signals of 4-class using visual imagery paradigm \cite{sousa2017pure,koizumi2018development}: `Hovering', `Splitting', `Dispersing', and `Aggregating'. The classes used in the experimental paradigm consist of the most basic commands for controlling a drone swarm. To best of our knowledge, this is the first attempt that demonstrates the feasibility of classifying the high-level commands which consist of 4-class. Second, we achieved robust classification performance in the 4-class high-level commands compared with the chance-level accuracy (0.25).

The rest of this paper is organized as follows. Section {\Romannumeral 2} gives a description of the experimental protocols, EEG signals acquisition, a drone swarm control simulator and the data analysis. Section {\Romannumeral 3} presents the results of performance accuracies for 4-class classification and discussions about our study. In session {\Romannumeral 4}, conclusion and future works are described.\\


\section {Materials and Methods}
\subsection{Experimental Protocols}
Seven healthy subjects, who were naive  BCI users, have recruited in the experiment (aged 22-33, five males and two females). Before the experiment, each subject was informed of the experimental protocols and procedures. After they had understood, all of them provided their given written consent according to the Declaration of Helsinki. All experimental protocols and environments were reviewed and approved by the Institutional Review Board at Korea University (KUIRB-2020-0013-01). First, the subjects sat in front of the experimental desk as depicted in Fig. 2. The monitor display for visual instruction was put at the distance of 90cm from the subjects. The subjects were asked to perform visual imagery according to the four different scenarios. Each trial was composed of four phases such as rest, visual cue/preparation, stare fixation point, and imagination. In the rest phase, the subject took a comfortable rest with restraining eye and body movement for 3 s. After the rest phase, the monitor displayed one of the scenarios as a visual cue and then subjects prepared the visual imagery task according to the cue. Then, the subjects stared the fixation point during 3 s to avoid an afterimage effect. During the 4 s, The subjects conducted visual imagery task. We asked subjects to perform 200 trials in total (i.e., 50 trials 4 classes) (Fig. 3).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
\centerline{\includegraphics[scale=0.8]{figure2.pdf}}
\caption{Experimental environment.}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
\centerline{\includegraphics[width = \columnwidth]{figure3.pdf}}
\caption{Experimental paradigm for visual imagery.}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{EEG Signal Acquisition}
We acquired the EEG signals with respect to drone swarm control scenarios using BrainVision Recorder (BrainProducts GmbH, Germany). EEG signals were acquired using 64 Ag/AgCl electrodes following 10/20 international systems. The ground and reference channels were FCz and FPz positions, respectively. The sampling rate was 1,000 Hz, and a notch filter was applied to the acquired signals as 60 Hz. All electrode impedances were kept below 10 k$\Omega$ during the experiment (Fig. 4). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\centerline{\includegraphics[scale=0.9]{figure4.pdf}}
\caption{System architecture for EEG data acquisition with a drone swarm simulator environment.}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\centerline{\includegraphics[width = \textwidth]{figure5.pdf}}
\caption{The example representation of a drone swarm simulator for four different scenarios.}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Drone Swarm Control Simulator}
We designed a drone swarm control simulator using Matlab software (MathWorks, USA) with Mobile Robotics Simulation Toolbox. This toolbox provides utilities for robot simulation and algorithm development in the 2D grid maps. We modified a multi-robot lidar control to drone swarm control system which was composed of fifty unit drones as depicted in Fig. 3. The drone conducted four different scenarios such as `Hovering', `Splitting', `Dispersing', and `Aggregating' through the simulator (Fig. 5). The hovering cue indicated an initial position of the swarm drone. The subjects imagined the visual instruction for the hovering state of drones. The splitting cue showed that the swarm drone divided two different swarms. The dispersing cue represented randomly position of each unit drone with outspreading. Finally, the aggregating cue indicated the unit drones was positioned closely with each other. One of the scenarios showed to the subjects during the `Visual cue/preparation' phase. 


\subsection{Data Analysis}
For the acquired EEG data verification, we adopted a basic EEG classification procedure which is generally used conventional BCI studies \cite{C3,MRCP,B1,ECoG2}. The data were preprocessed by using a band-pass filter with a zero-phase 2nd Butterworth filter between [8-30] Hz. The spectral ranges also mostly used in the imagination decoding from EEG signals which is mu and beta band. We segmented the data into 4 s epoched data for each trial. Then, a common spatial pattern (CSP) algorithm \cite{FBCSP} was applied to extract dominant spatial features for training. A transformation matrix from CSP consisted of the logarithmic variances of the first three and the last three columns were used as a feature. A linear discriminant analysis (LDA) \cite{channel} was used for a classification method which classified four different class using one-versus-rest strategy. For a fair evaluation of classification performance, a 5-fold cross-validation was used.


\section {Results and discussion}

As Fig. 6, the grand-average classification accuracy for four different scenarios is 36.7 ($\pm$4.6)\% across all subjects. 
Subject 5 showed the highest classification performance as 41.3\%, but subject 3 indicated the lowest results as 28.4\%. However, those accuracies are higher than the chance level accuracy for the 4-class classification problem (approximately 25\%). That means, although we used the basic machine learning algorithm for evaluating classification performances, we could confirm that the EEG data acquired with high quality under a restrained environment. Actually, the subject 3, who showed the lowest performance, tended to have difficulty with the visual imagery task during the experiment. Through the experiment, we confirm that visual instructions in a form similar to the real-world environment are necessary such as actual drone swarm control. It could be more helpful the subjects could perform visual imagery tasks. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
\centerline{\includegraphics[width = \columnwidth]{figure6.pdf}}
\caption{Classification accuracies of the four scenarios across all subjects.}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Conclusion and Future works}
In this paper, we designed an experimental environment for acquiring EEG data with respect to visual imagery tasks. Through the experiment system, the subjects could perform the visual imagery for the drone swarm control of various scenarios. We have implemented the four main different classes for swarm flight such as `Hovering', `Splitting', `Dispersing', and `Aggregating'. These scenarios have been used as an important function of drone swarm control under a simulated wargame environment. The EEG classification performance of visual imagery achieved a little higher than the chance rate level yet, this experiment system could contribute to developing a brain-swarm interface system using the drone for military service, industrial disaster, and artificial intelligence development.

Hence, we will have investigated a drone swarm control system based on EEG signals it could possible to conduct high-level tasks. As a result, the EEG classification performance needs to be higher and be cover more multi-command. Therefore, we will adopt the deep learning approach to our developing system for drone swarm control robustly under real-world environments. It would greatly improve the interaction effect between the user and the drone.

\section{Acknowledgement}
The authors thanks to B.-H. Kwon for their help with the design of experimental paradigm.\\
\bibliographystyle{IEEEbib}
\bibliography{BCI-Drone}


\end{document}
