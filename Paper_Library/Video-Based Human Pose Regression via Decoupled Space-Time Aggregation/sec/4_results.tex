\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Settings}
%\textbf{Dataset and Evaluatin Metric.} 
We have conducted evaluations on three widely-utilized video-based benchmarks for human pose estimation: PoseTrack2017~\cite{PoseTrack2017_CVPR2017}, PoseTrack2018~\cite{PoseTrack2018_CVPR2018}, and PoseTrack21~\cite{PoseTrack21_CVPR2022}. These datasets contain video sequences of complex scenarios involving rapid movements of highly occluded individuals in crowded environments. 
%Specifically, PoseTrack2017 includes 250 video clips for training and 50 videos for validation , with a total of 80, 144 pose annotations. PoseTrack2018 considerably increases the number of clips, containing 593 videos for training, 170 videos for validation, and a total of 153, 615 pose annotations. Both datasets identify 15 keypoints, with an additional label for joint visibility. The training videos are densely annotated in the center 30 frames, and validation videos are additionally labeled every four frames. PoseTrack21 further enriches and refines PoseTrack2018 especially for annotations of small persons and persons in crowds, including 177, 164 human pose annotations. 
To assess the performance of our models, we utilize the Average Precision (AP) metric~\cite{HRNet_CVPR2019,PoseWarper_NIPS2019,DCPose_CVPR2021,RLE_ICCV2021}. The AP is calculated for each joint, and the mean AP across all joints is denoted as mAP.
Our method is implemented using PyTorch. 
Unless otherwise specified, the input image size is $384 \times 288$ when using the HRNet-w48 backbone, while for other backbones, the input image size is $256 \times 192$.
We pretrained the backbones on the COCO dataset. For further implementation details, please refer to the supplementary materials provided.



%\noindent \textbf{Implementation Details.} 
%Our method is implemented using PyTorch. 
%Unless otherwise specified, the input image size is $384 \times 288$ when using the HRNet-w48 backbone, while for other backbones, the input image size is $256 \times 192$.
%We pretrained the backbones on the COCO dataset. 
%We incorporate data augmentation including random rotation [-45°, 45°], random scale [0.65, 1.35], truncation (half body), and flipping during training.  The temporal span $T$ is set to 2, consisting of two preceding and two subsequent frames, totaling four auxiliary frames.
%We adopt the AdamW optimizer~\cite{Adam_ICTAI2019} to train the entire network for 40 epochs, with a base learning rate of 2\textit{e}-4, which is reduced by an order of magnitude at the 20\textit{th} and 30\textit{th} epochs. $\beta_1$ and $\beta_2$ are set to 0.9 and 0.999, respectively, and weight decay is set to 0.01. 

\subsection{Main Results}

\begin{table}
    \centering
    \begin{tabular}{l|ccc}
       Method  &  ResNet-50 &  HRNet-W48   &  ViT-H \\
       \hline
       \multicolumn{4}{l}{\textit{image-based}}   \\
       RLE~\cite{RLE_ICCV2021}  &  70.7  &  75.7  &  79.0 \\
       Poseur~\cite{Poseur_ECCV2022}  & 74.4  & 79.3 &  81.0 \\
       \hline
       \multicolumn{4}{l}{\textit{video-based}} \\
       \textbf{DSTA (Ours)}  &  \textbf{79.7}  &  \textbf{84.6}  &  \textbf{85.6}  \\
    \end{tabular}
    \vspace{-2mm}
    \caption{\textbf{Comparison with image-based regression} (mAP) on PoseTrack2017 val. set.}
    \label{tab:regression_compare}
    \vspace{-5mm}
\end{table}

\subsubsection{Comparison with Image-based Regression}
To study the effectiveness of the proposed regression method on video input, we compare it with existing state-of-the-art image-based regression methods, namely RLE~\cite{RLE_ICCV2021} and Poseur~\cite{Poseur_ECCV2022}.
For thorough and fair comparisons, we utilized three distinct backbone networks—ResNet-50, HRNet-W48, and ViT-H—and ensured that each approach applied the identical pre-trained model to each backbone network. The experimental results on the PoseTrack2017 validation set are presented in Table~\ref{tab:regression_compare}. As shown, our proposed video-based method achieves significant performance improvements across all backbone networks when compared to image-based methods. For instance, our method outperforms RLE~\cite{RLE_ICCV2021} by a notable margin of \textbf{8.9} mAP (or \textbf{9.0} mAP) when utilizing the HRNet-W48 (or ResNet-50) backbone. This demonstrates the importance of incorporating temporal cues from neighboring frames.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{qualitative_result.jpg}
    \caption{\textbf{Qualitative comparison} of a) our DSTA, b) DCPose~\cite{DCPose_CVPR2021}, c) Poseur~\cite{Poseur_ECCV2022}, and d) RLE~\cite{RLE_ICCV2021} on the PoseTrack datasets, featuring challenges such as occlusions, nearby-person interactions, and motion blur. Inaccurate predictions are marked
    with red solid circles.}
    \label{fig:qualitative_result}
    \vspace{-5mm}
\end{figure}

By leveraging temporal dependencies across consecutive frames, our video-based regression is better equipped to handle challenging situations such as occlusion or motion blur commonly encountered in video scenarios, as demonstrated in Fig.~\ref{fig:qualitative_result}. These experiments demonstrate the superior performance of our proposed video-based regression framework, which significantly outstrips the capabilities of prior image-based methods when handling the video input. 

\subsubsection{Comparison with State-of-the-art Methods}
Current state-of-the-art algorithms for video-based human pose estimation are predominantly based on heatmaps. We first conduct a performance comparison of our method with these heatmap-based approaches on the PoseTrack datasets. Subsequently, we compare the computational complexity between the heatmap-based methods and our proposed regression-based approach. Furthermore, we examine the varying impacts of input resolution on both the heatmap-based methods and our regression-based approach.

\textbf{Results on the PoseTrack Datasets.} 
Table~\ref{tab:sota_compare} presents the quantitative results of different approaches on PoseTrack2017 validation set.
%We benchmark our regression-based model against $\diamond$ existing heatmap-based methods. 
Our method achieves comparable performance to the state-of-the-art methods.
For example, employing the HRNet-W48 backbone, our method attains an mAP of \textbf{84.6}, which surpasses the adopted backbone network HRNet-W48~\cite{HRNet_CVPR2019} by \textbf{7.3} points. When compared to video-based approaches utilizing the same backbone, our method outperforms DCPose~\cite{DCPose_CVPR2021} by \textbf{1.8} points while maintaining a performance level on par with the state-of-the-art FAMI-Pose~\cite{FAMIPose_CVPR2022}.
Our approach is flexible and can be easily integrated into various backbone networks. When using the ViT-H backbone, our method further pushes the performance boundary and achieves an mAP of \textbf{85.6}. The performance enhancement for the relatively challenging joints is truly encouraging: an mAP of \textbf{82.6} ($\uparrow$ \textbf{2.6}) for wrists and an mAP of \textbf{77.8} ($\uparrow$ \textbf{0.8}) for ankles. 
It is worth noting that methods utilizing temporal information, such as PoseWarper~\cite{PoseWarper_NIPS2019}, DCPose~\cite{DCPose_CVPR2021}, DetTrack~\cite{DetTrack_cvpr2020}, FAMI-Pose~\cite{FAMIPose_CVPR2022}, and our DSTA, consistently outperform those relying solely on a single key frame, such as HRNet~\cite{HRNet_CVPR2019}. This reaffirms the importance of incorporating temporal cues from adjacent frames.  Qualitative results are shown in Fig.~\ref{fig:qualitative_result}.


\begin{table}
\footnotesize
    \centering
    \setlength{\tabcolsep}{0.5mm}{
    \begin{tabular}{l|c|ccccccc|c}
       Method  & Bkbone & Head	 & Should. & Elbow	& Wrist & Hip & Knee & Ankle & \textbf{Mean} \\
       \hline
       \multicolumn{10}{l}{\textit{heatmap-based}}   \\
       \fontsize{6}{1}\selectfont PoseTrack~\cite{PoseTrack_cvpr2018} & \fontsize{6}{1}\selectfont ResNet-101 &  67.5	& 70.2	& 62.0	& 51.7 & 60.7 &	58.7 & 49.8 & 60.6 \\
       \fontsize{6}{1}\selectfont PoseFlow~\cite{PoseFlow_arXiv2018} & \fontsize{6}{1}\selectfont ResNet-152 & 66.7  & 73.3 & 	68.3 & 	61.1 & 	67.5 & 	67.0 & 	61.3 & 	66.5 \\
       \fontsize{6}{1}\selectfont FastPose~\cite{fastpose_arXiv2019} & \fontsize{6}{1}\selectfont ResNet-101 & 	80.0 & 	80.3 & 	69.5 & 	59.1 & 	71.4 & 	67.5 & 	59.4 & 	70.3 \\
       \fontsize{6}{1}\selectfont SimBase.~\cite{SimplePose_ECCV2018} & \fontsize{6}{1}\selectfont ResNet-152 & 	81.7 &	83.4 &	80.0 &	72.4 &	75.3 &	74.8 &	67.1 &	76.7 \\
       \fontsize{6}{1}\selectfont STEmbed.~\cite{STE_CVPR2019} & \fontsize{6}{1}\selectfont ResNet-152 &	83.8 &	81.6 &	77.1 &	70.0 &	77.4 &	74.5 &	70.8 &	77.0 \\
       \fontsize{6}{1}\selectfont HRNet~\cite{HRNet_CVPR2019} & \fontsize{6}{1}\selectfont HRNet-W48 &	82.1 &	83.6 &	80.4 &	73.3 &	75.5 &	75.3 &	68.5 & 77.3 \\
       \fontsize{6}{1}\selectfont MDPN~\cite{MDPN_ECCV-W2018} & \fontsize{6}{1}\selectfont ResNet-152 &	85.2 &	88.5 &	83.9 &	77.5 &	79.0 &	77.0 &	71.4 & 80.7 \\
       \fontsize{6}{1}\selectfont Dyn.-GNN~\cite{DynGNN_CVPR2021} & \fontsize{6}{1}\selectfont HRNet-W48 &	88.4 &	88.4 &	82.0 &	74.5 &	79.1 &	78.3 &	73.1 &	81.1 \\
       \fontsize{6}{1}\selectfont PoseWarp.~\cite{PoseWarper_NIPS2019} & \fontsize{6}{1}\selectfont HRNet-W48 & 81.4 &	88.3 &	83.9 &	78.0 &	82.4 &	80.5 &	73.6 &	81.2 \\
       \fontsize{6}{1}\selectfont DCPose~\cite{DCPose_CVPR2021} & \fontsize{6}{1}\selectfont HRNet-W48 &	88.0 &	88.7 &	84.1 &	78.4 &	83.0 &	81.4 &	74.2 &	82.8 \\
       \fontsize{6}{1}\selectfont DetTrack~\cite{DetTrack_cvpr2020} & \fontsize{6}{1}\selectfont HRNet-W48 &	89.4 &	89.7 &	85.5 &	79.5 &	82.4 &	80.8 &	76.4 &	83.8 \\
      \fontsize{6}{1}\selectfont FAMIPose~\cite{FAMIPose_CVPR2022} & \fontsize{6}{1}\selectfont HRNet-W48 &	89.6 &	90.1 &	\textbf{86.3} &	\textbf{80.0} &	84.6 &	\textbf{83.4} &	\textbf{77.0} &	\textbf{84.8} \\
       \hline
       \multicolumn{10}{l}{\textit{regression-based}} \\
       \fontsize{6}{1}\selectfont \textbf{DSTA (Ours)} & \fontsize{6}{1}\selectfont ResNet-152  &88.3 &	88.1 &	83.3 &	76.0 &	82.5 &	81.1 &	70.0 &	81.8 \\
      \fontsize{6}{1}\selectfont \textbf{DSTA (Ours)} &  \fontsize{6}{1}\selectfont HRNet-W48  &  \textbf{89.8} &	\textbf{90.8} &	\textbf{86.2} &	79.3 &	\textbf{85.2} &	82.2 & 75.9 & \textbf{84.6} \\
      
       \cellcolor{light-gray}\fontsize{6}{1}\selectfont \textbf{DSTA (Ours)} & \cellcolor{light-gray}\fontsize{6}{1}\selectfont ViT-H  &	\cellcolor{light-gray}89.3 &	\cellcolor{light-gray}90.6 &	\cellcolor{light-gray}87.3 &	\cellcolor{light-gray}82.6 &	\cellcolor{light-gray}84.5 &	\cellcolor{light-gray}85.1 & \cellcolor{light-gray}77.8 &	\cellcolor{light-gray}85.6 \\
    \end{tabular}}
    \vspace{-2mm}
    \caption{\textbf{Comparison with the SOTA} on PoseTrack2017 val. set. Similar to FAMI-Pose~\cite{FAMIPose_CVPR2022}, our proposed DSTA sets the temporal span $T$ to 2, consisting of two preceding and two subsequent frames, totalling four auxiliary frames.}
    \label{tab:sota_compare}
    %\vspace{-2mm}
\end{table}

We further evaluate our model on the PoseTrack2018 and PoseTrack21 datasets.
%, and the results on their validation sets are presented in Tables $\diamond$ and $\diamond$, respectively.
Due to space limitation, we present these results in the supplementary materials. 
Based on these results, it is evident that our approach either outperforms or is on par with the state-of-the-art heatmap-based methods. Using the HRNet-w48 backbone, we achieve \textbf{82.1} mAP and \textbf{82.0} mAP on these two datasets, respectively, while using the ViT-H backbone, we further improve performance by \textbf{1.3} and \textbf{1.5} points, respectively.

\begin{table}
\small
    \centering
    \setlength{\tabcolsep}{1mm}{
    \begin{tabular}{l|cccc}
       \multirow{2}{*}{Method}  &  \multirow{2}{*}{\#Params} &  GFLOPs   &  GFLOPs & \multirow{2}{*}{mAP} \\
       & & of Backbone & of Net. Head & \\
       \hline
       \multicolumn{5}{l}{\textit{heatmap-based}}   \\
       PoseWarper~\cite{PoseWarper_NIPS2019}  &  71.1M  &  35.5  &  156.7 & 81.0 \\
       DCPose~\cite{DCPose_CVPR2021}  & 65.2M   & 35.5  & 11.0  &  82.8 \\
       \hline
       \multicolumn{5}{l}{\textit{regression-based}} \\
       \textbf{DSTA (Ours)}  &  63.9M  &  35.5  & \textbf{0.02} & \textbf{83.4}  \\
    \end{tabular}}
    \vspace{-2mm}
    \caption{\textbf{Computation complexity} with HRNet-W48 backbone. \#Params includes the parameters of entire network.  All methods utilize the same two auxiliary frames as  in~\cite{DCPose_CVPR2021}.}
    \label{tab:complexity_compare}
    \vspace{-5mm}
\end{table}




\begin{table*}
\small
\parbox{.37\linewidth}{
    \centering
    \setlength{\tabcolsep}{1.5mm}{
    \begin{tabular}{c|c|cc|ccc}
       \multicolumn{2}{c|}{JFD}      &  w/o   & w/  & \checkmark  & \checkmark &  \checkmark  \\
       \hline
       \multirow{2}{*}{STD} & SD     &     &     &  \checkmark   &  & \checkmark  \\
        & TD                         &  &  &  & \checkmark & \checkmark  \\
        \hline
        \multicolumn{2}{c|}{mAP}     & 73.8 & 74.8 & 71.4 & 78.1 & \textbf{78.6} \\
    \end{tabular}}
    \caption{\textbf{Ablation of different modules} in DSTA.}
    \label{tab:impact_components}
}
\parbox{.25\linewidth}{
\centering
\setlength{\tabcolsep}{1mm}{
    \begin{tabular}{c|cc}
       JFD Method  &  MFLOPs &  mAP \\
       \hline
       \cite{Poseur_ECCV2022} (a) & 0.3	& 74.6   \\
       \cite{Poseur_ECCV2022} (b)  & 19.3	& 77.9 \\
       \textbf{Ours}  & 5.0 &	\textbf{78.6} \\
    \end{tabular}}
    \caption{\textbf{Different methods for constructing joint embeddings.}}
    \label{tab:jfd_choice}
}
\parbox{.38\linewidth}{
 \centering
    \setlength{\tabcolsep}{1mm}{
    \begin{tabular}{c|ccc}
       %\cline{2-4}
       \#Auxiliary Frame  & ResNet-50 & HRNet-W48 & ViT-H  \\
       \hline
       1 \{-1\} & 78.0 & 82.6 & 82.6 \\
      2 \{-1, +1\} &	78.6 & 83.4	& 84.3 \\
      4 \{-2, -1, +1, +2\} & \textbf{79.7}  & \textbf{84.6} &	\textbf{85.6} \\
    \end{tabular}}
    \vspace{-3mm}
    \caption{\textbf{Different number of auxiliary frames}. `-' indicates previous frames while `+' indicates subsequent frames.}
    \label{tab:t_selection}
}
\vspace{-3mm}
\end{table*}

\begin{table}
\small
    \centering
    \setlength{\tabcolsep}{2mm}{
    \begin{tabular}{l|cccc}
    \hline
       \multirow{2}{*}{Method}  &  \multicolumn{4}{c}{Input Size}\\
       & 384×288  & 256×192 & 128×128 & 64×64 \\
       \hline
       \multicolumn{5}{l}{\textit{heatmap-based}}   \\
       DCPose~\cite{DCPose_CVPR2021}  & 82.8   & 81.2  & 71.7  &  35.1 \\
       \hline
       \multicolumn{5}{l}{\textit{regression-based}} \\
       \textbf{DSTA (Ours)}  &  \textbf{83.4} & \textbf{82.3} & \textbf{77.9} & \textbf{55.4}  \\
    \end{tabular}}
    \vspace{-2mm}
    \caption{\textbf{Performance with different input resolutions}. Note that, as in~\cite{DCPose_CVPR2021}, only two auxiliary frames are used in DSTA.}
    \label{tab:resolution_compare}
    \vspace{-5mm}
\end{table}

\textbf{Computation Complexity.}
We conduct experiments to assess computation complexity using the PoseTrack2017 validation set, and the results are presented in Table~\ref{tab:complexity_compare}.
To ensure a fair comparison, all methods utilize the identical HRNet-W48 backbone and adopt the identical two auxiliary frames. Our proposed method outperforms heatmap-based methods while utilizing significantly lower computation complexity and fewer model parameters. The FLOPs of our regression-based head are an almost negligible \textbf{1/7835} or \textbf{1/550} of those heatmap-based heads. We encourage our readers to refer to the supplementary materials for additional comparisons using smaller backbones, \textit{i.e.}, ResNet and MobileNet. The computational superiority of our proposed regression framework is of great value in the industry, particularly for real-time video applications. 

\textbf{Gains on Low-resolution Input.} 
In practical applications, especially on some edge devices with limited computation resources, it is common to use low-resolution images for reduced computational cost.  To explore the robustness of our model under different input resolutions, we compare our method with heatmap-based DCPose~\cite{DCPose_CVPR2021} on the PoseTrack2017 validation set. As shown in Table~\ref{tab:resolution_compare}, our method consistently outperforms DCPose across all input sizes. The results also show that the performance of heatmap-based methods decreases significantly with low-resolution input. For example, at an input resolution of $64 \times 64$, our proposed method outperforms DCPose by \textbf{20.3} mAP.

%\subsubsection{Comparison of Visual Results}

\subsection{Ablation Study}
We conduct ablation experiments to analyze the influence of each component using the PoseTrack2017 validation set. The temporal span $T$ is set to 1, consisting of one preceding and one subsequent frames, totalling 2 auxiliary frames, and the ResNet-50 backbone is employed.

\textbf{Impact of different modules.} 
Table~\ref{tab:impact_components} lists the performance impact of each module of our approach. When we adopt global pose features instead of modeling the temporal dependency at the joint level, \textit{i.e.}, without the JFD and STD modules, the algorithm achieves an mAP of 73.8, decreasing \textbf{4.8} points. When capturing spatiotemporal relations based on joints using Eq.~\ref{eqn:simple_dependency}, \textit{i.e.}, Space-Time coupling, the accuracy reaches \textbf{74.8} mAP. It aligns with our assumption that modeling temporal dependency at the joint level, as opposed to the entire pose, is more appropriate.
Furthermore, when using the SD and TD modules to separately model the temporal dynamic dependencies and spatial structure dependencies, the algorithm achieves the highest accuracy of \textbf{78.6} mAP. It proves that the temporal dependencies of each joint should be individually captured, as every joint exhibits an  independent temporal trajectory.
Meanwhile, we can see that the TD module capturing temporal dependencies has a much greater impact (\textbf{78.1}) on overall performance than the SD module capturing spatial dependencies (\textbf{71.4}). We believe this is mainly because the feature token extracted by the JFD module for each joint already contains its spatial context information within the pose. This means the spatial structure information complemented by the SD module is limited.

\textbf{Choice of JFD.} As discussed in Sec.~\ref{sec:JFD}, 
~\cite{Poseur_ECCV2022} proposes an alternative approach to constructing feature embeddings for each joint. We compare our JFD module with this method in Table~\ref{tab:jfd_choice}, where ~\cite{Poseur_ECCV2022} (b) augments the feature embedding of each joint with relevant contextual features while~\cite{Poseur_ECCV2022} (a) does not. As can be seen, our approach improves accuracy by \textbf{4.0} points with a relatively small computational overhead, achieving the highest accuracy.

\textbf{Auxiliary Frames.} 
In addition, we investigate the impact of using different numbers of auxiliary frames. 
The results presented in Table~\ref{tab:t_selection} consistently demonstrate that increasing the number of auxiliary frames leads to improved performance across various backbone networks.
It aligns with our intuition that more auxiliary frames can provide more complementary information, thereby facilitating the enhancement of pose estimation  for the key frame.









