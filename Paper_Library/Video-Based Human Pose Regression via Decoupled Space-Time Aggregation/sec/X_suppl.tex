\clearpage
\setcounter{page}{1}
\maketitlesupplementary


\section*{Appendix}
\label{sec:appendix}
% 

In the supplementary material, we provide:\\\\
\noindent{\S}\textcolor[rgb]{1.00,0.00,0.00}{A} Additional Implementation Details. \\\\
\noindent{\S}\textcolor[rgb]{1.00,0.00,0.00}{B} Computation Complexity on More Backbones.\\\\
\noindent{\S}\textcolor[rgb]{1.00,0.00,0.00}{C} Experiments on PoseTrack2018/21 Datasets.\\\\
\noindent{\S}\textcolor[rgb]{1.00,0.00,0.00}{D} Additional Ablation Study. \\\\
\noindent{\S}\textcolor[rgb]{1.00,0.00,0.00}{E} Qualitative Results.

\section*{A. Additional Implementation Details}
\label{sec:appendix_impl_details}

\noindent{\textbf{Extracing Joint Embedding on HRNet.}} In the module of Joint-centric Feature Decoder (JFD), the feature embedding is extracted for each joint from the given global feature maps $\mathcal{F}_i(t')$ with $t'\in[t-T,t+T]$. 
In our implementation with HRNet backbones, specifically the HRNet-W48 variant, the high-resolution branch of the HRNet backbone is succeeded by a 1×1 convolutional layer (CONV) and a joint-wise fully connected feed-forward network (FFN) that encompasses a fully connected layer. The CONV layer consists of $n$ kernels, where $n$ represents the number of pose joints, resulting in a dedicated feature map for each joint. Subsequently, each individual feature map corresponding to a specific joint is flattened and fed into the shared FFN network, ultimately yielding its 32-bit feature embedding. \\\\
\noindent\textbf{Dataset.}
We evaluate our models on three widely-utilized video-based benchmarks for human pose estimation: PoseTrack2017~\cite{PoseTrack2017_CVPR2017}, PoseTrack2018~\cite{PoseTrack2018_CVPR2018}, and PoseTrack21~\cite{PoseTrack21_CVPR2022}. 
Specifically, PoseTrack2017 includes 250 video clips for training and 50 videos for validation , with a total of 80, 144 pose annotations. PoseTrack2018 considerably increases the number of clips, containing 593 videos for training, 170 videos for validation, and a total of 153, 615 pose annotations. Both datasets identify 15 keypoints, with an additional label for joint visibility. The training videos are densely annotated in the center 30 frames, and validation videos are additionally labeled every four frames. PoseTrack21 further enriches and refines PoseTrack2018 especially for annotations of small persons and persons in crowds, including 177, 164 human pose annotations. \\\\
\noindent\textbf{Optimization.} We incorporate data augmentation including random rotation [-45°, 45°], random scale [0.65, 1.35], truncation (half body), and flipping during training.  
We adopt the AdamW optimizer~\cite{Adam_ICTAI2019} to train the entire network for 40 epochs, with a base learning rate of 2\textit{e}-4, which is reduced by an order of magnitude at the 20\textit{th} and 30\textit{th} epochs. $\beta_1$ and $\beta_2$ are set to 0.9 and 0.999, respectively, and weight decay is set to 0.01. 

\begin{table}
\small
    \centering
    \setlength{\tabcolsep}{1mm}{
    \begin{tabular}{l|cccc}
       \multirow{2}{*}{Method}  &  \multirow{2}{*}{\#Params} &  GFLOPs   &  GFLOPs & \multirow{2}{*}{mAP} \\
       & & of Backbone & of Net. Head & \\
       \hline
       \multicolumn{5}{l}{\textit{heatmap-based}}   \\
       PoseWarper~\cite{PoseWarper_NIPS2019}  &  39.1M  &  4.1  &  90.3 & 75.9   \\
       DCPose~\cite{DCPose_CVPR2021}  & 35.6M   & 4.1  & 21.7 & 77.1 \\
       \hline
       \multicolumn{5}{l}{\textit{regression-based}} \\
       \textbf{DSTA (Ours)}  &  \textbf{24.6M}  &  4.1  & \textbf{0.01} & \textbf{78.6}  \\
    \end{tabular}}
    %\\vspace{-2mm}
    \caption{\textbf{Computation complexity} with ResNet-50 backbone. \#Params includes the parameters of entire network.  All methods utilize the same two auxiliary frames as  in~\cite{DCPose_CVPR2021}.}
    \label{tab:complexity_compare_resnet}
    %\vspace{-2mm}
\end{table}

\begin{table}
\small
    \centering
    \setlength{\tabcolsep}{1mm}{
    \begin{tabular}{l|cccc}
       \multirow{2}{*}{Method}  &  \multirow{2}{*}{\#Params} &  GFLOPs  &  GFLOPs & \multirow{2}{*}{mAP} \\
       & & of Backbone & of Net. Head & \\
       \hline
       \multicolumn{5}{l}{\textit{heatmap-based}}   \\
       PoseWarper~\cite{PoseWarper_NIPS2019}  &  14.8M  &  0.35  &  73.5 & 67.7   \\
       DCPose~\cite{DCPose_CVPR2021}  & 11.3M   & 0.35  & 4.9 & 68.8 \\
       \hline
       \multicolumn{5}{l}{\textit{regression-based}} \\
       \textbf{DSTA (Ours)}  &  \textbf{2.4M}  &  0.35  & \textbf{0.01} & \textbf{71.0}  \\
    \end{tabular}}
    %\\vspace{-2mm}
    \caption{\textbf{Computation complexity} with MobileNet-V2 backbone. \#Params includes the parameters of entire network.  All methods utilize the same two auxiliary frames as  in~\cite{DCPose_CVPR2021}.}
    \label{tab:complexity_compare_mobilenet}
    %\vspace{-2mm}
\end{table}


\section*{B. Computation Complexity on More Backbones}
\label{sec:appendix_computation complexity}
Tables~\ref{tab:complexity_compare_resnet} and ~\ref{tab:complexity_compare_mobilenet} present additional comparisons of computation complexity between our regression-based method and the heatmap-based methods. These experiments were conducted on the PoseTrack2017 validation set using the ResNet-50 and MobileNet-V2   backbones, respectively. 
While implementing the heatmap-based PoseWarper~\cite{PoseWarper_NIPS2019} and DCPose~\cite{DCPose_CVPR2021}, we utilized their official open-source codes.
In their network heads, similar to SimpleBase~\cite{SimplePose_ECCV2018}, we employed 3 deconvolution layers to generate high-resolution heatmaps from the backbones.


As shown, our method outperforms heatmap-based methods in both backbones, while utilizing significantly lower computation complexity and fewer model parameters. In addition, when compared to the HRNet backbone's results presented in Table~\ref{tab:complexity_compare} of the main paper, our method achieves even greater savings in computational costs and model parameters on these smaller backbone networks.
For instance, when utilizing the MobileNet-V2 backbone, our regression-based network incorporates a mere \textbf{2.4} million parameters, whereas the heatmap-based networks demand a significantly higher number, specifically 14.8 million and 11.3 million parameters. On the other hand, when employing the ResNet-50 backbone, the FLOPs of our regression-based head are almost negligible, accounting for just \textbf{1/9030} or \textbf{1/2170} of those required by the heatmap-based heads. The superior computational and storage efficiency of our proposed regression framework holds immense value in the industry, especially for edge devices and real-time video applications. 

\section*{C. Experiments on PoseTrack2018/21 Datasets}
Tables~\ref{tab:sota_compare_posetrack2018} and \ref{tab:sota_compare_posetrack21} present the comparisons of our method with the state-of-the-art methods on the PoseTrack2018 and PoseTrack21 valdation sets, respectively.
These results further demonstrate that our proposed regression-based method achieves performance that is either superior to, or at the very least, on par with the state-of-the-art heatmap-based methods. 

\begin{table}
\footnotesize
    \centering
    \setlength{\tabcolsep}{0.5mm}{
    \begin{tabular}{l|c|ccccccc|c}
       Method  & Bkbone & Head	 & Should. & Elbow	& Wrist & Hip & Knee & Ankle & \textbf{Mean} \\
       \hline
       \multicolumn{10}{l}{\textit{heatmap-based}}   \\
       \fontsize{6}{1}\selectfont AlphaPose~\cite{AlphaPose_ICCV2017} & \fontsize{6}{1}\selectfont ResNet-50 & 63.9 &	78.7 &	77.4 &	71.0 &	73.7 &	73.0 &	69.7 &	71.9  \\
       \fontsize{6}{1}\selectfont MDPN~\cite{MDPN_ECCV-W2018} & \fontsize{6}{1}\selectfont ResNet-152&	75.4 &	81.2 &	79.0 &	74.1 &	72.4 &	73.0 &	69.9 &	75.0 \\
       \fontsize{6}{1}\selectfont Dyn.-GNN~\cite{DynGNN_CVPR2021} & \fontsize{6}{1}\selectfont HRNet-W48&80.6 &	84.5 &	80.6 &	74.4 &	75.0 &	76.7 &	71.8 &	77.9 \\
       \fontsize{6}{1}\selectfont PoseWarp.~\cite{PoseWarper_NIPS2019} & \fontsize{6}{1}\selectfont HRNet-W48 &	79.9 &	86.3 &	82.4 &	77.5 &	79.8 &	78.8 &	73.2 &	79.7 \\
       \fontsize{6}{1}\selectfont PT-CPN++~\cite{PTCPN_ECCVW2018} &\fontsize{6}{1}\selectfont CPN~\cite{CPN_CVPR2018}&	82.4 &	\textbf{88.8} &	\textbf{86.2} &	\textbf{79.4} &	72.0 &	80.6 &	\textbf{76.2} &	80.9 \\
       \fontsize{6}{1}\selectfont DCPose~\cite{DCPose_CVPR2021} & \fontsize{6}{1}\selectfont HRNet-W48 &	84.0 &	86.6 &	82.7 &	78.0 &	80.4 &	79.3 &	73.8 &	80.9 \\
       \fontsize{6}{1}\selectfont DetTrack~\cite{DetTrack_cvpr2020} & \fontsize{6}{1}\selectfont HRNet-W48 &	84.9 &	87.4 &	84.8 &	79.2 &	77.6 &	79.7 &	75.3 &	81.5 \\
      \fontsize{6}{1}\selectfont FAMIPose~\cite{FAMIPose_CVPR2022} & \fontsize{6}{1}\selectfont HRNet-W48 &	85.5 &	87.7 &	84.2 &	79.2 &	81.4 &	\textbf{81.1} &	74.9 &	\textbf{82.2} \\
       \hline
       \multicolumn{10}{l}{\textit{regression-based}} \\
       \fontsize{6}{1}\selectfont \textbf{DSTA (Ours)} & \fontsize{6}{1}\selectfont ResNet-152  &	85.2 &	87.1 &	80.5 &	74.4 &	79.6 &	78.0 &	69.7 &	79.6  \\
      \fontsize{6}{1}\selectfont \textbf{DSTA (Ours)} &  \fontsize{6}{1}\selectfont HRNet-W48  &  \textbf{86.2} &	\textbf{88.6} &	84.2 &	78.5 &	\textbf{82.0} &	79.2 & 73.7 & \textbf{82.1} \\
      
       \cellcolor{light-gray}\fontsize{6}{1}\selectfont \textbf{DSTA (Ours)} & \cellcolor{light-gray}\fontsize{6}{1}\selectfont ViT-H  &	\cellcolor{light-gray}85.9 &	\cellcolor{light-gray}88.8 &	\cellcolor{light-gray}85.0 &	\cellcolor{light-gray}81.1 &	\cellcolor{light-gray}81.5 &	\cellcolor{light-gray}83.0 & \cellcolor{light-gray}77.4 &	\cellcolor{light-gray}83.4 \\
    \end{tabular}}
    %\\vspace{-2mm}
    \caption{\textbf{Comparison with the SOTA} on PoseTrack2018 val. set. Similar to FAMI-Pose~\cite{FAMIPose_CVPR2022}, our proposed DSTA sets the temporal span $T$ to 2, consisting of two preceding and two subsequent frames, totalling four auxiliary frames.}
    \label{tab:sota_compare_posetrack2018}
    %\vspace{-5mm}
\end{table}

\begin{table}
\footnotesize
    \centering
    \setlength{\tabcolsep}{0.5mm}{
    \begin{tabular}{l|c|ccccccc|c}
       Method  & Bkbone & Head	 & Should. & Elbow	& Wrist & Hip & Knee & Ankle & \textbf{Mean} \\
       \hline
       \multicolumn{10}{l}{\textit{heatmap-based}}   \\
       \fontsize{6}{1}\selectfont SimBase.~\cite{SimplePose_ECCV2018} & \fontsize{6}{1}\selectfont ResNet-152 & 	80.5 &	81.2 &	73.2 &	64.8 &	73.9 &	72.7 &	67.7 &	73.9 \\
       \fontsize{6}{1}\selectfont HRNet~\cite{HRNet_CVPR2019} & \fontsize{6}{1}\selectfont HRNet-W48 &	81.5 &	83.2 &	81.1 &	75.4 &	79.2 &	77.8 &	71.9 &	78.8 \\
       \fontsize{6}{1}\selectfont PoseWarp.~\cite{PoseWarper_NIPS2019} & \fontsize{6}{1}\selectfont HRNet-W48 &82.3 &	84.0 &82.2 &	75.5& 	80.7 &	78.7 &	71.6 &	79.5 \\
       \fontsize{6}{1}\selectfont DCPose~\cite{DCPose_CVPR2021} & \fontsize{6}{1}\selectfont HRNet-W48 &	83.7 &	84.4 &	82.6 &	\textbf{78.7} &	80.1 &	79.8 &	74.4 &	80.7 \\
      \fontsize{6}{1}\selectfont FAMIPose~\cite{FAMIPose_CVPR2022} & \fontsize{6}{1}\selectfont HRNet-W48 &	83.3 &	85.4 &	82.9 &	78.6 &	81.3 &	\textbf{80.5} &	\textbf{75.3} &	81.2 \\
       \hline
       \multicolumn{10}{l}{\textit{regression-based}} \\
       \fontsize{6}{1}\selectfont \textbf{DSTA (Ours)} & \fontsize{6}{1}\selectfont ResNet-152  &	86.1 &	85.5 &	80.0 &	74.6 &	80.5 &	76.9 &	70.2 &	79.6  \\
      \fontsize{6}{1}\selectfont \textbf{DSTA (Ours)} &  \fontsize{6}{1}\selectfont HRNet-W48  &  \textbf{87.5} &	\textbf{86.6} &	\textbf{83.3} &	\textbf{78.7} &	\textbf{82.7} &	78.3 &	73.9 &	\textbf{82.0} \\
      
       \cellcolor{light-gray}\fontsize{6}{1}\selectfont \textbf{DSTA (Ours)} & \cellcolor{light-gray}\fontsize{6}{1}\selectfont ViT-H  &	\cellcolor{light-gray}87.5 &	\cellcolor{light-gray}87.0 &	\cellcolor{light-gray}84.2 &	\cellcolor{light-gray}81.4 &	\cellcolor{light-gray}82.3 &	\cellcolor{light-gray}82.5 &	\cellcolor{light-gray}77.7 &	\cellcolor{light-gray}83.5 \\
    \end{tabular}}
    %\\vspace{-2mm}
    \caption{\textbf{Comparison with the SOTA} on PoseTrack21 val. set. Similar to FAMI-Pose~\cite{FAMIPose_CVPR2022}, our proposed DSTA sets the temporal span $T$ to 2, consisting of two preceding and two subsequent frames, totalling four auxiliary frames.}
    \label{tab:sota_compare_posetrack21}
    %\vspace{-5mm}
\end{table}


\section*{D. Additional Ablation Study}
\textbf{Size of Joint Tokens.} In this additional study, we conduct experiemnts to examine the influence of the adopted size of the joints' feature embedding (\textit{i.e.}, joint token). Table~\ref{tab:tokensize} presents the performance variations resulting from different joint token sizes on the PoseTrack2017 validation set. As the size of the joint tokens increases, a gradual improvement in performance can be observed. However, beyond a size of 16, the performance tends to plateau, suggesting that further increases in token size do not yield commensurate improvements.
This indicates that each pose joint requires a sufficiently large feature token to store its relevant feature information, but a too large feature token will only cause spatial redundancy. Therefore, in our experiments, we have opted to use a token size of 32, striking a balance between capturing sufficient feature information and avoiding unnecessary spatial redundancy.

\begin{table}
    \centering
    \begin{tabular}{c|cccc}
       %\cline{2-4}
       \#Token Size  & 8 & 16 & 32 & 64  \\
       \hline
       mAP &  76.1 & 78.0 & 78.6 & 78.6 \\
    \end{tabular}
    %\vspace{-2mm}
    \caption{\textbf{Different sizes of joint tokens}. In the experimental setup, we utilized the ResNet-50 backbone along with two auxiliary frames.}
    \label{tab:tokensize}
\end{table}


\section*{E. Qualitative Results}
Additional qualitative results on PoseTrack datasets are shown in Fig.~\ref{fig:qualitative_result_supplementary}. Additional results can be found in the accompanying video material.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{qualitative_result_supplementary.jpg}
    \caption{\textbf{Additional qualitative results} of our DSTA on the PoseTrack datasets.}
    \label{fig:qualitative_result_supplementary}
\end{figure}