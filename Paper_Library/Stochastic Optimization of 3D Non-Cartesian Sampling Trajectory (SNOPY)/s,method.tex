\begin{figure*}
\centerline{\includegraphics[width=0.8\textwidth]{figure/fig,workflow.png}}
\caption{~Diagram of SNOPY.
The sampling trajectory (and possibly reconstruction parameters)
are updated using gradient methods.
The training/optimization uses the differentiable programming approach
to get the gradient required in the update.
\label{fig:workflow}}
\end{figure*}

This section describes the proposed gradient-based methods for 
trajectory optimization.
We use the concept of differentiable programming
to compute the Jacobian/gradient
w.r.t. sampling trajectories required
in the gradient-based methods.
The sampling trajectory and reconstruction parameters
are differentiable parameters,
whose gradients can be calculated
by auto-differentiation or chain rule.
To learn/optimize these parameters,
one may apply (stochastic) gradient descent-like algorithms.
\fref{fig:workflow} illustrates the basic idea.
Here the sampling trajectory can also be jointly optimized with
the parameters of a learnable reconstruction algorithm,
so that the learned sampling trajectory and reconstruction method
are in synergy and can produce high-quality images.
The SNOPY algorithm combines
several optimization objectives,
to ensure that the optimized sampling trajectories 
have desired properties.
\ref{subsec:obj} delineates these objective functions.
\ref{subsec:param} shows that the proposed method
is applicable to multiple scenarios with
different parameterization strategies.
For non-Cartesian sampling,
the system model usually involves non-uniform fast Fourier transforms (NUFFT).
\ref{subsec:jacob} briefly describes an
efficient and accurate way to calculate the gradient involving NUFFTs.
\ref{subsec:efficient}
suggests several engineering approaches to 
make this large-scale optimization problem practical and efficient.



\input{s,obj}

\subsection{Reconstruction}
In \eqref{eqn:image},
the reconstruction algorithm $f_{\thta, \om}(\cdot)$
can be various algorithms.
Consider a typical cost function
for regularized MR image reconstruction
\begin{equation}
\label{recon}
\xh = \argmin_{\x} \|\A(\om)\x-\y\|_{2}^2 + \mathcal{R}(\x).
\end{equation}
$\mathcal{R}(\x)$ here can be a Tikhonov regularization $\lambda \|\x\|_2^2$
(CG-SENSE \cite{maier:2021:CGSENSERevisitedResults}),
a sparsity penalty $\lambda \|\T\x\|_1$ 
(compressed sensing \cite{lustig:2008:CompressedSensingMRI}, $\T$ is a finite-difference operator),
a roughness penalty $\lambda \|\T\x\|_2^2$ 
(penalized least squares, PLS),
or a neural network (model-based deep learning, MoDL \cite{modl}).
The Results section shows that different reconstruction algorithms
lead to distinct optimized sampling trajectories.

To get a reconstruction estimation $\xh$, 
one may use corresponding iterative reconstruction algorithms.
Specifically, the algorithm should be step-wise
differentiable (or sub-differentiable)
to enable differentiable programming.
The backpropagation
uses the chain rule to traverse 
every step of the iterative algorithm
to calculate the gradient w.r.t. variables
such as \om.



\subsection{Parameterization}
\label{subsec:param}
\input{s,param}



\subsection{Efficient and accurate Jacobian calculation}
\label{subsec:jacob}
\input{s,jacob}

\subsection{Efficient optimization}
\label{subsec:efficient}
\input{s,efficient}
