\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{1.45pt}%
    \renewcommand{\arraystretch}{1.1}%
    \scalebox{0.75}{%
    \begin{NiceTabular}{lc*{14}{c}}[cell-space-limits=2pt]
    \CodeBefore
      \rectanglecolor{metabg}{8-2}{8-16}
      \rectanglecolor{metabg}{14-2}{14-16}
    \Body
    \toprule
      \Block{2-1}{\small{\textbfrm{Task}}\\\small{\textbfrm{type}}} &
      \Block{2-1}{{$\tau(t)$}} &
      \Block{1-2}{\textbfrm{\footnotesize{Best single agent}}} & &
      \Block{1-2}{\textbfrm{\textsc{wtp}}} & &
      \Block{1-2}{\textbfrm{\textsc{carrot}}} & &
      \Block{1-2}{\footnotesize{\textbfrm{\textsc{TO}-Router}}} & &
      \Block{1-2}{\textbfrm{\footnotesize{FrugalGPT}}} & &
      \Block{1-2}{\textbfrm{\textsc{sale} \footnotesize{w/o memory}}} & &
      \Block{1-2}{\textbfrm{\textsc{sale}}} & \\
      \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \cmidrule(lr){13-14} \cmidrule(lr){15-16}
      & & \small{Pass@1($\uparrow$)} & \small{$\$$/Mt($\downarrow$)}
        & \small{Pass@1($\uparrow$)} & \small{$\$$/Mt($\downarrow$)}
        & \small{Pass@1($\uparrow$)} & \small{$\$$/Mt($\downarrow$)}
        & \small{Pass@1($\uparrow$)} & \small{$\$$/Mt($\downarrow$)}
        & \small{Pass@1($\uparrow$)} & \small{$\$$/Mt($\downarrow$)}
        & \small{Pass@1($\uparrow$)} & \small{$\$$/Mt($\downarrow$)}
        & \small{Pass@1($\uparrow$)} & \small{$\$$/Mt($\downarrow$)} \\
    \midrule
    \Block{6-1}{\small{\textit{Deep}}\\\small{\textit{search}}} & $\le0.1\phantom{0}$
      & 87.5 & 0.36
      & 83.8 & 0.32
      & 85.0 & 0.27
      & 86.3 & 0.28
      & 86.3 & 0.47
      & \textbfrm{91.3} & 0.24
      & \textbfrm{91.3}$_{0.0}$ & \textbfrm{0.22}$_{0.01}$ \\
    & $\le0.5\phantom{0}$
      & 87.5 & 0.36
      & 86.3 & 0.33
      & 86.3 & 0.28
      & 86.3 & 0.32
      & 81.3 & 0.48
      & 87.5 & 0.24
      & \textbfrm{88.5}$_{0.5}$ & \textbfrm{0.22}$_{0.01}$ \\
    & $\le2.5\phantom{0}$
      & 68.8 & 0.36
      & 67.5 & 0.31
      & 66.3 & 0.29
      & 67.5 & 0.34
      & 66.3 & 0.53
      & 72.5 & 0.25
      & \textbfrm{73.5}$_{1.2}$ & \textbfrm{0.23}$_{0.01}$ \\
    & $\le12.5$
      & 32.9 & 0.36
      & 34.2 & 0.32
      & 29.3 & 0.29
      & 32.9 & 0.36
      & 30.5 & 0.50
      & 35.4 & 0.19
      & \textbfrm{37.1}$_{1.8}$ & \textbfrm{0.17}$_{0.01}$ \\
    & $\le60\phantom{.}\phantom{0}$
      & 12.5 & 0.36
      & 9.4 & 0.31
      & 9.4 & 0.32
      & 12.5 & 0.36
      & 12.5 & 0.60
      & 15.6 & 0.26
      & \textbfrm{16.3}$_{1.3}$ & \textbfrm{0.23}$_{0.02}$ \\
    & All
      & 63.8 & 0.36
      & 62.4 & 0.32
      & 61.3 & 0.28
      & 63.0 & 0.33
      & 61.0 & 0.51
      & 66.4 & 0.24
      & \textbfrm{67.3}$_{0.5}$ & \textbfrm{0.21}$_{0.00}$ \\
    \midrule
    \Block{6-1}{\small{\textit{Coding}}} & $\le0.1\phantom{0}$
      & 95.0 & 0.36
      & 93.8 & \textbfrm{0.16} 
      & 95.0 & 0.36
      & 95.0 & 0.36
      & 97.5 & 0.39
      & 97.5 & 0.22
      & \textbfrm{98.3}$_{1.0}$ & 0.18$_{0.00}$ \\
    & $\le0.5\phantom{0}$
      & 79.7 & 0.36
      & 76.0 & \textbfrm{0.15}
      & \textbfrm{82.3} & 0.25
      & 79.7 & 0.36
      & 69.6 & 0.61
      & \textbfrm{82.3} & 0.28
      & 82.0$_{0.5}$ & 0.27$_{0.01}$ \\
    & $\le2.5\phantom{0}$
      & 67.5 & 0.36
      & 60.0 & \textbfrm{0.15}
      & 60.0 & 0.26
      & 67.5 & 0.36
      & 56.3 & 0.61
      & 68.8 & 0.31
      & \textbfrm{69.0}$_{0.5}$ & 0.29$_{0.00}$ \\
    & $\le12.5$
      & 27.2 & 0.36
      & 14.8 & \textbfrm{0.05}
      & 27.2 & 0.36
      & 27.2 & 0.36
      & 18.5 & 0.61
      & 27.2 & 0.32
      & \textbfrm{30.4}$_{2.2}$ & 0.30$_{0.02}$ \\
    & $\le60\phantom{.}\phantom{0}$
      & 22.8 & 0.36
      & 6.3 & \textbfrm{0.05}
      & 21.5 & 0.35
      & 22.8 & 0.36
      & 10.1 & 0.61
      & 24.1 & 0.31
      & \textbfrm{26.1}$_{2.4}$ & 0.29$_{0.01}$ \\
    & All
      & 58.4 & 0.36
      & 50.1 & \textbfrm{0.11}
      & 57.1 & 0.31
      & 58.4 & 0.36
      & 50.4 & 0.57
      & 59.9 & 0.27
      & \textbfrm{61.1}$_{0.6}$ & 0.27$_{0.00}$ \\
    \bottomrule
    \end{NiceTabular}
    }
    \caption{
    Deep search and coding performance (pass@1) and price per million tokens ($\$$/Mt) across task-complexity bins. We compare \textsc{sale} with the best single agent, the Willingness-to-Pay router (WTP), the TensorOpera Router (TO-Router), FrugalGPT, and an ablated variant of \textsc{sale} without memory-based self-refinement (\textsc{sale} w/o memory). For \textsc{sale}, we report five runs with distinct randomized test-set orders, with standard deviations shown as subscripts.}
    \label{tab:results}
\end{table*}



% \begin{table*}[t]
%     \centering
%     \setlength{\tabcolsep}{1.5pt}%
%     \renewcommand{\arraystretch}{1.0}%
%     \scalebox{0.80}{%
%     \begin{NiceTabular}{lc*{12}{c}}[cell-space-limits=2pt]
%     \CodeBefore
%       \rectanglecolor{metabg}{8-2}{8-14}
%       \rectanglecolor{metabg}{14-2}{14-14}
%     \Body
%     \toprule
%       \Block{2-1}{\textbfrm{Task}\\\textbfrm{type}} &
%       \Block{2-1}{{$\tau(t)$}} &
%       \Block{1-2}{\textbfrm{\small{Best single agent}}} & &
%       \Block{1-2}{\textbfrm{\textsc{wtp}-\footnotesize{\textsc{knn}}}} & &
%       \Block{1-2}{\textbfrm{\textsc{wtp}-\footnotesize{\textsc{mlp}}}} & &
%       \Block{1-2}{\footnotesize{\textbfrm{\textsc{TO}-Router}}} & &
%       \Block{1-2}{\textbfrm{\textsc{sale} \small{w/o memory}}} & &
%       \Block{1-2}{\textbfrm{\textsc{sale}}} & \\
%       \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \cmidrule(lr){13-14}
%       & & Pass@1($\uparrow$) & $\$$/Mt($\downarrow$)
%         & Pass@1($\uparrow$) & $\$$/Mt($\downarrow$)
%         & Pass@1($\uparrow$) & $\$$/Mt($\downarrow$)
%         & Pass@1($\uparrow$) & $\$$/Mt($\downarrow$)
%         & Pass@1($\uparrow$) & $\$$/Mt($\downarrow$)
%         & Pass@1($\uparrow$) & $\$$/Mt($\downarrow$) \\
%     \midrule
%     \Block{6-1}{\textit{Deep}\\\textit{search}} & $\le0.1\phantom{0}$
%       & 87.5 & 0.36
%       & 87.5 & 0.33
%       & 87.5 & 0.34
%       & 86.3 & 0.28
%       & \textbfrm{91.3} & 0.24
%       & \textbfrm{91.3}$_{0.0}$ & \textbfrm{0.22}$_{0.01}$ \\
%     & $\le0.5\phantom{0}$
%       & 87.5 & 0.36
%       & 85.0 & 0.33
%       & 87.5 & 0.34
%       & 86.3 & 0.32
%       & 87.5 & 0.24
%       & \textbfrm{88.5}$_{0.5}$ & \textbfrm{0.22}$_{0.01}$ \\
%     & $\le2.5\phantom{0}$
%       & 68.8 & 0.36
%       & 68.8 & 0.32
%       & 68.8 & 0.35
%       & 67.5 & 0.34
%       & 72.5 & 0.25
%       & \textbfrm{73.5}$_{1.2}$ & \textbfrm{0.23}$_{0.01}$ \\
%     & $\le12.5$
%       & 32.9 & 0.36
%       & 31.7 & 0.33
%       & 34.2 & 0.34
%       & 32.9 & 0.36
%       & 35.4 & 0.19
%       & \textbfrm{37.1}$_{1.8}$ & \textbfrm{0.17}$_{0.01}$ \\
%     & $\le60\phantom{.}\phantom{0}$
%       & 12.5 & 0.36
%       & 3.1 & 0.25
%       & 9.4 & 0.31
%       & 12.5 & 0.36
%       & 15.6 & 0.26
%       & \textbfrm{16.3}$_{1.3}$ & \textbfrm{0.23}$_{0.02}$ \\
%     & All
%       & 63.8 & 0.36
%       & 62.2 & 0.32
%       & 63.8 & 0.34
%       & 63.0 & 0.33
%       & 66.4 & 0.24
%       & \textbfrm{67.3}$_{0.5}$ & \textbfrm{0.21}$_{0.00}$ \\
%     \midrule
%     \Block{6-1}{\textit{Coding}} & $\le0.1\phantom{0}$
%       & 95.0 & 0.36
%       & 95.0 & \textbfrm{0.13}
%       & 95.0 & 0.19 
%       & 95.0 & 0.36
%       & {97.5} & 0.22
%       & \textbfrm{98.3}$_{1.0}$ & 0.18$_{0.00}$ \\
%     & $\le0.5\phantom{0}$
%       & 79.7 & 0.36
%       & 74.7 & 0.16
%       & 75.9 & \textbfrm{0.15}
%       & 79.7 & 0.36
%       & \textbfrm{82.3} & 0.28
%       & {82.0}$_{0.5}$ & 0.27$_{0.01}$ \\
%     & $\le2.5\phantom{0}$
%       & 67.5 & 0.36
%       & 60.0 & 0.16
%       & 52.5 & \textbfrm{0.15}
%       & 67.5 & 0.36
%       & 68.8 & 0.31
%       & \textbfrm{69.0}$_{0.5}$ & 0.29$_{0.00}$ \\
%     & $\le12.5$
%       & 27.2 & 0.36
%       & 22.0 & \textbfrm{0.14}
%       & 25.9 & 0.27
%       & 27.2 & 0.36
%       & 27.2 & 0.32
%       & \textbfrm{30.4}$_{2.2}$ & 0.30$_{0.02}$ \\
%     & $\le60\phantom{.}\phantom{0}$
%       & 22.8 & 0.36
%       & 13.9  & \textbfrm{0.14}
%       & 16.5 & 0.28
%       & 22.8 & 0.36
%       & 24.1 & 0.31
%       & \textbfrm{26.1}$_{2.4}$ & 0.29$_{0.01}$ \\
%     & All
%       & 58.4 & 0.36
%       & 53.1 & \textbfrm{0.15}
%       & 53.9 & 0.21
%       & 58.4 & 0.36
%       & 59.9 & 0.27
%       & \textbfrm{61.1}$_{0.6}$ & 0.27$_{0.00}$ \\
%     \bottomrule
%     \end{NiceTabular}
%     }
%     \caption{
%     Deep search and coding performance (pass@1) and price per million tokens ($\$$/Mt) across task-complexity bins. We compare \textsc{sale} with the best single agent, the Willingness-to-Pay (WTP) router instantiated with KNN and MLP backends, the TO-Router, and an ablated variant of \textsc{sale} without memory-based self-refinement (\textsc{sale} w/o memory). For \textsc{sale}, we report five runs with distinct randomized test-set orders, with standard deviations shown as subscripts.}
%     \label{tab:results}
% \end{table*}