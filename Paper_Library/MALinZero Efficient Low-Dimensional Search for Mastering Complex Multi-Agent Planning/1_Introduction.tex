\section{Introduction}


Monte Carlo Tree Search (MCTS) has demonstrated great performance in solving complex planning problems such as game playing ~\cite{MCTSgame_playing}, robotic control ~\cite{MCTS_robotics}, and optimization~\cite{MCTS_optimization}. It achieves much higher data efficiency than value- or policy-based reinforcement learning (RL)~\cite{muzero} by leveraging Upper Confidence Bound for Trees (UCTs) to balance exploration and exploitation through randomized sampling and cumulative regret minimization~\cite{uct}. Integrated with deep learning (e.g., AlphaZero~\cite{alphazero} and MuZero~\cite{muzero}), MCTS algorithms have achieved groundbreaking results in solving complex games, such as Go, Chess and Shogi~\cite{muzero}, relying on little knowledge of domain expertise or game rules.


However, for planning problems involving multiple agents, MCTS is confronted with a large combinatorial action space that often grows exponentially with the number of agents~\cite{maddpg,marl_sruvey,li2024crowdsensing,zhang2025learning}. As the number of candidate actions increases, the branching factor of MCTS (during tree expansion) also increases exponentially, making it very difficult to efficiently explore and exploit during tree search~\cite{kwak2024efficientmontecarlotree,MAZero}. Existing works either focus on single-agent problems or limit tree search to a small set of state-conditioned action abstractions~\cite{kwak2024efficientmontecarlotree,painter2024montecarlotreesearch, chen2025perception}.

As a result, MCTS can get stuck in local optima or become slow to explore optimal actions. Recent proposals like MAZero~\cite{MAZero} facilitate distributed representation of state transitions and reward prediction in multi-agent MCTS, but again do not address the challenges relating to the combinatorial action space in multi-agent planning.


In this work, we propose MALinZero, a new approach to leverage low-dimensional representational structures and enable efficient MCTS in complex cooperative multi-agent planning. The main idea of MALinZero is to model the joint returns through a low-dimensional linear combination of the (latent) per-agent action rewards. Thus, by observing the joint returns resulted from multi-agent actions, we can formulate a contextual linear bandit problem~\cite{contextual_bandit} -- with the per-agent action rewards as an unknown parameter vector $\theta$ -- and derive a linear Upper Confidence Bound applied to trees (LinUCT), to enable novel LinUCT-based exploration and exploitation in this low-dimensional space of (latent) per-agent action rewards. The idea of enforcing representational structures on joint returns has been instrumental in multi-agent reinforcement learning (MARL), e.g., VDN~\cite{sunehag2017valuedecompositionnetworkscooperativemultiagent} with linear representations, and QMIX~\cite{QMIXmixmab}, NDQ~\cite{wang2020learningnearlydecomposablevalue}, and PAC~\cite{zhou2023pacassistedvaluefactorisation} with monotonic representations, as well as policy factorizations like DOP~\cite{DOP} and FOP~\cite{FOP}. 

However, these MARL results do not apply to multi-agent MCTS, which requires not only factorized action-values but also the use of concentration inequalities~\cite{concentration_inequality} to bound their probability distributions given observed samples, like in our LinUCT.


For a planning problem with $n$ agents and $d$ actions per agent, MALinZero effectively reduces the tree search from considering $d^n$ independent joint-action returns to learning $nd$ latent per-agent action rewards. Our solution can be viewed as projecting the returns into the low-dimensional space represented by MALinZero using a contextual linear bandit problem formulation~\cite{contextual_bandit, linucb}. To mitigate the potential representational limitations, we further introduce a strongly-convex, $\mu$-smooth distance measure $f$ into the projection (as a new contextual bandit loss), in order to place more importance on not underestimating the better joint actions, while not overestimating the less attractive joint actions~\cite{rashid2020weightedqmixexpandingmonotonic, fang2024learning}. We solve the resulting contextual linear bandit problem with this convex loss and prove that our LinUCT achieves an cumulative regret of $\hat{R}_T = O\bigl(nd\cdot\sqrt{ \mu T}\cdot \ln(T)\bigr)$ after $T$ steps for low-dimensional rewards. We further show that the joint action selection problem in our MALinZero is a maximization of a submodular objective and can be solved using an $(1-\tfrac1e)$-approximation algorithm. MALinZero achieves state-of-the-art performance in our evaluations on matrix games, SMAC~\cite{samvelyan19smac}, and SMACv2~\cite{ellis2023smacv2}, by enabling multi-agents MSCT via low-dimensional representations.

The primary contributions of this paper are as follows:
\vspace{-0.1in}
\begin{itemize}
    \item We propose MALinZero to leverage low-dimensional representational structures on joint-action returns and enable efficient MCTS in complex multi-agent planning.
    \vspace{-0.03in}
    \item We solve the resulting contextual linear bandit problem with a convex loss function and derive a novel LinUCT to facilitate exploration and exploitation in low-dimensional space.
    \vspace{-0.03in}
    \item We analyze the regret of MALinZero for low-dimensional rewards and proposes an $(1-\tfrac1e)$-approximation algorithm for joint action selection via a submodular maximization.
     \vspace{-0.03in}
    \item MALinZero demonstrates state-of-the-art performance on multi-agent planning benchmarks such as MatGame, SMAC, and SMACv2, outperforming both multi-agent RL and MCTS baselines in terms of faster learning speed and better performance.
\end{itemize}


