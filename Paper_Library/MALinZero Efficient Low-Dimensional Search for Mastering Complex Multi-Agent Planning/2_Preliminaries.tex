\section{Related Works and Background} \label{bg}


Multi-agent planning with joint rewards can be modeled as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP)\cite{tang2023edge, yu2025look, chen2023hierarchical}, as a tuple \(\left(\mathcal{I}, \mathcal{S}, \{\mathcal{A}\}_{i\in \mathcal{I}}, P, R, \{\Omega\}_{i\in\mathcal{I}}, \{\mathcal{O}\}_{i\in\mathcal{I}}, \gamma\right)\) \citep{oliehoek2016concise, xu2023cnn}, where \(\mathcal{I}={1,2,\dots,n}\) is the set of \(n\) agents, \(\mathcal{S}\) the global state space, \(\mathcal{A}_i\) the action space of agent \(i\), \(P\) the state transition probability distribution, \(R\) the joint reward function, \(\Omega_i\) the individual observation space of agent \(i\), \(\mathcal{O}\) the global observation function and \(\gamma\) the discount factor to weigh future rewards \cite{fang2024coordinate, ravari2024adversarial, hong2025poster}. At times step \(t\), agent \(i\) gets state \(s_t\) thus acquiring local observation \(o^i_t = \mathcal{O}^i(s_t)\), then chooses action \(a_t\in\mathcal{A}_i\) based on the acquired local observation \(o^i_{\le t}\). Given a joint action \(\mathbf{a}_t = \left(a^1_t, \dots, a^N_t\right)\), the environment transits to the next state \(s_{t+1}\) and returns a reward \(r = R(s_t, \mathbf{a}_t)\). Agents aim to learn a joint policy \(\boldsymbol{\pi}\) that maximizes the expectation of discounted return \(E_{\boldsymbol{\pi}}\left[\sum^{\infty}_{t=0} \gamma^t r_t | a^i_t \sim \pi^i_t(\cdot|o^i_{\le t}), i=1,\dots,N\right]\) \cite{li2025sfmdiffusion}.

\paragraph{MARL with factorized representations.} Factorization-based methods have been commonly used to cope with the exponentially growing joint state-action space in MARL \cite{jiang2022intelligent, zhang2024modeling, mei2023mac, zhou2023every, chen2024deep}. Under the notion of Centralized Training and Decentralized Execution (CTDE), algorithms like VDN~\cite{sunehag2017valuedecompositionnetworkscooperativemultiagent} learn a centralized joint action-value function $Q_{\rm tot}$ through a linear combination of local per-agent value functions. This is further extended to monotonic representations in QMIX~\cite{QMIXmixmab}, nearly decentralized representations in NDQ~\cite{wang2020learningnearlydecomposablevalue}, and counterfactual predictions in PAC~\cite{zhou2023pacassistedvaluefactorisation}. Policy-based factorizations have also been considered in DOP~\cite{DOP} and FOP~\cite{FOP}. To mitigate potential representation limitation,  QTRAN~\cite{qtran} has considered adding state-value correction terms, while Weighted QMIX~\cite{QMIXmixmab} introduces importance weights on dominant state-actions. 

The idea of enforcing these representational structures has been instrumental in developing decentralized, scalable MARL algorithms. However, these factorized representations in MARL do not apply to multi-agent MCTS, which requires the use of concentration inequalities to bound the return distributions given observed samples, in order to balance exploration and exploitation. 



\paragraph{MCTS-based planning.}
MCTS is widely applied to solve planning problems through sequential decision-making~\cite{mcts, yu2025optimizing, li2023ecg, mei2024bayesian}. Efficient search for optimal actions in a large decision space has been one of the central problems in MCTS~\cite{chen2024bayesadaptivemontecarlo,MAZero,muzero, zhang2025lipschitz}. Existing works have leveraged Boltzmann policies~\cite{painter2024montecarlotreesearch} and state-conditioned action abstractions~\cite{kwak2024efficientmontecarlotree}. 

The problem becomes more pronounced in multi-agent MCTS, as the joint action space increases exponentially as the number of agents grows~\cite{scalablemcts, dalmasso2021human, skrynnik2024decentralized}, leading to significantly increased complexity in tree expansion and search. Recent approaches like MAZero~\cite{MAZero} have considered multi-agent MCTS, but only considered distributed representation of state transitions and reward prediction, without addressing the combinatorial action space in multi-agent planning.

MCTS typically involves four stages, i.e., \textit{Selection} to choose actions using UCB-like strategies~\cite{ucb}, \textit{Expansion} to add new child nodes, \textit{Simulation} to sample payoffs, and \textit{Back-Propagation} to propagate payoffs and update node returns. Model-based MCTS algorithms like MuZero \cite{muzero} learn a dynamic model to replace \textit{Simulation}, thus improving the planning efficiency. MuZero involves three key learnable models: a representation model $h_\theta$ to map the real environment into a latent space, a dynamics model $g_\theta$ that computes the next state and the reward of this transition, and a prediction model $f_\theta$ for value and policy approximation. Given the observation history $\mathbf{o}_{\le t}$ at time step $t$, the model maps the observation into a latent space as $\mathbf{s}_{t,0}=h_{\theta}(\mathbf{o}_{\le t})$, then unrolls $K$ steps and predicts the corresponding $\mathbf{s}_{t,k}, r_{t,k}=g_\theta(\mathbf{s}_{t, k-1})$ and ${v}_{t,k},\mathbf{p}_{t,k} = f_{\theta}(s_{t,k})$ for each hypothetical step $k$ with $k = 0, 1, \dots, K$. During \textit{Selection}, MuZero traverses from the root node and applies the probabilistic Upper Confidence Tree (pUCT) rule to select actions for node transitions until reaching the leaf node of the current tree: 
\begin{equation}
    a = \arg\max_{a \in {\mathcal{A}}} \;\;
     \Phi(s,a) + c(s) P(s,a) \frac{\sqrt{ \sum_{b} 
 N(s,b)}} {{N(s,a)+1}}
 \label{ucb}
\end{equation}
where $s$, $a$ and ${\mathcal{A}}$ are abbreviations for $\mathbf{s}_{t,k-1}$, ${a}_{t,k}$ and action set respectively. $\Phi(s,a)$ is the estimation for the real value of nodes, $N(s,a)$ denotes the visiting count, $P(s,a)$ is the prior probability of selecting \(a\) in $s$, and $c(s)$ is the coefficient balance exploitation and exploration. When the leaf node is reached, new nodes will be expanded to the tree, then $\Phi(s,a)$ and $N(s,a)$ of nodes in the search path will be updated. Specifically, $\Phi(s,a)$ is updated based on a cumulative discounted reward $G_{t,k} = \sum_{\tau=0}^{l-1-k} \gamma^{\tau} r_{k+1+\tau} + \gamma^{l-k} v^l$ for $k=0, 1, \dots, l$ where $l$ is the search depth and thus calculated as  $\Phi(s,a) = \frac{N(s,a) \cdot \Phi(s, a) + G_{t,k}}{N(s,a)+1}$.

Sampled MuZero \cite{sampled_muzero} extends MuZero into a sampling-based framework to tackle larger action spaces for which MuZero can not construct all possible states as nodes. In \textit{Expansion}, only a subset \(T(s)\) of the complete action space will be considered by Sampled MuZero according to the sampling policy \(\beta\) and prior policy \(\pi\). Then the sampled action will be selected by \( a = \arg\max_{a \in {\mathcal{A}}} \;\;
     \Phi(s,a) + c(s) \frac{\hat{\beta}}{\beta} P(s,a) \frac{\sqrt{ \sum_{b} 
 N(s,b)}} {{N(s,a)+1}}\), where \(\hat{\beta}\) is the empirical action distribution.


