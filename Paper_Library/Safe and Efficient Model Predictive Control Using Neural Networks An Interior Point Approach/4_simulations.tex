\section{Simulations} \label{sec:sims}

\subsection{Test systems}

%We simulate the proposed policy on two standard test systems used for studying parameteric MPC (see, e.g.,  \cite{Domahidi2011,Chen2018d,Zeilinger2011}). 
%The first is a double integrator system with $n = 2$ and $m = 1$. The system matrices and constraints are \begin{gather}
%    A = \m{1&1\\0&1},\ B = \m{0 \\1}, \\
%    \|x\|_\infty \leq 1,\ \|u\|_\infty \leq 1,\ \|d\|_\infty \leq 0.1 \label{eqn:2-27-7}.
%\end{gather} 

%A time horizon of $\tau=5$ is used. The interior point function $\pi_0$ is synthesized using \eqref{eqn:3-17-4}:
%\begin{gather*}
%    W = \m{-0.520 & -1.37},\ w = 0.0108.
%\end{gather*}

We simulate the proposed policy using a modified example from \cite{Zeilinger2011} with $n=3$, $m=2,$ and $\tau=5$. The system matrices, constraints, costs, and Phase I solution (found using~\eqref{eqn:3-17-4}) are given below:
\begin{gather}
    A = \m{-.5&.3&-1 \\.2&-.5&.6\\1&.6&-.6},\ B = \m{-.601&-.890\\.955&-.715\\.246&-.184}, \\
    \|x\|_\infty \leq 5,\ \|u\|_\infty \leq 1,\ \|d\|_\infty \leq 0.1 \label{eqn:2-27-8}, \\
    l(x,u) = \|x\|_2^2 + c_1\|u\|_2^2, \ l_F(x) = c_2\|x\|_2^2 \\
    W = \m{0.116 &  0.210 & -0.370 \\
       -0.320 & -0.104 & -0.122}, w = \m{-0.157 \\ -0.0533} \nonumber
\end{gather} where $c_1$ and $c_2$ are positive constants. Although quadratic costs are used in the simulations, the proposed method can work with any differentiable cost. 

\revision{We evaluate the performance of a given policy in both open- and closed-loop experiments. In the open-loop experiments, we evaluate the MPC cost \eqref{eqn:2-26-8} and compare it to the optimal cost. The fraction suboptimality is \begin{align}
    \delta = \frac{c_{nn} - c_{mpc}}{c_{mpc}} \label{eqn:5-25-22-1}
\end{align} where $c_{nn}$ is the average cost \eqref{eqn:2-26-8} incurred by the control sequence $\mu_\theta$ on a validation set $\{x_0^j\}_{j=1}^{N_{val}} \subset \S$ and $c_{mpc}$ is the optimal cost.}

\revision{In the closed-loop experiments, we evaluate the performance of a policy $\pi_\theta(x_t): \R^n \rightarrow \R^m, t \geq 0$ which is derived from $\mu_\theta(x_t)$ by taking the first action in the sequence. We simulate \eqref{eqn:2-26-5} for $T \gg \tau$ time steps.}
The trajectory cost in the closed-loop experiments is computed as 
    $\sum_{t = 0}^{T-1} l(x_t,u_t) + l_F(x_T)$ %where the stage and terminal costs are given by $
    %l(x_t,u_t) = \|x_t\|_2^2 + c_1\|u_t\|_2^2,\ l_F(x_T) = c_2 \|x_T\|_2^2
%$ with $c_1, c_2 > 0$. 
and the disturbance is modeled as an autoregressive sequence \cite{Srinath1995}, 
$d_{t+1} = \alpha d_t + (1-\alpha) \hat{d}$ %\label{eqn:3-19-1}$ 
where $\alpha \in (0,1)$ and $\hat{d}$ is drawn uniformly over $\D$.

\subsection{Benchmarks}

We compare the proposed method to two of the most common approaches for learning a solution to \eqref{eqn:2-26-7}. The first benchmark is a penalty-based approach \cite{Drgona2020} which enforces the constraints \eqref{eqn:2-26-10} and \eqref{eqn:2-27-9} by augmenting the cost \eqref{eqn:2-27-10} with a linear penalty term on constraint violations given by $\beta \cdot \max\{0,F_x x_t-\tilde{g}_x\}$ where the $\max$ is evaluated elementwise and $\beta > 0$. Since the penalty-based approach does not encode state constraints in the policy, the policy is constrained to the Cartesian product $\U^\tau = \prod_{k=0}^{\tau-1} \U$ using scaled $\tanh$ functions elementwise.

The second benchmark is a projection-based approach \cite{Chen2018d} which constrains the policy to the set $\F(x_0)$ by solving a convex quadratic program in the output layer of a neural network \cite{Agrawal2019}. % Instead of using $\tanh$ activations to clamp the output of a neural network to $\B_\infty$ and then using the gauge map to constrain the policy to $\F(x_t)$, the projection-based approach uses scaled $\tanh$ activations to clamp the neural network output $\hat{\textbf{v}}_t$ to the product set $\U^\tau$ and then projects the result onto $\F(x_t) \subseteq \U^\tau$ by solving the following projection:
The optimization layer $\textbf{v} \rightarrow \textbf{u}$ returns
\begin{gather*}
    \underset{\u}{\arg \min} \|\textbf{v} - \u\|_2^2
    \text{ subject to } \u \in \F(x_0).
\end{gather*}

Another class of approaches to learning-based MPC seeks to learn the optimal solution to \eqref{eqn:2-26-7} using regression \cite{Parisini1995, Domahidi2011,Maddalena2020}. Specifically, data-label pairs $(x_0,u_0^*)$ are generated by sampling $x_0$ from $\S$, solving \eqref{eqn:2-26-7} for each sample, and extracting $u_0^*$ from the optimal solution $\u^*$. Then, a neural network or other function approximator is trained to learn the relationship between $x_0$ and $u_0^*$. Performance and constraint satisfaction are handled e.g. by bounding the approximation error with respect to the MPC oracle. We do not compare against this type of approach since it requires a large number of trained samples, making it difficult to compare with our and the other unsupervised examples. 

\subsection{Neural network design}

\revision{The neural networks were designed with $n$ inputs, $m\tau$ outputs, and two hidden layers with rectified linear unit (ReLU) activation functions. The width of the networks was chosen during hyperparameter tuning. In particular, we performed 30 iterations of random search over the width of the network (number of neurons per hidden layer) $\in \{64,\ldots,1024\}$, the batch size (number of initial conditions, $M$) $\in \{100,\ldots,3000\}$ and the learning rate (LR, the step size for gradient descent) $\in [10^{-5},10^{-3}]$. For each set of hyperparameters under consideration, we computed the validation score using \eqref{eqn:5-25-22-1} with $N_{val}=100$. The hyperparameters after tuning are reported in Table \ref{table:hparams}.}

\begin{table}
\centering
\caption{Hyperparameters for the three neural networks.}
\begin{tabular}{ |c|c|c|c| } 
 \hline
 Type & Width & LR & $M$\\ 
 \hline
 Gauge & $859$ & $4.7 \times 10^{-4}$ & $1655$ \\%& 0.0070 & .0015\\ 
 Penalty & $318$ & $8.7 \times 10^{-4}$ & 133 \\%&0.0083 \\ 
 Projection & $956$ & $9.0 \times 10^{-5}$ & $813$ \\%& .010 & .024\\
 \hline
\end{tabular}
\label{table:hparams}
\end{table}

\subsection{Simulation results}
Here we compare our proposed approach (Gauge NN), the penalty-based approach (Penalty NN), the projection-based approach (Projection NN) and the ``ground truth'' obtained by solving  \eqref{eqn:2-26-7} online in \texttt{cvxpy}. \revision{The results of the open-loop experiments are shown in Table \ref{table:open_loop}, with performance computed relative to the optimal MPC solution using \eqref{eqn:5-25-22-1} with $N_{val}= 100$ trials. The proposed Gauge NN achieves lower cost compared to the projection-based method, and has a much lower computational complexity (solve time is only 6\% of projection).  Table \ref{table:open_loop} only compares the NNs with safety guarantees because constraint violations are not accounted for in \eqref{eqn:5-25-22-1}.} 

\begin{table}
\centering
\caption{Open-loop test results.}
\begin{tabular}{ |c|c|c| } 
 \hline
 Type & $\delta$ \eqref{eqn:5-25-22-1} & Solve time (sec)\\ 
 \hline
 Gauge & 0.007 & .0015\\ 
 Projection & 0.010 & .024\\
 \hline
\end{tabular}
\label{table:open_loop}
\end{table}

Figure \ref{fig:train} shows the training curves for each type of network. The lower training cost achieved by the Gauge NN illustrates that it can be more efficient to explore the interior of the feasible set than the boundary. \revision{Since the MPC cost in the simulations is strictly convex, solutions with lower cost are closer to the optimal solution.}
%Thus, a lower training cost directly corresponds to a shorter distance between $\mu_\theta$ and the optimal solution.} %The large initial costs incurred by the Penalty NN are due to constraint violations, which the network learns to avoid (albeit without guarantees). 

%\begin{figure}[b]
%    \centering
%    \includegraphics[height=4cm]{Figures/loss_2n1m_False_copy.png}
%    \hfill
%    \includegraphics[height=4cm]{Figures/loss_3n2m_False_copy.png}
%    \caption{Training trajectories for the 2-state system (left) and 3-state system (right). Our proposed Gauge-based approach achieves lower cost at a much faster rate.}
%    \label{fig:train}
%\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=8cm]{Figures/loss_3n2m_False_no_zoom.png}
    \caption{Training trajectories for the three types of neural netwokrs. Our proposed Gauge-based approach achieves lower cost at a much faster rate.}
    \label{fig:train}
\end{figure}

Figure \ref{fig:pareto_3} compares the policies in terms of computation time and test performance. The box-and-whisker plots indicate the range of performance over 100 test trajectories of length $T=50$, while the vertical position of each box indicates the average time to compute a control action. Of the policies with safety guarantees (Gauge NN, Projection NN, and online MPC), \revision{the Gauge NN achieves Pareto efficiency in terms of average solve time and median trajectory cost.} 
%Interestingly, the Gauge NN can achieve a lower median cost than the online MPC. 
Our intuition behind the high performance of the neural networks is that \eqref{eqn:2-26-7} is a heuristic and the unsupervised learning approach can lead to better closed-loop policies.
%there is no reason why solving an MPC problem on horizon $\tau$ should serve as a lower bound on trajectory cost when testing on the longer horizon $T$ with disturbances. With the right architecture and training algorithm, NNs can learn better policies. 

\begin{figure}
    \centering
    \includegraphics[width=8cm]{Figures/pareto.png}
    \caption{Solve time vs. trajectory cost for the networks under consideration applied to the 3-state system. The Gauge NN is Pareto-efficient in terms of cost and computation time compared to the other techniques with safety guarantees (Online MPC and Projection NN).}
    \label{fig:pareto_3}
\end{figure}


%\todo{Add a figure showing solve time vs. horizon length for NN and MPC and a table of solve times for $\tau=5$. Alternatively, I could try to put the box plot y-axis on a log scale. Thoughts?}

\ifx

\subsection{Extension to scenario-based MPC}
A drawback to both standard online MPC and neural network based methods is that they minimize the cost of a nominal trajectory, where the disturbance is not explicitly taken into account. 
% A possible explanation for the differences in performance shown in Figures \ref{fig:pareto_2} and \ref{fig:pareto_3} is that both the online MPC and the neural networks seek to minimize the cost of a nominal trajectory of length $\tau$, but are tested on a length-$T$ trajectory with disturbances. 
One way to address the issue of disturbances is via the \emph{scenario approach} \cite{Lucia2018}, in which the cost \eqref{eqn:2-26-8} is averaged over $N$ sequences of disturbances sampled from $\D$ according to \eqref{eqn:3-19-1}. The MPC \eqref{eqn:2-26-7} is replaced by \begin{subequations}
\begin{gather}
    \min_{\hat{\u}_t} \frac{1}{N} \sum_{i=1}^N \sum_{k=t}^{t+\tau-1} l(\hat{x}_k^i,\hat{u}_k) + l_F(\hat{x}_{t+\tau}^i) \\
    \text{s.t. }
    \hat{x}_{k+1}^i = A\hat{x}_k^i + B\hat{u}_k + d_k^i \ \forall\ i,k;\ \hat{x}_t^i = x_t  \label{eqn:3-20-1}\\
    \tilde{x}_{k+1} = A\tilde{x}_k + B\hat{u}_k \ \forall\ k;\ \tilde{x}_t = x_t  \label{eqn:3-21-2}\\
    \tilde{x}_{k+1} \in \T,\ \hat{u}_k \in \U \ \forall\ i,k. \label{eqn:3-21-3}
\end{gather} \end{subequations} For feasibility under any disturbance sequence, \eqref{eqn:3-21-3} constrains the nominal trajectory in \eqref{eqn:3-21-2} to the target set.
For the NN, the policy class \eqref{eqn:3-24-1} remains unchanged but the loss function \eqref{eqn:2-27-10} is averaged over scenarios, becoming \begin{align}
    J(\theta) = \frac{1}{MN} \sum_{i=1}^N \sum_{j=1}^M \sum_{k=0}^{\tau-1} l(\hat{x}_k^{i,j},\hat{u}_k^{j}) + l_F(\hat{x}_{\tau}^{i,j})
\end{align} with the controls $\hat{\u}_0^{j}$ given by $\pi_\theta(x_0^j)$ and trajectories by \eqref{eqn:3-20-1} for each scenario $i$ and each initial condition $j$.

Figure \ref{fig:pareto_s} shows that when noise is considered using the scenario approach, the MPC computation times increase sharply while the Gauge NN remains competitive in terms of median trajectory cost. The efficiency gained by taking an interior point perspective is multiplied when the designer wishes to account for disturbances using a scenario approach.

\begin{figure}
    \centering
    \includegraphics[width=8cm]{Figures/pareto_3n2m_True.png}
    \caption{Solve time vs. trajectory cost in the scenario-based setting, for the 3-state system. The computation time for online MPC increases dramatically, while Gauge NN achieves both lowest computation time and trajectory cost.}
    \label{fig:pareto_s}
\end{figure}

\fi