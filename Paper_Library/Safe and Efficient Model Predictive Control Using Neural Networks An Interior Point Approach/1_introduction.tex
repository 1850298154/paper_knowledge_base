\section{Introduction}

Model predictive control (MPC) \cite{Rawlings2019} is a powerful technique for controlling systems that are subject to state and input constraints, such as agricultural \cite{Ding2018}, automotive \cite{Hrovat2012}, 
%chemical process \cite{Eaton1992}, 
and energy systems \cite{Ademola-Idowu2021}. However, many applications require fast decision-making which may preclude the possibility of repeatedly solving an optimization problem online \cite{Alessio2009}.
% Strategies for accelerating MPC computations can be classified as offline \cite{Alessio2009}, online \cite{Patrinos2014, Wang2010}, or hybrid \cite{Zeilinger2011}.

A popular approach for accelerating MPC is to move as much computation offline as possible~\cite{Alessio2009,Zeilinger2011}. These techniques, known as explicit MPC, involve precomputing the solution to the MPC problem over a range of parameters or initial conditions. Most of the research effort has focused on problems with linear dynamics and constraints, and linear or quadratic cost functions. In this case,  the explicit MPC solution is a piecewise affine (PWA) function defined over a polyhedral partition of the state constraints. However, many of the applications of interest have cost functions that are not necessarily linear or quadratic, or even convex. Further, the memory required to store the partition and affine functions can be prohibitive even for modestly-sized problems. %\todo{The original version make it reads like we only care about linear or quadratic costs, which is not true and make our contribution sound less impressive. I tried rewriting it, see if it works.}

% If the dynamics and constraints of the system are linear and the cost function is linear or quadratic, the explicit MPC solution is a piecewise affine (PWA) function defined over a polyhedral partition of the state constraints \cite{Alessio2009}. The control law is implemented efficiently online by searching over the partition, with the main drawback being the amount of memory required to store the partition and affine functions even for modestly-sized problems. In \cite{Kvasnica2015}, the authors provide a region-free approach for problems with strictly convex quadratic costs only. 

In order to reduce the complexity of explicit MPC, the optimal offline solution can be approximated. Approximations generally fall into two categories: partition-based solutions  \cite{Jones2007,Johansen2004,Grancharova2009} that generate piecewise control laws over coarser state space partitions, and learning-based solutions \cite{Akesson2006,Chen2018d,Parisini1995,Domahidi2011} that use function approximation to compactly represent the optimal MPC policy. In this paper, we focus on the latter with the key contribution of ensuring constraint satisfaction while exploring all feasible policies.

%\todo{Make the need of constraint satisfaction more explicit at the start of the paragraph.}
Constraint satisfaction is crucial in many engineering applications, and the ability of MPC to enforce constraints is a major factor in its popularity. However, it is not straightforward to guarantee that a learning-based solution will satisfy constraints. The main challenge arises from the fact that while neural networks can limit their outputs to be in simple regions, there is no obvious way of forcing complex constraint satisfaction at the output. In~\cite{Akesson2006,Maddalena2020}, supervised and unsupervised learning were used to approximate the solution of MPCs, but did not provide any feasibility guarantees. By contrast, \cite{Chen2018d} trains an NN using a policy gradient approach, and guarantees feasibility by projecting the NN output into the feasible action set. However, this extra optimization step slows down the speed of online implementation, making it difficult to use in applications that require high-frequency solutions~\cite{Zheng2020}. Supervised learning approaches that provide safety guarantees \cite{Domahidi2011, Parisini1995} rely on a choice of MPC oracle that is not obvious when persistent disturbances are present.
% Learning-based approximations to explicit MPC date back to the mid 1990s and many solutions exist. 
% The methods employed in e.g. \cite{Parisini1995, Domahidi2011,Maddalena2020,Hertneck2018} use supervised learning by fitting the relationship between the current state and the optimal control input with a neural network (NN) or kernel machine. However, querying the optimal control law can be time-consuming, and it is not obvious how to find a good oracle in systems with persistent disturbances. 


%\todo{Good to mention Gauge or Gauge map somewhere in the next two paragraphs.}
In this paper, we propose an NN architecture for approximating explicit solutions to finite-horizon MPC problems with linear dynamics, linear constraints, and arbitrary differentiable cost functions. The proposed architecture guarantees constraint satisfaction without relying on projections or MPC oracles. By exploring the \emph{interior} of the feasible set, we demonstrate faster training and evaluation, and comparable closed-loop performance relative to other NN architectures. %Exploring the interior of the feasible set also guarantees feasibility at any level of accuracy, removing the need for fidelity to an oracle. 
%Finally, we show that with the proposed architecture, the unsupervised approach is competitive and can even outperform online MPC solutions in closed-loop experiments with persistent disturbances, in terms of both performance and computation time.

The proposed approach has parallels in interior point methods for convex optimization \cite{Boyd2009}. Interior point methods first solve a \emph{Phase I} problem to find a strictly feasible starting point. This solution is used to initialize the \emph{Phase II} algorithm for optimizing the original problem. Our approach accelerates both phases. The Phase I solution is given by a simple function (e.g., affine map) and the Phase II problem is solved using an NN architecture that can encode arbitrary polytopic constraints (Fig. \ref{fig:intro}).

The Phase II solution builds on a technique first proposed in \cite{Tabas2021a}, which uses a \emph{gauge map} to establish equivalence between compact, convex sets. \revision{With respect to \cite{Tabas2021a}, the current work has three novel aspects. First, the reinforcement learning (RL) algorithm in \cite{Tabas2021a} only uses information about the constraints, and does not use information about the cost function or dynamics. The resulting policy is safe, but can exhibit suboptimal performance. The MPC formulation in the current paper gives rise to a training algorithm that can exploit knowledge about the system, improving performance. 
Second, the MPC formulation permits explicit consideration for future time steps. The RL formulation cannot optimize entire trajectories due to the presence of constraints. This inability to ``look ahead'' again limits the performance of the RL algorithm. 
Finally, the previous work required a Phase I that used a linear feedback to find a strictly feasible point. A linear feedback, however, may not exist for some problems. The current work proposes a more general class of Phase I solutions (piecewise affine), while providing a way to manage the complexity of the Phase I solution.}

% is a coarse PWA function which, in our simulations, only requires one region (no partition). 
% problem is reduced to an affine function at best, and to a multiparametric linear program with substantially lower complexity and accuracy requirements compared to the original MPC problem at worst. 
% The Phase II problem is solved using a novel NN architecture that can encode arbitrary polytopic constraints, provided the Phase I solution exists and is available. T

\begin{figure}[b]
    \centering
    \includegraphics[height=3cm]{Figures/intro_fig_2.png}
    \caption{Illustration of the interior point approach to learning-based MPC. The set $\F(x_0)$ represents the MPC feasible set, while $\mu_0(x_0)$ and $\mu_\theta(x_0)$ are control input sequences representing solutions to the Phase I and Phase II problems, respectively. The neural network $\mu_\theta$ moves the Phase I solution to a more optimal solution.}
    \label{fig:intro}
\end{figure}

We demonstrate the effectiveness of the proposed technique on a 3-state test system, and compare to standard projection- and penalty-based approaches for learning with constraints. The results show that the proposed technique achieves Pareto efficiency in terms of closed-loop performance and online computation effort. All code is available at \texttt{github.com/dtabas/gauge\_networks}.

% The rest of the paper is organized as follows. Section \ref{sec:pf} provides the problem formulation. Sections \ref{sec:p1} and \ref{sec:p2} detail the accelerated solutions to the Phase I and Phase II MPC problems, respectively. Section \ref{sec:sims} contains descriptions of the simulations as well as test results.

\textit{Notation:}
The \revision{$p$}-norm ball for $p \geq 1$ is $\B_p = \{z \mid \|z\|_p \leq 1\}$.
%, with dimensionality inferred from context.
A \textit{polytope} $\P \subset \R^n := \{z \in \R^n \mid Fz \leq g\}$ is the (bounded) intersection of a finite number of halfspaces. \revision{Scaling of polytopes by a factor $\lambda > 0$ is defined as $\lambda \P = \{\lambda z \in \R^n \mid Fz \leq g\} = \{z \in \R^n \mid Fz \leq \lambda g\}$.} Given a matrix $F$ and a vector $g$, the $i$th row of $F$ is denoted $F^{(i)T}$ and the $i$th component of $g$ is $g^{(i)}$. The interior of any set $\Q$ is denoted $\int \Q$. The value of a variable $y$ at a time interval $t$ is denoted $y_t$. \newrevision{A state or control trajectory of length $\tau$ is written as the vector $\textbf{x} = \m{x_1^T,\ldots,x_{\tau}^T}^T \in \R^{n\tau}$ or $\textbf{u} = \m{u_0^T,\ldots,u_{\tau-1}^T}^T \in \R^{m\tau}$.} The column vector of all ones is \textbf{1}.
%, with dimensionality inferred from context. 
\newrevision{The symbol $\circ$ denotes function composition.}% \revision{The Pontryagin set difference is $\P \ominus \Q = \{z \in \R^n \mid z + \Q \subseteq \P\}$. }