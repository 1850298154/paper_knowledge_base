\section{Experiment Details}\label{sec_add_exp_setup}

For our result, we measure $B$ as an upper bound on the loss value taken over the entire data distribution.
Across five datasets in our experiments, setting $B = 5.4$ suffices for all the training and testing examples in the datasets.

For comparing generalization bounds, we use two types of model architectures, including GCN \cite{kipf2016semi} and the MPGNN in \citet{liao2020pac}. Following the setup in \citet{liao2020pac}, we apply the same network weights across multiple layers in one model, i.e., $W^{(t)} = W$ and $U^{(t)} = U$ across the first $l-1$ layers. For GCNs, we set $\cU$ as zero, $\rho_t$ and $\psi_t$ as identity mappings, $\phi_t$ as ReLU function. For MPGNNs, we specify $\phi_t$ as ReLU, $\rho_t$ and $\psi_t$ as Tanh function. For both model architectures, we set the width of each layer $d_t = 128$ and vary the network depth $l$ in 2, 4, and 6.
On the three collaboration network datasets, we use one-hot encodings of node degrees as input node features. 
We train the models with Adam optimizer with a learning rate of $0.01$ and set the number of epochs as $50$ and batch size as $128$ on all three datasets.
We compute the generalization bounds following the setup in \citet{liao2020pac}. %
Theorem 3.4 from \citet{liao2020pac}: 
    {\small$$
        \sqrt{\frac{42^2}{\gamma^2 N}
        \Bigg( \max\limits_{(X, G, y) \sim \cD}\bignorms{X}^2 \Bigg)
        \Big(\max \Big(\zeta^{-l+1}, (\lambda\xi)^{\frac{l+1}{l}} \Big) \Big)^2 l^2h\log(4lh) (2s_1^2r_1^2 + s_l^2r_l^2))},
    $$}%
    where $\zeta = \min(s_1, s_l)$, $\lambda = s_1s_l$, $\xi = \frac{(ds_1)^{l-1} - 1}{ds_1-1}$, $d$ is the max degree, $h$ is the max hidden width, and $\gamma$ is the desired margin in the margin loss. Note that $s_i = s_1$ and $r_{i}=r_1$ for $1 \leq i \leq l-1$ since the first $l-1$ layers apply the same weight.  
Proposition 7 from \citet{garg2020generalization}: 
    {\small$$
        48 s_l h Z\sqrt{\frac{3}{\gamma^2 N}\log\big( 24s_l\sqrt{N}
        \max\big( Z, M\sqrt{h}
        \max\big(
        \kappa_2 s_1, \bar{R}s_1 \big) \big) \big)},
    $$}%
    where $M{=}\frac{(ds_1)^{l-1} - 1}{ds_1-1}$, $\bar{R}=d\cdot \min( \kappa_1 \sqrt{h}, \kappa_2 s_1 M)$, $Z = \kappa_2 s_1 + \bar{R}s_1$, $\kappa_1 = \max\limits_{x \in \real^{h}}\norm{\phi(x)}_{\infty}$, and  $\kappa_2 = \max\limits_{_{_{(X, G, y) \sim \cD}}}\bignorms{X}^2$.