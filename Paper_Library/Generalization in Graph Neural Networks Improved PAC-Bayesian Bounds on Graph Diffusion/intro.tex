\begin{abstract}
Graph neural networks are widely used tools for graph prediction tasks. Motivated by their empirical performance, prior works have developed generalization bounds for graph neural networks, which scale with graph structures in terms of the maximum degree. In this paper, we present generalization bounds that instead scale with the largest singular value of the graph neural network's feature diffusion matrix. These bounds are numerically much smaller than prior bounds for real-world graphs. We also construct a lower bound of the generalization gap that matches our upper bound asymptotically. To achieve these results, we analyze a unified model that includes prior works' settings (i.e., convolutional and message-passing networks) and new settings (i.e., graph isomorphism networks). Our key idea is to measure the stability of graph neural networks against noise perturbations using Hessians. Empirically, we find that Hessian-based measurements correlate with the observed generalization gaps of graph neural networks accurately.  Optimizing noise stability properties for fine-tuning pretrained graph neural networks also improves test performance on several graph-level classification tasks.
\end{abstract}

\section{Introduction}

A central measure of success for a machine learning model is the ability to generalize well from training data to test data.
For linear and shallow models, the generalization gap between their training performance and test performance can be quantified via complexity notions such as the Vapnikâ€“Chervonenkis dimension and Rademacher complexity.
However, formally explaining the empirical generalization performance of deep models remains a challenging problem and an active research area (see, e.g., recent textbook by \citet{hardt2021patterns}).
There are by now many studies for fully-connected and convolutional neural networks that provide an explanation for their superior empirical performance \cite{bartlett2017spectrally,neyshabur2018towards}.
Our work seeks to formally understand generalization in graph neural networks (GNN) \cite{scarselli2008graph}, which are commonly used for learning on graphs \cite{hamilton2017representation}.

As a concrete example for motivating the study of generalization performance, we consider the fine-tuning of pretrained graph neural networks \cite{hu2019strategies}.
Given a pretrained GNN learned on a diverse range of graphs, fine-tuning the pretrained GNN on a specific prediction task is a common approach for transfer learning on graphs.
An empirical problem with fine-tuning is that, on one hand, pretrained GNNs use lots of parameters to ensure representational power.
On the other hand, fine-tuning a large GNN would overfit the training data and suffer poor test performance without proper algorithmic intervention.
Thus, a better understanding of generalization in graph neural networks can help us identify the cause of overfitting and, consequently, inspire designing robust fine-tuning methods for graph neural networks.

A naive application of the generalization bounds from fully-connected feedforward networks \cite{bartlett2017spectrally,neyshabur2017pac} to GNNs would imply an extra term in the generalization bound that scales with $n^{l-1}$, where $n$ is the number of nodes in the graph, hence rendering the error bounds vacuous.
Besides, \citet{scarselli2018vapnik} shows that the VC dimension of GNN scales with $n$. Thus, although the VC dimension is a classical notion for deriving learning bounds, it is oblivious to the graph structure.
Recent works have taken a step towards addressing this issue with better error analysis. %
\citeay{verma2019stability} find that one-layer graph neural networks satisfy uniform stability properties \cite{verma2019stability}, following the work of \citet{hardt2016train}.
The generalization bound of \citet{verma2019stability} scales with the largest singular value of the graph diffusion matrix of the model.
However, their analysis only applies to a single layer and node prediction.
\citeay{garg2020generalization} analyze an $l$ layer message-passing neural network --- with $l-1$ graph diffusion layers and $1$ pooling layer --- for graph prediction tasks \cite{garg2020generalization}.
Their result scales with $d^{l-1}$, where $d$ is the maximum degree of the graph.
Subsequently, \citeay{liao2020pac} develop a tighter bound but still scales with $d^{l-1}$ \cite{liao2020pac}.
For both results, the graph's maximum degree is used to quantify the complexity of node aggregation in each diffusion step.


\begin{figure*}[t!]
    \centering
     \begin{subfigure}[b]{0.49\textwidth}
 		\centering
 		\includegraphics[width=0.9\textwidth]{./figures/graph_statistics.pdf}
        \caption{Spectral norm vs. max degree for five graphs}\label{fig_intro_gcn}
 	\end{subfigure}\hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
 		\includegraphics[width=0.9\textwidth]{./figures/generalization_corr_hessian.pdf}
        \caption{Comparing generalization bounds}\label{fig_intro_graph}
    \end{subfigure}
     \caption{The spectral norm bounds for graph diffusion matrices are orders of magnitude smaller than maximum degree bounds on real-world graphs; In Figure \ref{fig_intro_gcn}, we measure the spectral norm and the max degree for five graph datasets.
     In Figure \ref{fig_intro_graph}, the Hessian-based generalization measure (plotted in green, scaled according to the \emph{left axis}) matches the empirically observed generalization gaps of graph neural networks (plotted in yellow, scaled according to the \emph{left axis}). The blue line shows a uniform-convergence bound (scaled according to the \emph{right axis}) that is orders of magnitude larger than the observed gaps.}
     \label{fig_intro}
\end{figure*}

Our main contribution is to show generalization bounds for graph neural networks by reducing the max degree to the spectral norm of the graph diffusion matrix.
We achieve this by analyzing the stability of a graph neural network against noise injections.
To illustrate, denote an $l$-layer GNN by $f$.
By PAC-Bayesian analysis \cite{mcallester2013pac}, the generalization gap of $f$ will be small if $f$ remains stable against noise injections; otherwise, the generalization gap of $f$ will be large.
Empirically, quantifying the noise stability of $f$ via Lipschitz-continuity properties of its activation functions leads to nonvacuous bounds for feedforward networks that correlate with their observed generalization gaps \cite{arora2018stronger,ju2022robust,dziugaite2017computing,jiang2019fantastic}. %
Our theoretical analysis rigorously formalizes this with refined stability analysis of graph neural networks through Hessians, leading to tight generalization bounds on the graph diffusion matrix.

\medskip
\noindent\textbf{Our Contribution.}
The goal of this work is to improve the theoretical understanding of generalization in graph neural networks, and in that vein, we highlight two results below:

\begin{itemize}[leftmargin=15.0pt,topsep=2.5pt,itemsep=2.5pt] %
    \item First, we prove sharp generalization bounds for message-passing neural networks \cite{dai2016discriminative,gilmer2017neural,jin2018junction}, graph convolutional networks \cite{kipf2016semi}, and graph isomorphism networks \cite{xu2018powerful}.
    Our bounds scale with the spectral norm of $P_{_G}^{l-1}$ for an $l$-layer network, where $P_{_G}$ denotes a diffusion matrix on a graph $G$ and varies between different models (see Theorem \ref{thm_mpgnn} for the full statement).
    We then show a matching lower bound instance where the generalization gap scales with the spectral norm of $P_{_G}^{l-1}$ (see Theorem \ref{prop_lb}).

    \item Second, our stability analysis of graph neural networks provides a practical tool for measuring generalization. 
    Namely, we show that the trace of the loss Hessian matrix can measure the noise stability of GNN.
    The formal statement is given in Lemma \ref{lemma_trace_hess}, and our techniques, which include a uniform convergence of the Hessian matrix, may be of independent interest.
    We note that the proof applies to twice-differentiable and Lipschitz-continuous activations (e.g., tanh and sigmoid).
\end{itemize}
Taken together, these two results provide a sharp understanding of generalization in terms of the graph diffusion matrix for graph neural networks.
We note that the numerical value of our bounds in their dependence on the graph is much smaller than prior results \cite{garg2020generalization,liao2020pac}, as is clear from Figure \ref{fig_intro_gcn}.
Moreover, the same trend holds even after taking weight norms into account (see Figure \ref{fig_bound_measurement}, Section \ref{sec_compare}).
Further, the Hessian-based bounds (see Lemma \ref{lemma_trace_hess}, Section \ref{sec_proff_sketch}) are non-vacuous, matching the scale of empirically observed generalization gaps in Figure \ref{fig_intro_graph}.




Finally, motivated by the above analysis, we also present an algorithm that performs gradient updates on perturbed weight matrices of a graph neural network.
The key insight is that minimizing the average loss of multiple perturbed models  with independent noise injections is equivalent to regularizing $f$'s Hessian in expectation. We conduct experiments on several graph classification tasks with Molecular graphs that show the benefit of this algorithm in the fine-tuning setting.



\section{Related Work}\label{sec_related}


\textbf{Generalization Bounds:}
An article by \citeay{zhang2016understanding} finds that deep nets have enough parameters to memorize real images with random labels, yet they still generalize well if trained with true labels.
This article highlights the overparametrized nature of modern deep nets (see also a recent article by \citet{arora2021technical}), motivating the need for complexity measures beyond classical notions. 
In the case of two-layer ReLU networks, \citeay{neyshabur2018towards} show that (path) norm bounds better capture the ``effective number of parameters'' than VC dimension---which is the number of parameters for piecewise linear activations \cite{bartlett2019nearly}.

For multilayer networks, subsequent works have developed norm, and margin bounds, either via Rademacher complexities \cite{bartlett2017spectrally,golowich2018size,long2020generalization}, or PAC-Bayesian bounds \cite{neyshabur2017pac,arora2018stronger,li2021improved,ju2022robust}.
All of these bounds apply to the fine-tuning setting following the distance from the initialization perspective.
Our analysis approach builds on the work of \citeay{arora2018stronger} and \citeay{ju2022robust}. The latter work connects perturbed losses and Hessians for feedforward neural networks, with one limitation Hessians do not show any explicit dependence on the data.
This is a critical issue for GNN as we need to incorporate the graph structure in the generalization bound.
Our result instead shows an explicit dependence on the graph and applies to message-passing layers that involve additional nonlinear mappings.
We will compare our analysis approach and prior analysis in more detail when we present the proofs in Section \ref{sec_proff_sketch} (see Remark \ref{remark_tech}).

\medskip
\noindent\textbf{Graph Representation Learning:} Most contemporary studies of learning on graphs consider either node-level or graph-level prediction tasks.
Our result applies to graph prediction while permitting an extension to node prediction: see Remark \ref{remark_node} in Section \ref{sec_proff_sketch}.
Most graph neural networks follow an information diffusion mechanism on graphs \cite{scarselli2008graph}.
Early work takes inspiration from ConvNets and designs local convolution on graphs, e.g., spectral networks \cite{bruna2013spectral}, GCN \cite{kipf2016semi}, and GraphSAGE \cite{hamilton2017inductive} (among others).
Subsequent works have designed new architectures with graph attention \cite{velivckovic2017graph} and isomorphism testing \cite{xu2018powerful}.
\citeay{gilmer2017neural} synthesize several models into a framework called message-passing neural networks.
Besides, one could also leverage graph structure in the pooling layer (e.g., differentiable pooling and hierarchical pooling \cite{zhang2018end,ying2018hierarchical}). It is conceivable that one can incorporate the model complexity of these approaches into our analysis.
Recent work applies pretraining to large-scale graph datasets for learning graph representations \cite{hu2019strategies}.
Despite being an effective transfer learning approach, few works have examined the generalization of graph neural nets in the fine-tuning step.

Besides learning on graphs, GNNs are also used for combinatorial optimization \cite{selsam2018learning} and causal reasoning \cite{xu2019can}.
There is another approach for graph prediction using graph kernels \cite{vishwanathan2010graph} such as the graph neural tangent kernel \cite{du2019graph}.
Lastly, we remark that graph diffusion processes have been studied in earlier literature on social and information networks \cite{goel2014connectivity,goel2015note,zhang2019pruning}.
For further references about different applications of GNN, see review articles \cite{hamilton2017representation,errica2019fair,wu2020comprehensive,chami2020machine}.


\medskip
\noindent\textbf{Generalization in Graph Neural Networks:}
Recent work explores generalization by formalizing the role of the algorithm and the alignment between networks and tasks \cite{xu2020neural}.
\citet{esser2021learning} finds that transductive Rademacher complexity-based bound provides insights into the behavior of GNNs in the stochastic block model.
Besides, there are works about size generalization, which refer to performance degradation when models extrapolate to graphs of different sizes from the input \cite{selsam2018learning,yehudai2021local}.
It is conceivable that the new tools we have developed may be useful for studying extrapolation. %


\medskip
\noindent\textbf{Expressivity of Graph Neural Networks:}
The expressivity of GNN for graph classification can be related to graph isomorphism tests and has connections to one-dimensional Weisfeiler-Lehman testing of graph isomorphism \cite{morris2019weisfeiler,xu2018powerful}.
This implies limitations of GNN for expressing tasks such as counting cycles \cite{sato2019approximation,chen2020can,azizian2020expressive}.
The expressiveness view seems orthogonal to generalization, which instead concerns the sample efficiency of learning.
For further discussions and references, see a recent survey by \citet{jegelka2022theory}. %





