\section{Proofs}\label{app_proof}

This section provides the complete proofs for our results in Section \ref{sec_theory}. First, we state several notations and facts needed in the proofs. Then we provide the proof of the Hessian-based generalization bound for MPNN, as stated in Lemma \ref{lemma_gen_error}.
After that, in Appendix \ref{app_mpgnn}, we provide the proof of Theorem \ref{thm_mpgnn}, a key step of which is the proof of Lemma \ref{lemma_trace_hess}.
Next, in Appendix \ref{app_lb}, we state the proof of the lower bound.
Lastly, in Appendix \ref{app_gin}, we will provide proof for the case of graph isomorphism networks.

First, we state several facts about graphs and provide a short proof of them.

\begin{fact}\label{fact_graph}
Let $G = (V, E)$ be an undirected graph.  Let $d_{G}$ be the maximum degree of $G$. %
\begin{enumerate}[leftmargin=0.25in]
    \item[a)] Let $A$ be the adjacency matrix of $G$. Then, the adjacency matrix satisfies: $\sqrt{d_G} \le \bignorms{A} \le d_G$. %
    \item[b)] The symmetric and degree-normalized adjacency matrix satisfies $\bignorms{D^{-1/2} A D^{-1/2}} \le 1$.
\end{enumerate}
\end{fact}
\begin{proof}
    Based on the definition of the spectral norm, we get
    \begin{align*}
        \bignorms{A} = \max_{\bignorm{x} = 1} x^{\top} A x = \max_{\bignorm{x} = 1} \sum_{(i,j)\in E} x_ix_j \le \max_{\bignorm{x} = 1} \sum_{(i,j)\in E} \frac{1}{2}(x_i^2 + x_j^2) \le d_G \sum_{i\in V} x_i^2 = d_G.
    \end{align*}
    Assume that node $i$ has the maximum degree $d_G$. Denote edges set $E_i =  \{(i,i_k)\}_{k=1}^{d_G}\subseteq E$. Let $x_i = \frac{1}{\sqrt{2}}$, $x_{i_k} = \frac{1}{\sqrt{2d_G}}$ for all $k = 1,\dots,d_G$. The rest entries of $x$ are equal to zero. Thus, $x$ is a normalized vector. Next, we have
    \begin{align*}
        \bignorms{A} = \max_{\bignorm{x} = 1} \sum_{(i,j)\in E} x_ix_j \ge \max_{\bignorm{x} = 1} 2 \sum_{(i,j)\in E_i} x_ix_j = 2 d_G \cdot \frac{1}{\sqrt{2}} \frac{1}{\sqrt{2d_G}} = \sqrt{d_G}.
    \end{align*}
    An example in which $\bignorms{P_G}$ gets close to $\sqrt {d_G}$ is the star graph.
    An example in which $\bignorms{P_G}$ gets close to $d_G$ is the complete graph.

    \smallskip
    Next, we focus on case b).
    From the definition of the spectral norm, we know
    \begin{align*}
        \bignorms{D^{-1/2} A D^{-1/2}} &= \max_{\bignorm{x} = 1} x^{\top} \bigbrace{D^{-1/2} A D^{-1/2}} x = \max_{\bignorm{x} = 1} \sum_{(i,j)\in E} \frac{x_ix_j}{\sqrt{d_id_j}} \\
        &\le \max_{\bignorm{x} = 1} \sum_{(i,j)\in E} \frac{x_i^2}{2d_i} + \frac{x_j^2}{2d_j} = \sum_{i\in V} x_i^2 = 1.
    \end{align*}
    During the middle of the above step, we used the Cauchy-Schwartz inequality. The proof of this result is now completed.
\end{proof}

\noindent\textbf{Notations:}
For two matrices $X$ and $Y$ that are both of dimension $d_1$ by $d_2$, the Hadamard product of $X$ and $Y$, denoted as $X \odot Y$, is equal to the entrywise product of $X$ and $Y$.

\subsection{Proof of our PAC-Bayesian bound (Lemma \ref{lemma_gen_error})}\label{proof_pac_bayes}

To be precise, we will restate the conditions required in Theorem \ref{thm_mpgnn} separately below. The conditions are exactly the same as stated in Section \ref{sec_theory}.

\begin{assumption}\label{ass_1}
    Assume that all the activation functions $\phi_i(\cdot),\rho_i(\cdot),\psi_i(\cdot)$ for any $1\leq i\leq l-1$ and the loss function $\ell(x, y)$ over $x$ are twice-differentiable and $\kappa_0$-Lipschitz. Their first-order derivatives are $\kappa_1$-Lipschitz and their second-order derivatives are $\kappa_2$-Lipschitz.
\end{assumption}

Based on the above assumption, we provide the precise statement for Taylor's expansion, used in equation \eqref{eq_perturb}.

\begin{proposition}\label{prop_taylor}
    In the setting of Theorem \ref{thm_mpgnn}, suppose each parameter in layer $i$ is perturbed by an independent noise drawn from $\cN(0,\sigma_i^2)$. 
    Let $\tilde{\ell}(f(X,G),y)$ be the perturbed loss function with noise perturbation injection vector $\cE$ on all parameters $\cW$ and $\cU$. 
    There exist some fixed value $C_1$ that do not grow with $N$ and $1/\delta$ such that %
    \begin{small}
        \begin{align*}
             \bigabs{\tilde \ell(f(X, G), y) - \ell(f(X, G), y) - \frac 1 2 \sum_{i=1}^l\sigma_{i}^2 \bigtr{\bH^{{(i)}}[\ell (f(X,G),y)]} } 
            \le  C_1 \sum_{i=1}^l \sigma_{i}^3.
        \end{align*}
    \end{small}%
\end{proposition}

\begin{proof}
    By Taylor's expansion, the following identity holds
    \begin{align*}
        \tilde \ell(f(X, G), y) - \ell(f(X, G), y)
        = \exarg{\cE}{\cE^{\top} \nabla \ell(f) + \frac 1 2 {\cE}^{\top} \bH[\ell(f)] {\cE} + R(\ell(f),\cE)}.
    \end{align*}
    where $R(\ell(f),\cE)$ is the rest of the first-order and the second-order terms.
    Since each entry in $\cE$ follows the normal distribution, we have
    $\exarg{\cE}{\cE^{\top} \nabla \ell(f)} = 0$. The Hessian term turns to
    \begin{align*}
        {\cE}^{\top} \bH[\ell(f)] {\cE} = \sum_{i=1}^l \sigma_i^2 \bigtr{\bH^{(i)}[\ell (f(X,G),y)]}.
    \end{align*}
    Since the readout layer is linear, by Proposition \ref{prop_taylor}, there exists a fixed constant $\bar C$ that does not grow with $N$ and $\delta^{-1}$ such that
     $   |R(\ell(f),\cE)| \le \bar C \bignorm{\cE}^3$.
    Based on \citet[Lemma 2]{jin2019short}, for any $x$ drawn from a normal distribution $\cN(0, \sigma^2)$, we have ${\ex{x^3}} \le 6\sigma^3$. Hence, we get
       $\ex{R(\ell(f),\cE)} \le C_1 \sum_{i=1}^l {\sigma_{i}^3}$,
    where $C_1 = \order{h^2 \bar C}$ is a fixed constant. Thus, we have finished the proof.
\end{proof}

Next, we state a Lipschitz-continuity upper bound of the network output at each layer.
This will be needed in the $\epsilon$-covering argument later in the proof of Theorem \ref{thm_mpgnn}.
To simplify the notation, we will abbreviate explicit constants that do not grow with $N$ and $1/\delta$ in the notation $\lesssim$; more specifically, we use $A(n) \lesssim B(n)$ to indicate that there exists a function $c$ that does not depend on $N$ and $1/\delta$ such that $A(n) \le c \cdot B(n)$ for large enough values of $n$.

\begin{proposition}\label{claim_hess}
    In the setting of Theorem \ref{thm_mpgnn}, for any $j=1,\dots,l-1$, the change in the Hessian of output of the $j$ layer network $H^{(j)}$ with respect to $W_i$ and $U_i$ under perturbation on $W$ and $U$ can be bounded as follows:
    \begin{align}
        \bignormFro{\bH_{\cW}^{(i)}[\tilde H^{(j)}] - \bH_{\cW}^{(i)}[H^{(j)}]}  &\lesssim \sum_{t=1}^j\Bigbrace{\bignorms{\Delta U^{(t)}} + \bignorms{\Delta W^{(t)}}}. \label{eq_claim_hess_1} \\
        \bignormFro{\bH_{\cU}^{(i)}[\tilde H^{(j)}] - \bH_{\cU}^{(i)}[H^{(j)}]} &\lesssim \sum_{t=1}^j\Bigbrace{\bignorms{\Delta U^{(t)}} + \bignorms{\Delta W^{(t)}}}. \label{eq_claim_hess_2}
    \end{align}
    Above, the notation $\bH_{\cW}^{(i)}[\tilde H^{(j)}]$ is the perturbation of the Hessian matrix of $H^{(j)}$ by $\Delta \cW$ and $\Delta \cU$, specific to the variables of $\cW$; likewise, $\bH_{\cU}^{(i)}[\tilde H^{(j)}]$ is the perturbation of the Hessian matrix specific to the variables of $\cU$.
\end{proposition}

The proof of Proposition \ref{claim_hess} will be deferred until Appendix \ref{proof_prop_lip}. Based on Propositions \ref{prop_taylor} and \ref{claim_hess}, now we are ready to present the proof of Lemma \ref{lemma_gen_error}.

\begin{proof}[Proof of Lemma \ref{lemma_gen_error}]
    First, we separate the gap of $\cL(f)$ and $\frac{1}{\beta}\hat{\cL}(f)$ into three parts:
    \begin{align*}
         &\cL(f) - \frac{1}{\beta} \hat{\cL}(f) 
        =  \underbrace{\exarg{(X,G,y) \sim \cD}{\ell(f(X,G),y)} - \exarg{(X,G,y) \sim \cD}{\tilde{\ell}(f(X,G),y)}}_{E_1} + \exarg{(X,G,y) \sim \cD}{\tilde{\ell}(f(X,G),y)} \\
        & - \frac{1}{\beta} \Bigbrace{\frac{1}{N} \sum_{i=1}^N \tilde{\ell}(f(X_i,G_i),y_i)} + \underbrace{\frac{1}{\beta} \Bigbrace{\frac{1}{N} \sum_{i=1}^N \tilde{\ell}(f(X_i,G_i),y_i)} - \frac{1}{\beta} \Bigbrace{\frac{1}{N} \sum_{i=1}^N \ell(f(X_i,G_i),y_i)}}_{E_2}.
    \end{align*}
    for any $\beta \in (0,1)$.
    Above, $\tilde{\ell}(f(X,G),y)$ is the perturbed loss from $\ell(f(X, G), y)$ with noise injections $\cE$ added to all the parameters in $\cW$ and $\cU$. By Taylor's expansion from Proposition \ref{prop_taylor}, we can bound the difference between $\tilde{\ell}(f(X,G), y)$ and $\ell(f(X, G)$ with the trace of the Hessian.
    Therefore
    \begin{align*}
        \cL(f) - \frac{1}{\beta} \hat{\cL}(f) \le \,& -\exarg{(X,G,y) \sim \cD}{\frac 1 2 \sum_{i=1}^l \sigma_i^2 \bigtr{\bH^{(i)}[\ell(f(X,G),y)]}}  + \sum_{i=1}^l C_1 \sigma_i^3 \tag{by Prop. \ref{prop_taylor} for $E_1$}\\
        & + \Bigg( \exarg{(X,G,y) \sim \cD}{\tilde{\ell}(f(X,G),y)} - \frac{1}{\beta} \Bigbrace{\frac{1}{N} \sum_{i=1}^N \tilde{\ell}(f(X_i,G_i),y_i)} \Bigg) \\
        & + \frac{1}{2\beta}\sum_{i=1}^l \sigma_i^2  \Bigbrace{\frac{1}{N} \sum_{j=1}^N \bigtr{\bH^{(i)}[\ell(f(X_j,G_j),y_j)]}} + \frac{1}{\beta}\sum_{i=1}^l C_1 \sigma_{i}^3. \tag{by Prop. \ref{prop_taylor} for $E_2$}
    \end{align*}%
    By rearranging the above equation, we get the following:
    {\small\begin{align*}
         \cL(f) - \frac{1}{\beta} \hat{\cL}(f) 
        &\le  \frac 1 2 \sum_{i=1}^l \sigma_i^2 \Bigg( \underbrace{\frac{1}{N} \sum_{j=1}^N \bigtr{\bH^{(i)}[\ell(f(X_j,G_j),y_j)]} - \exarg{(X,G,y) \sim \cD}{\bigtr{\bH^{(i)}[\ell(f(X,G),y)]}}}_{E_3} \Bigg) \\
        & + \frac 1 2 \Bigbrace{\frac{1}{\beta} - 1} \underbrace{\sum_{i=1}^l \frac{\sigma_i^2}{N} { \sum_{j=1}^N \bigtr{\bH^{(i)}[\ell(f(X_j,G_j),y_j)]}}}_{E_4} \\
        & + \Bigbrace{1 + \frac{1}{\beta}} C_1 \sum_{i=1}^l \sigma_i^3 + \underbrace{\exarg{(X,G,y) \sim \cD}{\tilde{\ell}(f(X,G),y)} - {\frac{1}{\beta N} \sum_{i=1}^N \tilde{\ell}(f(X_i,G_i),y_i)}}_{E_5}.
    \end{align*}}%
    Based on Proposition \ref{claim_hess}, the Hessian operator $\bH^{(i)}$ is Lipschitz-continuous for some parameter that does not depend on $N$ and $1/\delta$, for any $i = 1,2\dots,l$.
    Therefore, from \citet[Lemma 2.4]{ju2022robust}, there exist some fixed values $C_2$, $C_3$ that do not grow with $N$ and $1/\delta$, such that with probability at least $1 - \delta$ over the randomness of the training set. Therefore, the matrix inside the trace of $E_3$ satisfies
    \begin{align}
        \bignormFro{\frac 1 N \sum_{j=1}^N\bH^{(i)}[\ell(f(X_j,G_j), y_j)] - \exarg{(X,G,y)\sim\cD}{\bH^{(i)}[\ell(f(X,G), y)]}} \le \frac{C_2\sqrt{\log (C_3 N/\delta)}}{\sqrt N}, \label{eq_trace_bound}
    \end{align}
    for any $i = 1,\dots,l.$
    Thus, by the Cauchy-Schwartz inequality, $E_3$ is less than $\sqrt{2h^2}$ times the RHS of equation \eqref{eq_trace_bound}.
    Suppose the loss function $\ell(f(X,G),y)$ lies in a bounded range $[0, B]$ given any $(X,G,y) \sim \cD$. By the PAC-Bayes bound of \citet[Theorem 2]{mcallester2013pac} (see also \citet{guedj2019primer}), we choose $\cU$ as a prior distribution and $\cW + \cU$ as a posterior distribution. For any $\beta \in (0,1)$ and $\delta \in [0,1)$, with probability at least $1 - \delta$, $E_5$ satisfies:
    \begin{align}
        \exarg{(X,G,y) \sim \cD}{\tilde{\ell}(f(X,G),y)} - {\frac{1}{\beta N} \sum_{i=1}^N \tilde{\ell}(f(X_i,G_i),y_i)}
        \le& \frac{B}{2\beta(1-\beta)N}\Bigbrace{\sum_{i=1}^l {\frac{\bignormFro{W^{(i)}}^2 + \bignormFro{U^{(i)}}^2}{2\sigma_{i}^2}} + \log \frac{1}{\delta}} \nonumber \\
        \le& \frac{B}{2\beta(1-\beta)N} \Bigbrace{\sum_{i=1}^l \frac{s_i^2 r_i^2}{\sigma_i^2} + \log\frac 1 {\delta}}. \label{eq_pac_bayes}
    \end{align}
    The above is because ${\cW}$ and ${\cU}$ are inside the hypothesis set $\cH$.
    For any $i = 1,\dots,l$, let
    \[ \alpha_i = \max\limits_{(X, G, y)\sim\cD} \bigtr{\bH^{(i)}[\ell(f(X,G),y)]}. \]
    Lastly, we use $\sigma_i^2 \alpha_i$ above to upper bound $E_4$.
    Combined with equations \eqref{eq_trace_bound} and \eqref{eq_pac_bayes}, with probability at least $1 - 2\delta$, we get
    \begin{align*}
        \cL(f) - \frac{1}{\beta} \hat{\cL}(f) \le \,&  
           \frac{C_2\sqrt{2 h^2 \log (C_3 N/\delta)}}{\sqrt N}  \sum_{i=1}^l \sigma_i^2 + \Bigbrace{1 + \frac{1}{\beta}} C_1 \sum_{i=1}^l \sigma_i^3 \\
          & + {\frac 1 2 \Bigbrace{\frac{1}{\beta} - 1} \sum_{i=1}^l \alpha_i \sigma_i^2 
          + \frac{B}{2\beta(1-\beta)N}\Bigg( \sum_{i=1}^l {\frac{s_i^2 r_i^2}{\sigma_{i}^2}} + \log \frac{1}{\delta} \Bigg)}.
    \end{align*}
    Next, we will select $\sigma_i$ to minimize the last line above.
    One can verify that this is achieved when
    \begin{align*}
        \sigma_i^2 = \frac{s_i r_i}{1-\beta}\sqrt{\frac{B}{\alpha_i N}}, \text{ for every } i = 1,2,\dots,l.
    \end{align*}
    With this setting of the noise variance, the gap between $\cL(f)$ and $\hat{\cL}(f)/ {\beta}$ becomes:
    \begin{align*}
        & \cL(f) - \frac{1}{\beta} \hat{\cL}(f) \\
        \le& \frac{1}{\beta} \sum_{i=1}^l \sqrt{\frac{B \alpha_i s_i^2 r_i^2}{N}} 
        + \frac{C_2\sqrt{2 h^2 \log (C_3 N/\delta)}}{\sqrt N} \sum_{i=1}^L \sigma_{i}^2 + \Bigbrace{1 + \frac{1}{\beta}} C_1\sum_{i=1}^l \sigma_i^3 + \frac{C}{2\beta(1-\beta)N}\log \frac{1}{\delta}.
    \end{align*}
    Let $\beta$ be a fixed value close to $1$ and independent of $N$ and $\delta^{-1}$; let $\epsilon = (1 - \beta)/\beta$. We get
    \begin{align*}
        &\cL(f) \le  \, (1 + \epsilon) \hat{\cL}(f) + (1 + \epsilon) \sum_{i=1}^l \sqrt{\frac{B \alpha_i r_i^2 s_i^2}{N}} + \xi, \text{ where } \\
        &\xi = \frac{C_2\sqrt{2 h^2 \log (C_3 N/\delta)}}{\sqrt N}  \sum_{i=1}^L \sigma_{i}^2 + \Bigbrace{1 + \frac{1}{\beta}} C_1 \sum_{i=1}^l {\sigma_{i}^3} + \frac{C}{2\beta(1-\beta)N}\log \frac{1}{\delta}.
    \end{align*}
    Notice that $\xi$ is of order $\order{N^{-3/4}\ + \log(\delta^{-1}) N^{-1}} \le \order{\log(\delta^{-1}) / N^{3/4}}$.
    Therefore, we have finished the proof of equation \eqref{eq_main_1}.
\end{proof}

\subsubsection{Proof of Proposition \ref{claim_hess}}\label{proof_prop_lip}

For any $j = 1,2,\dots,l$, let $\tilde H^{(j)}$ be the perturbed network output after layer $j$, with perturbations given by $\Delta \cW$ and $\Delta \cU$.
We show the following Lipschitz-continuity property for $H^{(j)}$.

\begin{claim}\label{claim_perturb}
    Suppose that Assumption \ref{ass_1} holds. For any $j=1,\dots,l-1$, the change in the output of the $j$ layer network $H^{(j)}$ with perturbation added to $\cW$ and $\cU$ can be bounded as follows:
    \begin{align}\label{eq_claim_perturb}
        \bignormFro{\tilde H^{(j)} - H^{(j)}}\lesssim \sum_{t=1}^j\Bigbrace{\bignorms{\Delta U^{(t)}}+\bignorms{\Delta W^{(t)}}}.
    \end{align}
\end{claim}

\begin{proof}
    We will prove using induction with respect to $j$. If $j = 1$, we have
    \begin{align*}
        &\bignormFro{\phi_1\Bigbrace{X \bigbrace{U^{(1)} + \Delta U^{(1)}} + \rho_1\bigbrace{P_{_G} \psi_1\bigbrace{X}} \bigbrace{W^{(1)} + \Delta W^{(1)}}} - \phi_1\Bigbrace{X U^{(1)} + \rho_1\bigbrace{P_{_G} \psi_1\bigbrace{X}} W^{(1)}}} \\
        \leq & \kappa_0\bignormFro{X \Delta U^{(1)} + \rho_1\bigbrace{P_{_G} \psi_1\bigbrace{X}} \Delta W^{(1)}} \lesssim \bignorms{\Delta U^{(1)}}+\bignorms{\Delta W^{(1)}}.
    \end{align*}
    Hence, we know that equation \eqref{eq_claim_perturb} will be correct when $j = 1$. Assuming that equation \eqref{eq_claim_perturb} is correct for any $j \geq 1$, the perturbation of layer $j+1$'s network output $H^{(j+1)}$ is less than
    {\begin{align*} 
        & \bignormFro{\tilde H^{(j+1)} - H^{(j+1)}} \\ 
        \le & \, \kappa_0\bignormFro{X \Delta U^{(j+1)} + \rho_{j+1}\bigbrace{P_{_G} \psi_{j+1}\bigbrace{\tilde H^{(j)}}} \bigbrace{W^{(j+1)} + \Delta W^{(j+1)}} - \rho_{j+1}\bigbrace{P_{_G} \psi_{j+1}\bigbrace{H^{(j)}}} W^{(j+1)}} \\
        \lesssim & \bignorms{\Delta U^{(j+1)}}+\bignorms{\Delta W^{(j+1)}} + \bignormFro{\tilde H^{(j)} - H^{(j)}}.
    \end{align*}}%
    Thus, we have finished the proof of the induction step.
\end{proof}

Next, for any $i$ and $j$, let $\frac{\partial \tilde H^{(j)}}{\partial W^{(i)}}$ be the perturbation of the partial derivative of $H^{(j)}$ with perturbations given by $\Delta \cW$ and $\Delta \cU$.

\begin{claim}\label{claim_grad}
    Suppose that Assumption \ref{ass_1} holds. For any $j=1,\dots,l-1$, the change in the Jacobian of the $j$-th layer's output $H^{(j)}$ with respect to $W^{(i)}$ and $U^{(i)}$ satisfies:
    \begin{align}
        \bignormFro{\frac{{{\partial \tilde H}}^{(j)}}{\partial W^{(i)}} - \frac{\partial H^{(j)}}{\partial W^{(i)}}}  &\lesssim \sum_{t=1}^j\Bigbrace{\bignorms{\Delta U^{(t)}} + \bignorms{\Delta W^{(t)}}}. \label{eq_claim_grad_1} \\
        \bignormFro{\frac{{\partial\tilde H}^{(j)}}{\partial  U^{(i)}}  - \frac{\partial H^{(j)}}{\partial U^{(i)}}} &\lesssim \sum_{t=1}^j\Bigbrace{\bignorms{\Delta U^{(t)}} + \bignorms{\Delta W^{(t)}}}. \label{eq_claim_grad_2}
    \end{align}
\end{claim}

\begin{proof}
    We will consider a fixed $i = 1,\dots,l-1$ and take induction over $j = i, \dots, l-1$.
    We focus on the proof of equation \eqref{eq_claim_grad_1}, while the proof of equation \eqref{eq_claim_grad_2} will be similar.
    To simplify the derivation, we use two notations for brevity.
    Let
    \begin{align*}
        F_j = P_{_G} \psi_{j}\bigbrace{H^{(j-1)}} W^{(j)} \text{ and }
        E_j = X U^{(j)} + \rho_{j}\bigbrace{F_j}.
    \end{align*}
    First, we consider the base case when $j = i$.
    By the chain rule, we have:
    \begin{align*}
        \bignormFro{\frac{\partial \tilde H^{(i)}}{\partial W^{(i)}}  - \frac{\partial H^{(i)}}{\partial W^{(i)}}} 
        =&  \bignormFro{\phi'_i\bigbrace{\tilde E_i} 
        \odot \frac{\partial \tilde E_i} {\partial W^{(i)}} - \phi'_i\bigbrace{E_i} \odot \frac{\partial E_i} {\partial W^{(i)}} } \\
        \lesssim&  \bignormFro{\phi'_i\bigbrace{\tilde E_i}  - \phi'_i\bigbrace{E_i} } + \bignormFro{\frac{\partial\tilde E_i} {\partial W^{(i)}} - \frac{\partial E_i} {\partial W^{(i)}} }. 
    \end{align*}
    From Claim \ref{claim_perturb}, we know
    \begin{align*}
        \bignormFro{\phi'_i\bigbrace{\tilde E_i}  - \phi'_i\bigbrace{E_i}} \leq \kappa_1 \bignormFro{\tilde E_i - E_i} \lesssim \bignorms{\Delta W^{(i)}} + \bignorms{\Delta U^{(i)}}.
    \end{align*}
    By the chain rule again, we get:
    \begin{align*}
        \bignormFro{\frac{\partial \tilde E_i} {\partial W^{(i)}} - \frac{\partial E_i} {\partial W^{(i)}} } 
        \lesssim \bignormFro{\rho'_i\bigbrace{\tilde F_i} - \rho'_i\bigbrace{F_i}} + \bignormFro{\frac{\partial \tilde F_i} {\partial W^{(i)}} - \frac{\partial F_i} {\partial W^{(i)}}} 
        \lesssim {\bignorms{\Delta W^{(i)}} + \bignorms{\Delta U^{(i)}}} \tag{by Claim \ref{claim_perturb} again}.
    \end{align*}
    Hence, we know that equation \eqref{eq_claim_grad_1} will be correct when $j=i$. Assuming that equation \eqref{eq_claim_grad_1} will be correct for any $j$ up to $j \ge i$, we have
    \begin{align*}
        & \bignormFro{\frac{\partial \tilde H^{(j+1)}}{\partial W^{(i)}} - \frac{\partial H^{(j+1)}}{\partial W^{(i)}}} \\
        \lesssim & \bignormFro{\phi'_{j+1}\bigbrace{\tilde E_{j+1}} - \phi'_{j+1}\bigbrace{E_{j+1}} } + \bignormFro{\frac{\partial \tilde E_{j+1}} {\partial W^{(i)}} - \frac{\partial E_{j+1}} {\partial W^{(i)}} } \\
        \lesssim & \sum_{t=1}^{j+1}\Bigbrace{\bignorms{\Delta U^{(t)}} + \bignorms{\Delta W^{(t)}}} + \bignormFro{\rho'_{j+1}\bigbrace{\tilde F_{j+1}} - \rho'_{j+1}\bigbrace{F_{j+1}}} + \bignormFro{\frac{\partial \tilde F_{j+1}} {\partial W^{(i)}} - \frac{\partial F_{j+1}} {W^{(i)}}} \\
        \lesssim & \sum_{t=1}^{j+1}\Bigbrace{\bignorms{\Delta U^{(t)}}+\bignorms{\Delta W^{(t)}}} + \bignormFro{\psi'_{j+1}\bigbrace{\tilde H^{(j)}} - \psi'_{j+1}\bigbrace{H^{(j)}}} +  \bignormFro{\frac{\partial \tilde H^{(j)}} {\partial W^{(i)}} - \frac{\partial H^{(j)}} {\partial W^{(i)}}} \\
        \lesssim & \sum_{t=1}^{j+1}\Bigbrace{\bignorms{\Delta U^{(t)}}+\bignorms{\Delta W^{(t)}}}. \tag{by Claim \ref{claim_perturb} and the induction step}
    \end{align*}
    The above steps all use Claim \ref{claim_perturb}.
    The last step additionally uses the induction hypothesis.
    From repeatedly applying the above beginning with $j = i$ along with the base case of equation \eqref{eq_claim_grad_1}, we conclude that equation \eqref{eq_claim_grad_1} holds.

    \smallskip
    Next, we consider the base case for equation \eqref{eq_claim_grad_2}. For the base case $j=i$, from the chain rule, by Claim \ref{claim_perturb}, we get:
    \begin{align*}
        \bignormFro{\frac{\partial \tilde H^{(i)}}{\partial U^{(i)}} - \frac{\partial H^{(i)}}{\partial U^{(i)}}} 
        \lesssim \bignormFro{\phi'_i\bigbrace{\tilde E_i} - \phi'_i\bigbrace{E_i} } + \bignormFro{\frac{\partial \tilde E_i} {\partial U^{(i)}} - \frac{\partial E_i} {\partial U^{(i)}} }
        \lesssim {\bignorms{\Delta W^{(i)}} + \bignorms{\Delta U^{(i)}}}.
    \end{align*}
    Hence, we know that equation \eqref{eq_claim_grad_2} will be correct when $j=i$. Assuming that equation \eqref{eq_claim_grad_2} will be correct for any $j$ up to $j \ge i$, we have
    \begin{align*}
        & \bignormFro{\frac{\partial \tilde H^{(j+1)}}{\partial U^{(i)}} - \frac{\partial H^{(j+1)}}{\partial U^{(i)}}} 
        \lesssim  \bignormFro{\phi'_{j+1}\bigbrace{\tilde E_{j+1}} - \phi'_{j+1}\bigbrace{E_{j+1}}} + \bignormFro{\frac{\partial \tilde E_{j+1}} {\partial U^{(i)}} - \frac{\partial E_{j+1}} {\partial U^{(i)}} } \\
        &\lesssim  \sum_{t=1}^{j+1}\Bigbrace{\bignorms{\Delta U^{(t)}} + \bignorms{\Delta W^{(t)}}} + \bignormFro{\rho'_{j+1}\bigbrace{\tilde F_{j+1}} - \rho'_{j+1}\bigbrace{F_{j+1}}} + \bignormFro{\frac{\partial \tilde F_{j+1}} {\partial U^{(i)}} - \frac{\partial F_{j+1}} {\partial U^{(i)}}}  \\
        &\lesssim  \sum_{t=1}^{j+1}\Bigbrace{\bignorms{\Delta U^{(t)}}+\bignorms{\Delta W^{(t)}}} + \bignormFro{\psi'_{j+1}\bigbrace{\tilde H^{(j)}} - \psi'_{j+1}\bigbrace{H^{(j)}}} +  \bignormFro{\frac{\partial \tilde H^{(j)}} {\partial U^{(i)}} - \frac{\partial H^{(j)}} {\partial U^{(i)}}} \\
        &\lesssim  \sum_{t=1}^{j+1}\Bigbrace{\bignorms{\Delta U^{(t)}}+\bignorms{\Delta W^{(t)}}}. \tag{by Claim \ref{claim_perturb} and the induction step}
    \end{align*}
    The second and third steps are based on Claim \ref{claim_perturb}.
    From repeatedly applying the above beginning with $j = i$ along with the base case of equation \eqref{eq_claim_grad_2}, we conclude that equation \eqref{eq_claim_grad_2} holds.
    The proof of claim \ref{claim_grad} is complete.
\end{proof}



\begin{proof}[Proof of Proposition \ref{claim_hess}]
    We will consider a fixed $i = 1,\dots,l-1$ and take induction over $j = i, \dots, l-1$.
    We focus on the proof of equation \eqref{eq_claim_hess_1}, while the proof of equation \eqref{eq_claim_hess_2} will be similar.
    To simplify the derivation, we use two notations for brevity.
    Let
    \begin{align*}
        F_j = P_{_G} \psi_{j}\bigbrace{H^{(j-1)}} W^{(j)} \text{ and }
        E_j = X U^{(j)} + \rho_{j}\bigbrace{F_j}.
    \end{align*}
    First, we consider the base case when $j = i$.
    By the chain rule, we have:
    We use the chain rule to get:
    \begin{align*}
        \frac {\partial^2 H^{(i)}} {\partial \bigbrace{W^{(i)}_{p,q}}^2}
        = \phi''_{i}(E_i) \odot \frac{\partial E_i}{\partial W_{p,q}^{(i)}} \odot \frac{\partial E_i}{\partial W_{p,q}^{(i)}}
        + \phi'_{i}(E_i) \odot \rho''_{i}(F_i) \odot {\frac{\partial F_i}{\partial W_{p,q}^{(i)}}} \odot {\frac{\partial F_i}{\partial W^{(i)}_{p,q}}}.
    \end{align*}
    Hence, the Frobenius norm of the Hessian of $H^{(i)}$ with respect to $W_i$ under perturbation on $W$ and $U$ turns to
    \begin{align*}
        \bignormFro{\bH_{\cW}^{(i)} [\tilde H^{(i)}] - \bH_{\cW}^{(i)} [H^{(i)}]} 
        & \lesssim \bignormFro{\phi''_i\bigbrace{\tilde E_i} - \phi''_i\bigbrace{E_i} } 
        + \bignormFro{\frac{\partial \tilde E_i} {\partial W^{(i)}} - \frac{\partial E_i} {\partial W^{(i)}} } + \bignormFro{\phi'_i\bigbrace{\tilde E_i} - \phi'_i\bigbrace{E_i} } \\
        & + \bignormFro{\rho''_i\bigbrace{\tilde F_i} - \rho''_i\bigbrace{F_i} } + \bignormFro{\frac{\partial \tilde F_i} {\partial W^{(i)}} - \frac{\partial F_i} {\partial W^{(i)}} }. 
    \end{align*}
    From Claim \ref{claim_perturb}, we know
    \begin{align*}
        \bignormFro{\phi''_i\bigbrace{\tilde E_i} - \phi''_i\bigbrace{E_i}} &\le \kappa_2 \bignormFro{\tilde E_i - E_i} \lesssim \bignorms{\Delta W^{(i)}} + \bignorms{\Delta U^{(i)}}, \\
        \bignormFro{\phi'_i\bigbrace{\tilde E_i} - \phi'_i\bigbrace{E_i} } &\le \kappa_1 \bignormFro{\tilde E_i - E_i} \lesssim \bignorms{\Delta W^{(i)}} + \bignorms{\Delta U^{(i)}},  \\
        \bignormFro{\rho''_i\bigbrace{\tilde F_i} - \rho''_i\bigbrace{F_i} } &\le \kappa_2 \bignormFro{\tilde F_i - F_i} \lesssim \bignorms{\Delta W^{(i)}} + \bignorms{\Delta U^{(i)}}. 
    \end{align*}
    From Claim \ref{claim_grad}, we have
    \begin{align*}
        \bignormFro{\frac{\partial \tilde E_i} {\partial W^{(i)}} - \frac{\partial E_i} {\partial W^{(i)}} } & \lesssim \bignorms{\Delta W^{(i)}} + \bignorms{\Delta U^{(i)}}, \\
        \bignormFro{\frac{\partial \tilde F_i} {\partial W^{(i)}} - \frac{\partial F_i} {\partial W^{(i)}} } & \lesssim \bignorms{\Delta W^{(i)}} + \bignorms{\Delta U^{(i)}}. 
    \end{align*}
    Hence, we know that equation \eqref{eq_claim_hess_1} will be correct when $j=i$. Assuming that equation \eqref{eq_claim_hess_1} will be correct for any $j$ up to $j \ge i$, we can get the following steps, by taking another derivative of the first-order derivative, we can get the following steps:
    \begin{small}
        \begin{align*}
            \frac {\partial^2 H^{(j+1)}} {\partial \bigbrace{W^{(i)}_{p,q}}^2}
            =& \, \phi''_{j+1}(E_{j+1}) \odot \frac{\partial E_{j+1}}{\partial W_{p,q}^{(i)}} \odot \frac{\partial E_{j+1}}{\partial W_{p,q}^{(i)}}
            + \phi'_{j+1}(E_{j+1}) \odot \rho''_{j+1}(F_{j+1}) \odot \frac{\partial F_{j+1}}{\partial W_{p,q}^{(i)}} \odot \frac{\partial F_{j+1}}{\partial W_{p,q}^{(i)}} \\
            & + \phi'_{j+1}(E_{j+1}) \odot \rho'_{j+1}(F_{j+1}) \odot {P_{_G} \Bigbrace{\psi''_{j+1}(H^{(j)}) \odot \frac{\partial H^{(j)}}{\partial W_{p,q}^{(i)}} \odot \frac{\partial H^{(j)}}{\partial W_{p,q}^{(i)}} + \psi'_{j+1}(H^{(j)}) \odot \frac{\partial^2 H^{(j)}}{\partial\bigbrace{W_{p,q}^{(i)}}^2}}W^{(j+1)}}. 
        \end{align*} 
    \end{small}%
    Thus, the Frobenius norm of the Hessian of $H^{(j+1)}$ with respect to $W^{(i)}$ satisfies:
    \begin{small}
        \begin{align*}
            & \bignormFro{\bH_{\cW}^{(i)} [\tilde H^{(j+1)}] - \bH_{\cW}^{(i)} [H^{(j+1)}]} 
            \lesssim \underbrace{\bignormFro{\phi''_{j+1}\bigbrace{\tilde E_{j+1}} - \phi''_{j+1}\bigbrace{E_{j+1}} }}_{A_1} 
            + \underbrace{\bignormFro{\frac{\partial \tilde E_{j+1}} {\partial W^{(i)}} - \frac{\partial E_{j+1}} {\partial W^{(i)}} }}_{B_1} \\
            + & \underbrace{\bignormFro{\phi'_{j+1}\bigbrace{\tilde E_{j+1}} - \phi'_{j+1}\bigbrace{E_{j+1}} }}_{A_2} 
            + \underbrace{\bignormFro{\rho''_{j+1}\bigbrace{\tilde F_{j+1}} - \rho''_{j+1}\bigbrace{F_{j+1}} }}_{A_3} \\
            +& \underbrace{\bignormFro{\frac{\partial \tilde F_{j+1}} {\partial W^{(i)}} - \frac{\partial F_{j+1}} {\partial W^{(i)}} }}_{B_2} 
            + \underbrace{\bignormFro{\rho'_{j+1}\bigbrace{\tilde F_{j+1}} - \rho'_{j+1}\bigbrace{F_{j+1}} }}_{A_4} \\
            + & \underbrace{\bignormFro{\psi''_{j+1}\bigbrace{\tilde H^{(j)}} - \psi''_{j+1}\bigbrace{H^{(j)}} }}_{A_5} 
            + \underbrace{\bignormFro{ \frac{\partial \tilde H^{(j)}} {\partial W^{(i)}} - \frac{\partial H^{(j)}} {\partial W^{(i)}} }}_{B_3} \\ 
            + & \underbrace{\bignormFro{\psi'_{j+1}\bigbrace{\tilde H^{(j)}} - \psi'_{j+1}\bigbrace{H^{(j)}}}}_{A_6} 
            + \underbrace{\bignormFro{\bH_{\cW}^{(i)} [\tilde H^{(j)}] - \bH_{\cW}^{(i)} [H^{(j)}]}}_{C_1}.
        \end{align*}
    \end{small}%
    Similarly, by Claim \ref{claim_perturb}, we get 
    \begin{align*}
        A_i \lesssim \sum_{t=1}^{j+1}\Bigbrace{\bignorms{\Delta W^{(t)}} + \bignorms{\Delta U^{(t)}}}, \text{ for } 1\le i\le 6.
    \end{align*}
    By Claim \ref{claim_grad}, we get
    \begin{align*}
        B_i \lesssim \sum_{t=1}^{j+1}\Bigbrace{\bignorms{\Delta W^{(t)}} + \bignorms{\Delta U^{(t)}}}, \text{ for } 1\le i\le 3.
    \end{align*}
    By the induction hypothesis, $C_1$ is also less than the above quantity. %
    From repeatedly applying the above beginning with $j = i$ along with the base case of equation \eqref{eq_claim_hess_1}, we conclude that equation \eqref{eq_claim_hess_1} holds.

    \smallskip
    Next, we consider the base case for equation \eqref{eq_claim_hess_2}. For the base case $j=i$, from the chain rule, we get:
    \begin{align*}
        \bignormFro{\bH_{\cU}^{(i)} [\tilde H^{(i)}] - \bH_{\cU}^{(i)} [H^{(i)}]}
        \lesssim & \bignormFro{\phi''_i\bigbrace{\tilde E_i} - \phi''_i\bigbrace{E_i} } + \bignormFro{\frac{\partial \tilde E_i} {\partial U^{(i)}} - \frac{\partial E_i} {\partial U^{(i)}} } \\
        \lesssim & \kappa_2 \bignormFro{\tilde E_i - E_i} + {\bignorms{\Delta W^{(i)}} +  \bignorms{\Delta U^{(i)}}} \tag{by Claim \ref{claim_grad}} \\
        \lesssim & {\bignorms{\Delta W^{(i)}} +  \bignorms{\Delta U^{(i)}}} \tag{by Claim \ref{claim_perturb}}.
    \end{align*}
    Hence, we know that equation \eqref{eq_claim_hess_2} will be correct when $j=i$. Assuming that equation \eqref{eq_claim_hess_2} will be correct for any $j$ up to $j \ge i$, we obtain the induction step
    %
    similar to the proof of equation \eqref{eq_claim_hess_1}, by Claim \ref{claim_perturb}, Claim \ref{claim_grad}, and the induction hypothesis, we conclude that equation \eqref{eq_claim_hess_2} holds.
\end{proof}


\subsection{Proof for message passing graph neural networks}\label{app_mpgnn}

Next, we present proof for message-passing graph neural networks.
First, in Appendix \ref{proof_trace}, we derive the trace bound, which separates the trace of the Hessian matrix into each entry of the weight matrices.
Then in Appendix \ref{proof_first} and \ref{proof_second}, we provide bounds on the first-order and second-order derivatives of the Hessian matrix.
Last, in Appendix \ref{proof_theorem}, building on these results, we finish the proof of Theorem \ref{thm_mpgnn}.

\subsubsection{Proof of Lemma \ref{lemma_trace_hess}}\label{proof_trace}

\begin{proof}[Proof of Lemma \ref{lemma_trace_hess}]
    Notice that $f(X, G) =  H^{(l)}$.
    Recall that in each layer for $1\le i\le l-1$, there are two weight matrices, a $d_{i-1}$ by $d_i$ matrix denoted as $W^{(i)}$, and a $d_0$ by $U^{(i)}$ matrix denoted as $U^{(i)}$.
    To deal with the trace of the Hessian $\bH^{(i)}$, we first notice that there are two parts in the trace:
    \begin{align*}
        \bigabs{\tr\big[\bH^{(i)}[\ell(H^{(l)}, y)]\big]} 
        \le \underbrace{\bigabs{\sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i}\frac{\partial^2 \ell(H^{(l)}, y)}{\partial \big(W^{(i)}_{p,q}\big)^2}}}_{T_1}
        + \underbrace{\bigabs{\sum_{p=1}^{d_0}\sum_{q=1}^{d_i} \frac{\partial^2 \ell(H^{(l)}, y)}{\partial\bigbrace{U_{p,q}^{(i)}}^2}}}_{T_2}.
    \end{align*}
    We can inspect $T_1$ and $T_2$ in the above step separately.
    First, we expand out the second-order derivatives in $T_1$.
    This will involve two terms by the chain rule.
    \begin{align}
        T_1 =\,& \bigabs{\sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i} \Bigg\langle\frac{\partial\ell(H^{(l)},y)}{\partial H^{(l)}},\frac{\partial^2 H^{(l)}}{\partial \big(W^{(i)}_{p,q}\big)^2}\Bigg\rangle} 
        + \bigabs{ \sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i} \Bigg\langle  \frac{\partial^2 \ell(H^{(l)}, y)}{\partial \big(H^{(l)}\big)^2} \frac{\partial H^{(l)}}{\partial W^{(i)}_{p,q}}, \frac{\partial H^{(l)}}{\partial W^{(i)}_{p,q}} \Bigg\rangle} \nonumber \\
        \leq\,& \sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i}\bignorm{\frac{\partial\ell(H^{(l)}, y)}{\partial H^{(l)}}}\bignorm{\frac{\partial^2 H^{(l)}}{\partial \big(W^{(i)}_{p,q}\big)^2}} + \sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i} \bignorms{\frac{\partial^2 \ell(H^{(l)}, y)}{\partial \big(H^{(l)}\big)^2}}\bignorm{\frac{\partial H^{(l)}}{\partial W^{(i)}_{p,q}}}^2 \nonumber \\
        \leq\,& \kappa_0 \sqrt{k} \sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i} \bignorm{\frac{\partial^2 H^{(l)}}{\partial \big(W^{(i)}_{p,q}\big)^2}} + \kappa_1  k \sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i} \bignorm{\frac{\partial H^{(l)}}{\partial W^{(i)}_{p,q}}}^2. \label{eq_lemma_loss}
    \end{align}
    The last step is because $\ell(\cdot)$ is $\kappa_0$-Lipschitz continuous and $\ell'(\cdot)$ is $\kappa_1$-Lipschitz continuous, under Assumption \ref{ass_1}.
    Thus, the Euclidean norm of ${\frac{\partial \ell(H^{(l)}, y)}{\partial H^{(l)}}} $ is at most $\kappa_0 \sqrt k$, since $H^{(l)}$ is a $k$-dimensional vector.
    Recall from step \eqref{eq_mpgnn_readout} that $H^{(l)} = \frac{1}{n}\bm{1}_n^{\top} H^{(l-1)}W^{(l)}$. Hence, we have
    \begin{align}
        \bignorm{\frac{\partial H^{(l)}}{\partial W^{(i)}_{p,q}}} 
        &= \bignorm{\frac{1}{n} \bm{1}_n^{\top} \frac{\partial H^{(l-1)}}{\partial W^{(i)}_{p,q}}W^{(l)}} \nonumber \\
        &\leq \bignorm{\frac{1}{n} \bm{1}_n^{\top}} \bignorms{\frac{\partial H^{(l-1)}}{\partial W^{(i)}_{p,q}}W^{(l)}}\leq \frac{1}{\sqrt{n}}\bignorms{\frac{\partial H^{(l-1)}}{\partial W^{(i)}_{p,q}}} \bignorms{W^{(l)}}. \label{eq_lemma_readout_1}
    \end{align}
    In a similar vein, the Euclidean norm of ${\frac{\partial^2 \ell(H^{(l)}, y)}{\partial (H^{(l)})^2}}$ is at most $\kappa_1 k$, since the second-order derivatives become a $k$ by $k$ matrix. Then, we get
    \begin{align}
        \bignorm{\frac{\partial^2 H^{(l)}}{\partial \bigbrace{W^{(i)}_{p,q}}^2}} &= \bignorm{\frac{1}{n} \bm{1}_n^{\top} \frac{\partial^2 H^{(l-1)}}{\partial \bigbrace{W^{(i)}_{p,q}}^2}W^{(l)}} \nonumber \\
        &\leq \bignorm{\frac{1}{n} \bm{1}_n^{\top}} \bignorms{\frac{\partial^2 H^{(l-1)}}{\partial \bigbrace{W^{(i)}_{p,q}}^2}W^{(l)}}\leq \frac{1}{\sqrt{n}}\bignorms{\frac{\partial^2 H^{(l-1)}}{\partial \bigbrace{W^{(i)}_{p,q}}^2}} \bignorms{W^{(l)}}. \label{eq_lemma_readout_2}
    \end{align}
    After substituting equations \eqref{eq_lemma_readout_1} and \eqref{eq_lemma_readout_2} into equation \eqref{eq_lemma_loss}, we get: 
    \begin{align*}
        T_1 &\leq \frac{\kappa_0 \sqrt{k}}{\sqrt{n}} \bignorms{W^{(l)}} \sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i} \bignorms{\frac{\partial^2 H^{(l-1)}}{\partial \big(W^{(i)}_{p,q}\big)^2}} + \frac {\kappa_1 k} n \bignorms{W^{(l)}}^2 \sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i} \bignorms{\frac{\partial H^{(l-1)}}{\partial W^{(i)}_{p,q}}}^2. \\
        &\leq \frac{\kappa_0 \sqrt{k}}{\sqrt{n}} \bignorms{W^{(l)}} \sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i} \bignormFro{\frac{\partial^2 H^{(l-1)}}{\partial \big(W^{(i)}_{p,q}\big)^2}} +  \frac {\kappa_1 k} n \bignorms{W^{(l)}}^2 \sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i}\bignormFro{\frac{\partial H^{(l-1)}}{\partial W_{p,q}^{(i)}}}^2.
    \end{align*}
    The proof for the case of $T_2$ concerning $U^{(i)}$ follows the same steps as above.
    Without belaboring all the details, one can get that
    \begin{align}
        T_2 \le  \frac{\kappa_0 \sqrt k}{\sqrt n} \bignorms{W^{(l)}} \sum_{p=1}^{d_{0}}\sum_{q=1}^{d_i} \bignormFro{\frac{\partial^2 H^{(l-1)}}{\partial\bigbrace{U_{p,q}^{(i)}}^2}}
        + \frac{\kappa_1 k}{n} \bignorms{W^{(l)}}^2 \sum_{p=1}^{d_0}\sum_{q=1}^{d_{i}}\bignormFro{\frac{\partial H^{(l-1)}}{\partial U_{p,q}^{(i)}}}^2.
    \end{align}
    This completes the proof of Lemma \ref{lemma_trace_hess}.
\end{proof}


\subsubsection{Dealing with first-order derivatives}\label{proof_first}

Based on Lemma \ref{lemma_trace_hess}, the analysis involves two parts,
one on the first-order derivatives of $H^{(j)}$ for all layers $j$,
and the other on the second-order derivatives of $H^{(j)}$ for all layers $j$.

\begin{proposition}\label{prop_first_mpgnn}
    In the setting of Theorem \ref{thm_mpgnn}, 
    the first-order derivative of $H^{(j)}$ with respect to $W^{(i)}$ and $U^{(i)}$ satisfies the following, for any $i = 1,\dots,l-1$ and $j\ge i$:
    \begin{align}
        \bignormFro{\frac{\partial H^{(j)}} {\partial W^{(i)}}}
        \le\,& \kappa_0^{3(j-i+1)} \sqrt{d_i} \bignorms{P_{_G}}^{j-i+1} \bignormFro{H^{(i-1)}} \prod_{t=i+1}^j \bignorms{W^{(t)}}, \label{eq_deri_W} \\
        \bignormFro{\frac{\partial H^{(j)}} {\partial U^{(i)}}} \le\,& \kappa_0^{3(j-i)+1} \sqrt{d_i} \bignorms{P_{_G}}^{j-i+1} \bignormFro{X} \prod_{t=i+1}^j \bignorms{W^{(t)}}. \label{eq_deri_U}
    \end{align}
\end{proposition}

\begin{proof}
    We will consider a fixed $i = 1,\dots,l-1$ and take induction over $j = i, \dots, l-1$.
    We focus on the proof of equation \eqref{eq_deri_W}, while the proof of equation \eqref{eq_deri_U} will be similar.
    First, we consider the base case when $j = i$.
    Let $W^{(i)}_{p,q}$ be the $(p,q)$-th entry of $W^{(i)}$, for any valid indices $p$ and $q$.
    Recall that $\phi_i(\cdot)$ is $\kappa_0$-Lipschitz continuous from Assumption \ref{ass_1}, for any $i = 1,\dots, l-1$.
    Therefore,
    \begin{align}
        \bignormMax{\phi'_i(x)} \le \kappa_0,~~
        \bignormMax{\psi'_i(x)} \le \kappa_0,~~\text{and}~~
        \bignormMax{\rho'_i(x)} \le \kappa_0. \label{eq_deri}
    \end{align} 
    For each $(p, q)$-entry of $W^{(i)}$, by the chain rule, we have:
    \begin{align}
        \bignormFro{\frac {\partial H^{(i)}} {\partial W^{(i)}_{p,q}}}
        &= \bignormFro{\phi'_i\bigbrace{X U^{(i)} + \rho_i\bigbrace{P_{_G} \psi_i(H^{(i-1)}) W^{(i)}}}
        \odot \frac{\partial \bigbrace{X U^{(i)} + \rho_i\bigbrace{P_{_G} \psi_i(H^{(i-1)}) W^{(i)}}}} {\partial W^{(i)}_{p,q}}} \label{eq_W_prime} \\
        &\le \kappa_0\bignormFro{\frac{\partial \rho_i\bigbrace{P_{_G} \psi_i(H^{(i-1)}) W^{(i)}}} {\partial W^{(i)}_{p,q}}} \tag{by equation \eqref{eq_deri}} \\
        &= \kappa_0\bignormFro{\rho'_i\bigbrace{P_{_G} \psi_i(H^{(i-1)}) W^{(i)}} \odot \frac{\partial\bigbrace{P_{_G} \psi_i(H^{(i-1)}) W^{(i)}}} {W^{(i)}_{p,q}}} \nonumber \\
        &\le \kappa_0^2\bignormFro{\frac{\partial \bigbrace{P_{_G} \psi_i(H^{(i-1)}) W^{(i)}}} {\partial W^{(i)}_{p,q}}}. \tag{again by equation \eqref{eq_deri}}
    \end{align}
    Notice that only the $q$-th column of the derivative $P_{_G} \psi_i(H^{(i-1)}) W^{(i)}$ is nonzero, which is equal to the $p$'th column of $P_{_G} \psi_i(H^{(i-1)})$. Thus, the Jacobian of $H^{(i)}$ over $W^{(i)}$ satisfies:
    \begin{align}
        \bignormFro{\frac{\partial H^{(i)}} {\partial W^{(i)}}}
        =& \sqrt{\sum_{p=1}^{d_{i-1}} \sum_{q=1}^{d_i} \bignormFro{\frac {\partial H^{(i)}} {\partial W_{p,q}^{(i)}}}^2} \nonumber \\
        \le& \kappa_0^2 \sqrt{\sum_{p=1}^{d_{i-1}} \sum_{q=1}^{d_i} \bignormFro{\frac {\partial\bigbrace{P_{_G} \psi_i(H^{(i-1)}) W^{(i)}}} {W_{p,q}^{(i)}}}^2}
        = \kappa_0^2 \sqrt{d_i} \bignormFro{P_{_G} \psi_i(H^{(i-1)})}. \label{eq_mpgnn_base}
    \end{align}
    Therefore, the above equation \eqref{eq_mpgnn_base} implies that equation \eqref{eq_deri_W} holds in the base case.
    Next, we consider the induction step from layer $j$ to layer $j+1$. The derivative of $H^{(j+1)}$ with respect to $W^{(i)}_{p,q}$ satisfies:
    \begin{align*}
        & \bignormFro{\frac {\partial H^{(j+1)}} {\partial W^{(i)}_{p,q}}} \\
        =& \bignormFro{\phi'_{j+1}\bigbrace{X U^{(j+1)} + \rho_{j+1}\bigbrace{P_{_G} \psi_{j+1}(H^{(j)}) W^{(j+1)}}}
        \odot \frac{\partial \bigbrace{X U^{(j+1)} + \rho_{j+1}\bigbrace{P_{_G} \psi_{j+1}(H^{(j)}) W^{(j+1)}}}} {\partial W^{(i)}_{p,q}}} \\
        \le& \kappa_0\bignormFro{\frac{\partial\rho_{j+1}\bigbrace{P_{_G} \psi_{j+1}(H^{(j)}) W^{(j+1)}}} {\partial W^{(i)}_{p,q}}} \tag{by equation \eqref{eq_deri}} \\
        \le& \kappa_0\bignormFro{\rho'_{j+1}\bigbrace{P_{_G} \psi_{j+1}(H^{(j)}) W^{(j+1)}} \odot \frac{\partial\bigbrace{P_{_G} \psi_{j+1}(H^{(j)}) W^{(j+1)}}} {\partial W^{(i)}_{p,q}}} \\
        \le& \kappa_0^2\bignormFro{P_{_G} \frac{\partial \psi_{j+1}(H^{(j)}) } {\partial W^{(i)}_{p,q}}W^{(j+1)}} \tag{again by equation \eqref{eq_deri}}
    \end{align*}
    By applying equation \eqref{eq_deri} w.r.t. $\psi'_{j+1}$, The above is less than:
    \begin{align*} 
         \kappa_0^2\bignorms{P_{_G}}\bignormFro{\psi_{j+1}'(H^{(j)}) \odot \frac{\partial H^{(j)} } {\partial W^{(i)}_{p,q}}}\bignorms{W^{(j+1)}} 
        \le \kappa_0^3\bignorms{P_{_G}}\bignorms{W^{(j+1)}}\bignormFro{\frac{\partial H^{(j)} } {\partial W^{(i)}_{p,q}}}. 
    \end{align*}
    Hence, the Jacobian of $H^{(j+1)}$ with respect to $W^{(i)}$ satisfies:
    \begin{align*}
        \bignormFro{\frac {\partial H^{(j+1)}} {\partial W^{(i)}}} \le \kappa_0^3\bignorms{P_{_G}}\bignorms{W^{(j+1)}}\bignormFro{\frac{\partial H^{(j)} } {\partial W^{(i)}}}.
    \end{align*} 
    From repeatedly applying the above beginning with $j = i$ along with the base case of equation \eqref{eq_mpgnn_base}, we conclude that equation \eqref{eq_deri_W} holds.

    \smallskip
    Next, we consider the base case for equation \eqref{eq_deri_U}. For each $(p, q)$-th entry of $U^{(i)}$, from the chain rule we get:
    \begin{align*}
        \bignormFro{\frac{\partial H^{(i)}} {\partial U^{(i)}_{p, q}}}
        &= \bignormFro{\phi'_i\Bigbrace{X U^{(i)} + \rho_i\bigbrace{P_{_G} \psi_i(H^{(i-1)}) W^{(i)}}} \odot \frac{\partial\Bigbrace{X U^{(i)} + \rho_i\bigbrace{P_{_G} \psi_i(H^{(i-1)}) W^{(i)}}}} {\partial U^{(i)}_{p,q}}} \\
        &\le \kappa_0 \bignormFro{\frac {\partial (X U^{(i)})} {\partial U^{(i)}_{p, q}}}. \tag{by equation \eqref{eq_deri}}
    \end{align*}
    Therefore, by summing over $p=1,\dots,d_0$ and $q=1,\dots,d_i$, we get:
    \begin{align}
        \bignormFro{\frac {\partial H^{(i)}} {\partial U^{(i)}}}
        &= \sqrt{\sum_{p=1}^{d_0} \sum_{q=1}^{d_i} \bignormFro{\frac {\partial H^{(i)}} {\partial U^{(i)}_{p, q}}}^2} \nonumber \\
        &\le \kappa_0 \sqrt{\sum_{p=1}^{d_0} \sum_{q=1}^{d_i} \bignormFro{\frac {\partial(X U^{(i)})} {\partial U^{(i)}_{p,q}}}^2}
        = \kappa_0 \sqrt{d_i} \bignormFro{X}. \label{eq_base_U}
    \end{align}
    Going from layer $i$ to layer $j+1$, the derivative of $H^{(j+1)}$ with respect to $U^{(i)}_{p,q}$ satisfies:
    \begin{align*}
        & \bignormFro{\frac{\partial H^{(j+1)}} {\partial U^{(i)}_{p, q}}} \\
        &= \bignormFro{\phi'_{j+1}\bigbrace{X U^{(j+1)} + \rho_{j+1}\bigbrace{P_{_G} \psi_{j+1}(H^{(j)}) W^{(j+1)}}} \odot \frac{\partial\bigbrace{X U^{(j+1)} + \rho_{j+1}\bigbrace{P_{_G} \psi_{j+1}(H^{(j)}) W^{(j+1)}}}} {\partial U^{(i)}_{p,q}}} \\
        &\le \kappa_0 \bignormFro{\frac {\partial \rho_{j+1}\bigbrace{P_{_G} \psi_{j+1}(H^{(j)}) W^{(j+1)}}} {\partial U^{(i)}_{p, q}}} \tag{by equation \eqref{eq_deri} w.r.t. $\phi'_{j+1}$}\\
        &\le \kappa_0^3\bignorms{P_{_G}}\bignorms{W^{(j+1)}}\bignormFro{\frac{\partial H^{(j)} } {\partial U^{(i)}_{p,q}}}. \tag{by equation \eqref{eq_deri} w.r.t. $\rho'_{j+1}, \psi'_{j+1}$}
    \end{align*}
    Hence, the Jacobian of $H^{(j+1)}$ with respect to $U^{(i)}$ satisfies:
    \begin{align*}
        \bignormFro{\frac {\partial H^{(j+1)}} {\partial U^{(i)}}} \le \kappa_0^3\bignorms{P_{_G}}\bignorms{W^{(j+1)}}\bignormFro{\frac{\partial H^{(j)} } {\partial U^{(i)}}}.
    \end{align*}
    By repeatedly applying the above step beginning with the base case of equation \eqref{eq_base_U}, we have proved that equation \eqref{eq_deri_U} holds.
    The proof of Proposition \ref{prop_first_mpgnn} is complete.
\end{proof}

\subsubsection{Deal with second-order derivatives}\label{proof_second}

In the second part towards showing Theorem \ref{thm_mpgnn} for MPNNs, we look at second-order derivatives of the embeddings.
This will appear later when we deal with the trace of the Hessian.
A fact that we will use throughout the proof is
\begin{align}
    \bignormMax{\phi''_i(x)} \le \kappa_1,~~
    \bignormMax{\psi''_i(x)} \le \kappa_1,~~\text{and}~~
    \bignormMax{\rho''_i(x)} \le \kappa_1, \label{eq_deri_sec}
\end{align}
for any $x$ and $i = 1,\dots,l-1$.
This is because $\phi'_i, \psi_i', $ and $\rho_i'$ are all $\kappa_1$-Lipschitz continuous from Assumption \ref{ass_1}.

\begin{proposition}\label{prop_second_mpgnn}
    In the setting of Theorem \ref{thm_mpgnn}, 
    the second-order derivative of $H^{(l)}$ with respect to $W^{(i)}$ and $U^{(i)}$ satisfies the following, for any $i = 1,\dots,l-1$ and any $j= i,\dots,l-1$:
    {\begin{align}
        \sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i} \bignormFro{\frac{\partial^2 H^{(j)}}{\bigbrace{W_{p,q}^{(i)}}^2}}
        &\le C_{i,j} \kappa_1 d_i \max(\bignorms{P_{_G}}^{j-i+2}, \bignorms{P_{_G}}^{2(j-i+1)})
        \bignormFro{H^{(i-1)}}^2
        \prod_{t=i+1}^{j} s_t^2, \label{eq_second_W} \\
        \sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i} \bignormFro{\frac{\partial^2 H^{(j)}}{\bigbrace{U_{p,q}^{(i)}}^2}}
        &\le \hat{C}_{i,j} \kappa_1 d_i \max(\bignorms{P_{_G}}^{j-i}, \bignorms{P_{_G}}^{2(j-i)}) \bignormFro{X}^2 \prod_{t=i+1}^j s_t^2, \label{eq_second_U}
    \end{align}}%
    where $C_{i,j}$
    $$ C_{i,j} = \left\{
        \begin{aligned}
            &\kappa_0^{3(j-i+1)}\frac{\kappa_0^{3(j-i)+2} - 1}{\kappa_0 - 1}, & \kappa_0 \neq 1, \\
            &3(j - i) + 2, & \kappa_0 = 1,
        \end{aligned}
    \right.
    $$
    and $\hat{C}_{i,j}$
    $$ \hat{C}_{i,j} = \left\{
        \begin{aligned}
            &\kappa_0^{3(j-i)}\frac{\kappa_0^{3(j-i)+1} - 1}{\kappa_0 - 1}, & \kappa_0 \neq 1, \\
            &3(j - i) + 1, & \kappa_0 = 1.
        \end{aligned}
    \right.
    $$
    are fixed constants that depend on the Lipschitz-continuity of the activation mappings.
\end{proposition}

\begin{proof} 
    First, we will consider equation \eqref{eq_second_W}.
    To simplify the derivation, we introduce two notations for brevity.
    Let
    \begin{align*}
        F_j = P_{_G} \psi_{j}\bigbrace{H^{(j-1)}} W^{(j)} \text{ and }
        E_j = X U^{(j)} + \rho_{j}\bigbrace{F_j}.
    \end{align*}
    In the base case when $j = i$, from the first-order derivative in equation \eqref{eq_W_prime}, we use the chain rule to get:
    \begin{align}
        \frac {\partial^2 H^{(i)}} {\partial \bigbrace{W^{(i)}_{p,q}}^2}
        =& \phi''_{i}(E_i) \odot \frac{\partial E_i}{\partial W_{p,q}^{(i)}} \odot \frac{\partial E_i}{\partial W_{p,q}^{(i)}}
        + \phi'_{i}(E_i) \odot \rho''_{i}(F_i) \odot {\frac{\partial F_i}{\partial W_{p,q}^{(i)}}} \odot {\frac{\partial F_i}{\partial W^{(i)}_{p,q}}}. \label{eq_Hi_chain}
    \end{align}
    Using equation \eqref{eq_deri_sec}, the maximum entries of $\phi''_{i}(\cdot), \rho''_{i}(\cdot)$ are at most $\kappa_1$.
    Using equation \eqref{eq_deri}, the maximum entry of
    $\phi'_{i}(\cdot)$ is at most $\kappa_0$.
    Notice that the derivative of $E_i$ can be reduced to the derivative of $F_i$ as follows:
    \begin{align}
        \bignormFro{\frac{\partial E_i}{\partial W_{p,q}^{(i)}}}^2
        = \bignormFro{\rho'_{i}(F_i) \odot \frac{\partial F_1}{\partial W_{p,q}^{(i)}}}^2 
        \le \kappa_0^2 \bignormFro{\frac{\partial F_i}{\partial W_{p,q}^{(i)}}}^2. \label{eq_second_p2}
    \end{align}
    Therefore, based on the conditions for first- and second-order derivatives (cf. \eqref{eq_deri} and \eqref{eq_deri_sec}), the Frobenius norm of the above equation \eqref{eq_Hi_chain} is at most:
    \begin{align*}
        \bignormFro{\frac{\partial^2 H^{(i)}} {\partial\bigbrace{W_{p,q}^{(i)}}^2}}
        \le \kappa_1 \bignormFro{\frac{\partial E_i} {\partial W_{p,q}^{(i)}}}^2 + \kappa_0\kappa_1 \bignormFro{\frac{\partial F_i}{\partial W_{p,q}^{(i)}}}^2
        \le (\kappa_0 + 1)\kappa_0\kappa_1 \bignormFro{\frac{\partial F_i}{\partial W_{p,q}^{(i)}}}^2.
    \end{align*}
    Notice that the derivative of $F_i$ with respect to $W^{(i)}_{p,q}$ is nonzero only in the $q$-th column of $F_i$, and is equal to the $p$-th column of $P_{_G} g_{i}(H^{(i-1)})$. Therefore, by summing over $p = 1,\dots,d_{i-1}$ and $q = 1,\dots,d_i$, we get:
    \begin{align*}
        \sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i} \bignormFro{\frac{\partial F_{i}}{\partial\bigbrace{W_{p,q}^{(i)}}^2}}^2
        \le d_i \bignormFro{P_{_G} \psi_i(H^{(i-1)})}^2.
    \end{align*}
    Therefore, we have derived the base case when $j = i$ as:
    \begin{align}
        \bignormFro{\frac{\partial^2 H^{(i)}}{\partial\bigbrace{W_{p,q}^{(i)}}^2}}
        \le (\kappa_0 + 1) \kappa_0^3 \kappa_1 d_i \bignorms{P_{_G}}^2 \bignormFro{H^{(i-1)}}^2. \label{eq_second_base_W}
    \end{align}
    Next, we consider the induction step from layer $j$ to layer $j+1$. This step is similar to the base case but also differs since $H^{(j)}$ is now dependent on $W^{(i)}$.
    Recall that the second-order derivatives satisfy equation \eqref{eq_deri_sec}.
    Based on the Lipschitz-continuity conditions, the Frobenius norm of the second-order derivatives satisfies:
    \begin{align}
        & \bignormFro{\frac{\partial^2 H^{(j+1)}}{\partial\bigbrace{W_{p,q}^{(i)}}^2}} \\
        &\le \kappa_1 \bignormFro{\frac{\partial E_{j+1}}{\partial W_{p,q}^{(i)}}}^2
        + \kappa_0\kappa_1 \bignormFro{\frac{\partial F_{j+1}}{\partial W_{p,q}^{(i)}}}^2
        + \kappa_0^2 \bignorms{P_{_G}} \bignorms{W^{(j+1)}} \Bigbrace{\kappa_1 \bignormFro{\frac{\partial H^{(j)}}{\partial W_{p,q}^{(i)}}}^2 + \kappa_0 \bignormFro{\frac {\partial^2 H^{(j)}} {\partial \bigbrace{W_{p,q}^{(i)}}^2}}} \nonumber \\
        &\le (\kappa_0 + 1)\kappa_0\kappa_1 \bignormFro{\frac {\partial F_{j+1}} {\partial W_{p,q}^{(i)}}}^2
        + \kappa_0^2\bignorms{P_{_G}} \bignorms{W^{(j+1)}} \Bigbrace{\kappa_1\bignormFro{\frac{H^{(j)}}{\partial W_{p,q}^{(i)}}}^2
        + \kappa_0\bignormFro{\frac {\partial^2 H^{(j)}} {\partial\bigbrace{W_{p,q}^{(i)}}^2}}}. \label{eq_second_p1}
    \end{align}
    The last step follows similarly as equation \eqref{eq_second_p2}.
    For the derivative of $F_{j+1}$, using the chain rule, we get:
    \begin{align*}
        \bignormFro{\frac{\partial F_{j+1}}{\partial W_{p,q}^{(i)}}}^2
        &= \bignormFro{P_{_G} \frac {\partial \psi_{j+1}(H^{(j)})} {\partial W_{p,q}^{(i)}} W^{(j+1)}}^2 \\
        &\le \bignorms{P_{_G}}^2 \bignorms{W^{(j+1)}}^2 \bignormFro{\frac{\partial \psi_{j+1}(H^{(j)})}{\partial W_{p,q}^{(i)}}}^2 \\
        &\le \bignorms{P_{_G}}^2 \bignorms{W^{(j+1)}}^2 \bignormFro{\psi'_{j+1}(H^{(j)}) \odot \frac{\partial H^{(j)}}{\partial W_{p,q}^{(i)}}}^2 \\
        &\le \kappa_0^2 \bignorms{P_{_G}}^2 \bignorms{W^{(j+1)}}^2 \bignormFro{\frac {\partial H^{(j)}} {\partial W_{p,q}^{(i)}}}^2.
    \end{align*}
    Therefore, combining the above with equations \eqref{eq_second_p1} together, we get the following result:
    \begin{align*}
        \bignormFro{\frac{\partial^2 H^{(j+1)}}{\partial\bigbrace{W_{p,q}^{(i)}}^2}}
        \le\,& \Bigbrace{(\kappa_0+1)\kappa_0^3\kappa_1 \bignorms{P_{_G}}^2 \bignorms{W^{(j+1)}}^2 + \kappa_0^2 \kappa_1 \bignorms{P_{_G}} \bignorms{W^{(j+1)}}} \bignormFro{\frac {\partial H^{(j)}} {\partial W_{p,q}^{(i)}}}^2 \nonumber \\
        &+ \kappa_0^3 \bignorms{P_{_G}} \bignorms{W^{(j+1)}} \bignormFro{\frac {\partial^2 H^{(j)}} {\partial \bigbrace{W^{(i)}_{p,q}}^2}} \nonumber \\
        \le& \max\Bigbrace{\bignorms{P_{_G}}, \bignorms{P_{_G}}^2}s_{j+1}^2
        \Bigbrace{(\kappa_0^2+\kappa_0+1)\kappa_0^2\kappa_1\bignormFro{\frac {\partial H^{(j)}} {\partial W_{p,q}^{(i)}}}^2
        + \kappa_0^3 \bignormFro{\frac {\partial^2 H^{(j)}} {\partial\bigbrace{W_{p,q}^{(i)}}^2}}}. %
    \end{align*}
    Based on equation \eqref{eq_deri_W} of Proposition \eqref{prop_first_mpgnn}, the first-order derivative of $H^{(j)}$ satisfies:
    \begin{align}
        \sum_{p=1}^{d_{i-1}} \sum_{q=1}^{d_i} \bignormFro{\frac{\partial H^{(j)}} {\partial W_{p,q}^{(i)}}}^2 \le \kappa_0^{6(j-i+1)}{d_i} \bignorms{P_{_G}}^{2(j-i+1)} \bignorm{H^{(i-1)}}^2 \prod_{t=i+1}^j s_t^2. \label{eq_mpgnn_second_base}
    \end{align}
    Applying equation \eqref{eq_mpgnn_second_base} to
    the above (and summing over $p = 1,\dots, d_{i-1}$ and $q = 1,\dots,d_i$) forms the induction step for showing equation \eqref{eq_second_W}:
    {\small\begin{align*}
         \sum_{p=1}^{d_{i-1}} \sum_{q=1}^{d_i} \bignormFro{\frac {\partial^2 H^{(j+1)}} {\partial \bigbrace{W_{p,q}^{(i)}}^2}}
        \le\,& \frac{\kappa_0^3 - 1}{\kappa_0 - 1} \kappa_0^{6(j-i+1)+2} \kappa_1 d_i \max\Bigbrace{\bignorms{P_{_G}}^{2(j-i)+3}, \bignorms{P_{_G}}^{2(j-i)+4}} \bignormFro{H^{(i-1)}}^2 \prod_{t=i+1}^{j+1} s_t^2 \\
        &+ \kappa_0^3\max\bigbrace{\bignorms{P_{_G}}, \bignorms{P_{_G}}^2} s_{j+1}^2
        \sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i}\bignormFro{\frac{\partial^2 H^{(j)}}{\partial\bigbrace{W_{p,q}^{(i)}}^2}}.
    \end{align*}}%
    By repeatedly applying the induction step along with the base case in equation \eqref{eq_second_base_W}, we have shown that equation \eqref{eq_second_W} holds:
    \begin{align}
        \sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i} \bignormFro{\frac{\partial^2 H^{(j)}}{\bigbrace{W_{p,q}^{(i)}}^2}}
        \le C_{i,j} \kappa_1 d_i \max(\bignorms{P_{_G}}^{j-i+2}, \bignorms{P_{_G}}^{2(j-i+1)})
        \bignormFro{H^{(i-1)}}^2
        \prod_{t=i+1}^{j} s_t^2, \label{eq_conclude_W}
    \end{align}
    where $C_{i,j}$ satisfies the following equation:
    $$ C_{i,j} = \left\{
        \begin{aligned}
            &\kappa_0^{3(j-i+1)}\frac{\kappa_0^{3(j-i)+2} - 1}{\kappa_0 - 1}, & \kappa_0 \neq 1, \\
            &3(j - i) + 2, & \kappa_0 = 1.
        \end{aligned}
    \right.
    $$
    \smallskip
    In the second part of the proof, we consider equation \eqref{eq_second_U} similar to the first part.
    However, the analysis will be significantly simpler.
    We first consider the base case. Similar to equation \eqref{eq_Hi_chain}, the second-order derivative of $H^{(i)}$ over $W^{(i)}_{p,q}$ satisfies, for any $p = 1,\dots, d_{0}$ and $q = 1,\dots, d_i$:
    \begin{align*}
        \bignormFro{\frac{\partial^2 H^{(i)}}{\partial\bigbrace{U_{p,q}^{(i)}}^2}}
        = \bignorm{\phi_i''(E_i) \odot \frac{\partial E_i}{\partial U^{(i)}_{p,q}} \odot \frac{\partial E_i}{\partial U^{(i)}_{p,q}}}
        \le \kappa_1 \bignormFro{\frac{\partial(X U^{(i)})} {\partial U^{(i)}_{p,q}}}^2.
    \end{align*}
    Therefore, by summing up the above over all $p$ and $q$, we get the base case result:
    \begin{align}
        \sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i} \bignormFro{\frac{\partial^2 H^{(i)}}{\partial\bigbrace{U_{p,q}^{(i)}}^2}}
        \le \kappa_1 d_i \bignormFro{X}^2.
    \end{align}
    Next, we consider the induction step from layer $j$ to layer $j+1$.
    This step follows the same analysis until equation \eqref{eq_conclude_W}, from which we can similarly derive that:
    \begin{align}
        \sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i} \bignormFro{\frac{\partial^2 H^{(j)}}{\bigbrace{U_{p,q}^{(i)}}^2}}
        \le \hat{C}_{i,j} \kappa_1 d_i \max(\bignorms{P_{_G}}^{j-i}, \bignorms{P_{_G}}^{2(j-i)}) \bignormFro{X}^2 \prod_{t=i+1}^j s_t^2.
    \end{align}
    where $\hat{C}_{i,j}$ satisfies the following equation:
    $$ \hat{C}_{i,j} = \left\{
        \begin{aligned}
            &\kappa_0^{3(j-i)}\frac{\kappa_0^{3(j-i)+1} - 1}{\kappa_0 - 1}, & \kappa_0 \neq 1, \\
            &3(j - i) + 1, & \kappa_0 = 1.
        \end{aligned}
    \right.
    $$
\end{proof}

\subsubsection{Proof of Theorem \ref{thm_mpgnn}}\label{proof_theorem}

Based on Propositions \ref{prop_first_mpgnn} and \ref{prop_second_mpgnn}, we are ready to present the proof of Theorem \ref{thm_mpgnn} for message passing GNNs.
First, we will apply the bounds on the derivatives back in Lemma \ref{lemma_trace_hess}.
After getting the trace of the Hessians, we then use the PAC-Bayes bound from Lemma \ref{lemma_gen_error} to complete the proof.

\begin{proof}[Proof of Theorem \ref{thm_mpgnn}]
    By applying equations \eqref{eq_deri_W} and \eqref{eq_second_W}  into Lemma \ref{lemma_trace_hess}'s result, we get that the trace of $\bH^{(l)}$ with respect to $W^{(i)}$ is less than:
    {\begin{align} % \bignormFro{H^{(i-1)}}^2{\prod_{t=i+1}^l s_t^2}
         &\left( \frac{\kappa_0 \sqrt{k}}{\sqrt n} C_{i,l-1} \kappa_1 d_i \max\big(\bignorms{P_{_G}}^{l-i+1}, \bignorms{P_{_G}}^{2(l-i)}\big)  %\nonumber \\
         + \frac{\kappa_1 k}{n} \kappa_0^{6(l-i)} \kappa_1 d_i \bignorms{P_{_G}}^{2(l-i)} \right) \bignormFro{H^{(i-1)}}^2 \prod_{t=i+1}^l s_t^2 \nonumber \\
        &\le (\kappa_0 C_{i,l-1} + \kappa_0^{6(l-i)}) \sqrt{\frac{{k}}{{n}}} \kappa_1 d_i \max\Bigbrace{\bignorms{P_{_G}}^{l-i+1}, \bignorms{P_{_G}}^{2(l-i)}}
        \bignormFro{H^{(i-1)}}^2 \prod_{t=i+1}^l s_t^2,\label{eq_mpgnn_pr2}
    \end{align}}%
    for any $i=1,2,\cdots,l-1$. Here we have
    $$ \kappa_0 C_{i,l-1} + \kappa_0^{6(l-i)} = \left\{
        \begin{aligned}
            &\kappa_0^{3(l-i)+1}\frac{\kappa_0^{3(l-i)} - 1}{\kappa_0 - 1}, & \kappa_0 \neq 1, \\
            &3(l - i) - 1, & \kappa_0 = 1.
        \end{aligned}
    \right.
    $$
    It remains to consider the Frobenius norm of $H^{(i-1)}$.
    Notice that this satisfies the following:
    \begin{align*}
        &\bignormFro{H^{(i-1)}}
        \le \kappa_0 \bignormFro{X U^{(i-1)} + \rho_{i-1}(P_{_G} \psi_{i-1}(H^{(i-2)}))W^{(i-1)}} \\
        &\le \kappa_0 \bignorms{U^{(i-1)}}\bignormFro{X}
        + \kappa_0^3 \bignorms{P_{_G}} \bignorms{W^{(i-1)}} \bignormFro{H^{(i-2)}}
        \le \kappa_0 s_i \bignormFro{X} + \kappa_0^3 \bignorms{P_{_G}} s_i \bignormFro{H^{(i-2)}}.
    \end{align*}
    By induction over $i$ for the above step, we get that the Frobenius norm of $H^{(i-1)}$ must be less than:
    \begin{align}
        \bigbrace{\kappa_0^{3(i-1)} + \sum_{j=0}^{i-2} \kappa_0^{3j+1}} {\sqrt{k}} \max_{(X, G, y) \sim \cD}\bignorms{X} \max \bigbrace{1,\bignorms{P_{_G}}^{i-1}} \prod_{j=1}^{i-1} s_j. \label{eq_mpgnn_pr1}
    \end{align}
    By applying the above \eqref{eq_mpgnn_pr1} back in \eqref{eq_mpgnn_pr2}, we have shown that the trace of $\bH^{(l)}$ with respect to $W^{(i)}$ is less than:
    \begin{align}\label{eq_mpgnn_pr3}
        C' \max_{(X, G, y) \sim \cD}\bignorms{X}^2 \kappa_1 d_i k \max(1, \norm{P_{_G}}^{2(l-1)}) \prod_{t=1: t\neq i}^l s_t^2,
    \end{align}
    where $C'$ satisfies the following equation:
    $$ C' = \left\{
        \begin{aligned}
            &\frac{(\kappa_0^{3l} - 1)(\kappa_0^{3(l-1)/2} - 1)^2}{(\kappa_0 - 1)^3}, & \kappa_0 \neq 1, \\
            &\frac{4}{9}l^3, & \kappa_0 = 1.
        \end{aligned}
    \right.
    $$
    To be specific, when $\kappa_0 = 1$, $(3(l-i)-1)i^2\leq \frac{4}{9}l^3$ . If $\kappa_0\neq 1$ and $i \geq 2$, we have
    \begin{align*}
        &\Bigbrace{\kappa_0^{3(l-i)+1}\frac{\kappa_0^{3(l-i)} - 1}{\kappa_0 - 1}}\bigbrace{\kappa_0^{3(i-1)} + \sum_{j=0}^{i-2} \kappa_0^{3j+1}}^2 \le \kappa_0^{3(l-i)+3}\frac{\kappa_0^{3(l-i)} - 1}{\kappa_0 - 1}\frac{(\kappa_0^{3(i-1)} - 1)^2}{(\kappa_0 - 1)^2}\\
        &= \frac{\kappa_0^{3l} - \kappa_0^{3(l-i+1)}}{(\kappa_0 - 1)^3}\Bigbrace{(\kappa_0^{3(l-i)} - 1)(\kappa_0^{3(i-1)} - 1)} 
        \le \frac{(\kappa_0^{3l} - 1)(\kappa_0^{3(l-1)/2} - 1)^2}{(\kappa_0 - 1)^3}.
    \end{align*}
    If $\kappa_0\neq 1$ and $i = 1$, we obtain 
    \begin{align*}
        \Bigbrace{\kappa_0^{3(l-i)+1}\frac{\kappa_0^{3(l-i)} - 1}{\kappa_0 - 1}}\Bigbrace{\kappa_0^{3(i-1)} + \sum_{j=0}^{i-2} \kappa_0^{3j+1}}^2 = \kappa_0^{3l - 2} \frac{\kappa_0^{3(l - 1)} - 1}{\kappa_0 - 1} \le \frac{(\kappa_0^{3l} - 1)(\kappa_0^{3(l-1)/2} - 1)^2}{(\kappa_0 - 1)^3}.
    \end{align*}
    
    The above works for the layers from the beginning until layer $l-1$.
    Last, we consider the trace of $\bH^{(l)}$ with respect to $W^{(l)}$ (notice that $\cU$ is not needed in the readout layer).
    Similar to equation \eqref{eq_lemma_loss}, one can prove that the trace of the Hessian with respect to $W^{(l)}$ satisfies:
    \begin{align*}
        \bigabs{\tr\big[\bH^{(l)}[\ell(H^{(l)}, y)]\big]}
        \leq\,& \kappa_0 \sqrt{k} \sum_{p=1}^{d_{l-1}}\sum_{q=1}^{d_l} \bignorm{\frac{\partial^2 H^{(l)}}{\partial \big(W^{(l)}_{p,q}\big)^2}} + \kappa_1 k \sum_{p=1}^{d_{l-1}}\sum_{q=1}^{d_l} \bignorm{\frac{\partial H^{(l)}}{\partial W^{(l)}_{p,q}}}^2 \\
        \leq\,& \kappa_0 \sqrt{k} \sum_{p=1}^{d_{l-1}}\sum_{q=1}^{d_l} \bignorm{\frac{1}{n} \bm{1}_n^{\top} H^{(l-1)}\frac{\partial^2 W^{(l)}}{\partial \big(W^{(l)}_{p,q}\big)^2}} + \kappa_1 k \sum_{p=1}^{d_{l-1}}\sum_{q=1}^{d_l} \bignorm{\frac{1}{n} \bm{1}_n^{\top} H^{(l-1)}\frac{\partial W^{(l)}}{\partial W^{(l)}_{p,q}}}^2 \\
        \leq\,& \kappa_1 k \sum_{p=1}^{d_{l-1}}\sum_{q=1}^{d_l} \bignorm{\frac{1}{n}\bm{1}_n}^2 \bignorms{H^{(l-1)}\frac{\partial W^{(l)}}{\partial W^{(l)}_{p,q}}}^2 \\
        =\,& \kappa_1 \frac{k}{n} {d_l \bignormFro{H^{(l-1)}}^2}
    \end{align*}
    By equation \eqref{eq_mpgnn_pr1}, the above is bounded by
    \begin{align*}
        & \kappa_1  \frac{k}{n} d_l \bigbrace{\kappa_0^{3(l-1)} + \sum_{j=0}^{l-2} \kappa_0^{3j+1}}^2 \max_{(X, G, y) \sim \cD}\bignormFro{X}^2 \max \bigbrace{1,\bignorms{P_{_G}}^{2(l-1)}} \prod_{j=1}^{l-1} s_j^2 \\
        \leq~ & C_l \max_{(X, G, y) \sim \cD}\bignorms{X}^2 \kappa_1 d_l k \max\Bigbrace{1, \bignorms{P_{_G}}^{2(l-1)}} \prod_{t=1: t\neq l}^l s_t^2,
    \end{align*}%
    since $\frac{\bignormFro{X}^2}{n} \le \bignorms{X}^2$,
    where $C_l$ satisfies the following equation:
    $$ C_l = \left\{
        \begin{aligned}
            &\kappa_0^2\frac{(\kappa_0^{3(l-1)} - 1)^2}{(\kappa_0 - 1)^2}, & \kappa_0 \neq 1, \\
            &l^2, & \kappa_0 = 1.
        \end{aligned}
    \right.
    $$
    Finally, let
    \begin{align}
        \tilde C &= \max(C',C_l). \label{eq_const} 
    \end{align}
    From the value of $C'$ above and the value of $C_l$, we get that $\tilde C$ is equal to
    \begin{align}
        \tilde C &= \left\{
        \begin{aligned}
            &\frac{(\kappa_0^{3l} - 1)(\kappa_0^{3(l-1)/2} - 1)^2}{(\kappa_0 - 1)^3}, & \kappa_0 \neq 1, \nonumber \\
            &\frac{1}{2}l^3, & \kappa_0 = 1. \nonumber
        \end{aligned}\right.
    \end{align}
    Similarly by applying equations \eqref{eq_deri_U} and \eqref{eq_second_U} into Lemma \ref{lemma_trace_hess}, the trace of $\bH^{(l)}$ with respect to $U^{(i)}$ is also less than equation \eqref{eq_mpgnn_pr3}.
    Therefore, we have completed the proof for message-passing neural networks. 
\end{proof}


\subsection{Proof of matching lower bound (Theorem \ref{prop_lb})}\label{app_lb}

For simplicity, we will exhibit the instance for a graph ConvNet, that is, we ignore the parameters in $\cU$ and also set the mapping $\rho_t$ and $\psi_t$ as the identity mapping.
Further, we set the mapping $\phi_t(x) = x$ as the identity mapping, too, for simplifying the proof.
In the proof, we show that for an arbitrary configuration of weight matrices $W^{(1)}, W^{(2)}, \dots, W^{(l)}$, there exists a data distribution such that for this particular configuration, the generation gap with respect to the data distribution satisfies the desired equation \eqref{eq_lb}.

\begin{proof}[Proof of Theorem \ref{prop_lb}]
    Recall that the underlying graph for the lower bound instance is a complete graph.
    Next, we will specify the other parts of the data distribution $\cD$.
    Let $Z = \prod_{i=1}^l W^{(i)}$ denote the product of the weight matrices.
    We are going to construct a binary classification problem.
    Thus, the dimension of $Z$ will be equal to $n$ by $2$.
    Let $Z = U D V^{\top}$ be the singular value decomposition of $Z$.
    Let $\lambda_{\max}(Z)$ be the largest singular value of $Z$, with corresponding left and right singular vectors $u_1$ and $v_1$, respectively.
    Within the hypothesis set $\cH$, $\lambda_{\max}(Z)$ can be as large as $\prod_{i=1}^l s_i$.
    Denote a random draw from $\cD$ as $X, G, y$, corresponding to node features, the graph, and the label:
    \begin{enumerate}[leftmargin=15pt]
        \item The feature matrix $X$ is is equal to $\bm{1}_n u_1^{\top}$;
        \item The class label $y$ is drawn uniformly between $+1$ and $-1$;
        \item Lastly, the diffusion matrix $P$ is the adjacency matrix of $G$, which has a value of one in every entry of $P$.
    \end{enumerate}
    Given the example and the weight matrices, we will use the logistic loss to evaluate $f$'s loss.
    Notice that $P = \bm{1}_n \bm{1}_n^{\top}$.
    Thus, one can verify  $\lambda_{\max}(P) = n$.
    Crucially, the network output of our GCN is equal to
    \begin{align*}
        H^{(l)} = \frac 1 n \bm{1}_n^{\top} P^{l-1} X W^{(1)} W^{(2)} \cdots W^{(l)}
        = n^{l-1} \Bigbrace{\frac{\bm{1}_n^{\top} X}{n} Z}
        = n^{l-1} \Bigbrace{u_1^{\top} U D V^{\top}}
        = \bigbrace{n^{l-1} \lambda_{\max}(Z)} v_1^{\top}.
    \end{align*}
    Let us denote $\alpha = n^{l-1} \lambda_{\max}(Z)$---the spectral norms of the diffusion matrix and the layer weight matrices.
    Let $v_{1,1}, v_{1,2}$ be the first and second coordinate of $v_1$, respectively.
    Notice that $y$ is drawn uniformly between $+1$ or $-1$.
    Thus, with probability $1/2$, the loss of this example is $\log(1 + \exp(- \alpha \cdot v_{1,1}))$;
    with probability $1/2$, the loss of this example is $\log(1 + \exp(\alpha\cdot v_{1,2}))$.
    Let $b_i$ be a random variable that indicates the logistic loss of the $i$-th example.
    The generalization gap is equal to
    \begin{align*}
        \epsilon = {\frac 1 N \sum_{i=1}^N b_i} - \frac 1 2\Bigbrace{\log(1 + \exp(- \alpha \cdot v_{1,1})) + \log(1 + \exp(\alpha \cdot v_{1,2}))}.
    \end{align*}
    By the central limit theorem, as $N$ grows to infinity, the generalization gap $\epsilon$ converges to a normal random variable whose mean is zero and variance is equal to
    \begin{align*}
        \frac 1 {4N} \Bigbrace{\log(1 + exp(-\alpha \cdot v_{1,1})) - \log(1 + \exp(\alpha \cdot v_{1,2}))}^2 \gtrsim \frac{\alpha^2}{N},
    \end{align*}
    for large enough values of $N$.
    As a result, with probability at least $0.1$, when $N$ is large enough, the generalization gap $\epsilon$ must be at least 
    \[ \bigo{\sqrt{\frac{\alpha^2}  N}}, \text{ where } \alpha = \bignorms{P_{_G}}^{l-1} \lambda_{\max}\Bigg(\prod_{i=1}^l W^{(i)}\Bigg). \]
    Notice that the product matrix's spectral norm is at most $\prod_{i=1}^l s_i$.
    Thus, we have completed the proof of equation \eqref{eq_lb}.
\end{proof}

\subsection{Proof for graph isomorphism networks (Corollary \ref{thm_gin})}\label{app_gin}

To be precise, we state the loss function for learning graph isomorphism networks as the averaged loss over all the classification layers:
\begin{align}\label{eq_loss_gin}
    \bar{\ell}(f(X, G), y) = \frac 1 {(l-1)} \sum_{i=1}^{l-1}  \ell\Bigbrace{\frac 1 n \bm{1}_n^{\top} H^{(i)} V^{(i)}, y}.
\end{align}
Thus, $\hat \cL_{GIN}(f)$ is equivalent to the empirical average of $\bar\ell$ over $N$ samples from $\cD$.
$\cL_{GIN}(f)$ is then equivalent to the expectation of $\bar\ell$ over a random sample from $\cD$.

\begin{proof}[Proof of Corollary \ref{thm_gin}]
    This result follows the trace guarantee from Lemma \ref{lemma_trace_hess}.
    For any $i=1,\dots,l-1$ and any $j = i,\dots,l-1$, we can derive the following result with similar arguments:
    \begin{align*}
        \bigabs{\tr\Big[\bH_{\cW}^{(i)}\big[\ell\Bigbrace{\frac 1 n \bm{1}_n^{\top} H^{(j)} V^{(j)}, y}\big]\Big]} 
        \leq \frac{\kappa_0 \sqrt{k}}{\sqrt{n}} \bignorms{V^{(j)}} \sum_{p=1}^{d_{i-1}}\sum_{q=1}^{d_i} \bignormFro{\frac{\partial^2 H^{(j)}}{\partial \big(W^{(i)}_{p,q}\big)^2}} + \frac {\kappa_1  k} n \bignorms{V^{(j)}}^2 \bignormFro{\frac{\partial H^{(j)}}{\partial W^{(i)}}}^2.
    \end{align*}
    Next, we repeat the steps in Propositions \ref{prop_first_mpgnn} and \ref{prop_second_mpgnn}, for any $i = 1,\dots,l-1$ and any $j = i,\dots,l-1$:
    \begin{align*}
        & \max_{(X, G, y) \sim \cD} \bigabs{\tr\big[\bH^{(i)}[\ell\bigbrace{\frac 1 n \bm{1}_n^{\top} H^{(j)} V^{(j)}, y}]\big]} \\
        \le&  2\kappa_1\tilde{C} d_i k\max_{(X, G, y) \sim \cD}\bignorms{X}^2 \max \Bigbrace{1, \bignorms{P_{_G}}^{2(j-i+1)}} \bignorms{V^{(j)}}^2 \prod_{t = 1:\, t \neq i}^j s_t^2. %
    \end{align*}
    Based on the above step, the trace of the loss Hessian matrix with respect to $W^{(i)}, U^{(i)}$ satisfies:
    {\begin{align*}
        & \max_{(X, G, y) \sim \cD} \bigabs{\bigtr{\bH^{(i)}\bigbracket{\bar{\ell}(f(X, G), y)}}} \\
        =& \max_{(X, G, y) \sim \cD} \bigabs{\bigtr{\bH^{(i)}\bigbracket{\frac{1}{(l-1)} \sum_{j=1}^{l-1} \ell\bigbrace{\frac 1 n \bm{1}_n^{\top} H^{(j)} V^{(j)}, y}}}} \\
        =& \frac{1}{l-1} \sum_{j=i}^{l-1} \max_{(X, G, y) \sim \cD} \bigabs{\bigtr{\bH^{(i)}\bigbracket{\ell\Bigbrace{\frac 1 n \bm{1}_n^{\top} H^{(j)} V^{(j)}, y}}}} \\
        \le & 2 \kappa_1 \tilde{C} d_i k \max_{j=1}^{l-1}\bignorms{V^{(j)}}^2\Bigbrace{\max_{(X, G, y) \sim \cD}\bignorms{X}^2  \sum_{j=1}^{l-1} \frac{\max \bigbrace{1, \bignorms{P_{_G}}^{2j}}}{{l-1}}} \prod_{t = 1:\, t \neq i}^{l-1} s_t^2.
    \end{align*}}%
    Within the above step, the propagation matrix satisfies:
    \[ \frac 1 {(l-1)} \sum_{j=1}^{l-1} \max\bigbrace{1, \bignorms{P_{_G}}^{2j}} \le \max\fullbrace{1, \bignorms{\frac 1 {l-1}\sum_{j=1}^{l-1} P_{_G}^j }^2}. \]
    Notice that $P_{GIN} = \frac 1 {l-1} \sum_{j=1}^{l-1} P_{_G}^j$.
    Thus, we have completed the generalization error analysis for graph isomorphism networks in equation \eqref{eq_gin_result}.
\end{proof}