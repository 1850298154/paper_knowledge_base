\section{Conclusion}\label{sec_conclude}

This work develops generalization bounds for graph neural networks with a sharp dependence on the graph diffusion matrix. The results are achieved within a unified setting that significantly extends prior works. In particular, we answer an open question mentioned in \citet{liao2020pac}: a refined PAC-Bayesian analysis can improve the generalization bounds for message-passing neural networks. These bounds are obtained by analyzing the trace of the Hessian matrix with the Lipschitz-continuity of the activation functions. Empirical findings suggest that the Hessian-based bound matches observed gaps on real-world graphs. Thus, our work also develops a practical tool to measure the generalization performance of graph neural networks. The algorithmic results with noise stability optimization further demonstrate the practical implication of our findings.

Our work opens up many interesting questions for future work. Could the new tools we have developed be used to study generalization in graph attention networks \cite{velivckovic2017graph}? Could Hessians be used for measuring out-of-distribution generalization gaps of graph neural networks?
We remark that in a subsequent paper \cite{ju2023noise}, we provide a rigorous analysis of the convergence rates of our algorithm presented in the experiments, showing matching upper and lower bounds given Lipschitz-continuous conditions of the gradients.

\section*{Acknowledgement}

Thanks to Renjie Liao, Haoyu He, and the anonymous referees for providing constructive feedback on our work.
Thanks to Yang Yuan for the helpful discussions.
H. Z. is grateful to Stefanie Jegelka for an invitation to present this paper at her lab meeting.
H. J. and D. L. acknowledge financial support from the startup fund of Khoury College of Computer Sciences, Northeastern University.
