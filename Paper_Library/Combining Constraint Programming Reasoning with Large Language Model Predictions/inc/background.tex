\section{Background}
\label{sec:prelim}

This section introduces the necessary background on LLM and CP.

\subsection{LLM Predictions Strategies}


\subsubsection{Decoding Strategies Combined with LLMs}
Large Language Models (LLMs), such as the GPT series, generate text by predicting the next token (word or character) given the history of previously generated words. Decoding in LLMs refers to the strategy used to select the next words to be generated.

\subsubsection{Greedy Decoding}
The simplest decoding strategy is greedy decoding. Here, the LLM selects the words with the highest probability at each time step. Although simple and efficient, this approach does not guarantee the best overall sequence, as it does not consider the effect of the current selection on future tokens.


\subsubsection{Beam Search}

Beam Search (BS) \cite{beamsearch1:2021,beamsearch2:2018,hokamp-liu:2017} is a refined version of greedy decoding. A beam is a candidate sequence of words. Instead of selecting the single best token at each time step, it usually keeps track of the $k$ most likely sequences (beams) at each step.

Although BS usually achieves better results than greedy decoding, it assumes that high-ranking token sequences consist of high-ranking tokens, which may only sometimes be the case. For a more stochastic and diverse output, top-$k$ sampling and top-$p$ sampling (also known as nucleus sampling) are used. In top-$k$ sampling, the model selects from the top $k$ highest probability predictions, while in top-$p$ sampling, it dynamically selects the number of top predictions to cover $p$ percent of the total probability mass.

\subsubsection{Perplexity}
Perplexity is an entropy metric derived from Shannon's information theory \cite{brown-etal:1992}. 
Since an LLM computes the probability of text, then it can compute text perplexity.
It can be expressed as the geometric mean of the inverse conditional likelihood of the sequence \cite{jurafsky:2009}. Let $S_n$ be the sequence of a succession of words of size $n$: $S_n = w_1w_2..w_n$. The perplexity (PPL) of $S_n$ is computed as follows:
\[ PPL(S_n) = \sqrt[n]{\frac{1}{P(w_1w_2w_3...w_n)}},\]
where probability $P(\cdot)$ is given by the LLM. 
PPL can be interpreted as the ``how likely a text is generated by a given model'' \cite{garbacea-arxiv-survey-nlp:2022}.
Usually, it is used to evaluate the LLM itself by checking that good samples are recognized as such (i.e., low PPL values). 

In NLP, the evaluation of text is still an open problem, and human evaluation remains the gold standard. Numerous metrics have been developed to address this issue. Among them, PPL remains an objective criterion associated with text produced by a given model. PPL is also much more convenient to use than pure probability. Its range is $[ 1 ; + \infty [$ . The lower, the better.


\subsection{Constraint Programming} 

Constraint Programming (CP) is a paradigm for solving combinatorial problems that draws on a wide range of techniques from artificial intelligence and operations research. 
% 
In CP a problem can be defined as a Constraint Satisfaction Problem (CSP). A CSP is a triplet: $\langle X, D, C \rangle$, where:
\begin{itemize}
\item $X = \{X_1, X_2, ..., X_n\}$ is the set of variables  of the problem.
\item $D = \{D_{X_1}, D_{X_2}, ..., D_{X_n}\}$ is the set of domains, where each domain $D_{X_i}$ corresponds to the set of possible values for the variable $X_i$.
\item $C = \{c_1, c_2, ..., c_m\}$ is the set of constraints of the problem. A constraints represent a property of the problem.
\end{itemize}
%
A solution is an assignment of all the variables to a value present in their respective domains, such that all the constraints are satisfied.


\subsubsection{Avoiding Static Definition of the CSP}



In traditional CP, for the task of \tg, a variable represents a word. Since the domains of variables have to be set beforehand, they will be of enormous size, containing every word/declination of words for a given language. 
% 
Furthermore, constraints between succession of words may lead to a combinatorial explosion.
Since traditional CP is not well suited, this work focuses on \otf, a CP based method recently introduced by RÃ©gin and De Maria \cite{FGCSPICTAI}. Instead of having the variables/domains/constraints set before the search, \otf  generates the variables/domains/constraints during the search for a solution, 
% 
avoiding the problem stated above and being expendable to permit the integration of an LLM.
The new version of \otf is called GenCP.
