\section{Introduction}
\label{sec:Intro}

 How can we perceive Constraint Programming beyond its traditional role in solving combinatorial optimization problems?
Once Eugene Freuder wrote \emph{
Constraint programming represents one of the closest approaches computer science has yet made to the Holy Grail of programming: the user states the problem, the computer solves it }\cite{freuder:97}. 


%
Nevertheless, some real-world problems are still beyond the reach of the current CP paradigm. This is particularly true when real-world problems involve vague notions such as ``meaning'' and ``melody'' for text and music. %
These are not easy to model in CP with the classical toolbox, mainly because these notions are hard to define formally. For instance, it is unclear how to formalize an objective function or a constraint to get closer to a meaningful sentence, a melodious song or a captivating painting. 
%
On the other hand, recent results in Machine Learning (ML), such as transformer-based models \cite{vasmani-et-al:2017}, have demonstrated the power of these techniques to capture a significant part of these vague concepts through data-driven statistical learning (e.g., Large Language Model (LLM) like the GPT series \cite{radford:2019}, stable-diffusion \cite{rombach2021highresolution}, ChatMusician \cite{yuan2024chatmusician}). In the article, we demonstrate that ML, and in particular LLM, can help CP to model and solve problems where such vague concepts can be found.  

In recent years, there has been a growing interest in text generation under constraints thanks to the rise of transformer-based models, like OpenAI ChatGPT (\cite{radford:2019}) and Meta LLaMa (\cite{touvron2023llama}). Nevertheless, even fine-tuned 
prompted LLMs fail to generate several constrained outputs (see the tasks introduced in \cite{yao2023collie}).
The goal of this paper is to present a new method for the task of \tg. This interest has a strong chance of continuing to grow insofar as many brands wish to integrate these technologies, in particular with their customers, and want to have control and guarantees on the behavior of these conversational agents. Hence, it may impact several critical marketing aspects (e.g., brand representation, legal issues, data privacy, \dots). Therefore, CP has the potential to become a strong safeguard of this kind of generative model. 

%

%
For the task of \tg, ML techniques face limitations when they have to manage structural constraints, such as limits on the number of words or characters (e.g. Text Summarization, Text Simplification, Text style transfer, Question Answering, Storytelling, Poetry or Lyrics Generation, Subtitle) \cite{garbacea-arxiv-survey-nlp:2022}. CP succeeds on these types of constraints, making the combination of CP and ML a natural fit for the task of \tg. 

% 
This paper proposes such a combination, to tackle a class of problems where neither CP and ML succeeds on their own (Fig.~\ref{fig:enter-label}).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.80\textwidth]{ima/CPML-80.pdf}
    \caption{
    % 
    Our approach aspires to explore the in-between area. In the blue (left-hand side) region, LLM guided searches solve weakly constrained problems \cite{lu-etal-2022-neurologic,beamsearch2:2018,hokamp-liu:2017} and in the green (right-hand side) region, CP-based generation tackles strongly constrained problems \cite{sprockeels-vanroy:2024expressing,bonlarron-regin:2024inter,bonlarron-regin:2024markov,bonlarron-et-al:2023,perez-regin:17b,papadopoulos-roy-etal:15,pachet-roy:2011}.
    }
    \label{fig:enter-label}
\end{figure}


 Combining Combinatorial Optimization (CO) and ML is a very active research area \cite{BENGIO-survey:2021}, however there is no easy way to integrate the ML ``expertise'' into CP as a constraint of the model \cite{bartolini-et-al-neuron:2011,Lombardi-et-al-EML:2017} and \emph{vice versa} \cite{willem}. Furthermore, there are many incentives to strengthen the interactions between ML and CO \cite{Khalil_et_al_mip:2016,Khalil-et-al-mip:2017,Song-et-al-LNS-MIP:2020}. Usually, the main motivation comes from the performance perspective, where the idea is to improve a solver's performance with ML (e.g., finding branching heuristics thanks to Reinforcement Learning \cite{Cappart-et-al:2021} or finding better  bounds with Clustering \cite{Nafar-Romer:2024}). This paper tackles it from the modeling point of view. Modeling is a crucial part of CO works. In the end, the model must account for the underlying solver that runs it. More in detail, here, the paper focuses on the interaction between CP and ML, more precisely through an ML-augmented CP approach \cite{kotary-survey:2021}. 

In the context of \tg, the domain of a variable represents a word. The base idea of the paper consists in letting ML manage the domain of variables and CP manage the constraints and the number of variables. In this manner, the sentence formed by variables has high chances to have a meaning and all the constraints will be satisfied. In traditional CP, the domains can not be managed by  ML because they have to be set beforehand. However, it is possible to rely on \OTF (\otf) \cite{FGCSPICTAI}, a CP based method where variables, domains and constraints are generated during the search for a solution. \color{black}


The main contribution of this paper is to propose a new version of \otf,
called GenCP,
where the generative function of the domain of variables is modified to allow CP variable domains to be computed by an LLM embedded in it, during the search for a solution. % 
More in detail, ML is used during  process solving but it is also used as an explicit part of the problem definition (i.e., domains are predicted by the LLM and can replace entirely static variable domains definition of a CSP.). Thus it bridges CP and ML through solving and modelling.
% Helping functions were introduced to separate implicit and explicit constraints: explicit constraints are constraints of the problem (a word does not contain an "e" for example), while implicit constraint represent all the other constraints, like ensuring that a variable corresponds to a word, not the beginning of a word etc$\dots$

The potential of the approach is showcased for the problem of \tg, against one the most used techniques in the Natural Language Processing (NLP) field: Beam Search (BS). Both methods 
% (BS and the new version of \otf) 
(BS and GenCP)
are compared on constrained sentences generation tasks extracted from benchmarks recently introduced  \cite{yao2023collie}. The approach highlights how CP can provide guarantees when combined with LLM predictions.
% Theses benchmarks are also interesting because the authors shows that various LLMs struggle to produce length-constrained sentences.

The paper is organized as follows: Sec.~\ref{sec:prelim} serves as background, Sec.~\ref{sec:mmethod} shows how to extend \otf
to GenCP
and how to implement an interaction between 
% \otf and LLM. 
GenCP and LLM.
Sec.~\ref{sec:experiments} presents the experimental results  in which the new approach is demonstrated on the task of \tg. 
% In Sec.~\ref{sec:discussion}, the paper delves into further discussion, offering additional insights into this work and providing perspectives for future research endeavors. 
Finally, Sec.~\ref{sec:discussion} delves into further discussion, offering additional insights into this work and providing perspectives for future research endeavors.





