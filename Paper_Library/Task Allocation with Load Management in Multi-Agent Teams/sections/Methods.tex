To enable intelligent teaming of heterogeneous agents in task allocation problems, it is assumed that all agents possess individual decision-making processes instead of following commands. In lieu of being subordinate members, agents are equally responsible for strategic planning and task execution. We characterize the heterogeneity in agent decision models by allocation preferences which can be customized by agent reward functions. While agents have the same type of attributes to handle tasks, sense the environment, and communicate information, the capability levels for each attribute are heterogeneous. Such heterogeneous and specialized agents need to collaborate to complete all the tasks within fewer steps. An efficient learning method is desired to achieve intelligent teaming behaviors with load management, by which agents stay idle when they are not needed without compromising the overall team effectiveness.

\subsection{Problem Formulation}
\label{sec:prob_form}
The decision-making process for a team of heterogeneous agents with load management inherits the formulation of our previous work on heterogeneous teaming decentralized partially observable markov decision process (HT Dec-POMDP) \cite{HT-DEC-POMDP} and incorporate two additional features allowing intelligent load management: 1) agent's option to idle, and 2) penalty on unnecessary task assignment depending on capabilities and current task assignment. The proposed framework heterogeneous teaming with load management (HTLM Dec-POMDP) is defined as a tuple $(\bar{G},\alpha,S,\bar{A},\mathcal{T},\bar{R},Z,\mathcal{O},M,\mathcal{I},h,b^0,\gamma)$ where:

\begin{itemize}
    \item $\bar{G}:=\{g_1,\dots,g_p,\bar{g}_1,\dots,\bar{g}_q\}$ is a finite set of $p$ tasks and $q$ idle locations
    \item $\alpha:=\{\alpha_1,\dots,\alpha_n\}$ is a finite set of $n$ agents
    \item $S:=S^D\times S^C, s\in S,$ is the overall state that is factored into $p$ tasks and described by task demand/severity levels $S^D:=S^D_1\times\dots\times S^D_p$ and joint agent capabilities $S^C:=S^C_1\times\dots\times S^C_p$
    \item $\bar{A}:=\times_i \bar{G}, a\in \bar{A},$ is the set of joint assignment decision by assigning $\alpha_i$ to $g_j$
    \item $\mathcal{T}:=Pr(s'|s,a)$ is the state transition probability
    \item $\bar{R}:=f_R(a_o,b(s),a)$ is the reward function, where $a_o$ records the previous assignment, $b$ is the belief defined as the probabilistic distribution over the problem state space, $b'$ is the belief for the next state
    \item $Z:=\times_i Z_i, z\in Z,$ is the set of joint observations
    \item $\mathcal{O}:=Pr(z|s',a)$ is the observation probability
    \item $M:=\times_iM_i, m\in M,$ is the set of joint information, where $M_i$ represents the state information collected by agent $\alpha_i$
    \item $\mathcal{I}=Pr(m|s',a,z)$ is the information probability
    \item $h$ is the planning time horizon
    \item $b^0$ is the initial belief 
    \item $\gamma$ is the discount factor
\end{itemize}

This formulation follows the fundamental modeling of environment dynamics in HT Dec-POMDP \cite{HT-DEC-POMDP}, and explicitly defines various types of attributes (task-specific, sensing, and communication capabilities) for agents and the corresponding probabilities that describe the stochastic environment and the agent's perception of the environment and information. In many studies, agents stay active throughout the operations, which might lead to excessive usage and risk exposure. Instead, HTLM Dec-POMDP provides agents with the option to stay idle by including idle locations as part of the assignment decisions described by $\bar{G}$ and $\bar{A}$. %Reviewer1Major2
Note that an action of an agent refers to the task assignment, i.e. the agent executes the assigned task. In addition, the reward function $\bar{R}$ not only defines the rewards for completing the tasks, but it also penalizes unnecessary assignment and reassignment to induce agent idle behaviors without compromising team performance. Since the formulation is decentralized, agents have their own reward functions representing their understandings of the situation and preferences on the task assignment, which might be heterogeneous among the team members.

The goal of the formulation is, for each agent, to find an optimal series of task allocation actions or policy $\pi_*$ that maximizes the operation performance so that $\pi_*=argmax\{V(\pi)\}$. The performance is represented as the policy value $V(\pi)$ defined as the expected total future discounted reward.
\begin{equation}
    V_\pi=\mathbb{E}[\sum_{t=0}^{t=h} \gamma^t f_R(a^{t-1},b^t(s),a^t)|\pi,b^0].
\end{equation}
To give more incentives for agents to accomplish tasks at earlier time steps, the reward is discounted by time ($\gamma^t$). At the beginning of an operation, each agent has an initial belief $b^0(s)$ over the operation state. At each time step of a time horizon $h$, agents execute decisions $\bar{A}$, receive partial observations $Z$, and receive information from others ${M}$. After experiencing the action-observation-information histories, agents update their beliefs over the state following Eq. (\ref{eq:belief_update}) with capability-dependent transition probability $\mathcal{T}$, observation probability $\mathcal{O}$, and information accuracy $\mathcal{I}$ \cite{HT-DEC-POMDP} in order to make the decisions for the next time step %Reviewer1Major2
(the prime symbol denotes the next time step), namely
\begin{align}
    \label{eq:belief_update}
   b'(s')&= Pr(s'|m,z,a,b)\nonumber\\
   &\propto\mathcal{I}(s',a,z,m)\mathcal{O}(s',a,z)\sum_s \mathcal{T}(s,a,s')b(s).
\end{align}
The approaches of defining the reward functions and learning the optimal policies based on beliefs are discussed in Sections \ref{sec:preference} and \ref{sec:learning}.

\subsection{Embedding Agent Preference in Reward Shaping}
\label{sec:preference}
%%% reward references
%1-	Liu, B., Singh, S., Lewis, R. L., & Qin, S. (2012, November). Optimal rewards in multiagent teams. In 2012 IEEE international conference on development and learning and epigenetic robotics (ICDL) (pp. 1-8). IEEE. 
%2-	Dong, Y., Tang, X., & Yuan, Y. (2020). Principled reward shaping for reinforcement learning via Lyapunov stability theory. Neurocomputing, 393, 83-90. 
%3-	Marashi, M., Khalilian, A., & Shiri, M. E. (2012, October). Automatic reward shaping in reinforcement learning using graph analysis. In 2012 2nd International eConference on Computer and Knowledge Engineering (ICCKE) (pp. 111-116). IEEE. 
%4-	Matignon, L., Laurent, G. J., & Le Fort-Piat, N. (2006, September). Reward function and initial values: Better choices for accelerated goal-directed reinforcement learning. In International Conference on Artificial Neural Networks (pp. 840-849). Springer, Berlin, Heidelberg. 
%%%
Defining a reward function, also referred to as reward shaping, plays a critical role in reinforcement learning. A carefully designed reward function can expedite the training process while satisfying the problem objectives. In our decentralized task allocation problem, there are two objectives: 1) completion of tasks and 2) load management. Moreover, reward functions could be customized to model agent preferences, meaning that agents can have different reward functions depending on their willingness to take risks and to reassign tasks. Providing rewards based on the actual operation state \cite{Noureddine2017MultiagentDR} or system score \cite{DQN} is not applicable since there does not exist a centralized informer. Therefore, agents reward themselves based on their belief over the status of the demands and the environment.

\begin{equation}
    \label{eq:htlm_reward}
    f_R(a_0, b(s), a)=R_{comp}+R_{LM},
\end{equation}
\begin{equation}
    \label{eq:comp_reward}
    R_{comp}=W^T_lR_l,
\end{equation}
\begin{equation}
    \label{eq:LM_reward}
    R_{LM}=R_{II}(b(s))+T_{RP}(a_0,a).
\end{equation}

Eq. (\ref{eq:htlm_reward}) defines the reward function for each agent in the proposed HTLM Dec-POMDP.
%reviewer2Major1
The agent preference in this context refers to the customizable reward shaping for each agent. In particular, different agents might have different confidence in mission completion, which is modeled in $R_{comp}$; $R_{II}$ models agents' different incentives on idling; and $T_{RP}$ is utilized to describe agent preferences on certain tasks or the locations of the tasks.
$R_{comp}$ is a belief-dependent reward computed based on the belief state and the belief reward threshold $b_{th}$. By deciding how much and how generous each agent informs the reward of mission completion based on partial completion weight $w$ and belief reward threshold $b_{th}$ respectively, $W_l=f(b(s);w,b_{th})$ and $R_l$ provides the reward for each task severity level $l$ \cite{HT-DEC-POMDP}.
%Reviewer2Major3
The function $f(b(s);w,b_{th})$ is defined such that agents receive partial reward for decreasing the demand levels to level $l$ with maximum reward received when all the tasks are completed, i.e. when $l=0$. For example, if an agent believes, with a probability of at least $b_{th}$, that the severity levels of half of the tasks ($w=0.5$) is at level 2 $(l=2)$ and the other half of the tasks are completed $l=0$, then the $R_{comp}$ reward is computed as $R_{comp}=0.5R_0+0.5R_2$. The rewards $R_l$ depend on the studied scenario and need to be tuned for an efficient training, but $R_j>R_i,  j<i$ should be used to guide agents towards completing tasks.

% To construct $W_l=[w(n_0),\dots,w(n_l)]$, we first count the tasks $n_j$ that satisfy the belief threshold condition $b_{th}$ at each level $j=0,\dots,l$, and then get the partial completion weight $w(n_j)\in[0,1]$ for each level. For example, if the agent believes the severity levels of all tasks are $0$ with probability of at least $b_{th}$, $w(n_0)=1$, $W_l=[1,0,\dots,0]$, and $R_{comp}=R_0$.
%For example, if the agent believes the severity levels of all tasks are $0$ ($l=0$) with probability of at least 90 percent ($b_{th}=0.9$), $W_0$ equals to 1 ($w=1$ since full level completion at level 0), and $W_i=0$ ($w=0$ since no level completion at the other levels $i=1,2,\dots,l$), and then the agent receives completion reward $R_{comp}= W_0\times R_0$.
% $R_{comp}$ is belief-dependent because the number of tasks considered as completed at different levels are depended on the belief state and the belief reward threshold $b_{th}$}.

With only $R_{comp}$ reward, the team of agents learn to accomplish the desired tasks effectively, but there exists a number of optimal solutions. The deep learning approach randomly converges to one of the optimal solutions, but fails to avoid task overload. The second part of the reward function is the reward of load management $R_{LM}$ in HTLM Dec-POMDP defined in Eq. (\ref{eq:LM_reward}). $R_{LM}$ has two features - idle incentive $R_{II}$ and reassignment penalty $T_{RP}$ - in order to achieve the following behaviors without the compromise of operation effectiveness: idling as much as possible while avoiding excessive capability usage and unnecessary task reassignment. Load management serves as the secondary objective after mission completion. Therefore, the maximum possible $R_{LM}$ is much smaller than $R_{comp}$. Additionally, 
%Reviewer2Major3
the belief-dependent $R_{II}$ is positive (reward) if the agent is assigned to idle at one of the locations and negative (penalty) if the capability level of the agent $i$ is much higher than the believed severity level of the assigned task $j$ ($c_i>>b(s_j)$), penalizing the assignment of an agent with excessive capability to an easy task.
% namely being too capable of handling the task. 
% For example, if the maximum $R_{comp}$ is 1, $R_{II}=0.1$ would encourage more idling behaviors in addition to task completion. If $R_{II}=-0.2$ would prevent the agent from using too much of its capability on a easy task.}
The reassignment penalty $T_{RP}$ is a cost matrix defining the cost of changing task assignment from $a_0$ to $a$. %Reviewer2Minor5
The task reassignment penalty becomes the traversing cost when only the locations of tasks are used to calculate the penalty, i.e. when there is no cost of switching tasks at the same location.

In the task set $\bar{G}$, there are idle locations corresponding to the task locations. We assume that there is no cost of going into and out-of idling within the same location, and there is penalty for being reassigned to a different location. A sample $T_{RP}$ cost matrix for 2 locations is shown in Table \ref{tb:cost_matrix}, where the traversing cost between two locations is $tc\leq0$.
\begin{table}[ht]
\caption{Two-Location Traversing Cost Matrix}
\centering
    \begin{tabular}{|c|c|c|c|c|}
         \hline
         & Loc 1 & Loc 2 & Loc 1 & Loc 2 \\
         & Task & Task & Idle & Idle\\
         \hline
         Loc 1 Task & 0 & $tc$ & 0 & $-\infty$\\
         Loc 2 Task & $tc$ & 0 & $-\infty$ & 0\\
         Loc 1 Idle & 0 & $tc$ & 0 & $-\infty$\\
         Loc 2 Idle & $tc$ & 0 & $-\infty$ & 0\\
         \hline
    \end{tabular}
\label{tb:cost_matrix}
\end{table}

\noindent If only one idling action is considered for multiple locations with no cost of going into idling and out-of idling, agents can bypass the reassignment penalty by going idle and being assigned to a different task with two steps, which has a indirect impact on team effectiveness. Including multiple idle locations could explicitly define the reassignment penalty and have a direct impact on agent's decision for each time step. To avoid the additional computation due to the larger action space, we generate a reduced action space for each agent $\hat{A}=\{g_1\dots g_p, \bar{g}_o|a_o\}$ to restrict idling to a different location. Only the idle location $\bar{g}_0$ that has the same location as the previous assignment $a_0$ is included in the reduced action space while other idle locations are eliminated. For example, as shown in Table \ref{tb:cost_matrix}, the actions with $\infty$ cost would be eliminated given the previous assignment. The reduced action space for each agent at each step is $p+1$, where $p$ is the number of tasks and the additional action is idling at the same location. Note that by removing idle locations and setting $R_{LM}=0$, the framework would reduce to HT Dec-POMDP. The investigation on the effect of $R_{LM}$ on team behaviors and performance when idle actions are available is discussed in Section \ref{sec:R_LM_effect}.

\subsection{Decentralized Deep Q-Learning for Load Management}
\label{sec:learning}
To achieve load management by avoiding excessive capability usage and unnecessary assignment, an agent has to acknowledge the severity of the operation and record the previous assignment. To approach the optimal solution, the proposed method extends the decentralized deep Q-learning with beliefs \cite{HT-DEC-POMDP} as shown in Algorithm~\ref{alg:decdql_LM} (underlined parts indicate the contribution of the proposed learning method for load management), and an agent makes intelligent decisions based on a knowledge state $\mathcal{K}=\{b(s),a_o\}$ which consists of its belief of the operation state $b(s)$ and the previous assignment $a_o$. The knowledge state update (Alg.~\ref{alg:decdql_LM} Line 8) inherits the Markov property since the next belief $b'(s')$ only depends on the current belief $b(s)$ as described in Eq. (\ref{eq:belief_update}), and the decision made becomes the previous assignment in the next time step.

\begin{algorithm}
\caption{Dec Deep Q-Learning for Load Management}
\begin{algorithmic}[1]
\label{alg:decdql_LM}
\STATE Initialize replay memories $D$ and Q-Networks with random weights \textit{for all agents}
\FOR{each training episode}
\STATE Reset operation and \underline{knowledge state $\mathcal{K}$} \textit{for all agents}
\FOR{each operation step}
\STATE \textit{For all agents}
\STATE Select decisions \underline{from $\hat{A}$} with $\epsilon$-greedy and execute
\STATE Observe and communicate observations
\STATE \underline{Update $\mathcal{K}=\{b(s),a_o\}$}
\STATE \underline{Compute $f_R(\mathcal{K},a)$ and store $\mathcal{X}$ into $D$}
\STATE Train Q-Network with a randomly sampled minibatch of transitions in $D$
\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

Given the reward function discussed in Section \ref{sec:preference}, the discount factor $\gamma$, and the time horizon $h$, %Reviewer1Major4
the action-value or Q-Value $Q^{\pi}(\mathcal{K},a)$ given a policy is defined as the maximum possible expected total reward when having knowledge state $\mathcal{K}^t=\mathcal{K}=\{b, a_o\}$ and taking action $a=\pi(\mathcal{K})$ under policy $\pi$.
\begin{equation}
    \label{eq:Q*ba}
    Q^{\pi}(\mathcal{K},a)=\underset{\pi}{\max}\mathbb{E}[\sum_{t=t'}^{t=h}\gamma^{t-t'}f_R(\mathcal{K}^t,a^t)|\mathcal{K}^t=\mathcal{K}, a^t=\pi(\mathcal{K})].
\end{equation}

\noindent Under \textit{Bellman Equation}, the Q-Value $Q^{\pi}(\mathcal{K},a)$ can be rewritten using the next knowledge-action pair $(\mathcal{K}',a')$.
\begin{equation}
    \label{eq:bellman}
    Q^{\pi}(\mathcal{K},a)=\mathbb{E}_{\mathcal{K}'}[f_R(\mathcal{K},a)+\gamma \text{max}Q^{\pi}(\mathcal{K}',a')|\mathcal{K},a].
\end{equation}




Instead of exploring the full continuous knowledge state space, a neural network as shown in Fig. \ref{fig:q_network_K}. The Q-Network $Q(\mathcal{K},a;\theta)$ is used to approximate the Q-Value with the best policy $Q(\mathcal{K},a;\theta)\approx Q^{\pi}(\mathcal{K},a)$. The optimal policy for each agent can then be computed by $\pi^*(\mathcal{K})=\text{argmax}_{\hat{a}} Q(\mathcal{K},a;\theta)$. An agent selects the action with the highest Q-Value from the reduced action space $\hat{A}$. Recall that $\hat{A}\subseteq \bar{A}$. The input of the Q-Network is the knowledge state which includes the belief state over all task severity levels in probability space and the one-hot encoded previous assignment. The total number of input neurons is $pL+|\bar{G}|$, with $p$ being the number of tasks and $L$ being the number of task severity levels. The output of the Q-Network is the Q-Value for each action, and the size of the output is $|\bar{G}|$. The Q-Network output includes all tasks and idle locations since agents can be assigned to any of them throughout an operation. The reduced action space is helpful when generating random actions and selecting the optimal action (Alg.~\ref{alg:decdql_LM}, Line 6) so that agents do not waste effort on reassigning to the location with $\infty$ cost. The benefit of using a deep learning approach in decentralized multi-agent teams is that the state space and the actions space are linear with respect to the number of tasks.

During each training iteration $i$, a set of transition data $\mathcal{X}=<\mathcal{K},a,\mathcal{K}',f_R(\mathcal{K},a)>$ is collected into replay memories (Alg.~\ref{alg:decdql_LM}, Line 9). With the Bellman Eq. (\ref{eq:bellman}), the Q-Network $Q(\mathcal{K},a;\theta)$ should converge to a target Q-Value $y_i=f_R(\mathcal{K},a)+\gamma \text{max}Q(\mathcal{K}',a';\hat{\theta}_i)$, where the parameter of the target Q-Network $\hat{\theta_i}$ is updated with $\theta$ less frequently. The difference in Q-Value and the target Q-Value is the loss $\mathcal{L}_i$ to be minimized to reach Bellman Optimality as defined in:
\begin{equation}
    \label{eq:loss}
    \mathcal{L}_i(\theta_i)=\mathbb{E}_\mathcal{X}[y_i(\mathcal{K}',a',\hat{\theta}_i)-Q(\mathcal{K},a;\theta_i)].
\end{equation}

\begin{figure}[!t]
    \centering
    \vspace{2mm}
    \includegraphics[width=0.48\textwidth]{figs/q_network_K.png}
    \caption{Schematic of a Q-Network with an agent's belief over severity levels of 2 tasks and previous assignment as input and Q-Values of 2 actions as output. In the proposed approach, agent's knowledge state $\mathcal{K}$ is used as an input to the Q-Network and the outputs are the corresponding Q-Values $Q(\mathcal{K},\cdot)$ in the action space.}
    \label{fig:q_network_K}
\end{figure}