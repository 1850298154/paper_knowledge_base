
We report further details and results on the \heterogeneitygaps obtained in the multi-agent multi-task matrix game.

\subsection{Game formulation}
In \autoref{tab:matrix_game_formulation} we provide an example of the pay-off matrix in this game for $N=M=3$.

\begin{table}[!h]
    \centering
     \caption{Example of a Multi-Agent Multi-Task matrix game for $N=M=3$. Agents choose their actions $A=(r_{ij})$ and receive the global reward $R(\mathbf{A}) = \outeragg_{j=1}^M \inneragg_{i=1}^N r_{ij}$.}
    \label{tab:matrix_game_formulation}
\begin{tabular}{llccc} 
\toprule 
& & \multicolumn{3}{c}{Tasks} \\ 
& & {1} & {2} & {3} \\ 
\midrule 
\multirow{3}{*}{\hfill\rotatebox[origin=c]{90}{Agents}\hfill} 
& 1 & $r_{11}$ & $r_{12}$ & $r_{13}$ \\ 
& 2 & $r_{21}$ & $r_{22}$ & $r_{23}$ \\ 
& 3 & $r_{31}$ & $r_{32}$ & $r_{33}$ \\ 
\bottomrule 
\end{tabular}
\end{table}



\subsection{$N=M=2$}

We train with $N=2,M=2$ for 100 training iterations (each consisting of 60,000 frames). We report the results for the \textbf{continuous} case in \autoref{tab:matrix_game_cont} and for the \textbf{discrete} case in \autoref{tab:matrix_game_disc}. The evolution of the \heterogeneitygaps over training is shown in \autoref{fig:matrix_game}.


\begin{table}[!h]
    \centering
     \caption{\Heterogeneitygap $\Delta R \in \R_{0\leqslant x \leqslant 1}$ of the \textbf{continuous} matrix game with $N=M=2$. The results match the theoretical analysis in the Table of \autoref{fig:deltaR-vs-softmax}. We report mean and standard deviation after 6 million training frames over 9 different random seeds.}
    \label{tab:matrix_game_cont}
\begin{tabular}{llccc} 
\toprule 
& & \multicolumn{3}{c}{$T$} \\ 
& & {Min} & {Mean} & {Max} \\ 
\midrule 
\multirow{3}{*}{\hfill\rotatebox[origin=c]{0}{$U$}\hfill} 
& Min & $-0.002 \pm 0.002$ & $0.000 \pm 0.003$ & $\boldsymbol{0.504} \pm 0.007$ \\ 
& Mean & $-0.002 \pm 0.002$ & $0.000 \pm 0.000$ & $\boldsymbol{0.496} \pm 0.001$ \\ 
& Max & $-0.003 \pm 0.002$ & $-0.001 \pm 0.001$ & $0.003 \pm 0.001$ \\ 
\bottomrule 
\end{tabular}
\end{table}

\begin{table}[!h]
    \centering
       \caption{\Heterogeneitygap $\Delta R \in \R_{0\leqslant x \leqslant 1}$ of the \textbf{discrete} matrix game with $N=M=2$. The results match the theoretical analysis in \autoref{fig:deltaR-vs-softmax}. We report mean and standard deviation after 6 million training frames over 9 different random seeds.}
    \label{tab:matrix_game_disc}
\begin{tabular}{llccc} \toprule & & \multicolumn{3}{c}{$T$} \\ & & {Min} & {Mean} & {Max} \\ \midrule \multirow{3}{*}{\hfill\rotatebox[origin=c]{0}{$U$}\hfill} & Min & $0.0 \pm 0.0$ & $\boldsymbol{0.5} \pm 0.0$ & $\boldsymbol{1} \pm 0.0$ \\ & Mean & $0.0 \pm 0.0$ & $0.0 \pm 0.0$ & $\boldsymbol{0.5} \pm 0.0$ \\ & Max & $0.0 \pm 0.0$ & $0.0 \pm 0.0$ & $0.0 \pm 0.0$ \\ \bottomrule \end{tabular}
\end{table}

\begin{figure}[!ht] 
    \centering  % Centers the figure
    \includegraphics[width=\textwidth]{images/matrix_games_2_agents_graytext.pdf}  % Adjust width as necessary, replace 'example-image' with your image file path
    \caption{\Heterogeneitygap for the discrete and continuous matrix games with $N=M=2$ over training iterations. We report mean and standard deviation after 6 million training frames over 9 different random seeds. The final results match the theoretical predictions in \autoref{fig:deltaR-vs-softmax}.}  % Add your figure caption
    \label{fig:matrix_game}  % Add a label for referencing the figure in text
\end{figure}

\subsection{$N=M=4$}

In the case $N = M = 4$, the evolution of the \heterogeneitygaps during training is shown in \autoref{fig:matrix_game_4_agents}.
We further report the final obtained gains for the \textbf{continuous} case in \autoref{tab:matrix_game_cont_4} and for the \textbf{discrete} case in \autoref{tab:matrix_game_disc_4}. 


\begin{table}[!ht]
    \centering
     \caption{\Heterogeneitygap $\Delta R \in \R_{0\leqslant x \leqslant 1}$ of the \textbf{continuous} matrix game with $N=M=4$. The results match the theoretical analysis in \autoref{fig:deltaR-vs-softmax}. We report mean and standard deviation after 12 million training frames over 9 different random seeds.}
    \label{tab:matrix_game_cont_4}
\begin{tabular}{llccc} 
\toprule 
& & \multicolumn{3}{c}{$T$} \\ 
& & {Min} & {Mean} & {Max} \\ 
\midrule 
\multirow{3}{*}{\hfill\rotatebox[origin=c]{0}{$U$}\hfill} 
& Min & $-0.003 \pm 0.002$ & $0.000 \pm 0.001$ & $\boldsymbol{0.690} \pm 0.026$ \\ 
& Mean & $-0.002 \pm 0.000$ & $0.000 \pm 0.000$ & $\boldsymbol{0.722} \pm 0.002$ \\ 
& Max & $-0.037 \pm 0.023$ & $-0.009 \pm 0.005$ & $0.029 \pm 0.006$ \\ 
\bottomrule 
\end{tabular}
\end{table}

\begin{table}[!h]
    \centering
       \caption{\Heterogeneitygap $\Delta R \in \R_{0\leqslant x \leqslant 1}$ of the \textbf{discrete} matrix game with $N=M=4$. The results match the theoretical analysis in the Table of \autoref{fig:deltaR-vs-softmax}. We report mean and standard deviation after 12 million training frames over 9 different random seeds.}
    \label{tab:matrix_game_disc_4}
\begin{tabular}{llccc} \toprule & & \multicolumn{3}{c}{$T$} \\ & & {Min} & {Mean} & {Max} \\ \midrule \multirow{3}{*}{\hfill\rotatebox[origin=c]{0}{$U$}\hfill} & 
Min & $0.0 \pm  0.0$ & $\boldsymbol{0.25} \pm 0.0$ & $\boldsymbol{1.0} \pm 0.0$ \\ & 
Mean & $0.0 \pm 0.0$ & $0.0 \pm 0.0$ & $\boldsymbol{0.75} \pm 0.0$ \\ & 
Max & $0.0 \pm 0.0$ & $0.0 \pm 0.0$ & $0.0 \pm 0.0$ \\ \bottomrule \end{tabular}
\end{table}

\subsection{$N=M=8$}

To test scalability, we further report results for the discrete matrix game with $N=M=8$ in \autoref{tab:matrix_game_disc_8}. As discussed in \autoref{sec:experiments}, due to the larger agent scale and action dimensionality (8 agents and 8 tasks), for some reward structures the empirical heterogeneity gain is negative, since the neurally heterogeneous agents we train require more time to discover the optimal policy, hence lag behind their homogeneous counterparts under a fixed training budget. Increasing the number of training steps steers the negative $\Delta R$ values to $0$. This nuance aside, what is important is that the results still follow our theory exactly in terms of which reward structures yield a \textit{positive} heterogeneity gain. For such reward structures, our theory also precisely predicts the numerical value of $\Delta R$ (\autoref{fig:deltaR-vs-softmax}) . 

\begin{table}[!h]
    \centering
       \caption{\Heterogeneitygap $\Delta R \in \R_{0\leqslant x \leqslant 1}$ of the \textbf{discrete} matrix game with $N=M=8$. We report mean and standard deviation after 12 million training frames over 8 different random seeds.}
    \label{tab:matrix_game_disc_8}
\begin{tabular}{llccc} \toprule & & \multicolumn{3}{c}{$T$} \\ & & {Min} & {Mean} & {Max} \\ \midrule \multirow{3}{*}{\hfill\rotatebox[origin=c]{0}{$U$}\hfill} & 
Min & $0.0 \pm  0.0$ & $\boldsymbol{0.125} \pm 0.0$ & $\boldsymbol{1.0} \pm 0.0$ \\ & 
Mean & $-0.094 \pm 0.068 $ & $0.0 \pm 0.0$ & $\boldsymbol{0.875}\pm 0.0$ \\ & 
Max & $-0.75 \pm 0.5$ & $0.0 \pm 0.0$ & $0.0 \pm 0.0$ \\ \bottomrule \end{tabular}
\end{table}
