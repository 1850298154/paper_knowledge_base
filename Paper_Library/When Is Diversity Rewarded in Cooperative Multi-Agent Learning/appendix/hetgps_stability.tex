\section{Stability of Bilevel Optimization in HetGPS}

\label{app:stability_of_bilevel_optimization}

The Heterogeneity Gain Parameter Search (HetGPS) algorithm employs a bilevel optimization framework to simultaneously optimize environment parameters and agent policies. This appendix discusses the structure of this optimization problem, its convergence properties, practical stability, and alternatives for non-differentiable environments.

\subsection{HetGPS as a Stackelberg Game}
HetGPS can be formalized as a Stackelberg game, a hierarchical optimization problem involving a leader and followers~\citep{simaan1973stackelberg}. In our setting:
\begin{enumerate}
    \item The \textbf{Leader} is the environment designer (the outer loop of HetGPS), which aims to maximize the heterogeneity gain $HetGain^{\theta}$ by adjusting the environment parameters $\theta$.
    \item The \textbf{Followers} are the homogeneous and heterogeneous multi-agent teams (the inner loop), which aim to maximize their respective returns $G^{\theta}(\pi)$ by optimizing their policies $\pi_{het}$ and $\pi_{hom}$ within the environment defined by $\theta$.
\end{enumerate}
The leader's objective function (the heterogeneity gain) depends on the optimized policies of the followers, which, in turn, depend on the parameters $\theta$ set by the leader. Formally, the objective is:
\begin{equation}
    \max_{\theta} \left[ G^{\theta}(\pi^*_{\text{het}}(\theta)) - G^{\theta}(\pi^*_{\text{hom}}(\theta)) \right]
\end{equation}
where $\pi^*(\theta)$ represents the optimized policies for a given environment configuration $\theta$.

\subsection{Convergence and Stability}

Generally speaking, multi-agent reinforcement learning is a concurrent optimization process that faces non-stationarity as agents constantly adapt to one another's evolving policies~\citep{BasarOverview}. HetGPS extends this challenge as agents must also adapt to a changing environment. Consequently, formal convergence guarantees to a *global* optimum remain an open question with regards to HetGPS in particular, but also MARL algorithms in general. However, recent theoretical work in environment co-design has established conditions under which convergence of bilevel optimization processes similar to HetGPS to *local* optima can be guaranteed, such as requiring sufficient smoothness of the environment dynamics and policy updates~\citep{gao2024codesign}. 

Despite the theoretical complexities inherent in multi-agent learning and bilevel optimization, as shown in \autoref{sec:experiments} and \autoref{app:adverseinitializations}, HetGPS demonstrates strong empirical stability even under adversarial initializations. This stability is expected, as it mirrors the practical success observed in related co-design and automated curriculum learning literature~\citep{dennis2020emergent, gao2024codesign}.

\subsection{Advantage of Differentiable Simulation}

In our experiments, HetGPS increased training time by roughly 25\% compared to training agents in an environment with a fixed reward structure. Hence, it is highly efficient and does not impose much overhead. A key strength contributing to the efficiency of HetGPS is its use of differentiable simulation (e.g., VMAS~\citep{bettini2022vmas}). By leveraging backpropagation through the entire rollout, HetGPS computes the exact gradient $\nabla_{\theta}HetGain^{\theta}$. This approach is  more  sample-efficient than alternative methods that treat the environment design as a separate RL problem (e.g., PAIRED~\citep{dennis2020emergent} or Designer-RL~\citep{gao2024codesign, amir2025recodereinforcementlearningbaseddynamic}). Such methods rely on high-variance policy gradient estimates for the outer loop and often struggle with exploration inefficiency~\citep{parker-holder2021that, jiang2021replayguided, xu2022accelerated}. By utilizing exact gradients, HetGPS mitigates these issues.

\subsection{Handling Non-Differentiable Environments}
A requirement for the implementation of HetGPS presented in Alg. 1 is access to a differentiable simulator. When the environment involves non-smooth physics or black-box components, direct backpropagation is infeasible.

In such cases, the environment optimization step (Line 6 of \autoref{alg:HetGPS}) can be replaced with the gradient-free methods mentioned above, such as PAIRED \citep{dennis2020emergent}, the bilevel method from \citep{ gao2024codesign}, or evolutionary strategies~\citep{stanley2019designing}. While these methods have empirically been shown to be stable and robust in other co-design settings, and may enable the extension of HetGPS to non-differentiable settings, they typically require more samples and may exhibit more noise compared to the direct backpropagation approach utilized in this work.