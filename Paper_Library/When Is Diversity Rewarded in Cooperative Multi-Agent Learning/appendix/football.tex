\section{Football Experiments}
\label{app:football}

\begin{table}[t]
\centering
\caption{Football heterogeneity gains across different reward formulations. Results obtained after 500 training iterations of 240k frames each (6 seeds). Opponent speed annealed from 0\% to 100\%.}
\label{tab:reward_football}
\begin{tabular}{@{}lccp{0.35\textwidth}@{}}
\toprule
Reward & $\Delta R$ & Theory $\Delta R>0$? & Reward meaning \\ \midrule
$U=\min,\; T=\max$ & $\boldsymbol{1.76} \pm 0.72$  & Yes & One agent should attend the ball, the other the opponent; reward capped by the less-covered task. \\
$U=\text{mean},\; T=\max$ & $\boldsymbol{1.18} \pm 0.11$ & Yes & Similar to $(\min,\max)$, but reward is dictated by average task performance. \\
$U=\text{mean},\; T=\text{mean}$ & $0.009 \pm 0.075$ & No & Agents should attend both the opponent and the ball. \\
$U=\min,\; T=\min$ & $-0.08 \pm 0.73$ & No & At least one agent should attend at least the opponent or the ball. \\ \bottomrule
\end{tabular}
\end{table}

In some environments, the reward structure might not entirely follow the double-generalized-aggregator structure we study in this work, but at least some part of the reward function might obey this structure. In our study of the VMAS football scenario \citep{bettini2022vmas}, we ask what happens when this is the case.

Football is a complex, embodied, long-horizon scenario that  requires the agents to learn low-level dribbling skills as well as high-level strategy purely from a shared cooperative reward. The VMAS scenario uses reward shaping to enable agents to learn such behaviors. We \textit{add} a reward structure $U(T(r_{11}^t,r_{21}^t),T(r_{12}^t,r_{22}^t))$ on top of this and ask how this affects heterogeneity. 

In our experimental scenario, two learning agents spawn at midfield. A ball is located between them and the goal to the right; a heuristic defender spawns to their left and chases the ball. Agents receive a global reward that increases when the ball moves toward the goal and the defender stays away from it. Additionally, we reuse the reward structure from our Multi‑Goal‑Capture to define rewards for two tasks: tackling the ball, and tackling the opponent.

The effort at time $t$ is:
$$r_{ij}^t=(1-\frac{d_{ij}^t}{\sum_j d_{ij}^t})/d_{ij}^t,$$
where $d_{ij}$ is distance of agent $i$ to ball or opponent). The global reward given to all agents is then computed as: $$R^t=U(T(r_{11}^t,r_{21}^t),T(r_{12}^t,r_{22}^t))+\beta[(d_{ball,goal}^{t-1}-d_{ball,goal}^t)-(d_{opp,ball}^{t-1}-d_{opp,ball}^t)],$$
where $\beta$ weighs the global football reward. 

Since this reward structure does not follow our theory entirely, we ask whether, when $U,T$ are, respectively, strictly Schur-concave and strictly Schur-convex, we should expect $\Delta R > 0$ as in our other scenarios. We test this for $U=\min, T=\max$ and $U=\text{mean},T=\max$. To control for the possibility that football is heterogeneous ``by default'', we also test the aggregator combinations $U=\text{mean}, T=\text{mean}$ and $U=\min,T=\min$ as controls. 

We report heterogeneity gains after training homogeneous and heterogeneous policies in \autoref{tab:reward_football}.
This shows that our curvature test predicts the heterogeneity gain of different reward structures, despite only being a component in the overall reward structure. This insight is important, as it indicates our theoretical insights (the curvature test) may extend beyond environments that strictly follow our task allocation setting.

The resulting policies are reported in \autoref{fig:football} with videos \href{\websiteurlfootball}{here}.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{images/football.png}
    \caption{\textbf{Left:} Results of training a heterogeneous policy on VMAS Football where agents are trained with $U=\min, T=\max$ aggregators. Our learning agents are drawn in blue; the heuristic opponent in red; and the ball in black. The learning agents split their efforts, tackling both the ball and the opponent. \textbf{Right:} Results of training a homogeneous policy. Agents are unable to split their efforts, so either they both tackle the ball, or both tackle the opponent. This results in lower reward, hence $\Delta R > 0$. These policies are visualized on \href{\websiteurlfootball}{our website}.}
    \label{fig:football}
\end{figure}