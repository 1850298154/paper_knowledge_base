\section{Colonel Blotto \& Level-Based Foraging}
\label{appendix:examples_of_marl_environments}

We describe how two well-known settings from the literature fit into our theoretical framework, and check what our theoretical results say about their  \heterogeneitygap. 

\subsection{Team Colonel Blotto (fixed adversary)}
The \textit{Colonel Blotto game} is a well-known allocation game studied in both game theory and MARL \citep{Roberson2006,noel2022reinforcementcolonelblotto}. It is used to model election strategies and other resource-based competitions. In the team variant with fixed adversary,  \(N\) friendly colonels (agents) each distribute a (fixed and equal)  budget of troops \(r_{ij}\!\ge 0, \sum_{j=1}^M r_{ij} = 1\) across \(M\) battlefields \(j\in\{1,\dots,M\}\) (our tasks).  
A fixed adversary selects a \emph{stochastic} opposing allocation
strategy, i.e.\ a distribution \(\pi_{\mathrm{adv}}\) over vectors
\(a=(a_1,\dots,a_M)\) which is fixed throughout
training and evaluation.  
Let \(s_j=\sum_{i=1}^{N} r_{ij}\) denote the team force committed to
battlefield \(j\). Our agents win against the adversary if the troops they allocate to a given field surpass the troops allocated by the adversary. The
expected value secured on battlefield \(j\) is therefore 

\[
T_j(\mathbf a_j)\;=\;v_j\,
\mathbb E_{a\sim\pi_{\mathrm{adv}}}\!\bigl[\mathbf 1\![\,s_j>a_j\,]\bigr]
   \;=\;v_j\,\Pr_{a\sim\pi_{\mathrm{adv}}}\!\bigl[s_j>a_j\bigr],
\]

where $1[x > y]$ denotes the indicator function. This is a \textbf{thresholded-sum} that remains symmetric and
coordinate-wise non-decreasing in every agent’s contribution \(r_{ij}\).
Aggregating across battlefields with a value-weighted sum yields the team
reward

\[
R(\mathbf A)\;=\;\sum_{j=1}^{M} T_j(\mathbf a_j)
           \;=\;\underbrace{\sum_{j=1}^{M}}_{U}\;
               T_j\!\Bigl(\underbrace{\sum_{i=1}^{N}}_{\inneragg}
                          r_{ij}\Bigr),
\]

so the game fits the double-aggregation
structure \(R(\mathbf A)=\outeragg_{j}\inneragg_{i}r_{ij}\) assumed in
our analysis.  

\underline{\HeterogeneityGap:} This is a continuous allocation game, and the inner aggregator $T_j$ is an indicator function over the sum of troop allocations to battlefield $j$. This function is Schur-concave (and Schur-convex at the same time!). Hence, by \autoref{thm:heterogeneity-gap-schurconcave}, heterogeneous colonel teams, where each colonel has a distinct troop allocation strategy, have no advantage over homogeneous teams, where all colonels employ the same allocation strategy: $\Delta R = 0$. This makes sense, as it makes no difference whether two different colonels allocate $x/2$ troops to a battlefield, or one colonel allocates $x$ troops to the battlefield.

Our analysis also tells us what happens when we change $T_j$: this provides insights for generalizations of the Colonel Blotto game. For example, maybe the troops of different colonels don't cooperate as well with each other, such that two colonels allocating $x/2$ troops to a battlefield results in a lower $T_j$-value than a single colonel allocating $x$ troops. In this case, $T_j$ becomes strictly Schur-convex, and \autoref{thm:heterogeneity-gap-schurconvex} tells us that $\Delta R > 0$ as long as the optimal allocation is non-trivial. Hence, heterogeneous teams are advantaged.

\subsection{Level-Based Foraging} 
The well-known \textit{level-based foraging} (LBF) benchmark, based on the  knapsack problem \citep{garey1990guidenpcompleteknapsack}, is a deceptively challenging, embodied MARL environment, where \(N\) agents are placed on a grid with \(M\) food items, and are tasked with collecting them. Each item \(j\) has an integer level \(L_j\) that must be met or exceeded by the combined skills of the agents standing on that cell before it can be collected  \citep{papoudakis2021benchmarking_multilevelforaging}.  
Let agent \(i\)’s skill be \(e_i\).  At a given step the binary variable  

\[
r_{ij}\;\in\;\{0,e_i\},
\qquad 
\text{with}\; \sum_{j=1}^{M} r_{ij}\le e_i ,
\]

denotes whether \(i\) contributes its skill to item \(j\).  In our setting, we assume all agents are equally skilled, so $e_i = 1\;\forall i$. Collecting these variables thus yields an allocation matrix \(\mathbf A=[r_{ij}] \in \{0,e_i\}^{N\times M}\), which again matches our framework.

\paragraph{Inner aggregator.}  
A food item is harvested if the summed skill on its cell reaches the threshold, so  

\[
T_j(\mathbf a_j)\;=\;L_j\;
\mathbf 1\!\bigl[\textstyle\sum_{i=1}^{N} r_{ij}\;\ge\;L_j\bigr],
\qquad 
\mathbf a_j=(r_{1j},\dots,r_{Nj})^\top .
\]

This \textbf{threshold–sum} is symmetric and monotone, depending only on the
sum of its arguments and therefore simultaneously Schur-convex and
Schur-concave.

\paragraph{Outer aggregator.}  
The stepwise team reward is the sum of harvested item
values,

\[
R(\mathbf A)
\;=\;
\sum_{j=1}^{M} T_j\!\Bigl(\sum_{i=1}^{N} r_{ij}\Bigr)
\;=\;
\underbrace{\sum_{j=1}^{M}}_{U}\;
      T_j\!\bigl(\underbrace{\sum_{i=1}^{N}}_{\inneragg} r_{ij}\bigr)
\;=\;
\outeragg_{j=1}^{M}\inneragg_{i=1}^{N} r_{ij},
\]

so LBF also conforms to the
double-aggregation form \(R(\mathbf A)=\outeragg_{j}\inneragg_{i}r_{ij}\).

\paragraph{MARL Environment Reward.}  In the level-foraging environment, items that are picked up either disappear; replace themselves with different items; or replace themselves with the same item (possibly at a different cell). In all of these cases we can represent the cumulative reward as \(\sum_{t= 0}^T\gamma^{t}\,R_t\bigl(\mathbf{A}_{t}\bigr)\) for some sequence $(R_t)_{t=1,\ldots T}$ of rewards adhering to the above reward structure.


\underline{\HeterogeneityGap:} We analyze the heterogeneity gap of a specific stepwise reward $R$. 

Because this is an embodied environment where each agent can either stand on an item (\(r_{ij}=1\)) or not
(\(r_{ij}=0\)), effort allocations are \emph{discrete}.  Our continuous
curvature test therefore does not apply directly, but the discrete
analysis in \autoref{fig:deltaR-vs-softmax} (left panel) does.

The table in \autoref{fig:deltaR-vs-softmax} tells us something about the case where all items have level \(L_j=1\). In this case, since we assumed \(e_i=1\) for all agents, the inner aggregator reduces to
\[
T_j(\mathbf a_j)=\mathbf 1\!\Bigl[\textstyle\sum_{i=1}^{N} r_{ij}\ge 1\Bigr]
            =\max_i r_{ij},
\]
while the outer aggregator is an unnormalized sum, which becomes the \textit{mean} when divided by $M$. Hence \(R(\mathbf A)=\sum_j\max_i r_{ij}\), which, up to the constant
\(1/M\), is exactly the case
\(U=\text{mean},\,T=\text{max}\) of
\autoref{fig:deltaR-vs-softmax}.  That table shows
\[
\frac{1}{M} \Delta R \;=\;\frac{\min\{M,N\}-1}{M},
\]
so the heterogeneity gap is \emph{strictly positive} whenever the team
could in principle cover more than one item (\(\min\{M,N\}>1\)).
Intuitively, a homogeneous team can only collect one item per step
(all agents flock to the same cell), whereas heterogeneous agents may
spread out and capture up to \(\min\{M,N\}\) items simultaneously.

This analysis can be extended to the case where all items have the same level \(L>1\) and \(L\mid N\) by grouping agents into \(\tilde N:=N/L\) \emph{agent teams}, each bundle contributing exactly \(L\) units of skill. This yields
\[
\frac{1}{ML} \Delta R \;=\;\frac{\min\{M,\tilde N\}-1}{M}.
\]
(We omit the formal analysis, which is not difficult). Thus, if the team can form at least two such bundles
(\(\tilde N>1\)), heterogeneity is again advantageous. If it cannot, then $\Delta R = 0$, and there is no advantage to heterogeneity. 

When the levels \(\{L_j\}\) differ, an exact closed form is harder, but in general we expect $\Delta R > 0$ whenever there is some combination of items that the heterogeneous team can collect, which in total is worth more than the largest single item that can be collected if all $N$ agents stand on its cell.

In LBF, therefore, our theory suggests that behavioral diversity is often advantageous. Note that (unlike the Colonel Blotto game) since LBF is an embodied, time-extended MARL environment, this analysis does not \textit{formally guarantee} an advantage to RL-based heterogeneous agent teams: rather, it identifies that there are  effort allocation strategies that will give these teams an advantage over homogeneous teams. The agents must still \textit{learn} and be able to execute these strategies to gain this advantage (e.g., they must learn how to move to attain the desired allocations).
