
In {\capturetheflag}, agents need to navigate to goals. Each agent observes the relative position to the goals, and agent actions are continuous 2D forces that determine their direction of motion. 
The entries $r^t_{ij}$ of matrix $\mathbf{A}^t$ at time $t$ represent the local reward of agent $i$ towards goal $j$,  computed as $r^t_{ij} =\left (1-d^t_{ij}/\sum_{j=1}^M d^t_{ij}\right )/(M-1),$
where $d^t_{ij}$ is the distance between agent $i$ and goal $j$. This makes it so that $ \sum_{j=1}^M r^t_{ij} = 1$ and $r^t_{ij}\geqslant0$. At each step, the agents receive the global reward $R(\mathbf{A}^t)$, with aggregators $U,T\in\{\min,\mathrm{mean},\max\}$.

Our results, shown in Fig. \ref{fig:ctf_gap}, show that  our curvature theory reliably predicts when there is a heterogeneity gain (\autoref{fig:deltaR-vs-softmax}): it is positive \emph{only} for the concave–convex pairs $U=\min,\,T=\max$ and $U=\mathrm{mean},\,T=\max$.  The heterogeneity gain is smaller in the latter case because learning dynamics matter: with $U=\min,\,T=\max$ the best homogeneous policy is unique (every agent must steer to the midpoint between the two goals) so homogeneous learners seldom find it, leaving room for heterogeneous policies to excel (see \autoref{app:ctf}). By contrast, $U=\mathrm{mean},\,T=\max$ admits a continuum of good homogeneous policies, which homogeneous teams execute more easily.  For $U=\max,\,T=\min$ and $U=\max,\,T=\mathrm{mean}$, the theoretical $\Delta R$ is $0$, yet the empirical  heterogeneity gap is negative (\autoref{fig:ctf_gap}). This occurs because the reward peaks only when all agents coordinate on the same goal. Neurally heterogeneous teams learn this uniform behavior slower than homogeneous teams, so they underperform within the fixed training budget. Additional training would  close this gap to $\Delta R = 0$.  

In \autoref{fig:ctf_traj} we juxtapose two representative  $N = M = 2$ roll-outs of the \textsc{\capturetheflag} environment for \emph{homogeneous} teams (top row) and \emph{heterogeneous} teams (bottom row) when~$U~=~\min,\,T=\max$. Consistent with the discussion in \autoref{sec:experiments}, homogeneous agents steer to the geometric midpoint between the two goals, producing almost overlapping paths--this is suboptimal, as they cannot cover both goals. On the other hand,  heterogeneous agents exaggerate their differences, taking sharply diverging trajectories and ensuring one goal each.

\begin{figure}[!h]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/min_max_hom_1.png}
    \caption{Homogeneous run 1}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/min_max_hom_2.png}
    \caption{Homogeneous run 2}
  \end{subfigure}\\[0.8em]
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/min_max_het_2.png}
    \caption{Heterogeneous run 1}
  \end{subfigure}
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/min_max_het_1.png}
    \caption{Heterogeneous run 2}
  \end{subfigure}\hfill
  \caption{\textbf{Behaviour under the concave–convex aggregator $U=\min,\,T=\max$.}
  Each dot is an agent position; line segments indicate instantaneous velocity;
  green squares mark goal locations.
  Homogeneous policies collapse to a single ``mid-point'' route, while heterogeneous
  policies split and follow distinct paths to cover both goals. Note how the heterogeneous agents \textit{exaggerate} the difference in their trajectories, rather than head directly to the goal: this is an outcome of the reward structure, which encourages maximal diversity.}
  \label{fig:ctf_traj}
\end{figure}


% In \autoref{fig:ctf} we show the rendered scenario. Videos of learnt policies for the various settings are available in the attached supplementary material.


% \begin{figure}[!h] 
%     \centering  % Centers the figure
%     \includegraphics[width=0.5\textwidth]{images/ctf.png}  % Adjust width as necessary, replace 'example-image' with your image file path
%     \caption{\textsc{\capturetheflag} scenario. Two agents (spawned at random positions on the left-hand side of a workspace) need to navigate to two goals (spawned at random positions on the left-hand side).
% Each agent observes the relative position to both goals. The agent actions are 2D forces that determine their direction of motion }  % Add your figure caption
%     \label{fig:ctf}  % Add a label for referencing the figure in text
% \end{figure}

