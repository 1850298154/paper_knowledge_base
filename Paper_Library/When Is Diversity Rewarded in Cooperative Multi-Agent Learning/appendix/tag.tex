\section{2v2 Tag Experiments}
\label{app:tag}


\begin{figure}[ht]
    \centering
    \setlength{\tabcolsep}{2pt}
    \renewcommand{\arraystretch}{0}

    \begin{tabular}{ccccc}
        % Seed 0000
        \begin{adjustbox}{valign=t}
        \includegraphics[width=0.22\textwidth]{images/seed_0000_homogeneous.png}
        \end{adjustbox} &
        \begin{adjustbox}{valign=t}
        \includegraphics[width=0.22\textwidth]{images/seed_0001_homogeneous.png}
        \end{adjustbox} &
        \begin{adjustbox}{valign=t}
        \includegraphics[width=0.22\textwidth]{images/seed_0002_homogeneous.png}
        \end{adjustbox} &
        \begin{adjustbox}{valign=t}
        \includegraphics[width=0.22\textwidth]{images/seed_0003_homogeneous.png}
        \end{adjustbox} &
        \begin{adjustbox}{valign=t}
        % \includegraphics[width=0.22\textwidth]{images/seed_0004_homogeneous.png}
        \end{adjustbox} \\

        % Seed 0000 heterogeneous row
        \begin{adjustbox}{valign=t}
        \includegraphics[width=0.22\textwidth]{images/seed_0000_heterogeneous.png}
        \end{adjustbox} &
        \begin{adjustbox}{valign=t}
        \includegraphics[width=0.22\textwidth]{images/seed_0001_heterogeneous.png}
        \end{adjustbox} &
        \begin{adjustbox}{valign=t}
        \includegraphics[width=0.22\textwidth]{images/seed_0002_heterogeneous.png}
        \end{adjustbox} &
        \begin{adjustbox}{valign=t}
        \includegraphics[width=0.22\textwidth]{images/seed_0003_heterogeneous.png}
        \end{adjustbox} &
        \begin{adjustbox}{valign=t}
        % \includegraphics[width=0.22\textwidth]{images/seed_0004_heterogeneous.png}
        \end{adjustbox}
    \end{tabular}

    \caption{Comparison of homogeneous (top row) and heterogeneous (bottom row) 2v2 tag policies for chaser agents, trained with the reward structure $U=\min, T=\max$ across different initializations. Every column shows the trajectory of the homogeneous (top) and heterogeneous (bottom) policies. (Note that trajectories here are smoothened; agents don't go over obstacles in actual execution). The heterogeneous policies prioritize capturing both agents, whereas the homogeneous policies focus on just one. In the $U=\min, T=\max$ setting, this gives heterogeneous agents greater reward, hence $\Delta R > 0$. Please find more visualizations on \href{\websiteurltag}{our website}.}
    \label{fig:tag_hom_vs_het}
\end{figure}


The goal of our tag experiment is to showcase that our theoretical results, which predict the value of $\Delta R$ based on the curvature of the aggregators, hold for discrete, sparse rewards. Specifically, our results for discrete efforts in  \autoref{fig:deltaR-vs-softmax} predict that only $(U,T) = (\min,\max), (\min, \text{mean}), (\text{mean}, \max)$ will have positive heterogeneity gain, with $(\min,\max)$ maximizing the gain. We show in \autoref{fig:tag_gains} that this holds in 2v2 tag, despite the fact that this is a challenging, embodied, long-horizon, whereas our formal results are for instantaneous allocation games\footnote{The gain for $(\text{mean},\max)$ is small compared to the other two aggregator combinations, but still positive at $\Delta R \approx 0.37$. This is also significantly higher than aggregator combinations for which we predict $\Delta R$ is not positive, the largest of which attained $\Delta R < 0.01$.}. Note that this is a highly interpretable result: a $(\min,\max)$ means that agents are only awarded when \textit{both} escapers are caught, incentivizing heterogeneous strategies where chaser agents split their behaviour so that the chasing efforts $r^t_{ij}$ are equally distributed between both escapers. \autoref{fig:tag_hom_vs_het} visualizes the trajectories learned by agents trained under this $(\min,\max)$  reward structure, showing distinct emergent pursuit strategies emerging depending on whether the agents are neurally heterogeneous or neurally homogeneous. 