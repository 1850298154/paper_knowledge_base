\section{Limitations and Open Questions}
\label{appendix:limitations}

We list a number of limitations, open questions, and possible extensions.

\subsection{Theoretical scope}
\begin{itemize}[leftmargin=1.5em]

\item \textbf{Beyond task-allocation RL domains.}  
      The benchmark domains we study  and the additional settings covered in \autoref{appendix:examples_of_marl_environments} all fit into our abstract task-allocation
      framework: we can interpret agents' state, such as goal proximity in \capturetheflag, or whether they captured an escaping agent in tag, abstractly as ``efforts'' $r_{ij}$ and represent the reward in terms of such efforts.  This is what enables us to make predictions about these environments. Although our framework is quite general, and accommodates environments that one might not traditionally view as ``task allocation'' (such as football and tag), several notable multi-agent RL domains, e.g., multi-robot manipulation, might not be representable within this  framework. Our heterogeneity analysis does not directly apply to these settings, and extending our results to them is important for getting a complete picture of the benefits of heterogeneity.
\end{itemize}

\subsection{Algorithmic assumptions}
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Differentiable simulation.}  
          HetGPS requires $\nabla_\theta G^\theta(\boldsymbol\pi)$, hence a simulator that is
          end-to-end differentiable and tractable to back-propagate through.  We assume differentiability mainly for considerations of training efficiency. However, many realistic environments still rely on non-smooth physics or black-box generators, requiring us to modify HetGPS for these settings. We note that there are good, established methods for learning environment parameters in non-differentiable settings (at the cost of efficiency/increased noise). We discuss these in detail in \autoref{app:stability_of_bilevel_optimization}. However, we did not test such variants of HetGPS, and leave these extensions to future work. 
\end{itemize}

\subsection{Open questions}
\begin{enumerate}[leftmargin=2em,label=\roman*.]
    \item \textbf{What is the connection between the transition function and heterogeneity?}  
          Our analysis is reward-centric: the curvature criterion reasons only about the team reward.  
          In a Dec-POMDP, however, heterogeneity can be beneficial purely because agents are constrained by \emph{state transitions}.  When do state transition dynamics benefit heterogeneity?

    \item \textbf{Learning dynamics vs.\ reward structure.}  
          The theory predicts whether a given reward structure \textit{enables} an advantage to heterogeneous teams,  not whether a
          particular learning algorithm will learn in response to it. This is connected to the difference between \textit{neural} and \textit{behavioral} heterogeneity that we emphasize throughout the paper. Our experiments suggest, empirically, that neurally heterogeneous agents will, in practice, learn to exploit heterogeneous reward structures (i.e., be behaviorally heterogeneous); but can a formal link be established between our reward structure insights and what reward the learning dynamics converge to in practice?
\end{enumerate}

Tackling these challenges would sharpen our understanding of \emph{when} and \emph{how}
diversity should be engineered in cooperative multi-agent learning.
