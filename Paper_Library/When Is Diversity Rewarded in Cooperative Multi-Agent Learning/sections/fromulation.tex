Consider a set of \(N\) agents and \(M\) tasks. Each agent \( i \in \{1,\ldots,N\} \)
allocates \textit{effort} among the tasks according to the budget constraints:
\(
r_{i1}, r_{i2}, \ldots, r_{iM} \geq 0 
 \text{ with }  
\sum_{j=1}^{M} r_{ij} \leq 1
\), where $r_{ij}$ is \textit{defined} as the effort agent $i$ puts into task $j$. Here, ``effort'' $r_{ij}$ is a scalar input to the reward function representing the agent's contribution to the task, such as resource allocation (\autoref{appendix:examples_of_marl_environments}) or realized goal proximity (\autoref{sec:experiments}).
%Here, ``effort'' is an abstract quantity that can represent various things such as resource allocation or goal proximity. 
We can consider both continuous allocations ($r_{ij}$ can be any real number) and discrete allocations ($r_{ij}$ restricted to some finite set of options), with most results in this work focusing on the continuous case. We collect all
agents' allocations into an \(N \times M\) matrix:
\(
\mathbf{A} 
= 
[r_{ij}]
\)\footnote{All results in this work can be extended to the case where \(
r_{i1}, r_{i2}, \ldots, r_{iM} \geq B_{min} 
 \text{ and }  
\sum_{j=1}^{M} r_{ij} \leq B_{max}
\) for some arbitrary $B_{min}, B_{max} \in \mathbb{R}$.}.

For each task \(j\) let the \(j\)-th column of the effort matrix be \( \mathbf a_j=[r_{1j},\dots,r_{Nj}]^\top\).  A \emph{task-level aggregator} \(T_j:\R^{N}\!\to\!\R\) maps these efforts to a \textit{task score}, and an \emph{outer aggregator} \(U:\R^{M}\!\to\!\R\) combines the \(M\) scores into the team reward, \(R(\mathbf A)=U\bigl(T_1(\mathbf a_1),\dots,T_M(\mathbf a_M)\bigr)\).  Both \(T_j\) and \(U\) are \emph{generalised aggregators}: symmetric and coordinate-wise non-decreasing, mirroring the familiar properties of \(\sum\). When every task shares the same inner aggregator we simply drop the subscript and write \(T\). To highlight the analogy with a double sum we also write \(R(\mathbf A)=\outeragg_{j=1}^{M}\inneragg_{i=1}^{N} r_{ij}\), where (in abuse of notation) the outer symbol \(\outeragg\) denotes \(U\) and the inner symbol \(\inneragg\) denotes \(T_j\).  



\headline{Homogeneous vs. Heterogeneous Strategies} A \emph{homogeneous strategy} is one where all agents have the same allocation (i.e., devote the same amount of effort to a given task $j$):
\(
r_{ij} = c_j 
 \forall\, i, j
\). In this case, the allocation matrix \(\mathbf{A}\) consists of identical rows. We define
\(
R_{\mathrm{hom}}
=
\max_{(c_1,\ldots,c_M) \in \Delta_{\!\le}^{M-1}} 
\;
R\Bigl(\mathbf{A}\Bigr)
\)
where \( \Delta_{\!\le}^{M-1} = \{(c_1,\ldots,c_M)\mid c_j\ge 0,\; \sum_j c_j \leq 1\}\) is the closed unit simplex. A \emph{heterogeneous strategy} allows each agent \( i \) to choose any 
\((r_{i1}, \ldots, r_{iM}) \in \Delta_{\!\le}^{M-1}\) independently. Then
\(
R_{\mathrm{het}}
=
\max_{\mathbf{A} \in (\Delta_{\!\le}^{M-1})^N}
\;
R\Bigl(\mathbf{A}\Bigr).
\)
%
We define the \textit{\heterogeneitygap} as:
\(
\Delta R = R_{\mathrm{het}} - R_{\mathrm{hom}}.
\)
This quantity measures how much greater the overall reward can be when agents are
allowed to specialize differently across tasks, compared to when they must behave identically.  Characterizing when \(\Delta R > 0\) is our main focus in this work. 

\headline{MARL extension}
In MARL, the effort value $r_{ij}$ represents the contribution of agent $i$ to task $j$ \textit{as computed by the environment based on agent $i$'s actions}. The aggregate reward $R(A)$ can represent:
%In MARL, the value $r_{ij}$ represents the `local rewards' agents receive based on task performance, and the aggregate reward \(R(\mathbf{A})\) can represent:
(i) the payoff of a one-shot effort-allocation game, (ii) the return or sparse terminal reward of an episode, or (iii) the stepwise reward, giving the discounted return \(\sum_{t= 0}^T\gamma^{t}\,R\bigl(\mathbf{A}_{t}\bigr)\) for a sequence \(\bigl(\mathbf{A}_{t}\bigr)_{t=1,\ldots T}\) of allocations\footnote{To extend this further, our theoretical results hold even if the reward function varies over time, $R_t(A_t)$.}. \(\Delta R>0\) implies that the best heterogeneous policies outperform the best homogeneous ones.
In practice, this is \textit{evidence of} an advantage~to~heterogeneity and not~a formal~guarantee, as learning agents may not always converge to optimal policies.

% \emph{if} the heterogeneous agents can always attain the optimal allocation; if not (as in some \autoref{sec:experiments} experiments), this is \textit{evidence of} an advantage~to~heterogeneity--not~a formal~guarantee.

\headline{Examples} \autoref{appendix:parametrized_aggregator_table} contains examples of generalized aggregators. Our framework is flexible, and can be applied to many settings, including ones not ordinarily thought of as ``task allocation'': in \autoref{sec:experiments}, we apply it to one-shot allocation games, multi-agent navigation, tag, and football. Furthermore, in \autoref{appendix:examples_of_marl_environments}, we analyze the \heterogeneitygap of two well-known environments from the literature: Team Colonel Blotto games \citep{noel2022reinforcementcolonelblotto} and level-based foraging \citep{papoudakis2021benchmarking_multilevelforaging}.
