
Collective systems, from robot fleets to insect colonies, tend to adopt one of two structures: a uniform shared blueprint or a set of distinct, specialized roles. In multi-agent learning, this is reflected in the choice between behavioral homogeneity (all agents behave identically) and behavioral heterogeneity (agents specialize) \citep{bettini2023hetgppo,bettini2023snd,rudolph2021desperate}. Such behavioral diversity can be achieved, e.g., via distinct policies (neural heterogeneity) or shared policies conditioning on diverse inputs, such as agent roles \citep{leibo2019malthusian}. Although diversity unlocks role specialization and asymmetric information use, it also introduces extra coordination cost, representation overhead, and learning complexity \citep{chenghao2021celebrating}. This trade-off leads us to ask: under what conditions will heterogeneous agents outperform the best homogeneous baseline?

% this dichotomy is reflected in the design choice between behavioral homogeneity (where all agents share one policy) and heterogeneity (where each agent learns its own) \citep{bettini2023hetgppo,bettini2023snd,rudolph2021desperate}. 



A natural setting to study this question in is \emph{multi-agent task allocation}, where $N$ agents allocate effort across $M$ concurrent tasks. Here, we define \textit{effort} as an \textit{abstract quantity} representing the agent's contribution to a given task (e.g., proximity to a goal, or quantity of a task-specific resource the agent gathered), computed in an environment-specific manner. The focus of our work is \textit{behavioral, outcome-based} heterogeneity, defined through these efforts: a homogeneous team is one where all agents have the same effort allocations (e.g., every agent allocates $0.75$ of its effort to task A and $0.25$ to task B), whereas a heterogeneous team allows agents to achieve specialized allocations.
%A homogeneous team is forced to adopt a single allocation vector (e.g., every agent spends $0.75$ of its effort on task~A and $0.25$ on task~B), whereas a heterogeneous team allows agents to specialize differently.
 We relate this abstract effort to environmental rewards in many diverse environments, including cooperative navigation, tag, football (\autoref{sec:experiments}), Colonel Blotto games, and level-based foraging (\autoref{appendix:examples_of_marl_environments})  \citep{Roberson2006,noel2022reinforcementcolonelblotto,papoudakis2021benchmarking_multilevelforaging,terry2021pettingzoo}. We ask: what effort-based reward functions require heterogeneous behaviors to be maximized?


\headline{Theoretical Insights} We first study a pure, non-spatial and instantaneous variant of multi-agent task allocation: each agent commits its effort allocation once, and the team is rewarded immediately (\autoref{sec:problemsetting}). 
We start from the observation that team reward in many effort–allocation problems can be expressed as a \emph{generalized double aggregation},
\(
R(\mathbf A)=\outeragg_{j=1}^{M}\inneragg_{i=1}^{N} r_{ij},
\)
where $\mathbf A=(r_{ij})$ is the $N \times M$ matrix of agent effort allocations, the inner operator~$\inneragg$ aggregates over the $N$ agents’ efforts in each task and the outer operator~$\outeragg$ aggregates over resulting $M$ task scores into a scalar global reward.  Choosing both operators as sums recovers the  $\sum_{j}\sum_{i} r_{ij}$ reward common in RL, whereas alternatives such as \textsc{max}, \textsc{min}, power means, or soft-max encode very different effort–reward relationships. Assuming such a reward structure, we compare the optimal heterogeneous reward, $R_{\mathrm{het}}$, with the best reward attainable under a homogeneous allocation, $R_{\mathrm{hom}}$, and define their difference as the \emph{\heterogeneitygap} $\Delta R = R_{\mathrm{het}}-R_{\mathrm{hom}}$ (\autoref{fig:hero}).  
Our main insight is that $\Delta R$ is determined by the \emph{curvature} of the two aggregators: specifically, whether they are \emph{Schur-convex} or \emph{Schur-concave}. These criteria immediately enable us to characterize the heterogeneity gain of broad families of reward functions (Table~\ref{tab:param-agg-extended}); for instance, the soft-max operator switches from Schur-concave to Schur-convex as its temperature increases. We also find exact expressions for $\Delta R$ in several important cases.  These results help explain, for example, why a reward structure that involves a $\min$ aggregator (usually used to enforce that only one agent should pursue a goal) will require behavioral diversity from the agents~\citep{bettini2024controlling}.  We relate our findings to multi-agent reinforcement learning (MARL), where environments may be embodied and time-extended, by setting \(R(\mathbf{A_t})\)~as~the~stepwise~reward~over an~allocation~sequence~$(A_t)_{t=1,\ldots,T}$.

% A positive gain \(\Delta R>0\) then implies heterogeneous agents can outperform homogeneous ones \emph{if} the het. agents can execute the optimal allocation; if not, \(\Delta R>0\) is \textit{evidence of} an advantage to heterogeneity, but does not guarantee it.

\headline{Algorithmic Search}  To study heterogeneity in MARL settings not covered by our theoretical  analysis, we develop \emph{Heterogeneity Gain Parameter Search} (HetGPS), a gradient-based algorithm that optimizes parameters~$\theta$ of underspecified, differentiable MARL environments via backpropagation to find configurations that maximize or minimize the empirical $\Delta R$ (we assume differentiability for training efficiency, but consider non-differentiable environments in \autoref{app:stability_of_bilevel_optimization}). While HetGPS can in principle optimize any differentiable environment feature to influence $\Delta R$, we use it here  to explore reward structures exclusively, as a means of verifying and extending our theoretical insights.
%a gradient-based co-design algorithm that learns environment parameters~$\theta$ in under-specified tasks via backpropagation, leveraging differentiable simulation to find parameterizations that either maximize or minimize $\Delta R$. In our experiments $\theta$ parameterizes the reward structure, i.e., the inner- and outer-aggregators, but any differentiable feature of the environment (e.g., obstacle placements) can be optimized.  
Maximizing the heterogeneity gain allows us to discover reward functions where behavioral diversity is essential. Minimizing the gain leads us to settings where homogeneous policies are sufficient.

%, which may be desirable, e.g., when parameter sharing is preferred for learning efficiency and some parameters in the reward function are flexible  (\autoref{sec:HetGPS}).
%Maximizing the \heterogeneitygap through HetGPS allows us to discover previously unknown types of reward functions where heterogeneity is essential; 


% We introduce \emph{Heterogeneity Gain Parameter Search} (HetGPS), a gradient-based co-design algorithm that learns environment parameters~$\theta$ in under-specified tasks via backpropagation, leveraging differentiable simulation to find parameterizations that either maximize or minimize $\Delta R$. In our experiments $\theta$ parametrizes the inner- and outer-aggregators, but any differentiable feature of the environment (e.g., obstacle placements) can be optimized.  Tasking HetGPS with maximizing the \heterogeneitygap lets us \emph{create} and discover new types of tasks where heterogeneity is essential; minimizing it steers an underspecified task toward regimes where homogeneous policies are sufficient, which may be useful, e.g., when parameter sharing is preferred for learning efficiency.

\headline{Experiments} We validate our theoretical insights, and HetGPS, in simulation, by evaluating in both single-shot and long-horizon reinforcement learning environments whose reward structure instantiates the kinds of aggregation operators studied. 
First, in a continuous and a discrete  matrix game, we test reward structures based on all nine possible combinations of $\{\min,\mathrm{mean},\max\}$, and find that the \heterogeneitygaps that result from the agents' learned policies match our theoretical predictions: concave outer aggregators and convex inner aggregators benefit heterogeneous teams. Next, we test the same aggregators in embodied, partially observable environments: \capturetheflag, tag, and football. We find that our theory also transfers to such long-horizon MARL settings, and show that reward structures that maximize heterogeneity are meaningful and practically useful. Finally, we find that the empirical \heterogeneitygap disappears as the richness of agents' observations is increased, recovering the finding that rich observations allow agents with~identical~policy~networks~to~be~behaviorally~heterogeneous~\citep{bettini2023hetgppo,leibo2019malthusian}. 

% Finally, we find that the empirical \heterogeneitygap disappears as the task observability is increased, recovering the finding that rich observations allow neurally homogeneous agents \citep{bettini2023hetgppo} to be behaviorally heterogeneous~\citep{leibo2019malthusian}. 

We then turn to HetGPS.  Across two parameterizable families of aggregators (Softmax and Power-Sum), we show that, despite running on embodied environments, HetGPS rediscovers the reward regimes predicted by our curvature theory to maximize the heterogeneity gain, validating both HetGPS and the connection between our theoretical insights and MARL reward design (\autoref{sec:experiments}).