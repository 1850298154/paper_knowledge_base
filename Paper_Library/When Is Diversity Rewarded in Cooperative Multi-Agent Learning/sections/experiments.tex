To empirically ground our theoretical analysis, we conduct a three-stage experimental study in cooperative MARL.  We first analyze a one-step, observation-free matrix game in which each agent allocates effort $r_{ij}$ over $M$ tasks, and consider reward structures defined by aggregator pairs $U,T\in\{\min,\mathrm{mean},\max\}$. We find that the agents' learned policies recover the exact \heterogeneitygaps derived in the theory (\autoref{fig:deltaR-vs-softmax}).  Next, we transfer the same reward structures into embodied, time-extended environments:  \capturetheflag, 2v2 tag, and football. We  show that our curvature theory continues to be and informative in these settings as well. We discuss the learning dynamics that result, and perform further experiments highlighting the difference between \textit{neural} and \textit{behavioral} heterogeneity~\citep{bettini2023hetgppo}, important for understanding our insights. 
% , and discuss interesting learning dynamics emerging from the agents' embodiment. We also expose a trade-off between the heterogeneity gain and agents' observations, showing that when  homogeneous agents can access richer observations, they can learn incrementally heterogeneous behaviors by leveraging behavioral typing and thus minimizing the gain: this highlights the difference between \textit{neural} and \textit{behavioral} heterogeneity~\citep{bettini2023hetgppo}.
% We run a set of experiments to empirically validate the results obtained from the theory and show their applicability to MARL scenarios.
% We begin by considering aggregators from the set $U,T\in\{\min,\mathrm{mean},\max\}$. Firstly, we introduce a matrix game where agent actions are represented by the task distributions $r_{ij}$ and empirically confirm the \heterogeneitygaps derived in 
% the theory.
% %\autoref{tab:softmaxextremes} and \autoref{tab:discrete-finite-tau}.
% Next, we extend the matrix game to a more realistic embodied {\capturetheflag} scenario. We confirm our results on \heterogeneitygaps in this scenario and further discuss interesting learning dynamics emerging from the agents' embodiment.
% We then show that, in the case where the \heterogeneitygap is maximized, there exists a trade-off between the gain value and the observability of homogeneous agents and how, by allowing homogeneous agents to access more observations, they can learn incrementally heterogeneous behaviors by leveraging behavioral typing and thus minimizing the gain: this highlights the difference between \textit{neural} and \textit{behavioral} heterogeneity~\citep{bettini2023hetgppo}.
Finally, to study HetGPS, we parametrize the reward structure of {\capturetheflag} using either parametrized Softmax or Power-Sum  aggregators (\autoref{appendix:parametrized_aggregator_table}), and run HetGPS to learn parameterizations that maximize the heterogeneity gain.
HetGPS learns the theoretically optimal aggregator instantiations, validating its effectiveness at discovering heterogeneous missions. Implementation details and visualizations are available in \autoref{app:code_details} and \autoref{app:ctf}. 

\headline{(i) Task Allocation}
% \label{sec:matrix_game}
We consider a one-step observationless matrix game where $N$ agents need to choose between $M$ tasks. Their actions are effort allocations $r_{ij}$ with $r_{ij}\geq 0$,$\sum_jr_{ij}=1 $,  composing matrix $\mathbf{A}$.
With aggregators taken from the set $U,T\in\{\min,\mathrm{mean},\max\}$, our goal is to empirically confirm the \heterogeneitygaps derived in the theory \textit{in a learning context}. 
Each time the game is played, all agents obtain the global reward $R(\mathbf{A})$ computed through the double aggregator.
We consider two setups: (1) \textit{Continuous} ($r_{ij}\in\R_{0\leqslant x\leqslant 1}$): agents can distribute their efforts across tasks, (2) \textit{Discrete} ($r_{ij}\in\{0,1\}$): agents choose only one task.
We train with $N=M=4$ for 12 million steps. \autoref{fig:matrix_game} shows the evolution of the \heterogeneitygaps. The final results match \textit{exactly} the theoretical predictions of \autoref{fig:deltaR-vs-softmax} and our curvature theory:  \textit{concave} outer and \textit{convex} inner aggregators favor heterogeneity. Additional details and results, e.g., for $N=M=2,8$, are in \autoref{app:matrix_game}.



\begin{figure}[t] 
    \centering  % Centers the figure
    \includegraphics[width=0.8\textwidth]{images/matrix_games_4_agents_graytext.pdf}  % Adjust width as necessary, replace 'example-image' with your image file path
    \caption{\Heterogeneitygap for the discrete and continuous matrix games with $N=M=4$ over training iterations. We report mean and standard deviation after 12M frames over 9 random seeds. The final results match the theoretical predictions in the Table of \autoref{fig:deltaR-vs-softmax}. Solid lines indicate reward structures predicted by theory to have $\Delta R > 0$ in either the discrete or continuous setting; dashed lines indicate predicted no gain in both settings.}  % Add your figure caption
    \label{fig:matrix_game_4_agents}  % Add a label for referencing the figure in text
\end{figure}

\headline{(ii-1) \capturetheflag}
%\label{sec:ctf}
Next, we investigate a time-extended, embodied scenario called {\capturetheflag}, based on multi-goal navigation missions   \citep{terry2021pettingzoo}. In {\capturetheflag}, agents need to navigate to goals, and efforts $r^t_{ij}$ are continuous scalars computed based on their proximity to these goals. We provide details in \autoref{app:ctf}. Our goal is to show that the results obtained in the matrix game still hold in this embodied, long-horizong setting.
We again consider aggregators $U,T\in\{\min,\mathrm{mean},\max\}$. After 30M training frames (Fig.~\ref{fig:ctf_gap}) the empirical \heterogeneitygaps differ, numerically, from those of the  static matrix-game because agents now realize their allocations $r_{ij}$ through time-extended motion.  \emph{Nonetheless, our curvature theory reliably predicts when there is a heterogeneity gain} (\autoref{fig:deltaR-vs-softmax}): it is positive \emph{only} for the concaveâ€“convex pairs $U=\min,\,T=\max$ and $U=\mathrm{mean},\,T=\max$.  We further explain these results (including the interesting presence of ``negative'' heterogeneity gains) in \autoref{app:ctf}.
Note that the aggregator pairs in this experiment are not contrived: they encode practically meaningful global objectives.  For example, $U=\max,T=\max$ implies ``at least one agent should go to at least one goal''; $U=\max,T=\min$ implies ``all agents should go to the same goal'', and so on.
$U=\min,T=\max$, a concave-convex setting shown by our theory to favor heterogeneity, implies ``each agent should go to a different goal and all goals should be covered'' which is a natural goal for this scenario.
This is because $T=\max$ encodes a task that needs just one agent to be completed (e.g., find an object), while $U=\min$ encodes that all tasks should be attended (i.e., agents need to diversify their choices).



% \begin{figure}[t] 
%     \centering  % Centers the figure
%     \includegraphics[width=\textwidth]{images/ctf_embodied_vanilla.pdf}  % Adjust width as necessary, replace 'example-image' with your image file path
%     \caption{\Heterogeneitygap for {\capturetheflag} over training iterations. We report mean and standard deviation after 30 million training frames over 9 different random seeds.}  % Add your figure caption
%     \label{fig:ctf_gap}  % Add a label for referencing the figure in text
% \end{figure}


\begin{figure}[!tbp]
  \centering
 \begin{subfigure}{0.5\textwidth}
        \centering
    \includegraphics[width=0.8\textwidth]{images/ctf_embodied_vanilla_graytext_2.pdf}
    \caption{\capturetheflag.}
    \label{fig:ctf_gap}
  \end{subfigure}%
   \begin{subfigure}{0.5\textwidth}
   \centering
     \includegraphics[width=0.8\textwidth]{images/tag_graytext_darker2.pdf} 
    \caption{2v2 Tag.
    }  % Add your figure caption
    \label{fig:tag_gains} 
  \end{subfigure}
  \caption{\Heterogeneitygap for \capturetheflag and 2v2 Tag  throughout training. We report mean and standard deviation for 30 million training frames over 9 random seeds.}
\end{figure}




%For $U=\max,\,T=\min$ and $U=\max,\,T=\mathrm{mean}$ the empirical  heterogeneity gap is actually \emph{negative}: this is because the reward peaks only when \emph{all} agents choose the \emph{same} goal, a coordination that heterogeneous teams learn more slowly, so they lag within the fixed training budget. Crucially, while additional training will eventually result in $\Delta R = 0$ in the latter case, 
% the positive heterogeneity gains we report are \emph{theoretically irreducible}, pinpointing scenarios where behavioral diversity is needed.



% We train for 30M frames and report the resulting
% \heterogeneitygaps in \autoref{fig:ctf_gap}. These empirical gains are not
% identical to those of the static matrix game, as agents are not directly choosing their allocations $r_{ij}$, but must reach their goal over time through movement actions. \emph{Nonetheless, our curvature theory predicts whether there is a heterogeneity gain} (see \autoref{fig:deltaR-vs-softmax}): a positive heterogeneity gain emerges \textit{only} for the concave-convex aggregator pairs \(U=\min\, T=\max\) and
% \(U=\mathrm{mean}, T=\max\). Interestingly, the gain for \(U=\mathrm{mean},\,T=\max\) is smaller
% than for \(U=\min,\,T=\max\), even though the static matrix game predicts the
% same maximal gap for both aggregator pairs.  The reason is learning dynamics: when \(U=\min,\,T=\max\) the \emph{best} homogeneous
% policy is unique: every agent must steer to the exact midpoint between the two
% goals (see \autoref{app:ctf}).  Homogeneous learners therefore struggle to discover it, leaving more
% room for heterogeneous agents to outperform them.  In contrast, under
% \(U=\mathrm{mean},\,T=\max\), many optimal policies are available.
% Because this set of ``good'' homogeneous policies is much larger, homogeneous
% learners find one quickly, shrinking the empirical gain. On the converse, for the pairs \(U=\max,\,T=\min\) and \(U=\max,\,T=\mathrm{mean}\) we even
% observe a \emph{negative} gain. In theory the gain could be zero; in practice
% it becomes negative because the reward is maximized only when \emph{all}
% agents converge on the \emph{same} goal. Heterogeneous agents can learn this
% behavior, but they require more samples, so during the limited training
% budget they under-perform their homogeneous counterparts.  The difference between these two cases is that, while heterogeneous agents will be able to bridge the negative gains with training, the positive gains are theoretically impossible to bridge, highlighting the scenarios where heterogeneity is needed.

% Interestingly, due to the learning dynamics, the gain for the latter case is smaller, even though in theory, both setups could achieve the same gain (as shown in the matrix game). It is harder in practice  for homogeneous agents to learn a good policy in the $U=\min,T=\max$ case, where the best homogeneous policy is to go exactly to the midpoint between the goals. On the other hand, when $U=\mathrm{mean},T=\max$, there are more policies leading to the best homogeneous reward, which consist in the team traveling together to any point between the goals. Since the set of best policies available to homogeneous agents in the latter case is larger, it is easier for them to reach a smaller gain.
% We also observe a negative gain for $U=\max,T=\min$ and $U=\max,T=\mathrm{mean}$. Similarly, this negative gain, that in theory could be 0, as shown in the matrix game, is due to the learning dynamics. In fact, these are the setups that require more homogeneity, where the reward is maximized when all agents go to the same goal.
% Despite heterogeneous agents could learn this behavior, they will require more training samples, and thus we see the emergence of a negative gain that reduces over time.
% The difference between these two cases is that, while heterogeneous agents will be able to bridge the negative gains, the positive gains are theoretically impossible to bridge, highlighting the scenarios where heterogeneity is needed.

\textit{Observability-Heterogeneity Trade-Off:} To understand our theoretical results, it is important to solidify the difference between \textit{neural heterogeneity} (agents having different neural networks) and \textit{behavioral heterogeneity} (agents acting differently). Our insights concern behavioral heterogeneity, which need not be neurally induced. We show this in \autoref{app:observehettradeoff}, showing that: as the observability of neurally homogeneous agents increases (allowing them to sense each other), these agents can become behaviorally heterogeneous, and thus optimize the heterogeneity gain. This result is visualized \href{{\websiteurlctf}}{here}.

\headline{(ii-2) 2v2 tag} In our tag experiment, two learning chasers pursue two heuristic escapees in a randomized obstacle field. We define the effort $r_{ij}^t$ to be $1$ if chasing agent $i$ manages to capture escaping agent $j$ by time $t$, and $0$ otherwise. Whereas \capturetheflag had continuous effort allocations, here they are \textit{discrete}. The global reward is again computed with aggregators $U, T \in \{\min, \text{mean}, \max\}$, and is awarded at every time step. This is a \textit{sparse} reward signal only awarded upon mission success. For example, $U=\min, T=\max$ pays out only if both escapees are each caught by a chaser, encouraging heterogeneity. Training outcomes are summarized in \autoref{fig:tag_gains}. We again see that our theoretical results in \autoref{fig:deltaR-vs-softmax} (discrete efforts) predict \textit{exactly} which aggregators will exhibit $\Delta R > 0$. More details and visuals are available in \autoref{app:tag} and \href{{\websiteurltag}}{here}.

\headline{(ii-3) Football} We evaluate our theory in a complex continuous control football game to explore what happens when our reward structure $R(A)$ is only part of a global cooperative reward. To this end, we design a drill in the VMAS Football environment \citep{bettini2022vmas}, where one agent is tasked to score, while the other has to block the incoming opponent. \autoref{app:football} shows that, also in this case, our theory is highly predictive, with visuals available \href{{\websiteurlfootball}}{here}.


\headline{(iii) Heterogeneous Reward Design}
We apply HetGPS to {\capturetheflag}, and ask whether it finds the same aggregator parameterizations predicted by our theory to maximize the \heterogeneitygap.
We turn the environment into a PDec-POMDP by parameterizing the reward as $ {R}^\theta (\mathbf{A}^t) = \bigoplus_{j=1}^M \!^{\theta} \bigoplus_{i=1}^N \!^{\theta} r^t_{ij}$, with parametrized inner and outer aggregators $U^\theta = \bigoplus^{\theta}, T^\theta=\bigoplus^{\theta}$.
Our goal is to learn the parameters $\theta=(\tau_1,\tau_2)$, parametrizing $T$ and $U$ respectively, that maximize the \heterogeneitygap.
We consider two parametrized aggregators from \autoref{tab:param-agg-extended}: Softmax and Power-Sum.
\textit{Softmax}:
we parameterize both $U^\theta$ and $T^\theta$
using Softmax. We initialize $\tau_1 = \tau_2 = 0$, so $U$ and $T$ are initially \textit{mean}, and run HetGPS (in \autoref{app:adverseinitializations} we show that HetGPS is robust to adversarial initializations).
In \autoref{fig:HetGPS_softmax}, we show that, to maximize the heterogeneity gain, HetGPS learns to maximize $\tau_1$, making $T$ Schur-convex, while minimizing $\tau_2$, making $U$ Schur-concave. Hence, it rediscovers the theoretically optimal reward function. The large variance in final parameters  occurs because the Softmax aggregator saturates for large magnitudes (e.g., $|\tau| > 5$); HetGPS \textit{correctly} identifies this, leading seeds to converge to arbitrary large values within it.
%HetGPS learns to maximize $\tau_1$, making the inner aggregator $T$ Schur-convex, while minimizing $\tau_2$, making the outer aggregator $U$ Schur-concave. The result is exactly the reward function predicted by our theory to maximize the gain.
\textit{Power-Sum}: we parametrize both aggregators with  Power-Sum. We initialize both functions to $\tau_1 = \tau_2 = 1$, representing \textit{sum}, and run HetGPS. We constrain $\tau_{1,2} \in [0.3,6]$ to stabilize learning.
In \autoref{fig:HetGPS_powersum}, we show that HetGPS learns to maximize $\tau_1$, making $T$ Schur-convex, while minimizing $\tau_2$, making $U$ Schur-concave; again rediscovering the optimal parametrization our theory predicts. 
These results simultaneously validate HetGPS and our curvature theory, since each arrives at the same reward structure independently. 


\begin{figure}[t]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/ctf_embodied_softmax_design.pdf}
        \caption{HetGPS with Softmax aggregators ($\tau\in \R$). $\tau \geq 0$ is Schur-convex; $\tau \leq 0$ is Schur-concave.}
        \label{fig:HetGPS_softmax}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/ctf_embodied_powersum_design.pdf}
        \caption{HetGPS with Power-Sum aggregators ($\tau\in [0.3,6]$). $\tau \geq 1$ is Schur-convex; $\tau \leq 1$ is Schur-concave.}
        \label{fig:HetGPS_powersum}
    \end{subfigure}
    \caption{HetGPS results in  {\capturetheflag}. The two leftmost columns report the evolution of aggregator parameters through training, while the rightmost column shows the obtained \heterogeneitygap.
    This result empirically demonstrates that HetGPS rediscovers the reward structure predicted by our theory to maximize the gain, \textit{making the inner aggregator convex, and the outer aggregator concave}. We report mean and standard deviation for 90M training frames over 13 random seeds.}
    \label{fig:HetGPS_experiments}
\end{figure}


