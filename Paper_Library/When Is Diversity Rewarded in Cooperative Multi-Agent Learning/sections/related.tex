
\headline{Behavioral Diversity in MARL}
Behavioral heterogeneity, where capability-identical agents learn distinct policies, can markedly improve exploration, robustness, and reward \citep{bettini2023hetgppo}. Yet heterogeneity reduces parameter sharing and thus sample-efficiency, so a core practical question is \emph{when} its benefits outweigh that cost. Existing MARL methods typically adopt one of two poles: endowing each agent with its own network, or enforcing parameter sharing so all agents follow a single policy~\citep{gupta2017cooperative,rashid2018qmix,foerster2018counterfactual,kortvelesy2022qgnn,sukhbaatar2016learning}. A large body of work explores the efficiency–diversity trade-off~\citep{christianos2021scaling,fu2022revisiting} by interpolating between these extremes: e.g., injecting agent IDs into the observation~\citep{foerster2016learning,gupta2017cooperative}, masking different subsets of shared weights~\citep{NEURIPS2024_274d0146}, sharing only selected layers~\citep{chenghao2021celebrating}, pruning a shared network into agent-specific sub-graphs~\citep{kim2023parameter}, or producing per-agent parameters with a hypernetwork~\citep{tessera2024hypermarl}. Further, several methods for promoting behavioral diversity in MARL have been proposed, such as:
conditioning agents' policies on a latent representation~\citep{wang2020roma}, 
decomposing and clustering action spaces~\citep{wang2021rode}, dynamically grouping agents to share parameters~\citep{yang2022ldsa},
applying structural constraints to the agents' policies~\citep{bettini2024controlling},
or by intrinsic rewards that maximize diversity~\citep{chenghao2021celebrating,jaques2019social,wang2019influence,jiang2021emergence,mahajan2019maven,liu2023contrastive,liu2024interaction,li2024learning}. While these studies demonstrate \textit{how} to obtain diversity, they presume tasks where heterogeneity is advantageous. Our work addresses the orthogonal question of \textit{when} diversity is beneficial, giving a principled characterization of which reward structures create that incentive in the first place.

%While these studies demonstrate \emph{how} to obtain diversity, they all presume tasks where heterogeneity is already advantageous; what is still missing, and what we supply here, is a principled characterization of \emph{which} reward~structures~create~that~incentive~in~the~first~place.

\headline{Task Allocation}
Classic resource–allocation settings, in which a team must divide finite effort among simultaneous objectives, are a central proving ground for cooperative MARL. In robotics, potential-field and market-based learning are the dominant tools for coverage, exploration, and load-balancing tasks \citep{Gupta2017,Lowe2017}. Game-theoretic analysis and, recently, MARL, play the same role in discrete counterparts such as Colonel-Blotto contests, where players decide how to spread forces over several ``battlefields'' \citep{Roberson2006,noel2022reinforcementcolonelblotto}. Embodied benchmarks like level-based foraging are heavily studied in MARL, and expose the tension between uniform and specialized effort allocations \citep{papoudakis2021benchmarking_multilevelforaging}. The survey of \citep{BasarOverview} highlights how cooperative performance is governed by the shape of the shared reward and the equilibria it induces. Our contribution sharpens this perspective: we prove that the \emph{curvature} of nested aggregation operators characterizes when heterogeneous allocations dominate homogeneous ones, and introduce algorithmic tools for further exploring settings where diversity is needed. 

\headline{Environment Co-design}
Co-design is a paradigm where agent policies \textit{and} their mission or environment are simultaneously optimized \citep{gao2024codesign,amir2025recodereinforcementlearningbaseddynamic}. Our HetGPS algorithm is related to PAIRED~\citep{dennis2020emergent}, a method which automatically designs environments in a curriculum such that an \textit{antagonist} agent succeeds while the \textit{protagonist} agent fails. This makes it so that resulting environments are challenging enough without being unsolvable. Similarly, HetGPS designs environments that are advantageous to heterogeneous teams, while disadvantaging homogeneous teams.
The key differences are: (1) the environment designer uses direct regret gradient backpropagation via a differentiable simulator instead of RL; this enables higher efficiency by directly leveraging all the environment gradient data available during collection while preventing RL-related issues identified in subsequent works~\citep{jiang2021replayguided,parker-holder2021that} such as exploration inefficiency and the need for a reward signal; and
(2), the protagonist and antagonist are independent multi-agent teams instead of single agents.  