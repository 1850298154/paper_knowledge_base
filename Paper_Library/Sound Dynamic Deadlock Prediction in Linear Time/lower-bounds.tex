%!TEX root = main.tex

\section{The Complexity of Dynamic Deadlock Prediction}
\seclabel{lower-bounds}

Detecting deadlock patterns and predictable deadlocks is clearly a problem in $\NP$,
as any witness for either problem can be verified in polynomial time.
However, little has been known about the hardness of the problem in terms of rigorous lower bounds.
Here we settle these questions, by proving strong intractability results.
Due to space constraints, we state and explain the main results here, and refer to\begin{pldi}~our technical report \cite{arxiv}\end{pldi}\begin{arxiv}~\cref{sec:sec:app_proofs_lower_bounds}\end{arxiv} for the full proofs.

\myparagraph{Parametrized hardness for detecting deadlock patterns}{
We show that the basic problem of checking the existence of a deadlock \emph{pattern} is itself
hard parameterized by the size $k$
of the pattern.
% \ucomment{Can be rewritten as: We first consider the problem of checking the existence of deadlock \emph{patterns}. Intuitively, a solution to the problem of deadlock prediction must resolve this basic problem first. Our first result shows that this problem is hard, parametrized by the size of the pattern we are looking for. In particular, the following result implies}

\begin{restatable}{theorem}{patternwonehardness}
\thmlabel{pattern-w1-hardness-pattern}
Checking if a trace $\tr$ contains a deadlock pattern of size $k$
is $\W{1}$-hard in the parameter $k$.
Moreover, the problem remains $\NP$-hard even when the lock-nesting depth of $\tr$ is constant.
\end{restatable}
}
\input{w1-hardness-pattern-construction}


\thmref{pattern-w1-hardness-pattern} implies that the problem is not only $\NP$-hard, 
but also unlikely to be \emph{fixed parameter tractable} in the size $k$ of the deadlock pattern.
In fact, under the well-believed Exponential Time Hypothesis (ETH), the parametrized
problem INDEPENDENT-SET(c) cannot be solved in time $f(c) \cdot n^{o(c)}$~\cite{chen2006strong}.
The above reduction preserves the parameter $k=c$, thus under ETH,
detecting deadlock patterns of size $k$ is unlikely to be solvable 
in time complexity $f(k) \cdot \NumEvents^{g(k)}$, where 
$g(k)$ is $o(k)$ (such as $g(k) = \sqrt{k}$ or even $g(k) = k/\log(k)$).
% It is known that INDEPENDENT-SET(c) cannot
% be solved in f (c) · n
% o(c)
% time under ETH [11]. As our reduction
% to rf-poset realizability and dynamic data-race prediction uses
% k = O(c) threads, each of these problems does not have a
% f (k) · n
% o(k)
% -time algorithm under ETH.
% In turn this means that it is unlikely to obtain an algorithm for detecting deadlock patterns that runs in time $O(\NumEvents^{c}\cdot f(k))$, where $c$ is a constant and $f$ is \emph{any} function independent of $n$.
% This is important as $k$ is typically constant; 
% $\W{1}$-hardness indicates that we can only expect algorithms with running time $O(\NumEvents^{g(k)})$, i.e., the degree of the polynomial depends on the size of the deadlock patterns which can be as high as the number of threads.
The problem of checking the existence of deadlock patterns 
is, intuitively, a precursor to the deadlock prediction problem. 
Thus, an approach for 
deadlock prediction that first identifies the existence of arbitrary deadlock patterns and 
then verifying their feasibility is unlikely to be tractable. 
In practice, the synchronization patterns corresponding to the hard instances are uncommon in executions, and our proposed algorithms (\secref{syncp} and \secref{otf}) can effectively expose predictable deadlocks (\secref{experiments}).


\myparagraph{Fine-grained hardness for deadlock pattern detection}{
\begin{comment}
	We now establish a fine-grained hardness for detecting
	deadlock patterns --- for each $k \geq 2$, we cannot check for the existence of 
	patterns of size $k$ in time $O(\NumEvents^{k-\epsilon})$ for any $\epsilon > 0$,
	under the popular Orthogonal Vectors hypothesis (OV).
	%This result is based on a reduction from the popular Orthogonal Vectors (OV) problem.
	For a fixed $k \geq 2$, the $k$-OV problem takes
	$k$ sets of $d$-dimensional vectors
	$A_1, A_2, \ldots, A_k \subseteq \set{0, 1}^d$, each of cardinality $|A_i|= n$
	($1 \leq i \leq k$) as input, 
	and asks if
	there are vectors $a_1 \in A_1, \ldots, a_k \in A_k$
	such that the extended dot product $a_1 \cdot a_2 \cdots a_k = \sum_{p =1}^d (a_1[p]\cdot a_2[p] \cdots a_k[p])= 0$.
	For a $k\geq 2$, the $k$-OV hypothesis states that for any 
	$\epsilon >0$, there is no $O(n^{k-\epsilon}\cdot \poly{d})$ algorithm for $k$-OV.
	The Strong Exponential Time Hypothesis (SETH) implies $k$-OV ~\cite{williams2005}.
	Our next theorem is based on showing that, for every $k \geq 2$, 
	detecting deadlock patterns of size $k$ is at least as hard as solving $k$-OV.
 Note the difference between \thmref{pattern-w1-hardness-pattern} and \thmref{pattern-ov-hardness}:~the former allows algorithms with running time of the form $\NumEvents^{k/2}$ (even under ETH), but the latter excludes them, requiring that $k$ fully appears in the exponent.
 The two results are based on different hypotheses and also incomparable since 
 it could turn out that $k$-OV is false but ETH is true.
 We thus establish both results, in order to develop a deeper understanding of the intricacies of the problem.
 \end{comment}
% \input{ov-hardness-pattern-construction}
% \myparagraph{The complexity of size-$2$ patterns}{
We next consider the problem of
detecting deadlock patterns of size $2$, as these form the most common case in practice~\cite{Lu08}.
Observe that \thmref{pattern-w1-hardness-pattern} has no implications on this case, as here $k$ is fixed.
The problem admits a folklore $O(\NumEvents^2)$ time algorithm, by iterating over all pairs of lock-acquisition events of the input trace, and checking whether any such pair forms a deadlock pattern.
Perhaps surprisingly, here we show that, despite its simplicity, this algorithm is optimal, i.e., we cannot hope to improve over this quadratic bound.
This result is based on a reduction from the popular Orthogonal Vectors (OV) problem.
Given two sets of $d$-dimensional vectors
$A, B \subseteq \set{0, 1}^d$
of cardinality $|A| = |B| = n$, the OV problem asks if there are $a \in A, b \in B$
such that $a \cdot b = \sum_i a[i]\cdot b[i]= 0$.
The OV hypothesis states that for any 
$\epsilon >0$, there is no $O(n^{2-\epsilon}\cdot \poly{d})$ algorithm for solving OV.
This is also a consequence of the famous Strong Exponential Time Hypothesis (SETH)~\cite{williams2005}.
We next show that detecting deadlock patterns of size $2$ is at least as hard as solving OV.
% }

\begin{restatable}{theorem}{patternovhardness}
\thmlabel{pattern-ov-hardness}
Given a trace $\tr$ of size $\NumEvents$ and $\NumLocks$ locks, 
for any $\epsilon>0$,
there is no algorithm that determines in $O(\NumEvents^{2-\epsilon}\cdot \poly{\NumLocks})$ time
whether $\tr$ has a deadlock pattern of size $2$, under the OV hypothesis.
\end{restatable}

% \input{ov-hardness-pattern-construction}
\input{2-ov-construction-old}


\myparagraph{The complexity of deadlock prediction}{
Finally, we settle the complexity of the prediction problem for deadlocks,
and show that, even for deadlock patterns of size $2$, the problem is $\W{1}$-hard
parameterized by the number of threads.
In contrast, recall that the $\W{1}$-hardness of 
\thmref{pattern-w1-hardness-pattern} concerns deadlock patterns of arbitrary size.
Our result is based on a similar hardness that was established recently for predicting data races~\cite{Mathur2020b}.


\begin{restatable}{theorem}{wonehardness}
\thmlabel{w1-hardness-pattern}
The problem of checking if a trace $\tr$ has a predictable deadlock of size $2$
is $\W{1}$-hard in the number of threads $\NumThreads$ appearing in $\tr$, and thus
is also $\NP$-hard.
\end{restatable}
}