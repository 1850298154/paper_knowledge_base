\subsection{Toward an Implementation}
\label{subsect:implementation}

We describe an example scenario that we are targeting for an
embryonic implementation.

Beforehand, a number of contracts have been executed that bind the
adult parents $P_1$ and $P_2$ in a home $H$, and also bind a number of
artificial agents in $H$, including a TAI agent ($\tau$) that oversees
the home.  (Strictly speaking, the agents wouldn't have entered into
contracts, but they would know that their human owners have done so,
and they would know what the contracts are.)

\begin{quote}{Itâ€™s winter in Berlin NY.  Night.  Outside, a blizzard.  The
  mother and father of the home $H$, and their two toddler children,
  are fast asleep.  The smartphone of each parent is set to ``Do Not
  Disturb'', with incoming clearance for only close family.  There is
  no landline phone.  A carbon monoxide sensor in the basement, near
  the furnace, suddenly shows a readout indicating an elevated level,
  which proceeds to creep up.  $\tau$ perceives this, and forms
  hypotheses about what is causing the elevated reading, and believes
  on the basis of using a cognitive calculus that the reading is
  accurate (to some likelihood factor).  The nearest firehouse is
  notified by $\tau$ .  No alarm sounds in the house.  $\tau$ runs a
  diagnostic and determines that the battery for the central auditory
  alarm is shot.  The reading creeps up higher, and now even the
  sensors in the upstairs bedrooms where the humans are asleep show an
  elevated, and climbing, level.  $\tau$ perceives this too.}
\end{quote}

Unfortunately, $\tau$ reasons that by the time the firemen arrive,
permanent neurological damage or even death may well (need again a
likelihood factor) be caused in the case of one or more members of the
family.  Should the alarm company have programmed the sensor to report
to a central command, still, any human command is fallible. The
company may be negligent, or a phone call may be the only option at
their disposal, or they may dispatch personnel who arrive too
late. Without enlisting the help of other \textit{artificial} agents
in planning and reasoning, $\tau$ can't save the family; $\tau$ knows
this on the basis of proof/argument.

However, $\tau$ can likely wake the family up, starting with the
parents, in any number of ways.  However, each of these ways entails
violation of at least one legal prohibition that has been created by
contracts that are in place.  These contracts have been analyzed by an
IBM service, which has stocked the mind of $\tau$ with knowledge of
legal obligations in \DCEC --- or rather in a dialect that has
separate obligation operators for legal $\ought_l$ and moral
$\ought_m$ obligations.  The moral obligation to save the family
overrides the legal prohibitions, however.  $\tau$ turns on the TV in
the master bedroom at maximum volume, and flashes a warning to leave
the house immediately because of the lethal gas building up.  (There
are many other alternatives, of course.  TAI could break through Do
Not Disturb, eg).

\subsection{Toward Using Smart-City Infrastructure}
\label{subsect:toward_smartcity}

The European Initiative on Smart Cities \cite{europa} is an effort by
the European Commission \cite{ec} to improve the quality of life
throughout Europe, while progressing toward energy and climate
objectives.  Many of its goals are relevant to and desirable in the
world at large.  TAI has the potential to be instrumental in achieving
many of these, such as smart appliances (in the manner discussed in
the previous sub-section) and intelligent traffic management.  Indeed,
the scope and objectives of the Initiative may conceivably be
considerably broadened with a pervasive application of TAI.

We briefly point at a simple scenario that expands on the vision of
the European Initiative's smart-transportation goals.

\begin{quote}{Parking space is very scarce on a work-day in mid-town
  Manhattan.  A busy executive will need to park near several offices
  over the course of the day, and these locations change over the
  week.}
\end{quote}

The executive's car consults her calendar.  Based on past patterns, it
interpolates locations where it believes she intends to park.  It
communicates with other cars parked at these locations, and determines
when their owners are likely to return, based on their expressed (and
inferable) intentions and current locations.  Adjusting for the
location of our executive, traffic conditions and changes in her
agenda, it determines the optimal parking locations dynamically,
throughout her busy day.  Of course, in the spirit of TAI, all other
cars would have their movement adjusted accordingly, through
time.\footnote{TAI applications like this give rise to privacy concerns which could possibly
  be resolved by employing either \textbf{differential privacy} 
  \cite{dwork2008differential} or privacy based on
  \textbf{zero-knowledge proofs} \cite{gehrke2011towards}.}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
