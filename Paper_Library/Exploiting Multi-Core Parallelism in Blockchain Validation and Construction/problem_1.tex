\section{Ordered-Block Scheduling (OBS)} \label{sec:problem1}

In this section we propose algorithms to solve the Ordered-Block Scheduling (OBS) problem. All solutions start by creating a dependency graph. Given a block containing an ordered sequence of transactions $T=(\tau_1, \ldots, \tau_n)$, we construct a \textbf{directed dependency graph} $G=(T,E)$, where each node represents a transaction and each directed edge $(i,j) \in E$ indicates that transaction $\tau_i$ must precede $\tau_j$. An edge exists if (i) the two transactions conflict (as defined in \Cref{subsubsec:conflicts}), and (ii) $i<j$, ensuring consistency with the block’s total order. This graph captures all read/write dependencies that restrict concurrent execution. 
%Due to space constraints, we defer the MILP formulation for OBS and its discussion to~\Cref{sec:OBS_MILP}.
 
 %Create a directed conflict graph $G=(T,E)$, so that the nodes are the transactions in $T$, and there is a link from $\tau_i$ to $\tau_j$ (i.e., $(i,j) \in E$) if (i) $\tau_i$ and $\tau_j$ have a conflict (explained in Section \ref{subsubsec:conflicts}) and (ii) $i<j$.

\subsection{OBS MILP Formulations}
%\section{Ordered Block Scheduling (OBS) - MILPs} 
\label{sec:OBS_MILP}

\input{MILP/pbm1_milp_simple}

\input{MILP/pbm1_milp_simple_and_complex}

\subsubsection{Homogeneous Transactions}

We first consider the homogeneous case in which all the transactions have identical execution time. Since each transaction takes the same amount of time $\executiontime{\tau}$, we discretize time into rounds $R$ of uniform length. MILP \ref{milp-pbm1-simple} defines the corresponding MILP formulation. Here, $R$ denotes the maximum number of rounds, which must be an upper bound of the rounds required and has to be provided. 

%\smallskip
\textbf{Correctness.}
In the solution of MILP \ref{milp-pbm1-simple}, $x_{ir}=1$ iff $\tau_i$ is executed in round $r$; and $y_r$ indicates whether the round $r$ is used by any transaction. The objective is to minimize the number of rounds used, minimizing total execution time (makespan).

Constraint~\eqref{con:p-cores} enforces the per-round parallelism limit. 
Since the binary variable $x_{ir}$ indicates that transaction~$\tau_i$ executes in round $r$,
the sum $\sum_i x_{ir}$ counts the number of transactions assigned to that round. 
At most $p$ cores are available, and hence we require $\sum_i x_{ir} \le p \cdot y_r$, where $y_r = 1$ only if round~$r$ is actually used. 
Therefore, no round can host more than $p$ concurrent transactions, and unused rounds ($y_r = 0$) cannot schedule any work.
Observe that in this case, the solution does not need to assign transactions to specific cores.

Constraint~\eqref{con:order} enforces precedence. 
For every directed edge $(i,j) \in E$, transaction~$\tau_i$ must complete before~$\tau_j$ begins. 
The term $\sum_{t=1}^r x_{jt}$ equals~1 if $\tau_j$ is scheduled in a round $\le r$, while $\sum_{t=1}^{r-1} x_{it}$ equals~1 if $\tau_i$ is scheduled strictly before round~$r$. 
The inequality
\[
\sum_{t=1}^r x_{jt} - \sum_{t=1}^{r-1} x_{it} \le 0, \quad \forall r,
\]
therefore excludes any assignment in which $\tau_j$ is placed before $\tau_i$. 
Together, constraints~\eqref{con:p-cores} and~\eqref{con:order} ensure that (i) no more than $p$ transactions execute in parallel in any round and (ii) all precedence edges are respected in the resulting schedule.

\subsubsection{Heterogeneous Transactions}

We now extend the model to the heterogeneous case, in which a block contains both simple and complex transactions with different execution times. MILP~\ref{milp-pbm1-simple&complex} encodes this mixed setting. The total execution time (makespan) found is denoted by $M$, and $B$ is a parameter that must be set to be larger than any possible $M$. The objective is to minimize the makespan $M$ subject to the assignment, timing, and ordering constraints.

Each transaction $\tau_i$ is assigned to exactly one core via a binary variable $x_{ic}$ and scheduled with start and end times $s_i$ and $e_i$, respectively.  Precedence edges $(i,j) \in E$ enforce dependency order via \Cref{con:order-c1}, ensuring that conflicting transactions follow the sequence order. For non-conflicting pairs, overlap is permitted unless the two transactions are assigned to the same core. Same-core assignment is captured by the auxiliary variable $w_{ijc}$ (the logical AND of $x_{ic}$ and $x_{jc}$), and $z_{ij}$ indicates whether the pair shares any core. If $z_{ij}=1$, the binary selector $y_{ij}$ determines their order: $y_{ij}=1$ means $\tau_i$ precedes $\tau_j$; otherwise, $y_{ij}=0$ means the reverse. 
%
In \Cref{con:order1} and \Cref{con:order2}, observe the following cases:
\begin{itemize}
    \item $z_{ij}=0$: the constraints have no effect due to the runtime limit term $B$.
    \item $z_{ij}=1, y_{ij}=1$:  \Cref{con:order2} has no effect, only \Cref{con:order1} applies, yielding $e_i \le s_j$.
    \item $z_{ij}=1, y_{ij}=0$: \Cref{con:order1} has no effect, only \Cref{con:order2} applies, yielding $e_j \le s_i$.
\end{itemize}
%
These conditional constraints ensure that same-core transactions are serialized.
%, while all others may execute concurrently.

%\smallskip
\textbf{Correctness.}
Constraint~\eqref{con:onetrans-c1} ensures that every transaction is assigned to exactly one core. 
Precedence edges $(i,j) \in E$ are enforced by~\eqref{con:order-c1}, which requires that any dependent transaction $\tau_j$ starts only after $\tau_i$ has completed. 
Together, these constraints preserve the total order implied by the dependency graph.
%
For non-conflicting pairs, the coupling of $w_{ijc}$, $z_{ij}$, and $y_{ij}$ ensures that same-core pairs are serialized by 
\eqref{con:order1}--\eqref{con:order2}, while pairs on different cores remain unconstrained. 
Together, these properties ensure that MILP \ref{milp-pbm1-simple&complex} produces a feasible schedule that respects all dependencies and minimizes the overall makespan~$M$.

%\smallskip
\subsubsection{Complexity} 

Both MILP formulations grow quickly with the number of transactions $n$ and cores $p$. 
The homogeneous case (MILP \ref{milp-pbm1-simple}) involves $\mathcal{O}(nR)$ binary variables and $\mathcal{O}(nR + |E|R)$ constraints, while the heterogeneous case (MILP~\ref{milp-pbm1-simple&complex}) expands to $\mathcal{O}(n^2p)$ binary variables, $\mathcal{O}(n)$ integer variables, and $\mathcal{O}(n^2p + |E|)$ constraints. 
Each formulation generalizes well-known NP-hard multiprocessor scheduling problems, implying that exact solutions become computationally prohibitive as $n$ and $p$ increase. 
%In practice, solver time rises steeply with block size and heterogeneity, motivating the heuristics introduced next, which trade a small loss in optimality for orders-of-magnitude speed.

\input{Heuristics/pbm1_DAG}

\input{Heuristics/pbm1_preprocess}

\input{Heuristics/pbm1_scheduling}

\input{Heuristics/pbm1_main}

\subsection{Heuristics}

Our heuristics (\Cref{heu:build-DAG,heu:preprocessing_problem1,heu:scheduling_problem1,heu:problem1}, 
%deferred to~\cref{sec:OBS_heuristicalgorithms}) 
use a conflict-aware greedy scheduler, guided by transaction priorities. It proceeds in three stages:

%\begin{itemize}
    %\item \textbf{DAG Construction:} For the given set of transactions $T$, we build a conflict DAG following the definition of conflicts presented in \cref{subsubsec:conflicts}.
    %\item 
    \noindent
    \textbf{(1) DAG construction.} For the given block $T=(\tau_1,\dots,\tau_n)$, we build a dependency DAG $G=(V,E)$ (\Cref{heu:build-DAG}). Each node is a transaction $\tau \in T$, and we add an edge $(\tau_i,\tau_j)$ whenever $\tau_i$ and $\tau_j$ conflict (as defined in \Cref{subsubsec:conflicts}) and $i<j$. This DAG encodes all precedence constraints that must be respected at execution time.
    %\item \textbf{Priority Scores:} First, the node-level dependency profile is extracted from the DAG. A single backward pass computes per-node height (critical-path execution length) and volume (sum of subtree execution times). The priority key is $\priority{[\tau]} \gets (-\height{[\tau]},-\volume{[\tau]},-|\successor{[\tau]}|,\id{\tau})$, favouring long critical paths, then heavier subtrees, then higher fan-out. The transaction id breaks ties deterministically.
    
    %\item 
       \noindent
       \textbf{(2) Priority scoring.} We then extract structural information from $G$ (\Cref{heu:preprocessing_problem1}). For each transaction $\tau$, we compute:
    \begin{itemize}
        \item $\height{[\tau]}$: the critical-path length rooted at $\tau$ (i.e., $\tau$'s own execution time plus the maximum downstream height),
        \item $\volume{[\tau]}$: the total execution volume of $\tau$ and its descendants,
        \item $|\successor{[\tau]}|$: the fan-out of $\tau$.
    \end{itemize}
    A single backward pass over the DAG computes these values. We assign each transaction a deterministic priority key
    $
        \priority{[\tau]} := \big(-\height{[\tau]},\; -\volume{[\tau]},\; -|\successor{[\tau]}|,\; \id{\tau}\big),
    $
    which favors long critical paths first, then heavier subtrees, then higher fan-out, and $\id{\tau}$ breaks ties.
    %\item \textbf{Transaction Scheduling:} We maintain a set of ready transactions and a set of running jobs. Whenever one or more cores become free, we dispatch the best ready job. When jobs finish, we decrease their successors’ in-degrees and enqueue those successors that reach zero in-degrees to ready. This works for both simple-only and simple-complex transaction workloads.
    
    %\item    
    \noindent
    \textbf{(3) Transaction scheduling.} Finally, we simulate the block execution on $p$ cores (\Cref{heu:scheduling_problem1}). The scheduler maintains (i) a set of \emph{ready} transactions (initially those with in-degree zero in $G$), (ii) a set of \emph{running} transactions with assigned cores and finish times, and (iii) the current time $now$. Whenever any core becomes free, we schedule the highest-priority ready transaction to that core. When a transaction finishes, we decrement the in-degree of its successors and add any successor with zero in-degree to the set of ready transactions. This event-driven loop continues until all transactions are scheduled. \Cref{heu:problem1} combines these steps into a complete pipeline.
%\end{itemize}

%\smallskip
\textbf{Complexity.}
The DAG construction in \Cref{heu:build-DAG} compares each ordered pair $(\tau_i,\tau_j)$ with $i<j$, and is therefore $\mathcal{O}(n^2)$ in the worst case.\footnote{In practice, this can often be pruned with sparse access lists, but we analyze the dense upper bound.} The preprocessing pass in \Cref{heu:preprocessing_problem1} performs a single reverse topological traversal plus constant work per edge, i.e., $\mathcal{O}(n + |E|)$. The scheduler in \Cref{heu:scheduling_problem1} runs as an event-driven simulation with at most $n$ schedule events and $n$ completion events; using a priority queue for the ready set, this is $\mathcal{O}\big(n \log n + |E|\big)$. Overall, the heuristic end-to-end runtime is dominated by DAG construction and is $\mathcal{O}(n^2)$ in the worst case.
Regarding the approximation ratio, since the heuristic implements a list scheduling, we know that it gives a
schedule with makespan at most $2-1/p$ times the optimal.

