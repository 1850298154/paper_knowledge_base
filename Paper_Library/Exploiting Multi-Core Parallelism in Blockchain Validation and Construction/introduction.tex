\section{Introduction} \label{sec:introduction}

\subsection{Motivation}

Blockchains process a continuous stream of user transactions that modify a shared replicated state. In most deployed designs (e.g., Ethereum-style execution), validators\footnote{Nodes responsible for proposing or verifying blocks, and executing transactions.} validate and execute blocks sequentially following a \emph{fixed total order.}
%, and block execution is typically carried out sequentially in that order. 
While this simplifies reasoning about state, it underutilizes modern multi-core CPUs. In practice, many transactions affect disjoint sets of accounts or storage slots and can be safely executed in parallel without altering the final state. Sequential execution, therefore, limits throughput and increases time-to-finality (i.e., the delay from block proposal to having its transactions finalized on-chain).
%
However, parallelizing block execution is not trivial, since the blockchain state is shared. Reordering or executing concurrently two transactions that conflict\footnote{Both write, or one writes and the other reads, the same account or storage slot.} 
when accessing the state may change the final state. Any viable solution must therefore preserve deterministic, order-equivalent semantics while exploiting conflict-free concurrency.

There is also an economic dimension when validators build blocks. They select transactions from a \emph{mempool} of pending transactions with heterogeneous runtimes and rewards. Since blocks
are limited in size by execution time (e.g., with a \textit{gas budget} or \textit{runtime limit}),
the question for validators is not ``how fast can we execute a given block?'' but ``how do we select the transactions in a block to maximize the reward?''
%“which transactions should we include, and how should we place them across cores, to maximize reward under a budget?”


In this paper, we study how a validator can leverage multi-core hardware without violating blockchain semantics or economic constraints. We study two complementary validator-side optimization problems, one with a fixed block order and one with flexible selection:

\smallskip
\noindent\textbf{Ordered-Block Scheduling (OBS). % (fixed order).
}
Given a block with a fixed transaction set and total order, how should a validator exploit its cores to minimize total execution time while guaranteeing equivalence to sequential execution?
This is a \textbf{scheduling} problem: transactions that access disjoint sets of accounts or storage slots can run together, while conflicting transactions must respect the block order. We capture conflicts through a \textbf{dependency DAG} and aim for a parallel schedule that minimizes the time to execute the whole block (\emph{makespan}), leveraging the inherent parallelism present in existing blocks.
This is a classical scheduling problem known as Precedence-Constrained Scheduling, known to be NP-hard on 3 cores and on 2 cores when transactions have execution times in \{1,2\}\cite{garey2002computers}.

\smallskip
\noindent\textbf{Parallel-Block Construction (PBC). %(flexible selection).}
}
On the other hand, when assembling a new block under a runtime limit, the validator has to choose from a mempool of pending heterogeneous transactions that differ in both reward and execution time. Here, the problem is deciding which transactions should be included, and how should they be assigned to cores, so that 
(i) conflicts are respected, (ii) the overall schedule fits within the runtime limit, and (iii) the total validator reward is maximized. Unlike with OBS, we can now trade off which transactions to include against how efficiently they can be scheduled in parallel. This couples selection and scheduling, reflecting the real economic and system constraints that a validator faces in practice.
To our knowledge, prior work has not studied this specific combination of conflict constraints, runtime limit, and validator reward maximization. Observe that PBS is NP-hard, since it generalizes classical NP-hard problems such as knapsack with conflict graphs, and parallel-machine scheduling with conflicts.

\subsection{Contributions}

    \noindent
    \textbf{(1) Problem definitions:} We \textbf{formalize two validator-side optimization problems:} (i)~Ordered-Block Scheduling (OBS), scheduling an ordered block on $p$ cores minimizing makespan, and 
    (ii)~Parallel-Block Construction (PBC), selecting and scheduling a subset of mempool transactions on $p$ cores under a runtime limit $B$ to maximize validator reward, all while respecting read/write conflicts.

    \noindent
    \textbf{(2) MILP formulations:} We present exact \textbf{Mixed-Integer Linear Programming (MILP) formulations}\footnote{Some formulations use only integer or Boolean variables. We refer to all of them as MILP for simplicity.} that capture conflicts and precedence, core capacity, and budget constraints. These MILPs provide optimal baselines on small-to-medium instances, but become intractable at scale; we therefore use them primarily for benchmarking and insight.

    \noindent
    \textbf{(3) Fast heuristics:} We design \textbf{deterministic heuristics} that produce feasible schedules rapidly, trading optimality for orders-of-magnitude higher speed.
    E.g., the heuristic for OBS is guaranteed to find a
schedule with makespan at most $2-1/p$ times the optimal.
%These heuristics can handle larger more realistic instances, and trade a small loss in optimality for orders-of-magnitude higher speed.
%, guided by lightweight priority keys—critical-path length, dependency degree, and reward-per-time—combined with event-driven dispatch.
%Our heuristics trade a small loss in optimality for orders-of-magnitude speed. The MILPs capture the feasibility space precisely (order/precedence, conflicts, core capacity, and budget), and the heuristics leverage lightweight priority keys (critical path, degree, reward-per-time) with event-driven dispatch. 

    \noindent
    \textbf{(4) Empirical study:} 
    We quantify empirically
%These MILP formulations and heuristics let us quantify 
the limits of parallelism in block execution and construction.
%how much speedup parallel execution can unlock on fixed blocks and how much additional reward can be realized by constructing blocks that are “friendly” to parallel execution. 
Using Ethereum mainnet traces, we compare MILPs and heuristics on both the time taken to find a
solution and its quality.
We include in the comparison a Solana-inspired declared-access baseline (Sol) for OBS and a simple reward-greedy baseline (RG) for PCB, instantiated using the access sets extracted from Ethereum traces.
%, as a point of comparison.

We observe that using multiple cores can significantly reduce makespan and increase the block reward relative to sequential block execution.
For instance, for OBS we observe that with only 2 cores we can reduce the makespan by a factor of $1.57$, and with 8 cores by more than $2$.
For PBC, the throughput can be increased almost linearly with the number of cores, and the reward
increased by a factor of more than $2$ even with $4$ cores.
%
We also observe that the runtime for solving the MILPs grows steeply with transaction heterogeneity, core count, and runtime limit. 
However, our heuristics achieve makespans and rewards close to MILP optima when available (and remain competitive against LP-relaxation upper bounds when exact optimization times out), while running in milliseconds.
    
%These results demonstrate that \textbf{efficient scheduling and block selection can unlock substantial parallelism}, narrowing the gap between current sequential pipelines and the parallel potential of modern hardware.


% - Definition of the problems

% - Formulation of mathematical programs (MILP) to optimally solve the problems

% - Heuristics that solve the problems possibly without optimality but fast.

% - Evaluation of the benefits of parallelizing, and the performance and time with each option.


    



\subsection{Structure of the Rest of the Paper}
The article is structured as follows: 
Section~\ref{sec:related} reviews existing literature related to this work.
%
Section~\ref{sec:problemstatement} presents in detail our model and problems statements.
%
Section~\ref{sec:problem1} %presents the MILP formalization of OBS and 
discusses the heuristics for OBS.
%
Section~\ref{sec:problem2} presents the MILP formalization of PBC and discusses the heuristics for this problem.
%
Section~\ref{sec:experiments} describes the experiment details.
%
Finally, Section~\ref{sec:results}, presents and discusses the experimental 
results, while Section~\ref{sec:conclusion} concludes the paper.
%
%A section with related work is deferred to~\cref{sec:related} due to space constraints.