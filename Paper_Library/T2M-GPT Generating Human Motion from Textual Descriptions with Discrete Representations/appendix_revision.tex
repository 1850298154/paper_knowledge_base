\clearpage
\appendix
\vspace*{1em}{\centering\large\bf%
Appendix
\vspace*{1.5em}}

In this appendix, we present:
\begin{itemize}
    \item Section~\ref{sec:supp_trans_arch}: ablation study of T2M-GPT architecture.
    \item Section~\ref{sec:supp_vq_loss}: ablation study of the reconstruction loss ($\mathcal{L}_{re}$ in Equation [3]) for motion VQ-VAE.
    \item Section~\ref{sec:supp_rep_ratio}: ablation study of $\tau$ for the corruption strategy in T2M-GPT training.
    \item Section~\ref{sec:supp_dilation_code}: ablation study of the number of codes in VQ-VAE.
    \item Section~\ref{sec:supp_metrics}: more details on the evaluation metrics and the motion representations.
    \item Section~\ref{sec:supp_vq_arch}: the detail of the Motion VQ-VAE architecture.
    \item Section~\ref{sec:supp_limitation}: limitations of our proposed approach.
    \item Section~\ref{sec:supp_fund}: more funding information.
\end{itemize}


\section{Ablation study of T2M-GPT architecture}
\label{sec:supp_trans_arch}

\begin{table*}[t]
    \centering\setlength{\tabcolsep}{12pt}
    \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{cccccc}
        \toprule
        Num. layers & Num. dim & Num. heads & FID $\downarrow$ & Top-1 $\uparrow$ & Training time (hours). \\
        \midrule
        4 & 512 & 8 & \et{0.469}{.014} & \et{0.469}{.002} & 17 \\  
        8 & 512 & 8 & \et{0.339}{.010} & \et{0.481}{.002} & 23 \\
        8 & 768 & 8 & \et{0.338}{.009} & \et{0.490}{.003} & 30 \\
        8 & 768 & 12 & \et{0.296}{.009} & \et{0.484}{.002} & 31 \\
        12 & 768 & 12 & \et{0.273}{.007} & \et{0.487}{.002} & 40 \\
        12 & 1024 & 16 & \et{0.149}{.007} & \et{0.489}{.002} & 55 \\
        16 & 768 & 12 & \et{0.145}{.006} & \et{0.486}{.003} & 47 \\
        16 & 1024 & 16 & \et{0.143}{.007} & \et{0.490}{.004}  & 59 \\
        18 & 768 & 12 & \etb{0.130}{.006} & \et{0.483}{.003} & 51 \\
        18 & 1024 & 16 & \et{0.141}{.005} & \etb{0.492}{.003} & 78 \\
        \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Ablation study of T2M-GPT architecture on HumanML3D~\cite{guo2022generating} test set.} For all the architectures, we use the same motion VQ-VAE. The T2M-GPT is trained with $\tau \in \mathcal{U}[0, 1]$. The training time is evaluated on a single Tesla V100-32G GPU.}
    \label{tab:supp_trans_arch}
\end{table*}

In this section, we present results with different transformer architectures for T2M-GPT. The results are provided in Table~\ref{tab:supp_trans_arch}. We notice that better performance can be obtained with a larger architecture. We finally leverage an 18-layer transformer with 16 heads and 1,024 dimensions. 

\section{Impact of the reconstruction loss in motion VQ-VAE}
\label{sec:supp_vq_loss}


\begin{table}[h]
    \centering\setlength{\tabcolsep}{10pt}
    \resizebox{0.4\textwidth}{!}{
    \begin{tabular}{lccc}
        \toprule
        \multirow{2}{*}{$\mathcal{L}_{cons}$} & \multirow{2}{*}{$\alpha$} &
        \multicolumn{2}{c}{Reconstruction} \\ \cmidrule{3-4}
         & & FID $\downarrow$ & Top-1 (\%)  \\ \midrule
        L1 & 0 & \et{0.095}{.001} & \et{0.493}{.002}  \\
        L1 & 0.5 & \et{0.144}{.001} & \et{0.495}{.003}  \\
        L1 & 1 & \et{0.160}{.001} & \et{0.496}{.003}  \\
        \midrule
        L1Smooth & 0 & \et{0.112}{.001} & \et{0.496}{.003}  \\
        L1Smooth & 0.5 & \etb{0.070}{.001} & \etb{0.501}{.002}  \\
        L1Smooth & 1 & \et{0.128}{.001} & \et{0.499}{.003}  \\
        \midrule
        L2 & 0 & \et{0.321}{.002} & \et{0.478}{.003}  \\
        L2 & 0.5 & \et{0.292}{.002} & \et{0.483}{.002}  \\
        L2 & 1 & \et{0.213}{.002} & \et{0.490}{.003} \\
        
         \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Ablation of losses for VQ-VAE on HumanML3D~\cite{guo2022generating} test set.} We report FID and Top1 metric for the models trained 300K iterations.}
\label{tab:supp_loss}
\end{table}

In this section, we study the effect of the reconstruction loss ($\mathcal{L}_{re}$ in Equation [3]) and the hyper-parameter $\alpha$ (Equation [3]). The results are presented in Table~\ref{tab:supp_loss}. We find that L1 Smooth achieves the best performance on reconstruction, and the performance of L1 loss is close to L1 Smooth loss. For the hyper-parameter $\alpha$, we find that $\alpha = 0.5$ leads to the best performance.

\section{Impact of $\tau$ for the corruption strategy in T2M-GPT training}
\label{sec:supp_rep_ratio}

\begin{table}[h]
    \centering\setlength{\tabcolsep}{12pt}
    \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{cccc}
    \toprule
        $\tau$  & FID $\downarrow$ & Top-1 $\uparrow$  & MM-Dist $\downarrow$ \\
        \midrule
        $0.0$ & \et{0.140}{.006} & \et{0.417}{.003} & \et{3.730}{.009}\\
        $0.1$ & \et{0.131}{.005} & \et{0.453}{.002} & \et{3.357}{.007}\\
        $0.3$ & \et{0.147}{.006} & \et{0.485}{.002} & \et{3.157}{.007}\\
        $0.5$ & \etb{0.116}{.004} & \et{0.491}{.003} & \etb{3.118}{.011}\\ 
        $0.7$ & \et{0.155}{.006} & \et{0.480}{.004} & \et{3.183}{.011}\\
        $\mathcal{U}[0, 1]$ & \et{0.141}{.005} & \etb{0.492}{.003} & \et{3.121}{.009} \\ 
        \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Analysis of $\tau$ on HumanML3D~\cite{guo2022generating} test set.}}
    \label{tab:mask}
\end{table}

In this section, we study $\tau$, which is used for corrupting sequences during the training of T2M-GPT. The results are provided in Table~\ref{tab:mask}. We can see that the training with corrupted sequences $\tau = 0.5$ significantly improves over Top-1 accuracy and FID compared to $\tau = 0$. Compared to $\tau \in \mathcal{U}[0, 1]$, $\tau = 0.5$ is probably preferable for HumanML3D~\cite{guo2022generating}, as it achieves comparable Top-1 accuracy compared to $\tau \in \mathcal{U}[0, 1]$ but with much better FID. 

\section{Ablation study of the number of codes in VQ-VAE}
\label{sec:supp_dilation_code}

\begin{table}[t]
    \centering\setlength{\tabcolsep}{12pt}
    \resizebox{0.4\textwidth}{!}{
    \begin{tabular}{ccc}
    \toprule
        \multirow{2}{*}{Num. code} & 
        \multicolumn{2}{c}{Reconstruction}  \\ \cmidrule{2-3}
         &  FID $\downarrow$ & Top-1 (\%)  \\ \midrule
        256 & \et{0.145}{.001} & \et{0.497}{.002}  \\
        512 & \etb{0.070}{.001} & \etb{0.501}{.002}  \\
        1024 & \et{0.090}{.001} & \et{0.498}{.003}  \\ \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Study on the number of code in codebook on HumanML3D~\cite{guo2022generating} test set.}}
    \label{tab:supp_codenum}
\end{table}

We investigate the number of codes in the codebook in Table~\ref{tab:supp_codenum}. We find that the performance of 512 codes is slightly better than 1,024 codes. The results show that 256 codes are not sufficient for reconstruction. 


\section{More details on the evaluation metrics and the motion representations.}
\label{sec:supp_metrics}

\subsection{Evaluation metrics}

We detail the calculation of several evaluation metrics, which are proposed in ~\cite{guo2022generating}. We denote ground-truth motion features, generated motion features, and text features as $f_{gt}$, $f_{pred}$, and $f_{text}$. Note that these features are extracted with pretrained networks in~\cite{guo2022generating}.

\paragraph{FID.}
FID is widely used to evaluate the overall quality of the generation. We obtain FID by
\begin{equation}
\text{FID} = \lVert \mu_{gt} - \mu_{pred}\rVert^2 - \text{Tr}(\Sigma_{gt} + \Sigma_{pred} - 2(\Sigma_{gt}\Sigma_{pred})^{\frac{1}{2}})
\label{formula:fid}
\end{equation}
where $\mu_{gt}$ and $\mu_{pred}$ are mean of $f_{gt}$ and $f_{pred}$. $\Sigma$ is the covariance matrix and $\text{Tr}$ denotes the trace of a matrix.

\paragraph{MM-Dist.}
MM-Dist measures the distance between the text embedding and the generated motion feature. Given N randomly generated samples, the MM-Dist measures the feature-level distance between the motion and the text. Precisely, it computes the average Euclidean distances between each text feature and the
generated motion feature as follows:
\begin{equation}
\text{MM-Dist} = \frac{1}{N}\sum_{i=1}^{N}\lVert f_{pred,i} - f_{text,i}\rVert
\label{formula:mm-dis}
\end{equation}
where $f_{pred,i}$ and  $f_{text,i}$ are the features of the i-th text-motion pair. 

\paragraph{Diversity.} Diversity measures the variance of the whole motion sequences across the dataset. We randomly sample $S_{dis}$ pairs of motion and each pair of motion features is denoted by $f_{pred,i}$ and $f_{pred,i}'$. The diversity can be calculated by
\begin{equation}
\text{Diversity} = \frac{1}{S_{dis}}\sum_{i=1}^{S_{dis}}||f_{pred,i} - f_{pred,i}'||
\label{formula:diversity}
\end{equation}
In our experiments, we set $S_{dis}$ to 300 as \cite{guo2022generating}.


\paragraph{MModality.} MModality measures the diversity of human motion generated from the same text description. Precisely, for the i-th text description, we generate motion 30 times and then sample two subsets containing 10 motion. We denote features of the j-th pair of the i-th text description by ($f_{pred,i,j}$, $f_{pred,i,j}'$). The MModality is defined as follows:
\begin{equation}
\text{MModality} = \frac{1}{10N}\sum_{i=1}^{N}\sum_{j=1}^{10}\lVert f_{pred,i,j} - f_{pred,i,j}'\rVert
\label{formula:mmodality}
\end{equation}


\subsection{Motion representations}

We use the same motion representations as ~\cite{guo2022generating}. Each pose is represented by $(\dot{r}^a, \dot{r}^x, \dot{r}^z, r^y, j^p, j^v, j^r, c^f)$, where $\dot{r}^a \in \mathbb{R}$ is the global root angular velocity; $\dot{r}^x \in \mathbb{R}, \dot{r}^z \in \mathbb{R}$ are the global root velocity in the X-Z plan; $j^p \in \mathbb{R}^{3j}, j^v \in \mathbb{R}^{3j}, j^r \in \mathbb{R}^{6j}$ are the local pose positions, velocity and rotation with j the number of joints; $c^f\in \mathbb{R}^4$ is the foot contact features calculated by the heel and toe joint velocity.



\section{VQ-VAE Architecture}
\label{sec:supp_vq_arch}
We illustrate the detailed architecture of VQ-VAE in Table~\ref{tab:supp_vqarch}. The dimensions of the HumanML3D~\cite{guo2022generating} and KIT-ML~\cite{plappert2016kit} datasets feature are 263 and 259 respectively.

\begin{table}[t]
    \centering\setlength{\tabcolsep}{12pt}
    \resizebox{0.4\textwidth}{!}{
    \begin{tabular}{ccc}
    \toprule
        \multirow{2}{*}{Dilation rate} & 
        \multicolumn{2}{c}{Reconstruction} \\ \cmidrule{2-3}
         &  FID $\downarrow$ & Top-1 (\%)  \\ \midrule
        1, 1, 1 & \et{0.145}{.001} & \et{0.500}{.003}  \\
        4, 2, 1 & \et{0.138}{.001} & \etb{0.502}{.002}  \\
        9, 3, 1 & \etb{0.070}{.001} & \et{0.501}{.002}  \\ 
        16, 4, 1 & \et{57.016}{.084} & \et{0.032}{.001} \\\bottomrule
    \end{tabular}
    }
    \caption{\textbf{Ablation study of different dilation rate in VQ-VAE on HumanML3D~\cite{guo2022generating} test set.}}
    \label{tab:supp_dilation}
\end{table}


\paragraph{Dilation rate.}

We investigate the impact of different dilation rates of the convolution layers used in VQ-VAE, and the results are presented in Table~\ref{tab:supp_dilation} for reconstruction. We notice that setting the dilation rate as (9, 3, 1) gives the most effective and stable performance.

\section{Limitations}
\label{sec:supp_limitation}
Our approach has two limitations: \textit{i)} for excessively long texts, the generated motion might miss some details of the textual description. Note that this typical failure case exists for all competitive approaches. \textit{ii)} some generated motion sequences slightly jitter on the legs and hands movement, this can be seen from the visual results provided in the appendix. We think the problem comes from the VQ-VAE architecture, with a better-designed architecture, the problem might be alleviated. For a real application, the jittering problem could be addressed using a temporal smoothing filter as a post-processing step.


\section{Funding Support}
\label{sec:supp_fund}
This work is supported by:
\begin{itemize}
    \item Natural Science Foundation of China (No. 62176155).
    \item Natural Science Foundation of Jilin Province (20200201037JC).
    \item Provincial Science and Technology Innovation Special Fund Project of Jilin Province (20190302026GX)
    \item Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102)
\end{itemize}


\begin{table*}[h]
    \centering\setlength{\tabcolsep}{12pt}
    \begin{tabular}{ll}
    \toprule
        Components & Architecture \\ \midrule
        VQ-VAE Encoder & (0): Conv1D($D_{in}$, 512, kernel\_size=(3,), stride=(1,), padding=(1,)) \\
        ~ & (1): ReLU() \\
        ~ & (2): 2 $\times$ Sequential( \\
        ~ &   ~~~~(0): Conv1D(512, 512, kernel\_size=(4,), stride=(2,), padding=(1,)) \\
        ~ &   ~~~~(1): Resnet1D( \\
        ~ &   ~~~~~~~~    (0): ResConv1DBlock( \\
        ~ &   ~~~~~~~~~~~~      (activation1): ReLU() \\
        ~ &   ~~~~~~~~~~~~      (conv1): Conv1D(512, 512, kernel\_size=(3,), stride=(1,), padding=(9,), dilation=(9,)) \\
        ~ &   ~~~~~~~~~~~~      (activation2): ReLU() \\
        ~ &   ~~~~~~~~~~~~      (conv2): Conv1D(512, 512, kernel\_size=(1,), stride=(1,))) \\
        ~ &   ~~~~~~~~    (1): ResConv1DBlock( \\
        ~ &   ~~~~~~~~~~~~      (activation1): ReLU() \\
        ~ &    ~~~~~~~~~~~~     (conv1): Conv1D(512, 512, kernel\_size=(3,), stride=(1,), padding=(3,), dilation=(3,)) \\
        ~ &   ~~~~~~~~~~~~      (activation2): ReLU() \\
        ~ &    ~~~~~~~~~~~~     (conv2): Conv1D(512, 512, kernel\_size=(1,), stride=(1,))) \\
        ~ &   ~~~~~~~~    (2): ResConv1DBlock( \\
        ~ &   ~~~~~~~~~~~~      (activation1): ReLU() \\
        ~ &   ~~~~~~~~~~~~      (conv1): Conv1D(512, 512, kernel\_size=(3,), stride=(1,), padding=(1,)) \\
        ~ &   ~~~~~~~~~~~~      (activation2): ReLU() \\
        ~ &   ~~~~~~~~~~~~      (conv2): Conv1D(512, 512, kernel\_size=(1,), stride=(1,))))) \\
        \midrule
        Codebook & nn.Parameter((512, 512), requires\_grad=False) \\
        \midrule
        VQ-VAE Decoder & (0): 2 $\times$ Sequential( \\
        ~ &   ~~~~(0): Conv1D(512, 512, kernel\_size=(3,), stride=(1,), padding=(1,)) \\
        ~ &   ~~~~(1): Resnet1D( \\
        ~ &   ~~~~~~~~    (0): ResConv1DBlock( \\
        ~ &   ~~~~~~~~~~~~      (activation1): ReLU() \\
        ~ &   ~~~~~~~~~~~~      (conv1): Conv1D(512, 512, kernel\_size=(3,), stride=(1,), padding=(9,), dilation=(9,)) \\
        ~ &   ~~~~~~~~~~~~      (activation2): ReLU() \\
        ~ &   ~~~~~~~~~~~~      (conv2): Conv1D(512, 512, kernel\_size=(1,), stride=(1,))) \\
        ~ &   ~~~~~~~~    (1): ResConv1DBlock( \\
        ~ &   ~~~~~~~~~~~~      (activation1): ReLU() \\
        ~ &    ~~~~~~~~~~~~     (conv1): Conv1D(512, 512, kernel\_size=(3,), stride=(1,), padding=(3,), dilation=(3,)) \\
        ~ &   ~~~~~~~~~~~~      (activation2): ReLU() \\
        ~ &    ~~~~~~~~~~~~     (conv2): Conv1D(512, 512, kernel\_size=(1,), stride=(1,))) \\
        ~ &   ~~~~~~~~    (2): ResConv1DBlock( \\
        ~ &   ~~~~~~~~~~~~      (activation1): ReLU() \\
        ~ &   ~~~~~~~~~~~~      (conv1): Conv1D(512, 512, kernel\_size=(3,), stride=(1,), padding=(1,)) \\
        ~ &   ~~~~~~~~~~~~      (activation2): ReLU() \\
        ~ &   ~~~~~~~~~~~~      (conv2): Conv1D(512, 512, kernel\_size=(1,), stride=(1,))))) \\
        ~ &   ~~~~(2): Upsample(scale\_factor=2.0, mode=nearest) \\
        ~ &   ~~~~(3): Conv1D(512, 512, kernel\_size=(3,), stride=(1,), padding=(1,)) \\
        ~ & (1): ReLU() \\
        ~ & (2): Conv1D(512, $D_{in}$, kernel\_size=(3,), stride=(1,), padding=(1,))
 \\
         \bottomrule
    \end{tabular}
    \caption{\textbf{Architecture of our Motion VQ-VAE.}}
    \label{tab:supp_vqarch}
\end{table*}