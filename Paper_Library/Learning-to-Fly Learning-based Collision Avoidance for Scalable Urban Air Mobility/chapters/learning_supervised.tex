%\subsection{\crLSTM: A Supervised Learning Approach}
\subsection{Learning-based conflict resolution}
\label{sec:learning_supervised}

Motivated by Theorem~\ref{th:MILP_CAMPC_relation},
%on the 
%connection between the MILP solution and the existence of the zero-slack solution of CA-MPC 
%optimizations, 
we propose to learn the conflict resolution policy 
from the MILP solutions.
To do so, we use a \textit{Long Short-Term Memory} (LSTM)~\cite{hochreiter1997long} recurrent neural network augmented with fully-connected layers.
LSTMs perform better than traditional recurrent neural networks on sequential prediction tasks~\cite{gers2002learning}.
%We have chosen LSTM-based model for learning the sequence of maneuvers because LSTMs perform better than traditional recurrent neural networks on sequential prediction and sequential classification tasks~\cite{gers2002learning}.
 
 \begin{figure}[tb]
 	\begin{center}
 		\includegraphics[width=0.49\textwidth, trim={0cm 0.5cm 0cm 0cm}]{figures/lstm_arc2.pdf}
 	\end{center}
 	{\caption{\small Proposed LSTM model architecture for CR-S. LSTM layers are shown unrolled over $N$ time steps. The inputs are 
 			$z_k$ which are the differences between the planned UAS positions, and the outputs are decisions $d_k$ for conflict resolution at each time $k$ in the horizon.}
 		\label{fig:lstm_arc}}
 	\vspace{-15pt}
 \end{figure} 

The network is trained to map a difference trajectory $\mathbf{z}=\mathbf{x}_1-\mathbf{x}_2$ (as in eq.~\eqref{eq:noconf}) to a decision sequence $\mathbf{d}$ that deconflicts pre-planned trajectories $\mathbf{x}_1$ and $\mathbf{x}_2$. For creating the training set, $\mathbf{d}$ is produced by solving the MILP problem~\ref{eq:CentralMILP}, i.e. obtaining 
a sequence of binary decision variables $\mathbf{b}\in\{0,1\}^{6(N+1)}$ and translating it into the decision sequence $\mathbf{d}\in\{1,\ldots,6\}^{N+1}$. 

%For the two given pre-planned trajectories $\mathbf{x_1}$ and $\mathbf{x_2}$, the feasible MILP solution defines a sequence of binary decision variables $\mathbf{b}\in\{0,1\}^{6(N+1)}$, which translate into the decision sequence $\mathbf{d}\in\{1,\ldots,6\}^{N+1}$. 
%Sequences $\mathbf{d}$ are the training outputs. 
%Trajectories $\mathbf{z}$ obtained from the set of $\mathbf{x_1}$ and $\mathbf{x_2}$ using~\eqref{eq:noconf} are the training inputs.

The proposed architecture is presented in 
 Figure~\ref{fig:lstm_arc}.
%The input shape is $(N+1,3)$, where $N$ is a number of time steps in 
%a sequence $\mathbf{z}$ defined by~\eqref{eq:noconf}. 
The input layer is 
connected to the block of three stacked LSTM layers.
The output layer is a time distributed dense layer with a 
softmax activation function such that each value is a  
decision $d_k$, $k=\{0,\ldots,N\}$. 
%The final model output is used as the sequence of deconfliction maneuvers $\mathbf{d}$.
%%of the size $(N+1)$.

