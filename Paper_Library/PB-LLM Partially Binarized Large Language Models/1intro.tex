\section{Introduction}
\label{sec:intro}
Recently, large language models (LLMs) have gained significant traction in artificial intelligence. It can be attributed to the success of models such as ChatGPT~\citep{brown2020language,ouyang2022training}. 
Following its lead, other LLMs such as OPT~\citep{zhang2022opt}, BLOOM~\citep{scao2022bloom}, and LLaMA~\citep{touvron2023llama} have emerged, proving that an increase in model size typically results in enhanced capabilities. 
As a result, models with tens to hundreds of billions of parameters have become the norm. 
However, their vast size poses considerable deployment challenges on memory-constrained devices. A model such as the LLAMA-65B (with 65 billion parameters) requires at least 130GB of memory for inference - a number that often exceeds the capacity of a single GPU or server. 

Many methods have been proposed to reduce the memory consumption of LLMs~\citep{zhu2023survey}. Those methods can be categorized into weight quantization~\citep{dettmers2022llm}, network pruning~\citep{frantar2023sparsegpt}, and low-rank factorization~\citep{zhang2023pruning}. 
Among these compression paradigms, weight quantization is particularly prominent and widely adopted for LLMs. 
Since it preserves the original model architecture and leverages well-trained LLMs' full-precision checkpoints, the compression process is greatly simplified~\citep{zhu2023survey}. 
However, state-of-the-art LLM quantization methods show a marked decline in quality beyond 4 bits~\citep{liu2023llm}.

More aggressive compression methods are required to push the LLM quantization into the lower bit range. 
The network binarization technique stands out, reducing the bit-width of weights to just one bit~\citep{helwegen2019latent,rusci2020memory,qin2020forward,qin2023bibench}.  
The binarized models take little storage and memory, and accelerate the inference by efficient bitwise operations. 
Compared to other aggressive compression technologies like high-sparsity pruning, network binarization has potent topological generics, as it only applies to parameters. 
Binarization is widely studied in academic research as a standalone compression technique, rather than simply a 1-bit specialization of quantization. 
Some SoTA binarization algorithms have even achieved full-precision performance on large-scale tasks, \eg ReActNet \citep{liu2020reactnet} for ImageNet classification~\citep{deng2009imagenet}. 
It is theoretically possible to significantly lower the LLM quantization if we generalize the idea of binarizing the weights of LLMs. 

In this paper, we explore network binarization specifically for LLM quantization and propose Partially-binarized LLMs (abbreviated as \pbllm). 
This methodology aims to achieve extreme quantization to the lowest possible bit, while maintaining the language reasoning capacity inherent in LLMs. 
The explorations indicate that simple adaptations of existing binarization algorithms do not work well for LLM quantization. 
As a result of this realization, attention is directed towards the salient-weight property of LLM quantization. In order to achieve the desired extreme low-bit quantization, salient weights must be fully exploited. 
We investigate the salient weights in aspects of their detection criteria and granularity, as well as the storage costs. Then, we propose the partially binarized matrix, storing the salient weights in higher bits.
After establishing the foundation of \pbllm, the exploration extends to regain the lost reasoning capacity of the quantized LLMs, under the frameworks of post-training quantization (PTQ) and quantization-aware training (QAT).  
In the view of PTQ, inspired by the concepts of GPTQ~\citep{frantar2022gptq}, we reconstruct the \pbllm~matrix guided by the Hessian matrix and successfully recover the reasoning capacity of \pbllm~in low-bit. 
In the view of QAT, salient weights are frozen throughout the binarization process for efficient training. 
In addition, from the perspective of quantization error minimization, we explore how binarized LLMs should be scaled based on the ideal scaling factor. We scale the binarized weight based on the derived scaling strategy shown in Fig.~\ref{fig:brief-pbllm}. 
Low-bit quantized LLMs can significantly improve their performance with such explorations. 
Benefited from explorations of PTQ and QAT, \pbllm~can efficiently obtain an extremely low-bit LLM with comparable reasoning capacity (see Fig.~\ref{fig:brief-performance}). 
The methodologies applied and the insights gained within this study stand to contribute substantially to the advancement of knowledge and development in the field of network binarization for LLMs.

% In this paper, we explore network binarization specifically for Large Language Models (LLMs) quantization, and propose Partially-binarized LLMs, abbreviated as \pbllm. 
% This methodology aims to achieve extreme quantization to the lowest possible bit, while maintaining the language reasoning capacity inherent in the quantized LLMs. 
% The exploration indicates that simple adaptations of existing binarization algorithms do not work well for LLM quantization. 
% As a result of this realization, attention is directed towards the salient-weight property characteristic of LLM quantization. In order to achieve the desired extreme low-bit quantization, salient weights must be fully exploited. As part of the proposed methodology, these salient weights are frozen throughout the binarization process and subsequently stored in a higher bit.
% In addition, from the perspective of quantization error minimization, the paper explores how binarized neural networks should be scaled based on the ideal scaling factor. The paper suggests scaling residual weights into binary based on the derived scaling strategy illustrated in Fig.~\ref{fig:brief-pbllm}. After establishing the foundational framework of \pbllm, the exploration extends to regain the lost capacity of the quantized LLMs. 
% The problem is approached from the viewpoints of Post-Training Quantization (PTQ) and Quantization-aware Training (QAT). 
% Low-bit quantized LLMs can significantly improve their performance with such explorations. 
% Benefited from explorations of PTQ and QAT, \pbllm~can efficiently obtain an extremely low bit LLM with comparable reasoning capacity (see Fig.~\ref{fig:brief-performance}). 
% The methodologies applied and the insights gained within this study stand to contribute substantially to the advancement of knowledge and development in the field of network binarization for Large Language Models.

\begin{figure}[!t]
\begin{minipage}{\textwidth}
    \begin{subfigure}{0.715\textwidth}
    \includegraphics[width=\textwidth]{figures/pipeline_2.pdf}
    \captionsetup{font=small}
    \caption{One basic block of the Partially-Binarized LLM.}
    \label{fig:brief-pbllm}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.28\textwidth}
    \includegraphics[width=\textwidth]{figures/line17-1.pdf}
    \captionsetup{font=small}
    \caption{Performance on BoolQ.}
    \label{fig:brief-performance}
    \end{subfigure}
\end{minipage}
\vspace{-0.1in}
\captionsetup{font=small}
\caption{\textbf{(a)} We introduce Partially-Binarized Large Language Model (\pbllm), where a small subset of the weights of the LLM are frozen and preserved with higher bit precision, while the remaining weights are binarized utilizing an optimal scaling factor strategy; \textbf{(b)} By using \pbllm, an extremely low-bit LLM can be acquired efficiently (\ie quantization-aware training converges quickly) while maintaining its language reasoning capabilities.}
\vspace{-0.2in}
\end{figure}

% The contributions can be summarized in three-fold: \textbf{(i)} This work is the first to introduce and comprehensively explore network binarization in LLM quantization; \textbf{(ii)} We propose \pbllm~in which we employ optimal scaling strategies and salient weights freezing to retain quantized model functionality, and then delve into the potentials of PTQ and QAT to optimize and recover the performance of models; \textbf{(iii)} \pbllm, our proposed methodology, realizes LLMs under 2-bit overall quantization, preserving their reasoning capacities.

