\section{Partially Binarizing Large Language Models (\pbllm)}
\label{sec:method}
In this section, we elaborate on the methodology of Partially Binarizing Large Language Models, named \pbllm. 
To begin, a review of the foundational framework of binarized neural networks is presented, showcasing its applicability and limitation to LLM quantization.  
Subsequently, a novel format for the quantized matrix is formulated, specifically tailored for the binarization of LLMs. 
Taking advantage of the proposed partially-binarized weight matrix, we delve into its potential in the realms of post-training quantization and training-aware training for LLMs, to break the trade-off between bit-width and performance. 
It is crucial to note that, due to constraints in computational resources, the methodology exploration predominantly utilizes OPT-1.3B~\citep{zhang2022opt} to perform the majority of experiments. 
% This model is selected as it is conducive to implementation on a single GPU and its quantization time is comparatively reasonable. 
Given the space constraints, this section primarily focuses on key aspects of the methodology. For detailed discussions, exact result values, and specific implementation details in codes, readers are referred to the supplemental materials.

\subsection{Preliminary: Network Binarization}
To begin with, we briefly review the general concept of network binarization and binarized neural networks (BNNs) in~\citep{courbariaux2015binaryconnect, hubara2016binarized}. 
% In this study, we concentrate on weight quantization, setting aside the quantization of activations~\citep{kim2023squeezellm}. 
As most optimizable quantized structures of LLMs are linear layers (see Fig.~\ref{fig:brief-pbllm}) in LLMs, we use a one-layer Perceptron to show the training and inference processes of the BNN. 
The one-layer neural network is defined as $f(\mathbf{x}) = (\mathbf{W})(\mathbf{a})$, where $\mathbf{a}\in\mathbb{R}^{d_i}$ is the input activation and $\mathbf{W}:\mathbb{R}^{d_i} \longmapsto \mathbb{R}^{d_o}$ stands for the weight matrix, with $d_i$ and $d_o$ representing the sizes of the input and output of the layer, respectively. 

The goal of network binarization is to represent floating-point (FP) weights, denoted as $\mathbf{W}_F$, and/or FP activations $\mathbf{a}_F$ as 1-bit (\ie., $\pm 1$) values~\citep{qin2020binary}. Networks utilizing this representation are referred to as BNNs. BNNs diverge from FP neural networks in their forward operations and in the approximation of backward gradients.
In the forward propagation, the sign function is used for binarizing FP values of weights: 
\begin{equation}
\text{Forward:~~~} \texttt{sign}(x) = \left\{
    \begin{array}{ll}
        +1 & \quad  x  \geq 0 \\
        -1 & \quad x < 0.
    \end{array}
    \right.
    \label{eq:critic_new}
\end{equation}
Specifically, in the training process of binarized network, the BNN maintains FP latent weights $\mathbf{W}_F$ for gradient updates, and the updated weight matrix $\mathbf{W}_F$ is binarized into the binary weight matrix $\mathbf{W}_B$ via the binarize function $\texttt{sign}(\cdot)$, \textit{i.e.} $\mathbf{W}_B = \texttt{sign}(\mathbf{W}_F)$. Then the intermediate activation map (full-precision) of this layer is produced by $\mathbf{A}_{F,o} = \mathbf{W}_B \mathbf{A}_{F,i}$. 
For inference efficiency, BNNs with 1-bit weights significantly reduce the memory cost of inference. 
Theoretically, BNNs can binarize both weights and activations to 1-bit, providing a 32x compression in memory cost and a 64x acceleration in inference speed, by replacing FP multiplications in conventional floating-point networks with Xnor-Bitcount operations. 
However, recent studies highlight that the weights of LLMs as the main contributor to memory overhead~\citep{kim2023squeezellm}, and thus we primarily aim to curtail memory costs. 
Therefore, in this pivotal exploration of binarized LLMs, our attention is specifically centered on weight binarization, foregoing the simultaneous binarization of weights and activations.

\begin{wrapfigure}{r}{0.4\textwidth} 
  \vspace{-0.2in}
  \centering
  \includegraphics[width=0.4\textwidth]{figures/existing_binarization_1.pdf}
  \vspace{-0.2in}
  \captionsetup{font=small}
  \caption{We implement five renowned binarization methods on LLMs and assess the resultant binarized LLMs across seven zero-shot common sense reasoning tasks. \textcolor{red}{Random} represents the hypothetical worst baseline, indicating random guesses, while \textcolor{blue}{FP} stands as the optimal baseline, representing full-precision OPT-1.3B. The exact values corresponding to this radar graph are detailed in the Appendix.}
  \label{fig:existing_binarization}
  \vspace{-0.3in}
\end{wrapfigure}
In the backward propagation, the main challenge is that the pervasive \texttt{sign} functions are theoretically non-differentiable, and thus extremely destroy the gradient chain in the backward propagation. To address this problem, researchers widely exploit the straight-through estimator (STE)~\citep{bengio2013estimating} to numerically approximate the derivative of the whole BNN~\citep{qin2020binary}, \ie
\begin{equation}
\text{Backward:~~~} \frac{\partial\mathcal{L}}{\partial x}= \left\{
    \begin{array}{ll}
        \frac{\partial\mathcal{L}}{\partial \texttt{sign}(x)} & \quad \left | x \right | \leq 1 \\
        0 & \quad \left | x \right | > 1,
    \end{array}
    \right.
    \label{eq:critic_new}
\end{equation}
which makes the optimization of BNN accessible. 

We first investigate the \textbf{possibility of implementing binarization to LLM quantization}. 
Specifically, following the binarization benchmark in BiBench~\citep{qin2023bibench}, we generalize some representative binarization methods into LLM quantization scenarios. 
BNN~\citep{hubara2016binarized}, XNOR~\citep{rastegari2016xnor}, Bi-Real~\citep{liu2020bi}, ReCU~\citep{xu2021recu} and FDA~\citep{xu2021learning} are re-implemented to quantize LLMs, particularly to OPT~\citep{zhang2022opt}. Training details are illustrated in the Sec.~\ref{sec:exp}. 
The results evaluated on seven zero-shot common sense reasoning tasks are shown in Fig.~\ref{fig:existing_binarization}. We can see that the LLMs binarized via the existing popular binarization algorithms perform worse than random guesses, showing that the existing binarization methods are not suitable for LLM binarization. 



\subsection{Partially Binarized Weight Matrix}
\label{sec:pbweight}
In the low-bit quantization of Transformers, a significant challenge is managing the salient weights, as they can unnecessarily extend the quantization range~\citep{kovaleva2021bert}. Several outlier-aware quantization methods have been explored to tackle this issue~\citep{dettmers2022llm,wei2022outlier,kim2023squeezellm,lin2023awq}. Notably, SqueezeLLM \citep{kim2023squeezellm} provides a generalized methodology for handling outliers in weight values during 4-bit LLM post-training quantization. Concurrently, AWQ~\citep{lin2023awq} demonstrates that preserving only $1\%$ of significant weights can benefit 4-bit LLM quantization. 
Motivated by existing research, this study also seeks to optimize the treatment of salient weights while binarizing most of weights. We present Partially-Binarized LLMs (\pbllm), a method involving the selective binarization of the LLMs' weight matrix, wherein a minor fraction of weights is kept in high bits for enhanced language capacity. 


\subsubsection{Salient Weight: Criteria, Granularity, and Cost}
Beyond the most straightforward method of choosing salient weights—selecting based on magnitude element-wise—we conduct a thorough investigation into salient weight detection from two perspectives: criteria and granularity. For criteria, we compare Magnitude- and Hessian-based methods, and for granularity, we explore both element-wise and column-wise approaches. 
In addition, we discuss the cost of storing matrix weights in a mixed-precision manner. 

\textbf{Criteria: Magnitude vs. Hessian.} 
Beyond the identification of salient weights through magnitude, alternative criteria have also been examined. 
The Hessian metric emerges as a crucial factor in LLM quantization, as elucidated in~\citep{dong2019hawq,frantar2022gptq,frantar2023sparsegpt}, particularly in relation to post-training quantization for LLMs (details regarding the Hessian criteria for PTQ can be found in Sec.~\ref{sec:ptq}).
However, we observe that the selection of salient weights, whether by magnitude or Hessian, does not significantly impact the efficacy of PTQ. 
Consequently, magnitude is elected as the preferred criterion for the identification of salient weights in both PTQ and QAT, primarily due to its simplicity and efficacy in distinguishing critical weight components.

\begin{wrapfigure}{r}{0.2\textwidth}
\centering
\vspace{-0.2in}
\includegraphics[width=0.2\textwidth]{figures/outlier_distribution.pdf}
\vspace{-0.2in}
\captionsetup{font=small}
\caption{Distribution of $5\%$ salient weight.}
\vspace{-0.3in}
\label{fig:outlier_distribution}
\end{wrapfigure}
\textbf{Granularity: Element-wise vs. Column-wise.} Our investigations reveal that adopting a column-wise approach for selecting salient weights has the potential to impair the performance of binarization. Visualization of the salient weights’ distribution within the matrix, as depicted in Fig.~\ref{fig:outlier_distribution} (where the white dots represent the filtered salient weights), disclosed a random and uniform scattering of these weights. Given the absence of any discernable column-wise pattern in the distribution of salient weights, a column-wise filtration method is deemed unsuitable. This scattered and uniform distribution necessitates an element-wise approach for effective filtration in the binarization process.

\textbf{Salient Weight Storing Cost.} The additional overhead for storing the salient weights is acceptable. 
The overall bit number, $N_{bit}$ must adhere to the following condition:
\begin{equation}
N_{bit} \leq \overbrace{1*r_{binary}}^{\text{for binary weights}}+\overbrace{N_{salient-bit}*(1-r_{binary})}^{\text{for salient weights}}+\overbrace{~~~~~~~~1~~~~~~~~}^{\text{for index storing, could be optimized}},
\label{eq:bitmap}
\end{equation}
\begin{wrapfigure}{r}{0.3\textwidth}
\centering
\vspace{-0.2in}
\includegraphics[width=0.3\textwidth]{figures/bitmap.pdf}
\vspace{-0.2in}
\captionsetup{font=small}
\caption{Variation in overall bit number $N_{bit}$ with the ratio of the salient weights $r_{binary}$, where salient weights are stored in 8-bit.}
\vspace{-0.2in}
\label{fig:bitmap}
\end{wrapfigure}
Here, $r_{binary}$ denotes the ratio of the binarized weights, $N_{salient-bit}$ represents the number of bits allocated for storing salient weights (\eg 8 bits), and the additional 1 bit is allocated for using the bitmap mechanism~\citep{chan1998bitmap} for index saving. It’s important to note that employing bitmap for index storage is not the most efficient method and can be optimized further using sparse matrix storage methods such as Compressed Sparse Row (CSR) or Compressed Sparse Column (CSC)~\citep{borvstnik2014sparse}; hence the use of $\leq$ instead of $=$ in Eq.~\ref{eq:bitmap}. Given this research's emphasis on the theoretical aspects of binarization for LLM quantization, we do not delve into saving the cost of storing the index. The relationship between the ratio of salient weights and the overall bit number is illustrated in Fig.~\ref{fig:bitmap}, depicting that a lower ratio corresponds to a reduced overall bit number. For example, retaining $10\%$ of weights in 8 bits and binarizing the remaining $90\%$ equates to, at most, a 2.7-bit quantization.


\subsection{Post-training Quantization for PB-LLMs}
\label{sec:ptq}
After defining the partially-binarized matrix format, the next step is to recover the performance (\ie the reasoning capacity in the literature of LLMs) of the quantized \pbllm. 
In this section, we explore the weight binarization with post-training quantization (PTQ) methods. 
PTQ methods hold a prominent position in the realm of quantization techniques for LLMs due to their ease of implementation. 
They enable direct quantization of pre-trained LLMs without the need for a training dataset and additional training overhead. 
Therefore, we first explore the weight binarization within the PTQ framework.

\begin{table}[t]
\centering
\captionsetup{font=small}
\caption{Perplexity of C4 on OPT-1.3B quantized with RTN (without GPTQ) and \texttt{PB-GPTQ}. Magnitude criteria or Hessian criteria is used for detecting salient weights.}
\label{tab:ptq}
\vspace{-0.1in}
\scalebox{0.8}{
\begin{tabular}{@{}lcccc@{}}
\toprule
Salient Fraction        & 50\%    & 20\%      & 10\%      & 5\%       \\ \midrule
RTN Magnitude           & 24.5675 & 5892.0898 & 4889.0385 & 8023.1132 \\
RTN Hessian             & 20.2512 & 2109.8522 & 7508.7788 & 6173.1611 \\
\texttt{PB-GPTQ} Magnitude       & 18.3674 & 46.4093   & 895.0322  & 2880.6157 \\
\texttt{PB-GPTQ} Hessian         & 17.7567 & 42.1157   & 165.6767  & 528.4877  \\
\texttt{PB-GPTQ} Magnitude g=128 & 18.0293 & 57.2164   & 1230.8537 & 2662.7114 \\
\texttt{PB-GPTQ} Hessian g=128   & 17.6000 & 45.9811   & 157.8825  & 646.3616  \\ \bottomrule
\end{tabular}}
\vspace{-0.2in}
\end{table}

GPTQ~\citep{frantar2022gptq} is the most efficient and effective method for weight quantization~\citep{zhu2023survey}, capable of quantizing LLMs to 4-bit or even 2-bit. Therefore, we generalize the idea of  GPTQ to the partial-binarization setting. 
Specifically, GPTQ quantizes the weights in LLM layer-by-layer to minimize the layer-wise quantization error:
\begin{equation}
    \argmin_{\hat{\mathbf{W}}} ||\mathbf{W}\mathbf{X} - \hat{\mathbf{W}}\mathbf{X}||_2^2
\end{equation}
% 同时始终更新所有尚未量化的权重，以补偿量化单个权重所产生的误差。 AWQ写法
GPTQ quantizes a weight $w_q$ to $\hat{w_q}$, calculates the compensation $\delta_{-q}$ for remaining weights $w_{-q}$, and then applies the compensation factor to the remaining weights:
\begin{equation}
    \delta_{-q}=\frac{w_q-\hat{w_q}}{[\mathbf{H}^{-1}]_{qq}}\cdot (\mathbf{H}^{-1})_{:,q}, \qquad
    {w}_{-q} := w_{-q} + \delta_{-q},
\end{equation}
where the $\mathbf{H}$ is the Hessian matrix of the layer-wise quantization error with respect to the weights and $w_q$ is the $q$-th value in flattened weight matrix $\mathbf{W}$.
In GPTQ, weights are quantized iteratively and the remaining weights are updated until all weights have been quantized.

We propose to use GPTQ to iteratively bianrize the un-salient weights and quantize the salient weights to higher bit, and then apply the compensation to the remaining weights.
Specifically, we first detect the salient weights $\mathbf{W}^{sal}$ and un-salient (to-be-binarized) weights $\mathbf{W}^{unsal}$ in the weight matrix $\mathbf{W}=\mathbf{W}^{sal}+\mathbf{W}^{unsal}$.
Drawing inspiration from SparseGPT~\citep{frantar2023sparsegpt}, we calculate the saliency metric, represented as $v_i=w_i^2/[\mathbf{H}^{-1}]^2_{ii}$, for the purpose of detecting salient weights using Hessian criterion.
The un-salient weights will be binarized to $\hat{\mathbf{W}}^{unsal}$, and the salient weights will be quantized to higher bit $\hat{\mathbf{W}}^{sal}$.
We use asymmetric per-channel quantization for both salient and un-salient weights.
For un-salient weight, we use the per-channel mean as zero point and calculate the optimal scaling factor $\alpha$ for the un-salient weights using the method in Sec.~\ref{sec:opt_scale}.
We use MinMax metric to calibrate the scaling factor and zero point for salient weights.

In the quantization process, we iteratively quantize the columns in the weight matrix $\mathbf{W}$.
For each column, we binarize the un-salient weights and quantize the salient weights, and then calculate the compensation for remaining weights, and then apply the compensation factor to the remaining columns of weights.
This process is repeated until all the weights are quantized.
The proposed method is denoted as \texttt{PB-GPTQ}.
% The overall pipeline is illustrated in Fig.~\ref{fig:ptq_pipeline}. 
% 我们还探索了使用groupwise的GPTQ
We also explore the fine-grained \texttt{PB-GPTQ}, which quantizes the weights in a group-wise manner.
Specifically, the weight matrix is split into several groups, each group contains $g$ columns.
In each group, we detect the salient weights and un-salient weights, and then calibrate to set the scaling factor and zero point using the weights in this group.

The results are listed in Tab.~\ref{tab:ptq}. 
\texttt{PB-GPTQ} is significantly better than RTN. 
We note that the Hessian-based \texttt{PB-GPTQ} exhibits a superior performance compared to the Magnitude criterion \texttt{PB-GPTQ}.
The group-wise \texttt{PB-GPTQ} performs better or worse than the non-group-wise \texttt{PB-GPTQ}, but the difference is not significant. 
Our analysis suggests that the disparity in scaling factors is not the primary determinant of binarization performance; hence, the introduction of group-wise methodology does not yield an enhancement in binarization performance.
Subsequently, our next endeavor will involve the application of QAT to reduce the error introduced by weight binarization.





\subsection{Quantization-aware Training for PB-LLMs}
\label{sec:qat}
In order to further enhance the reasoning capacity of the Partially-Binarized Large Language Models (\pbllm), we extend our exploration by employing Quantization-aware Training (QAT) to meticulously train the quantized models. 
Because LLM training is difficult, we desire that \pbllm~training could be as efficient as possible. 
To realize efficient training for \pbllm, we propose the Salient Weights Frozen and Optimal Scaling Factor for Binary Weights, targeting the salient weights and binarized weights, respectively. 

\subsubsection{Salient Weights Frozen}
\begin{wrapfigure}{r}{0.3\textwidth}
\centering
\vspace{-0.2in}
\includegraphics[width=0.3\textwidth]{figures/training_curves.pdf}
\vspace{-0.3in}
\captionsetup{font=small}
\caption{\textbf{Training Loss Curves:} When only $2\%$ of weights are retained in their un-binarized state, the training loss converges more swiftly.}
\vspace{-0.3in}
\label{fig:training_curves}
\end{wrapfigure}
\label{sec:salient_weight_frozen}
To leverage the value of pre-trained weights, we propose freezing the salient weights, determined by weight magnitude, prior to the weight binarization process. 
As illustrated in Fig.~\ref{fig:brief-pbllm}, we initially filter out a number of weights from a pre-trained weight matrix—\eg $2\%$ by magnitude—at the beginning of quantization-aware training, maintaining their fixed state throughout the training process. 
Examination of training efficiency (refer to Fig.\ref{fig:training_curves}) suggests that these salient weights play a crucial role in LLM capacity. Maintaining the high bit representation of certain weights, thereby freezing them, aids in the training of quantized LLMs and reduces their optimization difficulty.

\subsubsection{Optimal Scaling Factor for Binary Weights.} 
\label{sec:opt_scale}
AWQ~\citep{lin2023awq} enhances the weight-only quantization method for LLMs by optimizing scaling factors to mitigate the quantization error of quantized weights. Specifically, AWQ demonstrates that searching for empirically optimal scaling factors proves to be an effective strategy for reducing quantization errors and recovering the performance of the quantized models. 
Fortunately, in the context of LLM binarization, we have a better choice for scaling the binarized weights. There’s no need to search for optimal scaling factors as they can be \textbf{analytically derived}. 
Specifically, we apply a column-wise scaling factor to binarized weights to \textbf{reduce the binarization error}, \ie enforcing $\mathbf{w}_F=\alpha\bar{\mathbf{w}}_B$. 
The optimal values of scaling factor $\alpha$ for the $\bar{\mathbf{w}}_B\in\{-1,1\}$ can be calculated by minimizing the L2 error:
\begin{equation}
    \alpha^{\star} = \arg\min_{\alpha \in \mathbb{R}_{+}}\mathcal{J}(\alpha),~\text{in which}~\mathcal{J}(\alpha)=\Vert\mathbf{w}_F - \alpha\bar{\mathbf{w}}_B\Vert_2^2
\end{equation}
Following XNOR-Net~\citep{rastegari2016xnor}, by expanding the below equation, we have
\begin{equation}
    \mathcal{J}(\alpha) = \alpha^2\bar{\mathbf{w}}_B^T\bar{\mathbf{w}}_B - 2\alpha\mathbf{w}_F^T\bar{\mathbf{w}}_B + \mathbf{w}_F^T\mathbf{w}_F
\end{equation}
\begin{wrapfigure}{r}{0.333\textwidth}
\centering
\vspace{-0.2in}
\includegraphics[width=0.333\textwidth]{figures/bar1.pdf}
\vspace{-0.2in}
\captionsetup{font=small}
\caption{\textbf{Perplexity (PPL) on C4:} When $50\%$ of the weights are maintained in their un-binarized state (equivalent to around 5-bit quantization), the untrained \pbllm~does not experience a total loss of reasoning capabilities.}
\vspace{-0.25in}
\label{fig:simple-pbllm}
\end{wrapfigure}
For the vector with $\mathbf{w}_F\in \mathbb{R}^n$ we follow the traditional methods of binarizing weights~\citep{hubara2016binarized} by taking the sign of real-valued weights: 
\begin{equation}
    \bar{\mathbf{w}}_B^i = \texttt{sign}(\mathbf{w}_F^i) = \left\{
    \begin{array}{ll}
        +1, & \quad  \mathbf{w}_F^i  \geq 0; \\
        -1, & \quad \mathbf{w}_F^i < 0.
    \end{array}
    \right.
    \label{eq:critic_new}
\end{equation}
In that case, $\bar{\mathbf{w}}_B^T\bar{\mathbf{w}}_B = n_{\mathbf{w}_F}$, where $n_{\mathbf{w}_F}$ is number of elements in $\mathbf{w}_F$, and $\alpha^{*}$ can be solved as:
\begin{equation}
    \alpha^{*} = \frac{\mathbf{w}_F^T\bar{\mathbf{w}}_B}{n_{\mathbf{w}_F}}=\frac{\Vert \mathbf{w}_F \Vert_1}{n_{\mathbf{w}_F}}.
\end{equation}
A counterintuitive outcome emerges from the incorporation of salient-frozen and optimal-scaling mechanisms: directly deploying those two mechanisms to pre-trained LLM even \textit{without any retraining or fine-tuning}, still results in commendable performance. For instance, applying these techniques to OPT-1.3B with 50\% salient weights (see Fig.~\ref{fig:simple-pbllm}) reveals that the partially-binarized OPT-1.3B retains a small amount of language capacity, corroborating the importance of a small number of salient weights in LLM quantization.
Consequently, implementing just these two techniques—Outlier Frozen and Optimal Scaling Factor for Binary Weights—on pre-trained LLMs serves as an efficient starting point for training \pbllm. 

Both of the above-proposed mechanisms are very effective when used during quantization-aware training of \pbllm. 
The consequential outcomes are delineated in Figs.\ref{subfig1}-\ref{subfig16}. 
Observations from the presented results elucidate that optimizing using the partially-binarized quantization format is notably more straightforward compared to single-bit quantization. This empirical evidence corroborates the discussion regarding the rapid convergence property found in Sec.\ref{sec:salient_weight_frozen}, highlighting the efficacy and adaptability of our proposed methodology in optimizing LLMs within the constraints of partial binarization.
From the perspective of QAT, \pbllm~emerges as more efficient in training compared to existing LLM QAT methods. For instance, while models like LLM-QAT~\citep{liu2023llm} necessitate up to 100K iterations for adequate training, \pbllm~remarkably achieves recovery of the performance of quantized LLMs in merely around 1-10K iterations. This substantial reduction in required iterations represents a leap in training efficiency, streamlining the path to achieving optimal performance in quantized LLMs with significantly reduced computational effort.

\begin{figure}[!t]
\begin{minipage}{\textwidth}
    \begin{subfigure}{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/line1.pdf}
    \captionsetup{font=tiny}
    \caption{BoolQ~\citep{clark2019boolq}}
    \label{subfig1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/line2.pdf}
    \captionsetup{font=tiny}
    \caption{PIQA~\citep{bisk2020piqa}}
    \label{subfig2}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/line3.pdf}
    \captionsetup{font=tiny}
    \caption{HellaSwag~\citep{zellers2019hellaswag}}
    \label{subfig3}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/line4.pdf}
    \captionsetup{font=tiny}
    \caption{WinoGrande~\citep{sakaguchi2021winogrande}}
    \label{subfig4}
    \end{subfigure}
\end{minipage}
\begin{minipage}{\textwidth}
    \begin{subfigure}{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/line5.pdf}
    \captionsetup{font=tiny}
    \caption{ARC-E~\citep{clark2018think}}
    \label{subfig5}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/line6.pdf}
    \captionsetup{font=tiny}
    \caption{ARC-C~\citep{clark2018think}}
    \label{subfig6}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/line7.pdf}
    \captionsetup{font=tiny}
    \caption{OBQA~\citep{mihaylov2018can}}
    \label{subfig7}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/line8.pdf}
    \captionsetup{font=tiny}
    \caption{\textbf{Average on Seven Tasks}}
    \label{subfig8}
    \end{subfigure}
\end{minipage}
\begin{minipage}{\textwidth}
    \begin{subfigure}{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/line9.pdf}
    \captionsetup{font=tiny}
    \caption{BoolQ~\citep{clark2019boolq}}
    \label{subfig9}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/line10.pdf}
    \captionsetup{font=tiny}
    \caption{PIQA~\citep{bisk2020piqa}}
    \label{subfig10}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/line11.pdf}
    \captionsetup{font=tiny}
    \caption{HellaSwag~\citep{zellers2019hellaswag}}
    \label{subfig11}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/line12.pdf}
    \captionsetup{font=tiny}
    \caption{WinoGrande~\citep{sakaguchi2021winogrande}}
    \label{subfig12}
    \end{subfigure}
\end{minipage}
\begin{minipage}{\textwidth}
    \begin{subfigure}{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/line13.pdf}
    \captionsetup{font=tiny}
    \caption{ARC-E~\citep{clark2018think}}
    \label{subfig13}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/line14.pdf}
    \captionsetup{font=tiny}
    \caption{ARC-C~\citep{clark2018think}}
    \label{subfig14}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/line15.pdf}
    \captionsetup{font=tiny}
    \caption{OBQA~\citep{mihaylov2018can}}
    \label{subfig15}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.245\textwidth}
    \includegraphics[width=\textwidth]{figures/line16.pdf}
    \captionsetup{font=tiny}
    \caption{\textbf{Average on Seven Tasks}}
    \label{subfig16}
    \end{subfigure}
\end{minipage}
\label{figure:qatlines}
\captionsetup{font=small}
\vspace{-0.1in}
\caption{\textbf{QAT training results with 30\% salient weights \pbllm~(upper two lines):} As fine-tuning epochs increase, quantized models swiftly regain their reasoning capacities, demonstrating the resilience and adaptability of \pbllm in sustaining cognitive functionalities within models, despite substantial quantization; \textbf{QAT training results with 5\% salient weights \pbllm~(bottom two lines)}: Existing LLM QAT methods exhibit an absolute failure when subjected to extremely-low bit conditions. In contrast, \pbllm triumphs in restoring the reasoning capacities of low-bit quantized LLMs. This underlines the efficacy of \pbllm~in balancing quantization and performance, preserving the essential reasoning abilities of LLMs even under rigorous bit reduction.}
\vspace{-0.4in}
\end{figure}
