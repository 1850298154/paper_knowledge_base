\begin{abstract}
This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. 
Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (\pbllm), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. 
Specifically, our exploration first uncovers the ineffectiveness of naïve applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. 
Thus, \pbllm~filters a small ratio of salient weights during binarization, allocating them to higher-bit storage, \ie partially-binarization. 
\pbllm~is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). 
Under PTQ, combining the concepts from GPTQ, we reconstruct the binarized weight matrix guided by the Hessian matrix and successfully recover the reasoning capacity of \pbllm~in low-bit. 
Under QAT, we freeze the salient weights during training, explore the derivation of optimal scaling factors crucial for minimizing the quantization error, and propose a scaling mechanism based on this derived scaling strategy for residual binarized weights. 
Those explorations and the developed methodologies significantly contribute to rejuvenating the performance of low-bit quantized LLMs and present substantial advancements in the field of network binarization for LLMs. 
The code is available at \href{https://github.com/hahnyuan/PB-LLM}{PB-LLM}.

% % Version:0 
% This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. 
% Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (\pbllm), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. 
% Specifically, our exploration first uncovers the ineffectiveness of naïve applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. 
% Thus, \pbllm~freezes the salient weights during binarization, allocating them to higher bit storage. 
% Further, we explore the derivation of optimal scaling factors crucial for the to-be-binarized weights and propose a scaling mechanism based on this derived scaling strategy for residual weights. 
% This framework is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). 
% Those explorations and the developed methodologies significantly contribute to rejuvenating the performance of low-bit quantized LLMs and present substantial advancements in the field of network binarization for LLMs. 
% The code is available at \href{https://github.com/hahnyuan/BinaryLLM}{PB-LLM}.
\end{abstract}