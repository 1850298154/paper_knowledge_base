\section{Related Work}
\label{related}

\subsection{Network Binarization.}

Binarization uses the sign function to binarize weights and activations to $\pm 1$. 
To eliminate the vanishing gradient issue caused by the sign function in the binarization, the straight-through estimator (STE)~\citep{bengio2013estimating} is utilized for the network backpropagation. Based on this archetype, copious studies contribute to improving the performance of BNNs. 
Binarization techniques can be broadly classified into three categories: the enhancement of training objectives, the reduction of gradient mismatch, and the minimization of quantization errors~\citep{qin2020binary,qin2023bibench,yuan2023comprehensive}. To illustrate:
\textit{Gradient Mismatch:} \cite{liu2020bi} introduce double residual connections paired with full-precision downsampling layers. This approach addresses the gradient vanishing problem that arises due to binarization.
\textit{Training Objectives:} \cite{martinez2020training} focus on optimizing the loss function during training. They suggest aligning the spatial attention maps derived from both binary and real-valued convolutions.
\textit{Quantization Error Minimization:} \cite{rastegari2016xnor} identify that the disparity in quantization between full-precision and binarized weights can impede the representational abilities of BNNs. As a solution, they introduce a scaling factor—determined by the L1 norm—for both weights and activation functions. 

While binarization has proven successful in computer vision, its exploration in natural language processing remains limited. 
Existing methods~\citep{bai2020binarybert,qin2022bibert,liu2022bit,liu2023binary} primarily target smaller language models (\eg BERT-base~\citep{devlin2018bert} with 110M parameters) potentially hindering their generalization to larger ones (\eg LLAMA-7B~\citep{touvron2023llama} with 7B parameters). 
We investigate binarization for LLMs comprehensively in this paper and propose \pbllm, which is an attempt to compress LLMs using binarization. 


\subsection{Large Language Model Quantization.}

Quantization, a prominent method in model compression, addresses the storage and computational overhead of deep learning models. 
Recent research efforts successfully apply quantization to compress Large Language Models (LLMs), including Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ).

In the domain of QAT, innovative strategies like LLM-QAT~\citep{liu2023llm} address challenges in acquiring training data for LLMs by leveraging pre-trained models for data-free distillation. 
Additionally, techniques such as QLORA~\citep{dettmers2023qlora} focus on parameter-efficient fine-tuning (PEFT), expediting model compression and inference acceleration.
In PTQ, approaches range from quantizing only the weights of LLMs to jointly quantizing both weights and activations. 
Methods like GPTQ~\citep{frantar2022gptq} and QuIP~\citep{chee2023quip} optimize matrix multiplications and propose novel layer-wise quantization techniques achieving high compression rates. 
SqueezeLLM~\citep{kim2023squeezellm} and SpQR~\citep{dettmers2023spqr} identify weights that lead to particularly large quantization errors and subsequently storing them with higher precision to mitigate the accuracy degradation caused by weight quantization.
AWQ~\citep{lin2023awq} and OWQ~\citep{lee2023owq} contend that when quantizing weights, it is crucial to account for the impact of activation outliers on weights.
Norm Tweaking~\citep{li2023norm} addresses the issue of activation value deviation by training LayerNorm.
For activation quantization, ZeroQuant~\citep{yao2022zeroquant} proposes a fine-grained quantization method that can be applied to both weights and activations. 
Methods like SmoothQuant~\citep{xiao2022smoothquant} and Outlier Suppression~\citep{wei2022outlier,wei2023outlier} shift the quantization challenge from activations to weights by proposing a mathematically equivalent per-channel scaling transformation. 
OmniQuant~\citep{2023omniquant} further enhances performance by training the quantization parameters.
RPTQ~\citep{yuan2023rptq} proposed proposes performance improvement through grouped quantization after clustering similar channels.
In this paper, our primary focus lies in the binarization of weights exclusively, employing both PTQ and QAT methodologies.

% While QAT methods have been shown to improve the accuracy of DNNs in some cases, they require significant computational resources to train the models. 
% For instance, LSQ introduces a differentiable quantization function, which enables gradient-based optimization during training~\cite{esser2019learned}. 
% LSQ involves quantizing and de-quantizing the activation and weights, which requires additional computations. 
% Additionally, LSQ involves optimizing the quantization parameters, which requires extra training epochs to solve and update the gradient of the parameters.
% This makes them less practical for large-scale language models (LLMs) that already have high training costs. 
% In contrast, PTQ methods are more feasible for LLMs, as they involve quantizing pre-trained models, which do not require additional training time.


% Recently, there are some multi-billion scale transformer quantization methods designed for LLMs.
% ZeroQuant~\cite{yao2022zeroquant} proposes a fine-grained quantization scheme that can be applied to both weights and activations. 
% It treats each layer of the transformer as a small neural network and uses the FP model to distill the quantization model.
% nuQmm~\cite{park2022nuqmm} utilizes group-wise binary-coding non-uniform quantization scheme, and propose a specialized multiplicative kernel to speed up the operation.
% LLM.int8()~\cite{dettmers2022llm} observes that a significant contributor to poor quantization performance is outliers in activations.
% The method fixes this with mixed-precision quantization.
% SmoothQuant~\cite{xiao2022smoothquant} migrates the quantization difficulty from activations to weights by proposing a mathematically equivalent per-channel scaling transformation. 
% This transformation smooths the magnitude across channels, making the model quantization-friendly.
% GPTQ~\cite{frantar2022gptq} uses second-order approximation to quantize weights, enabling the weight quantization of LLMs into 4-bit - the first post-training method to do so. 
% However, these methods can only achieve the quantization of activations to 8 bits.
% Comprehensive study~\cite{yao2023comprehensive} has improved ZeroQuant, treating each linear layer of the transformer as a small neural network for distillation, and achieved usable performance at W4A8 quantization.

% PTQ-SL~\cite{yuan2021ptq} proposed that adjusting the channel order of weights can lead to higher accuracy in finely-quantized networks. 
% However, PTQ-SL mainly focuses on the quantization of weights in convolutional networks, and does not address the quantization issues of activations. 
% PGQ~\cite{bondarenko2021understanding} employs a range-based permutation of the embedding dimensions and share quantization parameters among elements in the same group to address the problem of activation quantization.
% Nonetheless, it only consider for the dynamic range and utilizes uniformly divided groups, rendering it less efficacious for LLMs.
