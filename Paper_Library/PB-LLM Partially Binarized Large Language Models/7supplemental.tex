\section{Supplemental Materials}
\subsection{Exisiting Binarization Methods on LLM Quantization}
\begin{table}[!ht]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{l|cccccccc}
    \toprule
        Method & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-Easy & ARC-Challenge & OBQA & Mean \\ \midrule
        Random Performance & 0.5 & 0.5 & 0.25 & 0.5 & 0.25 & 0.25 & 0.25 & 0.36 \\ 
        FP & 0.595 & 0.63 & 0.415 & 0.595 & 0.54 & 0.22 & 0.25 & 0.46 \\ \hline
        BNN & 0.38 & 0.545 & 0.235 & 0.46 & 0.195 & 0.165 & 0.15 & 0.30 \\ 
        XNOR & 0.37 & 0.525 & 0.265 & 0.49 & 0.195 & 0.165 & 0.16 & 0.31 \\ 
        Bi-Real & 0.395 & 0.5 & 0.25 & 0.505 & 0.235 & 0.185 & 0.165 & 0.32 \\ 
        ReCU & 0.39 & 0.515 & 0.24 & 0.51 & 0.255 & 0.185 & 0.175 & 0.32 \\ 
        FDA & 0.39 & 0.485 & 0.265 & 0.49 & 0.265 & 0.19 & 0.17 & 0.32 \\ \bottomrule
    \end{tabular}}
    \label{table:existing-binarization}
    \caption{Table corresponds to Figure 2 in the main paper: We implement five renowned binarization methods on LLMs and assess the resultant binarized LLMs across seven zero-shot common sense reasoning tasks.}
\end{table}
We first investigate the possibility of implementing binarization to LLM quantization. 
Specifically, following the binarization benchmark in BiBench~\citep{qin2023bibench}, we generalize some representative binarization methods into LLM quantization scenarios. 
BNN~\citep{hubara2016binarized}, XNOR~\citep{rastegari2016xnor}, Bi-Real~\citep{liu2020bi}, ReCU~\citep{xu2021recu} and FDA~\citep{xu2021learning} are re-implemented to quantize LLMs, particularly to OPT~\citep{zhang2022opt}. Training details are illustrated in the Sec. 4. 
The results evaluated on seven zero-shot common sense reasoning tasks are shown in the above table. We can see that the LLMs binarized via the existing popular binarization algorithms perform worse than random guesses, showing that the existing binarization methods are not suitable for LLM binarization. 

% \subsection{Codes}
% Codes can be found anomalously in \href{https://anonymous.4open.science/r/BinaryLLM-4C31/README.md}{PB-LLM}.
