\vspace{-0.2in}
\section{Experiments}
\label{sec:exp}
Besides the exploration with OPT-1.3B in Sec.~\ref{sec:method}, we assess the effectiveness of \pbllm~by conducting experiments on LLaMA-7B~\citep{touvron2023llama} and presenting results on various tasks. 

\subsection{Experimental Setup} 
\noindent\textbf{Dataset.} 
In this study, the \pbllm~is trained using the RedPajama-simple-1B dataset, as the dataset for LLaMa training is not openly accessible. 
This dataset, RedPajama-1T, is structured to closely resemble the LLaMa paper and serves as a transparent, open-source alternative to LLM training dataset. 
It amalgamates data from diverse sources including Commoncrawl, C4, GitHub, Wikipedia, Gutenberg Books3, ArXiv, and Stackexchange. RedPajama-simple-1B, representing a 0.1\% subset of RedPajama-1T, is substantially smaller than the typical datasets used for training other LLMs, making it a convenient choice for our experiments.

\noindent\textbf{Training Details.} 
In the training process of our quantized network, we commence with a pre-trained model for initialization. The optimization of the model is facilitated through the AdamW optimizer~\citep{loshchilov2017decoupled}, applied with zero weight decay. We assign a batch size of 1 to each GPU and implement a learning rate of 2e-5, adhering to a cosine learning rate decay strategy. We only fine-tune our \pbllm~for 10K iterations. 


\noindent\textbf{Evaluated Tasks.} To eliminate the variance of evaluated performance, we evaluate the binarized LLMs on seven zero-shot common sense reasoning tasks, \ie BoolQ~\citep{clark2019boolq}, PIQA~\citep{bisk2020piqa}, HellaSwag~\citep{zellers2019hellaswag}, WinoGrande~\citep{sakaguchi2021winogrande}, ARC-Easy, ARC-Challenge~\citep{clark2018think}, OBQA~\citep{mihaylov2018can}. We also along eavulated the quantized moelds' perplexity scores on WikiText2~\citep{merity2016pointer} and C4~\citep{raffel2020exploring}.



\vspace{-0.1in}
\subsection{Results on LLaMA}
\vspace{-0.1in}
\begin{table}[t]
    \centering
    \captionsetup{font=small}
    \caption{Zero-shot performance on Common Sense Reasoning tasks within a 4-bit setting. Reported results of previous works are documented in their papers. \pbllm~30\% denotes the preservation of 30\% salient weights, and \pbllm~10\% implies the preservation of 10\% salient weights.}
    \vspace{-0.15in}
    \scalebox{0.81}{
    \begin{tabular}{l|cccccccc}
    \toprule
        Method & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-E & ARC-C & OBQA & Avg \\ \midrule
        FP LLaMA-7B & 76.8 & 79.3 & 76.1 & 70.0 & 73.0 & 48.0 & 57.6 & 68.7 \\ \hline
        RTN & 71.2 & 77.3 & 72.7 & 66.9 & 68.8 & 46.4 & 52.8 & 65.2  \\ 
        SmoothQuant & 67.7 & 76.0 & 69.4 & 66.7 & 66.9 & 43.0 & 50.6 & 63.0 \\ 
        LLM-QAT          & 75.5 & 78.3 & 74.0 & 69.0 & 70.0 & 45.0 & 55.4 & 66.6 \\ \hdashline
        \texttt{PB-GPTQ} 10\%                      & 62.3  & 55.9 & 27.7      & 49.3       & 29.3  & 20.1  & 10.6 & 36.5 \\
        \texttt{PB-GPTQ} 30\%                      & 73.5  & 74.9 & 47.5      & 64.9       & 61.3  & 32.4  & 25.2 & 54.2 \\

        \pbllm~10\% & 68.9 & 67.8 & 68.1 & 67.4 & 58.7 & 42.9 & 50.6 & 60.6 \\

        \pbllm~30\% & 75.7 & 78.0 & 74.3 & 69.7 & 69.0 & 45.6 & 55.8 & 66.9 \\ \bottomrule
    \end{tabular}}
    \vspace{-0.2in}
    \label{table:qat-llama7b}
\end{table}
\begin{wraptable}{r}{0.46\textwidth}
\centering
\vspace{-0.2in}
\captionsetup{font=small}
\caption{Perplexity of C4, wikitext2 and PTB on LLaMA-7b quantized with PTQ methods.}
\vspace{-0.1in}
\scalebox{0.74}{
\begin{tabular}{@{}l|ccc@{}}
\toprule
                     & C4      & WIKI      & PTB      \\ \midrule
FP                   & 7.3435                 & 5.6770                   & 41.1509                 \\
GPTQ 4b              & 8.6977                 & 8.1368                   & 57.9951                 \\
SparseGPT 50\%       & 15.5949                & 12.829483                & 505.1396                \\
\texttt{PB-GPTQ} 50\% & 8.1466                 & 6.3089                   & 54.8674                 \\
\texttt{PB-GPTQ} 20\% & 20.6057                & 17.1929                  & 280.4353                \\
\texttt{PB-GPTQ} 10\% & 72.1115                & 85.7838                  & 708.4120                \\
\texttt{PB-GPTQ} 5\%  & 401.6475               & 619.1054                 & 1687.1815               \\ \bottomrule
\end{tabular}}
\vspace{-0.1in}
\label{table:ptq-llama7b}
\end{wraptable}
Experiments were conducted on LLaMA-7B. 
The results of employing \texttt{PB-GPTQ} and \pbllm~are illustrated in Tabs.~\ref{table:qat-llama7b} and \ref{table:ptq-llama7b}. 
When employing PTQ, \texttt{PB-GPTQ} exhibited commendable performance, particularly when the salient weight exceeded 30\%. Nevertheless, a noteworthy decline in the performance of the quantized network was observed when the salient weight was reduced to 10\%.
On the other hand, employing QAT resulted in a notable improvement in the performance.
A comparison within a 4-bit quantization setting between \pbllm~30\% and LLM-QAT in Tab.~\ref{table:qat-llama7b} reveals superior performance by our method. 
It is notable that \pbllm~is only fine-tuned for 10K iterations, whereas LLM-QAT underwent 100K iterations of training, showing its fast convergence property (refer to Sec.~\ref{sec:pbweight}). The results under \pbllm~10\% represent the outcomes of \pbllm~where 10\% of salient weights are preserved. This demonstrates the potential for advancing LLM quantization towards a fully 1-bit state. 
% In Tab.~\ref{table:ptq-llama7b}, under the same sparsity, \pbllm~outperforms the SoTA of unstructured pruning for LLMs, SparseGPT~\citep{frantar2023sparsegpt}.