\vspace{-0.1in}
\section{Conclusion}
\vspace{-0.1in}
\label{sec:con}
In conclusion, this work is the first to implement network binarization for LLM quantification, introducing the novel Partially-binarized LLM (\pbllm) methodology. 
This approach is meticulously designed to maintain linguistic reasoning capabilities of LLMs, even under extreme low-bit quantization. The research unearthed the significant role of salient weights in achieving extreme quantization and proposed innovative strategies like optimal scaling for effective binarization. This framework is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). 
The methodology is a significant stride in the realm of network binarization for LLMs. 