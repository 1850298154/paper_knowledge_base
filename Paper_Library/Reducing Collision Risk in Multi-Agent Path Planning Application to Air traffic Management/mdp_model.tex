\section{Markov decision process congestion game}
\textbf{Notation}. We use $\reals(\reals_+)$ to denote the real (non-negative real) numbers, $[N]$ to denote $\{1,\ldots N\}$, $\mc{T}$ to denote $\{0,\ldots,T\}$, and $\Delta_N = \{y \in \reals_+^N \ | \ \sum_{i} y_i = 1\}$ to denote the simplex in $\reals^N$. 
\subsection{Individual Markov decision process (MDP)}
The \emph{finite-horizon MDP} for player $i$ is given by $([S], [A], \mc{T}, P^i, C^i, p^i_0)$, where $[S]$ is the \textbf{finite set of states}, $[A] $ is the \textbf{finite set of actions}, $\mc{T}$ is the \textbf{finite time horizon}, $C^i \in \reals^{(T+1)SA}$ are the \textbf{state-action costs}, $P^i \in \reals_+^{TSSA}$ is the \textbf{transition dynamics}, and $p^i_0 \in \Delta_S$ is player's \textbf{initial state probability distribution}. Assume that each action $a \in [A]$ is admissible from each state $s \in [S]$. 

At time $t \in \mc{T}$ and state $s \in [S]$, player $i$ selects an action $a \in [A]$ and incurs a cost $C_{tsa} \in \reals$. At $t+1$, the player transitions to state $s'$ with probability $P_{ts'sa}\geq 0$. This is repeated for $t\in\mc{T}$. At $t=0$, player $i$'s probability of being in state $s$ is given by $p_{0s}$.

Player $i$'s \textbf{state-action distribution} is $x^i \in \reals_+^{(T+1)SA}$, where $x^i_{tsa}$ is player's joint probability of taking action $a$ at $(t,s) \in \mc{T}\times [S]$. The set of feasible MDP state-action distributions is given by 
\begin{equation}
\textstyle \mc{X}(P^i, p^i_0) = \big\{ z \in \reals_+^{(T+1)SA} \ | \ \sum_{a}z_{0sa} = p^i_{0s}, 
     \sum_{a}z_{(t+1)sa} = \sum_{a, s'}P^i_{tss'a}z_{ts'a}, \forall (t,s) \in \mc{T}\times[S]\big\}.
\end{equation}
% The standard \textbf{MDP policy} $\pi^i(t,s,a)$ can be derived from $x^i$ as $\textstyle  \pi(t,s,a) = $ $\textstyle \frac{x^i_{tsa}}{\sum_{a'}x^i_{tsa'}}$ if $\textstyle \sum_{a'}x^i_{tsa'} > 0$, and $\textstyle \pi(t,s,a) =1/A$ otherwise. 
% \textbf{Policy.} At time $t$ and state $s$, the player selects an action $a$ with probability $\pi(t,s,a)$,  where $\pi(t, s, \cdot) \in \Delta_A$; it outputs the decision maker's probability of taking action $a$, given a time $t$ and state $s$. Then $x_{tsa}$ becomes the player's joint probability at time $t$ of both being in state $s$ and taking action $a$ according to its policy:
% \[x_{tsa}  = p_{ts} \pi(t,s,a).\]
% The optimal policy for each decision maker is the one that minimizes the expected value of the cost-to-go. The policy returns a probability for each action instead of a single determined action because we wish to plan ahead for future actions, which must respond to the stochastic behavior of the dynamics and the other players.
% Each player's state-action distribution $x$ embeds its policy $\pi$, which can be explicitly derived as
% \begin{equation}
%     \pi(t,s,a) = \begin{cases}
%     \frac{x_{tsa}}{\sum_{a'}x_{tsa}} & \sum_{a'}x_{tsa} > 0 \\
%     0& \text{otherwise}
%     \end{cases}, \ \forall t, s,a \in [T]\times[S]\times[A].
% \end{equation}

% Then the probability that a player takes some state-action pair $(s,a)$ is the joint probability of the player both being in state $s$ and selecting action $a$ according to its policy, which yields the relationship \[x_{tsa}  = p_{ts} \pi(t,s,a).\]

Player $i$'s \textbf{Q-value function} $Q^i\in\reals^{(T+1)SA}$ is the expected incurred cost within the MDP~\cite[Chp.4.2.1]{puterman2014markov}. When player $i$ is at state $s$ and time $t$, $Q^i_{tsa}$ is the expected total cost player incurs from state $s$ and time $t$ if it first takes action $a$ and plays optimally thereafter.
\begin{equation}\label{eqn:q_value}
   \textstyle  Q^i_{Tsa} := C^i_{Tsa}, \ 
  \textstyle   Q^i_{(t-1)sa}:= C^i_{(t-1)sa} + \sum_{s'} P^i_{ts'sa}\underset{a'}{\min}\, Q^i_{t,s'a'}, \ 
\forall \  (t, s, a) \in [T]\times[S]\times[A]. %\underset{s'}{\sum}
\end{equation}
\subsection{Multi-player MDPs under collision risk-based congestion}
Inspired by autonomous vehicles sharing an operation space, we consider the scenario in which $N$ players each solve the MDP $([S],[A],\mc{T}, P^i, \ell^i, p^i_{0})$ for $i \in [N]$. Distinct from individual MDPs, the MDP costs $\ell^i: \reals^{N\times (T+1)SA} \mapsto \reals^{(T+1)SA}$ depend on all players' state-action distributions $(x^1, \ldots, x^N)$. We denote this joint state-action distribution as $x = (x^1, \ldots, x^N)$ and the resulting cost as $\ell^i(x)$. The players jointly solve an \textbf{MDP congestion game} under costs $(\ell^1,\ldots,\ell^N)$. 
% and the joint state-action distribution excluding player $i$ as $x^{-i}$, such that $x = (x^i, x^{-i})$. 

\textbf{Probabilistic collision risks}. A player's collision risk at $(t,s)$ and $(t,s,a)$ are the probabilities that at least one other player is in the same state and state-action, respectively. 
\begin{lemma}
 Under $x=(x^1,\ldots, x^N)$, player $i$'s probability of encountering at least one other player in $s$ and $(s,a)$ at time $t$ are respectively denoted by $D^i_{ts}(x)$ and $G^i_{tsa}(x)$ and computed as
 \begin{equation}
   \textstyle   D^i_{ts}(x)  = 1 - \prod_{j\neq i}(1 - \sum_{a'} x^j_{tsa'}), \ G^i_{tsa}(x) = 1  - \prod_{j\neq i}(1 -  x^j_{tsa}) \ \forall \ i, t, s, a \in [N]\times [T]\times [S]\times [A].\label{eqn:DG_prob}
 \end{equation}
\end{lemma}
\begin{proof}
 The probability of player $j$ taking state-action $(s, a)$ at time $t$ is $x^j_{tsa}$. The probability that player $j$ does \textit{not} take state-action $(s, a)$ at time $t$ is $1 - x^j_{tsa}$. The probability that \textit{none} of the players $j \neq i$ take state-action $(s, a)$ at time $t$ is $\textstyle \prod_{j \neq i} (1 - x^j_{tsa})$. The probability of \textit{at least one} other player $j \neq i$ taking state-action $(s, a)$ at time $t$ is given by $G^i_{tsa}(x)$ in~\eqref{eqn:DG_prob}. To derive $\textstyle D^i_{ts}(x)$~\eqref{eqn:DG_prob}, we apply similar arguments to the probability of player $j$ being in state $s$ at time $t$, given by $\textstyle \sum_a{x^j_{tsa}}$.
\end{proof}
% Both $D_{ts}$ and $G_{tsa}$ have non-trivial collision risk interpretations. 
As shown in Section~\ref{sec:flight_model}, $D^i$ and $G^i$ are flight separation constraints in air traffic management.

\textbf{Collision risk-based congestion}. We augment players' individual costs $C^i$ with $D^i(x)$ and $G^i(x)$. \begin{equation}\label{eqn:congestion_cost}
    \ell^i_{tsa}(x) = C^i_{tsa} + k\big(D^i_{ts}(x) + G^i_{tsa}(x)\big), \ \forall (t,s,a) \in \mc{T}\times[S]\times[A],
\end{equation}
where $k \in \reals_+$ is a user-defined parameter that signifies the players' willingness to risk collisions. Players are collision ignorant at $k=0$, and collision-averse at $k\rightarrow\infty$. Unique from~\cite{calderone2017markov,li2022congestion}, $\ell^i$~\eqref{eqn:congestion_cost} is independent of $x^i$; when player $i$'s opponents fix their strategies, player $i$ solves a standard MDP. 

When all players simultaneously achieve the minimum $Q^i(x)$, $x$ is a Nash equilibrium. 
\begin{definition}[Nash equilibrium]\label{def:NE}\cite{li2022congestion}
The state-action distribution $x$ is a Nash equilibrium if every player exclusively takes actions that minimize their $Q$-value function, $Q^i(x)$~\eqref{eqn:q_value}. 
% every player's policy is optimal with respect to their own MDP.
\begin{equation}\label{eqn:nash}
   \textstyle  x^i_{tsa}> 0 \Rightarrow a \in \argmin\{Q^i_{tsa'}(x)\ | \ a' \in [A]\}, \ \forall (i, t, s, a) \in [N] \times [T]\times [S]\times [A]. 
\end{equation}
\end{definition}
We consider solving for the Nash equilibrium using the potential game formulation given by
% Under congestion costs of the form~\eqref{eqn:congestion_cost}, the Nash equilibrium condition~\eqref{eqn:nash} is equivalent to the first order KKT conditions of the optimization problem given by 
\begin{equation}\label{eqn:potential_game}
\begin{aligned}
  \textstyle   \min_{x^1,\ldots, x^N} & F(x^1,\ldots, x^N) & \text{s.t.} \  x^i \in \mc{X}(P^i, p^i), \ \forall \ i \in [N],
\end{aligned}
\end{equation}
where the objective  $F:\reals_+^{N\times(T+1)\times S\times A}\mapsto \reals$ is defined as 
\begin{multline}~\label{eqn:game_potential}
   \textstyle  F(x^1, \ldots, x^N)  =\sum_{i,t,s,a} x^i_{tsa}C^i_{tsa} +  \sum_{t,s} k\Big(\sum_{i,a} 2x^i_{tsa} + \prod_{i \in [N]} (1 - \sum_{a}x^i_{tsa}) + \sum_{a} \prod_{i \in [N]}(1 -x^i_{tsa})\Big). 
\end{multline}
\begin{lemma}\label{lem:potential_gradient}
The joint state-action distribution $x$ satisfies~\eqref{eqn:potential_game}'s first order KKT conditions if and only if it corresponds to a Nash equilibria of the MDP congestion game with costs $(\ell^1,\ldots, \ell^N)$~\eqref{eqn:congestion_cost}. 
\end{lemma}
\begin{proof}
From~\cite[Thm.1.3]{calderone2017markov}, the first order KKT conditions of~\eqref{eqn:potential_game} are equivalent to the Nash equilibrium condition if $F$'s gradients satisfy $\textstyle \partial F/\partial x^i_{tsa} = \ell^i_{tsa}(x)$ for all $(i,t,s,a)\in[N]\times[T]\times[S]\times[A]$. We compute $\textstyle \partial F/\partial x^i_{tsa}$ via~\eqref{eqn:game_potential}. With respect to $x^i_{tsa}$, the gradient of $\sum_{i,t,s,a} x^i_{tsa}C^i_{tsa}$ is $C^i_{tsa}$, the gradient of $\textstyle  k\sum_{i,t,s,a} 2x^i_{tsa}$ is $2k$, the gradient of $\textstyle \sum_{t,s} k \prod_{i \in [N]} (1 - \sum_{a}x^i_{tsa})$ is $\textstyle -k\prod_{j\neq i} (1 - \sum_{a}x^j_{tsa})$, and the gradient of $\textstyle  \sum_{t,s,a} k \prod_{i \in [N]} (1 - x^i_{tsa})$ is $\textstyle -k\prod_{j\neq i}(1 - x^j_{tsa})$. Their sum recovers $\ell^i_{tsa}$~\eqref{eqn:congestion_cost} for all $\textstyle (i,t,s,a) \in [N]\times\mc{T}\times[S]\times[A]$. 
\end{proof}
\textbf{Non-convexity}. Distinct from~\cite{li2022congestion,li2019tolling}, congestion costs~\eqref{eqn:congestion_cost} results in a non-convex and multilinear optimization objective~\eqref{eqn:game_potential}. However, the proposed Frank-Wolfe solution ~\cite{li2022congestion, li2019tolling} for finding the Nash equilibria will still converge sublinearly~\cite{lacoste2016convergence}. We refer to~\cite[Alg.1]{li2022congestion} for an algorithm outline.
% \subsection{Player Heterogeneity, Equilibrium Stability, and Equilibrium Uniqueness}
% \tc{blue}{May take out.}Consider a single state, single time step, and two action MDP congestion game with two players shown in Figure~\ref{fig:stability_equilibrium}. On the left, both players have identical transition dynamics and identical MDP congestion costs. When player one selects state-action distribution $x^1 = [0, 1]$ and player two selects $x^2 = [1, 0]$, and assuming that $k^1 = k^2 = 1$, their congestion costs become $\ell^1(x^1, x^2)  = [1, -1]$, $\ell^2(x^1, x_2) = [0, 0]$. It is simple to verify against Definition~\ref{def:NE} that this is a Nash equilibrium. Because both players have identical roles within the game, an alternative Nash equilibrium is $x^1 = [1, 0]$ and $x^2 = [0, 1]$. 
% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=0.2\columnwidth, height=20mm]{figures/stability_equilibrium} 
%     \ 
%     \includegraphics[width=0.2\columnwidth, height=20mm]{figures/stability_equilibrium_b}
%     \caption{Illustration of two different MDP congestion games with two players. On the left, both players have homogeneous costs and transition dynamics. On the right, players have different costs.}
%     \label{fig:stability_equilibrium}
% \end{figure}
% Instead, suppose that the players have different congestion sensitivities: $k_1 = 2$ and $k_2 = 10$, in this case, under the joint state-action distribution, $x^1 = [0, 1], x^2 = [1, 0]$, each player achieves costs $\ell^1 = [1, -1]$ and $\ell^2 = [0, 9]$. Here while multiple equilibria the equilibria at $(x^1, x^2) = ([0, 1], [1,0])$ and $(x^1, x^2) = ([1,0], [0,1])$ are now both stable. 

% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{figures/mdp_description.png}
%     \caption{Example MDP figure}
%     \label{fig:mdp_desc}
% \end{figure}