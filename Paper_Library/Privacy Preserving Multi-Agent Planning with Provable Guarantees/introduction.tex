\section{Introduction}
\label{Introduction}


As our world becomes better connected and more open ended, and autonomous agents are no longer  science fiction, a need arises for enabling groups of  agents to cooperate in generating a plan for diverse tasks that none of them can perform alone, in a cost-effective manner. Indeed, much like ad-hoc networks, one would expect various contexts to naturally lead to the emergence of ad-hoc teams of agents that can benefit from cooperation. Such teams could range from groups of manufacturers teaming up to build a product that none
can build on their own, to groups of robots sent by different agencies or countries to help in disaster settings. To perform complex tasks, these agents need to combine their diverse skills effectively. 
Planning algorithms can  help  achieve this goal.


Most planning algorithms require full information about the set of actions and state variables in the domain. However, often, various aspects of this information are private to an agent, and it is not eager to share them. For example, the manufacturer is eager to let everyone know that it can supply motherboards, but it will not want to disclose the local process used to construct them, its suppliers, its inventory level, and the identity of its employees. 
Similarly, rescue forces of country A may be eager to help citizens of country B suffering from a tsunami, but without having
to provide detailed information about the technology behind their autonomous bobcat to country B, or to country C's humanoid evacuation robots. In both cases, agents have public capabilities  they are happy to share, and private processes and information that support these capabilities, which they prefer (or possibly require) to be kept private.

With this motivation in mind, a number of algorithms have recently been devised for distributed privacy-preserving planning~\cite{Bonisoli14,FMAP14,LB14,NBJAIR}. In these algorithms,  agents supply a public interface only, and through a distributed planning process, come up with a plan that achieves the desired goal without being required to share a complete model of their actions and local state with other agents. But there is a major caveat: it is well known from the literature on secure multi-party computation~\cite{Yao82b} that the fact that a distributed algorithm does not require an agent to {\em explicitly\/} reveal private information does not imply that other agents cannot deduce such private information from other information communicated during the run of the algorithm. Consequently, given that privacy is the raison-d'etre for these algorithms, it is important to strive to improve the level of privacy provided, and to provide formal guarantees of such privacy properties.

To the best of our knowledge, to date, there have been two attempts to address this issue. In~\cite{TozickaSK17}, the authors describe
a secure planner for multi-agent systems. However, as they themselves admit, this planner is impractical, as it requires computing all possible solutions. \cite{Brafman15} describes \smafs\, a modification of the {\sc multi-agent forward search} algorithm~\cite{nissim2014distributed} in which an agent never sends similar states. \smafs\ is an efficient algorithm. In fact, an implementation of it based on an equivalent macro sending technique~\cite{MaliahSB16} shows state of the art performance.  But it is not clear what security guarantees
it offers. While~\cite{Brafman15} provides some privacy guarantees, they are restricted to very special cases, and it seems
most plausible that \smafs\ is not secure in general.

The goal of this paper is to place the development of \smafs\ on firm footing by developing appropriate notions of privacy 
that are useful and realizable in the context of search algorithms, to characterize the privacy preserving properties
of \smafs\ and to provide rigorous proofs for its correctness and completeness. 
We define a notion of $\beta$-indistinguishable secure computation, and more specifically, we suggest a notion of PST-secure computation which is not as strong as that of strong privacy,
but is meaningful and more stringent than weak privacy. Roughly speaking, given a function $\beta$ on planning instances,
we say that an algorithm is $\beta$-indistinguishable if it will send the same messages during computation for any two
instances whose $\beta$ value is identical. PST-secure computation refers to the special case in which $\beta$ returns
a projected version of the search space -- one in which only the value of public variables is available.


The paper is structured as follows: First, we describe the basic model of privacy-preserving classical multi-agent planning.
Then, we discuss some basic notions of privacy. Next, we gradually develop more practical versions of PST-secure planning
algorithms, eventually describing an algorithm that is, essentially \smafs, and prove that the latter is sound and complete, and
is PST-secure.
