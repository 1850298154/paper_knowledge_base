\section{Special Cases and Discussion on \textbf{\whred{GPT-4V}}}
In this section, we primarily present some special phenomena observed during the evaluation, provided for the reference of future researchers.

\begin{table}[!t]
    \centering
  \scalebox{0.82}{
  \setlength{\tabcolsep}{1.0pt}
    \begin{tabular}{lccccccccccc}
    \toprule
     & DTD  & SUN & RAF & Caltech & ImgNet & Aircraft & Flower & Cars & Food & Pets \\
    \midrule
    Batch     &  59.1  & 57.0 & 58.5 & 95.5  & 58.5 & 36.0 & 70.6 & 58.3 & 79.0  & 92.6 \\
    \rpink
    Single    &  57.7  & 59.2 & 68.7 & 93.7  & 63.1  & 56.6 & 69.1 & 62.7  & 86.2 & 90.8  \\
    \bottomrule
    \end{tabular}
    }
    \caption{Impact of Single \vs Batch Testing on GPT-4V performance evaluation, reporting top-1 accuracy (\%).}
    \label{tab:single}
\end{table}

\vspace{1mm}
\noindent\textbf{Batch Testing \vs Single Testing.} Our initial results were released before December 2023, during a period when we were constrained by OpenAI's daily API request limit of 100 per account. This led us to implement \emph{Batch Testing}, where one request yielded results for multiple samples.
Moving closer to March 2024, the limits were removed for tier 5 accounts, enabling us to switch back to standard \emph{Single Testing}, updating all datasets with one result per request. \Cref{tab:single} shows a comparison of these two methods. The results from batch testing do not align perfectly with those from standard single testing. This misalignment could stem from several factors: 1) There's a notable probability of result misalignment, \ie, the results intended for sample A may actually correspond to sample B. Furthermore, we've observed instances of both repeated predictions for the same sample and missing predictions for others.
2) Given that GPT-4V can process all samples in a batch simultaneously, the content of these samples might interfere with the results of individual samples.
Therefore, we recommend that future work should invariably employ single testing (\ie, the API processes only one sample at a time) to ensure the accuracy of the tests.
% \vspace{1mm}

\vspace{1mm}
\noindent\textbf{Predict categories beyond the given list.}
In some instances, GPT-4V predicted categories that were not included in the given category list. For example, during the evaluation of the texture recognition dataset DTD~\cite{dtd}, GPT-4V might respond with: ``\emph{Note: As there were no categories provided that perfectly matched some images (such as straw-like), I have used the most comparable or related terms from the list provided. Additionally, not all images may have an exact or obvious matching category within the provided list, and I've estimated the most relevant categories based on observable texture patterns in the images.}'' In such cases, we tried to restrict the predictions to the provided category list through the prompts, but this proved ineffective. To proceed with the evaluation, we chose to exclude these predictions that were not within the given list.

\vspace{1mm}
\noindent\textbf{Prediction based on image names.}
GPT-4V sometimes makes category inferences from image names. For streamlined data gathering, we set GPT-4V to output in a dictionary format, with sample ID from filename as the key, and predictions as the value. 
Notably, GPT-4V may rely on explicit category hints in filenames, like `banded\_0060.jpg' from the DTD~\cite{dtd} dataset, impacting its predictions more than the visual content itself. 
or instance, the DTD dataset's top-1 accuracy soared to 98\% with original filenames but normalized to 57\% when filenames were anonymized.
To address this, we hashed sample names to ensure GPT-4V focuses on visuals over filename cues.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{sec/figures/safety.pdf}
    % \vspace{-5mm}
    \caption{Some examples rejected by GPT-4V. (a) is an image from ImageNet-1K, (b) and (c) are videos from Kinetics-400.}
    \label{fig:safety}
\end{figure}

% \vspace{1mm}
\vspace{1mm}
\noindent\textbf{Safety system in GPT-4V.}
Throughout our dataset evaluations, we stumbled upon specific instances, as depicted in Figure~\ref{fig:safety}, where GPT-4V refused to generate predictions, 
stating: ``\emph{Your input image may contain content that is not allowed by our safety system.}''
We surmise that this precautionary mechanism is designed to ensure that GPT-4V adheres to ethical guidelines by avoiding engagement with potentially sensitive or inappropriate content.

\vspace{1mm}
\noindent\textbf{GPT-4V API Costs.} We offer an estimate that using the GPT-4V API for one testing round across all datasets costs about \$4000 for reader's reference.



\section{Conclusion and Limitation}
This work aims to quantitatively evaluating the linguistic and visual capabilities of the current state-of-the-art large multimodal model GPT-4 in zero-shot visual recognition tasks. To ensure a comprehensive evaluation, we have conducted experiments across three modalities—images, videos, and point clouds—spanning a total of 16 benchmarks. We hope our empirical study and experience will benefit the community, fostering the evolution of future multimodal models.

\noindent\textbf{Limitations:} 1) This study has focused solely on fundamental visual recognition tasks. A comprehensive quantitative analysis of other tasks, such as object detection, is necessary to truly gauge the breadth of these models' capabilities in analyzing complex visual information. 
2) This work is limited to the evaluation of GPT-4 alone. Future efforts could include quantitative comparisons of various multimodal models (\eg, LLaVA~\cite{liu2023llava}, MiniGPT-4~\cite{zhu2023minigpt}, \etc), enhancing the breadth and depth of our analysis.
3) The current method of prompting GPT-4V is quite straightforward, raising concerns that an overly lengthy category list will increase the number of input tokens, potentially leading to adverse effects. Designing more effective prompts may further unlock GPT-4V's capabilities.


% In summary, we believe that the evidence, challenges, and open questions in this study are worth knowing, if selfsupervised Transformers will close the gap in pre-training between vision and language. We hope our data points and experience will be useful to push this frontier.
