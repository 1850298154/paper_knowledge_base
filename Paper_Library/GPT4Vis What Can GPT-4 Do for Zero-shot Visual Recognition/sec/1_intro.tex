\section{Introduction}
\label{sec:intro}
\blfootnote{*~Research interns at Baidu \quad  \Envelope~Corresponding author} 
% 背景
ChatGPT~\cite{chatgpt}, lanuched in November 2022, marked a seismic shift in the application of AI, sparking a ``wow'' moment that galvanized the tech industry. This innovative product catalyzed a flurry of investment in Generative AI.
% , particularly in large language models (LLMs), and attracted a global user base due to its exceptional quality. 
% It set a new precedent for human-AI interaction, inspiring a wave of innovation in how we integrate intelligent systems into our daily digital experiences.
The innovation journey continued in March 2023 with the introduction of GPT-4\footnote{GPT-4 can accept a prompt of text and images. For clarity, in this paper, we refer to the version of the model with visual capabilities as ``GPT-4V'', following the OpenAI official report~\cite{gpt4v}.}, a large multimodal model, capable of processing both text and images, further captivated the industry by demonstrating the extensive capabilities of multimodal technologies.
By the end of September 2023, GPT-4 with Vision (GPT-4V) was fully integrated into the ChatGPT platform. Following this milestone, comprehensive user study reports~\cite{yang2023dawn,lin2023mm,shi2023exploring,yang2023performance,zhou2023exploring,wen2023road,li2023comprehensive} by computer vision researchers began to emerge, providing evaluations of its visual prowess.
More recently, on the first anniversary of ChatGPT, November 6, 2023, OpenAI hosted its first DevDay, during which the GPT-4V API was released. This release opens new doors for the academic community to conduct extensive evaluations of its performance across a range of visual benchmarks, offering quantitative metrics beyond the limited scope of user studies.


\begin{figure*}[t]
  \centering
   \includegraphics[width=1\linewidth]{sec/figures/datasets.pdf}
   \caption{An overview of 16 evaluated popular benchmark datasets, comprising images, videos, and point clouds. The image benchmarks include tasks such as texture recognition, satellite image classification, scene recognition, facial expression recognition, as well as fine-grained object classification. The video datasets encompass diverse human actions captured from various viewpoints and scenes. The point cloud datasets provide valuable information that can be projected onto multi-view depth maps for visual recognition.}
   \label{fig:datasets}
\end{figure*}


% 在这篇文章中，我们从语言能力和视觉能力两方面评测GPT-4在视觉识别任务（计算机视觉领域最基础的任务之一）的表现，没有任何训练（i.e., zero-shot）。（i）首先关于语言能力，我们探索利用GPT-4的语言能力来帮助视觉识别任务。大规模image-text预训练的CLIP为我们构建了视觉-文本的桥梁，他能计算不同category name embedding和image embedding的相似度来进行零样本视觉识别。基于此，我们考虑利用GPT-4的丰富知识对category name生成更丰富更细致的描述，以增加类内的多样性和类间的可区分性，来替代简单的category name来做zero-shot recognition。(ii)关于视觉能力的评估则很直观了，即我们直接输入图片和候选类别让GPT-4V进行相关性排序，即可得到Top-1/Top-5的预测结果。
In this paper, we evaluate GPT-4's performance in visual recognition tasks—one of the fundamental tasks in the field of computer vision—without any fine-tuning (\ie, in a zero-shot manner). We explore two main facets: \textbf{\whblue{linguistic}} and \textbf{\whred{visual}} capabilities. 
(i) Regarding linguistic capabilities, we investigate how GPT-4's language proficiency can bolster visual recognition. It's widely recognized that the large-scale image-text pre-trained model CLIP~\cite{clip} has built a bridge between vision and text, enabling zero-shot visual recognition by calculating similarity scores between category name embeddings and image embeddings. Building on CLIP's foundation, we aim to utilize GPT-4's extensive linguistic knowledge to craft more nuanced and detailed descriptions of category names, thus enhancing intra-class diversity and inter-class distinguishability, offering a refined alternative to the use of basic category names for zero-shot recognition.
(ii) As for visual capabilities, the evaluation is quite straightforward: we directly feed the image or images (applicable to video and point cloud data) along with candidate categories. By employing prompts, we instruct GPT-4V to organize the candidate categories by relevance to the visual content, thereby obtaining Top-5 prediction.









% 测评
% 为了进行全面的测评，我们考虑了图像、视频、点云三种模态，总计17个公开的流行的分类benchmark。具体来说，图像数据集包括纹理识别、卫星图像识别、场景识别、表情识别、目标识别、细粒度识别甚至大规模图像分类（e.g., ImageNet）。视频数据集包含不同视角不同场景下的行为识别甚至大规模行为识别（e.g., Kinetics-400, something-something），我们对视频在时序上均匀采样多帧作为多图输入。点云数据集包括主流的ModelNet10和ModelNet40, 我们将点云数据映射为6个视角的深度图作为多图输入。实验结果展示了GPT-4的语言能力在大部分场景下都能给CLIP带来进一步的性能提升，GPT-4V体现了不错的zero-shot识别水准，在实验章节中我们提供了具体的实验结果、prompt细节、实验分析和讨论。



To conduct a comprehensive evaluation, we included three distinct modalities: images, videos, and point clouds, across 16 well-known and publicly available classification benchmarks~\cite{dtd,helber2019eurosat,xiao2010sun,li2017reliable,caltech101,deng2009imagenet,aircraft,flower,Stanfordcar,bossard2014food,parkhi2012cats,ucf101,hmdb,i3d,sth-sth,modelnet40}, as showcased in Figure~\ref{fig:datasets}. 
For video datasets, we implemented a uniform sampling of frames to create multi-image inputs.
For point cloud data, we process the 3D shape into multiple 2D rendered images.
% As showcased in Figure~\ref{fig:datasets}, the image datasets span a diverse array of recognition challenges, including texture~\cite{dtd}, satellite imagery~\cite{helber2019eurosat}, scene categorization~\cite{xiao2010sun}, facial expression analysis~\cite{li2017reliable}, object recognition~\cite{caltech101}, fine-grained image classification involving aircraft~\cite{aircraft}, flowers~\cite{flower}, cars~\cite{Stanfordcar}, food~\cite{bossard2014food}, and pets~\cite{parkhi2012cats}, along with large-scale image classification (\eg, ImageNet~\cite{deng2009imagenet}).
% Video datasets capture action recognition across varied viewpoints and scenarios, including UCF-101, HMDB-51 and large scale datasets (\eg, Kinetics-400~\cite{i3d}, something-something~\cite{sth-sth}). For these, we implemented a uniform sampling of frames to create multi-image inputs.
% For point cloud data, we utilize the well-known dataset ModelNet10~\cite{modelnet40}, where we process the 3D shape into multiple 2D rendered images by rendering them from various viewpoints.
% With image input at hand, we can evaluate GPT-4's proficiency in linguistic and visual aspects within the context of zero-shot visual recognition tasks.
For each dataset, we offer the zero-shot performance of CLIP, a representative model among web-scale pre-trained vision-language models (VLMs), as a reference. The results includes four backbones: OpenAI CLIP (pre-trained on 400M image-text pairs) with ViT-B/32, ViT-B/16, and ViT-L/14, as well as the more recent and larger EVA CLIP's ViT-E/14, which boasts a staggering 4.4B parameters (14$\times$ that of ViT-L), and has been pre-trained on 2B image-text pairs.
Our research highlights that GPT-4's linguistic capabilities play a crucial role in boosting zero-shot visual recognition, delivering a notable average increase of 7\% in top-1 absolute accuracy across 16 datasets. This leap in performance is driven by GPT-4's rich knowledge, enabling it to craft detailed descriptions for diverse categories. Simultaneously, GPT-4's prowess in visual recognition, evaluated across 16 datasets, is almost on par with EVA-CLIP's ViT-E. This is particularly evident in video datasets such as UCF-101 and HMDB-51, where GPT-4 distinctly surpasses the performance of ViT-E, highlighting its effectiveness in handling contextual visual content reasoning.
% Our results reveal that GPT-4's linguistic capabilities notably bolster CLIP's performance in most cases, and GPT-4V exhibits a robust capability for zero-shot recognition.
For more detailed results, analyses, and experimental details, please refer to the experimental section.

To the best of our knowledge, this study is the first quantitative evaluation of zero-shot visual recognition capabilities using GPT-4V across three modalities—images, videos, and point clouds—over 16 popular visual benchmarks. We believe that the empirical evidence and prompts provided herein are worth knowing. We hope our data points and experience will contribute meaningfully to future research.


\begin{figure*}[t]
  \centering
   \includegraphics[width=0.98\linewidth]{sec/figures/method.pdf}
   \caption{Zero-shot visual recognition leveraging GPT-4's linguistic and visual capabilities. (a) We built upon the visual-language bridge established by CLIP~\cite{clip} and employed the rich linguistic knowledge of GPT-4 to generate additional descriptions for categories, exploring the benefits to visual recognition. (b) We present visual content (\ie, single or multiple images) along with a category list, and prompt GPT-4V to generate the top-5 prediction results.}
   \label{fig:method}
\end{figure*}





















