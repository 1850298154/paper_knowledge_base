\section{Methodology}
As demonstrated in Figure~\ref{fig:method}, we evaluate GPT-4's linguistic and visual capabilities in zero-shot visual recognition. This section will introduce the specific details.


\subsection{Data Processing}

\begin{figure}[h]
  \centering
   \includegraphics[width=0.98\linewidth]{sec/figures/data_process.pdf}
   \caption{Processing video and point cloud data into images.}
   \label{fig:data_process}
\end{figure}

To align with the input requirements of CLIP~\cite{clip} and GPT-4V~\cite{gpt4v}, apart from image classification tasks, the inputs for video and point cloud classification must be transformed into images. 
As illustrated in Figure~\ref{fig:data_process}, this process involves the transformation of video and point cloud data into image sets. Specifically, for input videos, we uniformly sample eight frames to serve as multi-image inputs. Regarding point clouds, we follow MVCNN~\cite{su2015multi} to render multiple views around the object in a uni-directional manner at an angle of 30 degrees. To reduce testing costs, we use six frontally rendered images in our process.
% we project the data onto six different planes—front, back, left, right, top, and bottom—to generate a set of six images. 
This prepares us to carry out subsequent evaluations. 






\subsection{Exploration of Linguistic Capabilities}
Our objective is to explore how the extensive linguistic knowledge of GPT-4 can be leveraged to enhance visual recognition performance. Building on the cross-modal bridge established by CLIP through large-scale image-text pre-training, we aim to enrich textual descriptions beyond using simple category names to better align with visual content. As shown in Figure~\ref{fig:method}(a), we begin by guiding GPT-4 to generate $K$ sentences describing each category in the dataset using appropriate prompts. These $K$ sentences are then converted into $K$ text embeddings via CLIP's frozen text encoder, while the visual signal is encoded into a vision embedding by CLIP's frozen image encoder (\eg, for video and point cloud data, the vision embedding is obtained by global averaging pooling over multiple frame embeddings or viewpoint embeddings). Subsequently, these text embeddings are compared with the vision embedding to calculate $K$ similarity scores. After normalization with a \emph{Softmax} function and averaging, we obtain a consolidated similarity score for each category in relation to the visual input.  Given a dataset with $C$ categories, each visual input yields $C$ similarity scores, which are then ranked from highest to lowest to determine the final prediction.



\subsection{Evaluation of Visual Capabilities}
The recent release of the GPT-4V API allows for a comprehensive evaluation of visual benchmarks, moving beyond limited case studies within the ChatGPT web interface. The evaluation process, outlined in Figure~\ref{fig:method}(b), is simple and straightforward. Visual samples, whether a single image or a collection, are inputted along with an appropriate text prompt. This prompt guides GPT-4V to assess the dataset's categories, sorting them based on their relevance to the provided visual content, and produces the top-5 prediction results. These predictions are subsequently compared against the ground truth of the dataset to derive both top-1 and top-5 accuracy metrics, providing a quantitative assessment of GPT-4V's visual understanding capabilities. 
Further details about our prompts is available in \href{https://github.com/whwu95/GPT4Vis}{Code Repo}.

