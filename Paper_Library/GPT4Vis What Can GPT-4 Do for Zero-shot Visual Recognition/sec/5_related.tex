\section{Related Works}

\noindent\textbf{Preliminary Explorations of GPT-4V.} Recent studies have undertaken detailed case studies on GPT-4V's capabilities across diverse tasks. 
Prior research~\cite{yang2023dawn} delved into the reasoning skills of foundational models within visual domains from a qualitative perspective. 
Subsequently, GPT-4V's performance has been examined in various
visual-language tasks, including but not limited to video understanding~\cite{lin2023mm}, optical character recognition (OCR)~\cite{shi2023exploring}, image context reasoning~\cite{liu2023hallusionbench}, recommender system~\cite{zhou2023exploring}, mathematical logic~\cite{lu2023mathvista}, medical imaging analysis~\cite{yang2023performance,li2023comprehensive}, anomaly detection~\cite{cao2023towards}, social media analysis~\cite{lyu2023gpt} and autonomous driving~\cite{wen2023road}.
% Nonetheless, the majority of these investigations have focused on qualitative, preliminary case studies, with a notable lack of quantitative analysis using widely-recognized visual benchmark datasets.
However, a gap remains in these studies: most have concentrated on qualitative, initial explorations without extensive quantitative analysis utilizing established visual benchmarks. Such analysis is essential for a comprehensive validation of GPT-4V's visual understanding capabilities. 
The recent availability of its API \footnote{gpt-4-vision-preview: https://platform.openai.com/docs/guides/vision} now enables large-scale quantitative evaluations.

\vspace{1mm}
\noindent\textbf{Enhancing Zero-shot Visual Recognition with LLMs.} The web-scale image-text pre-training model, CLIP~\cite{clip}, has established a pivotal connection between visual and textual domains. Numerous subsequent studies have extended this model to video understanding~\cite{text4vis,wu2023transferring,bike,wu2022cap4video,fang2023uatvr,wang2021actionclip,luo2022clip4clip} or point cloud recognition~\cite{zhang2021pointclip,Zhu2022PointCLIPV2}.
With the ascent of Large Language Models (LLMs), there is increasing interest in harnessing class-specific knowledge from LLMs can improve CLIPâ€™s prediction accuracy.
\cite{menon2022visual} leveraged GPT-3~\cite{gpt3} to create text descriptions for unseen class labels and compare the image embedding with the embeddings of the descriptions. \cite{ren2023chatgpt} further developed this concept by employing ChatGPT to structure the classes hierarchically to boost zero-shot image recognition.
In our study, rather than constructing a hierarchical structure, we prompt GPT-4 to produce multi-sentence descriptions for categories, examining the effectiveness of this straightforward approach in image, video, and point cloud recognition.


 