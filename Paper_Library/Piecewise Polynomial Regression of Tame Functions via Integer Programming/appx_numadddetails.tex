In this section, we provide additional figures and details on the numerical experiments presented in \Cref{sec:numer-exper}.

\Cref{fig:NN_architecture} displays the architecture of the Neural Network used in \cref{sec:regr-tame-funct}.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.6\columnwidth]{figures/NN_net.pdf}
  \caption{%
    The architecture of the example Neural Network. Input neurons are green, blue nodes are neurons in hidden layers, and the output neuron is red.
    Grey nodes represent the max-pooling layer.
    Above the connections are the names of activation functions used on the outputs of the layer to the left of each respective name.
    % Output has no activation because we perform regression.
    % The results using this architecture are in \Cref{fig:NN_fit} and \Cref{table:expsregression}.
  }
  \label{fig:NN_architecture}
\end{figure}

\Cref{fig:mip_process} shows the evolution of the optimization routine, as described in \cref{sec:note-optim-process}.

\begin{figure}[t]
    \centering
      \includegraphics[width=0.6\columnwidth]{figures/numres_plots/mip_process.png}
    \caption{Objective value (mean absolute error) of the incumbent solution during the optimization. The full optimization takes 50 minutes, although a solution with almost optimal error is found within seconds.}
    \label{fig:mip_process}
\end{figure}


\begin{table}[t]
  \caption{%
    \label{table:expsregression}
    Normalized error between tame functions and affine-hyperplane tree approximations \eqref{eq:OrtFormulationHplane}.
    % of a given depth.
    % (the cone function \eqref{eq:cone} plus the NN from \Cref{fig:NN_architecture})
  }
  \begin{center}
    \resizebox{0.48\textwidth}{!}{
      \begin{tabular}{lcccc}
        \toprule
        Function & depth & max. err. & mean err. & median err. \\ \midrule
        % $f$ (cone) - depth 2
        \multirow{2}{*}{$\fCone$ \eqref{eq:cone2dintro}} & 2 & \scinum{0.49887991165379475} & \scinum{0.04685379257004139} & \scinum{0.00010017119702482091} \\
                 & 3 & \scinum{0.3742223619441276} & \scinum{0.010007973010744676} & \scinum{6.371677832554213e-05} \\ % lower max (0.34) and slightly higher mean and median when it runs for 20 minutes
        \midrule
        \multirow{2}{*}{NN} & 2% \Ranexp{} net approx.
                         & \scinum{0.32716195149752386} & \scinum{0.036286603222585916} & \scinum{0.024595572191527453} \\
                 & 3% \Ranexp{} - depth 3
                         & \scinum{0.1886663085338612} & \scinum{0.012912554084793467} & \scinum{0.009264882675604167} \\
        \bottomrule
      \end{tabular}
    }
    \vskip -0.15in
  \end{center}
\end{table}

\begin{table}[p]
  \centering
  \caption{
    Normalized absolute error for the denoising scenarios in \Cref{fig:data_noisy}. The error is computed as the absolute difference between the ground truth signal and its approximation by the \emph{axis-aligned} trees, divided by the maximal value of the ground truth.
    % Axis-aligned tree \eqref{eq:OrtFormulation}.
    \label{table:expsdenoising}
  }
  \resizebox{0.48\textwidth}{!}{
  \begin{tabular}{lcccccc}
    \toprule
    % \multirow{2}{*}{Test function} & \multicolumn{3}{c}{Axis-aligned tree \eqref{eq:OrtFormulation}} \\
       Denoising & max. err. & mean err. & median err. \\ \midrule
    Scenario 1 & \scinum{1.0975286704379505} & \scinum{0.14299021402955037} & \scinum{0.09257820874197198} \\
    Scenario 2 & \scinum{1.4545038074634236} & \scinum{0.18311634112276917} & \scinum{0.09003334246760586} \\
    Scenario 3 & \scinum{0.9546384046329512} & \scinum{0.11935686801291823} & \scinum{0.054727166940183425} \\
    Scenario 4 & \scinum{0.8339429435772073} & \scinum{0.04129052618009546} & \scinum{0.01588706426515545} \\
    \bottomrule
  \end{tabular}
  }
\end{table}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main_neurips"
%%% End:
