In this section, we introduce a variant of the affine hyperplane regression tree, presented in \cref{sec:mip-form}, that accommodates using hyperplanes aligned with cartesian for partitioning the space.
\Cref{table:hyperparam,table:variablesaffhyp} summarize the hyperparameters and variables of the mixed-integer formulation.

\textbf{Axis-aligned partition of the space.} At each branching node $m\in\bNode$, a hyperplane splits the space in two subspaces
\begin{equation}\label{eq:splitstrictappx}
  a_m^{\top} x_i - b_{m}  < 0 \qquad a_m^{\top} x_i - b_{m}  \ge 0,
\end{equation}
that will be associated to the left and right children of node $m$.
The parameters $a_{m}\in \{0,1\}^{\ndim}$ and $b_{m} \in [0,1]$ are variables of the mixed integer program.
In the axis-aligned formulation, the hyperplane is aligned with an axis of the cartesian space.
This is enforced by requiring for all branching node $m\in\bNode$ that $a_{m}$ take boolean values, only one of which is one:
\begin{equation}
  \textstyle\sum_{j=1}^d{a_{jm}} = 1, \qquad 0 \le b_m \le 1.
\end{equation}
Since $x_{i}\in[0, 1]$, there holds $a_{m}^{\top}x_{i}\in[0, 1]$, so that $b_{m}$ is constrained to $[0, 1]$ without loss of generality.

In order to model the strict inequality in \eqref{eq:splitstrictappx}, we follow \citet{bertsimasOptimalClassificationTrees2017} and introduce the vector $\epsilon\in\bbR^{\ndim}$ of smallest increments between two distinct consecutive values in points $(x_{i})_{i=[\nSample]}$ in any dimension:
\begin{equation}
  % \resizebox{0.48\textwidth}{!}{
  \epsilon_j = \min \left\{x_j^{(i+1)} - x_j^{(i)}, \text{ for } i \in [\ndim-1] \;:\; x_j^{(i+1)} \ne x_j^{(i)}\right\}
% }
\end{equation}
where $x_j^{(i)}$ is the $i$-th largest value in the $j$-th dimension.
$\epsilon_{\max}$ is the highest value of $\epsilon_j$ and serves as a tight big-M bound, leading to the formulation
\begin{align}
a_m^{\top} x_i &\ge b_m - (1 - z_{it}) & \forall m \in A_R(t) \\ % & \forall i \in [\nSample], \; \forall t \in \mathcal{T}_L,\; \forall m \in A_R(t) \\
{a}_m^{\top} ({x}_i + {\epsilon}) &\le b_m + (1 + \epsilon_{\max})(1 - z_{it}) & \forall m \in A_L(t) % \forall i \in [\nSample], \; \forall t \in \mathcal{T}_L, \; \forall m \in A_L(t)
\end{align}
both for all $i$ in $[\nSample]$ and all $t$ in $\mathcal{T}_L$. Recall that $z_{it}$ takes binary values and is equal to one if sample $x_i$ belongs to leaf node $t$. 

Combining these elements yields the axis-aligned formulation:
\begin{subequations}
  % \tiny
  \label{eq:OrtFormulation}
  \begin{align}
    \min \; &
              \frac{1}{\nSample} \sum^{\nSample}_{i=1} \delta_i & \\
    \textrm{s.t.} \quad & \delta_i \ge \phi_{it} - (1 - z_{it})M && \forall i \in [\nSample], \quad \forall t \in \mathcal{T}_L \\
            & \delta_i \ge - \phi_{it} - (1 - z_{it})M && \forall i \in [\nSample], \quad \forall t \in \mathcal{T}_L \\
            & \phi_{it} = y_i - \textrm{poly}(x_i; c_t) && \forall i \in [\nSample], \quad \forall t \in \mathcal{T}_L \\
            & a_m^{\top} {x}_i \ge b_m - (1 - z_{it}) && \forall i \in [\nSample], \quad \forall t \in \mathcal{T}_L, \quad \forall m \in A_R(t)    \label{eq:axaligned_eee}\\
            & {a}_m^{\top} ({x}_i + {\epsilon}) \le b_m & & \notag\\
            & \quad + (1 + \epsilon_{\max})(1 - z_{it})& & \forall i \in [\nSample], \quad \forall t \in \mathcal{T}_L, \quad \forall m \in A_L(t)   \label{eq:axaligned_fff}\\
            & \textstyle\sum_{t \in \mathcal{T}_L}{z_{it}} = 1 && \forall i \in [\nSample]  \label{eq:aaa}\\
            & z_{it} \le l_t && \forall i \in [\nSample], \quad \forall t \in \mathcal{T}_L \label{eq:bbb}\\
            & \textstyle\sum_{i=1}^{\nSample}{z_{it}} \ge N_{\min} l_t && \forall t \in \mathcal{T}_L \label{eq:minN}\\
            & \textstyle\sum_{j=1}^d{a_{jt}} = 1 && \forall t \in \mathcal{T}_B \label{eq:ccc}\\
            & 0 \le b_t \le 1 && \forall t \in \mathcal{T}_B \label{eq:ddd}\\
            & z_{it}, l_{t} \in \{0, 1\} && \forall i \in [\nSample], \quad \forall t \in \mathcal{T}_L \label{eq:axaligned_lll}\\
            & a_{jt} \in \{0, 1\} && \forall j \in [\ndim], \quad \forall t \in \mathcal{T}_B \label{eq:axaligned_mmm}\\
            & \phi_{it} \in \mathbb{R} && \forall i \in [\nSample], \quad \forall t \in \mathcal{T}_L
  \end{align}
\end{subequations}
The constraints \cref{eq:axaligned_eee,eq:axaligned_fff,eq:aaa,eq:bbb,eq:minN,eq:ccc,eq:ddd,eq:axaligned_lll,eq:axaligned_mmm} are the optimal classification trees (OCT) of \citet{bertsimasOptimalClassificationTrees2017}, whereas the other constraints are our extensions thereof. Note that we do not use the complexity parameters of the OCT formulation ($d_t$) and replace them with 1, where appropriate.

\begin{table}[t]
  \caption{Summary of the variables of the axis-aligned regression tree formulation\label{table:variablesaxisaligned}}
  % \resizebox{0.48\textwidth}{!}{
  \centering
  \begin{tabular}{lll}
    \toprule
    variable & index domain & interpretation \\ \midrule
    $l_{t} \in \{0, 1\}$ & $t\in\lNode$ & 1 iff any point is assigned to leaf $t$ \\
    $z_{it} \in \{0, 1\}$ & $t\in\lNode$, $i\in[\nSample]$ & 1 iff point $x_{i}$ is assigned to leaf $t$ \\
    $a_{m} \in \{0, 1\}^{\ndim}$ & \multirow{2}{*}{$m\in\bNode$} & \multirow{2}{*}{coefficients of the axis-aligned cut} \\
    $b_{m} \in \bbR$ & & \\
    $\phi_{it} \in \bbR$ & $t\in\lNode$, $i\in[\nSample]$ & fit error of point $x_{i}$ by the polynomial of leaf $t$ \\
    $\delta_{i} \in \bbR$ & $i\in[\nSample]$ & fit error of point $x_{i}$ by the piecewise polynomial function \\
    $c_{t} \in \bbR^{\binom{\polyDeg + \ndim}{\ndim}} $ & $t\in\lNode$ & coefficients of the degree $\polyDeg$ polynomial associated with leaf $t$ \\
    \bottomrule
  \end{tabular}
  % }
\end{table}
