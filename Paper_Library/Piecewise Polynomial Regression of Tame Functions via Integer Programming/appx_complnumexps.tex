In this section, we give complementary experiments that illustrate the practical behavior of the axis-aligned and affine-hyperplane regression models.

\paragraph{Setup.}
The setup is identical to the one described in \cref{sec:numer-exper}.
We consider three additional regression problems, for which we plot the landscapes in the forthcoming figures and give approximation errors in \cref{table:expsappx}.

% \subsection{Experimental setup}
% We implemented the axis-aligned and the affine-hyperplane formulations, detailed in \cref{eq:OrtFormulationHplane,eq:OrtFormulation}, in Python and used Gurobi as a MIP solver.
% Each optimization ran with a 5-minute time limit, on a laptop.
% The trees were parametrized with depth $D=2$, unless stated otherwise, meaning that we expect at most four regions.
% We set the minimum number of points per region to $N_{min} = 1$, and the degree of the regression polynomials to $\polyDeg=1$.
% Finally, we set $\mu=10^{-4}$ in the affine-hyperplane formulation, and round to $4$ decimals the scaled values $x_i \in [0, 1]^\ndim$.
% % This value can cause numerical issues for the solver, so setting it to a high enough value is essential.
% % In experiments, we found benefits in rounding all scaled values $x_i \in [0,1]^d$ to 4 decimals also for numerical stability, so we set $\mu = 10^{-4}$.

% The test functions are two-dimensional; the regression data is a collection of $\nSample = 250$ samples distributed randomly within the approximation region.
% For each function, we report the level lines of the original function, and the level lines of the piecewise polynomial functions provided by the axis-aligned and affine-hyperplane formulations.
% In addition, we also report in \cref{table:expsappx} the error between each test function and its approximation by the axis aligned and affine hyperplane regression trees, if applicable.

% \gbcomment{\@ Jiri, what value for $N_{min}$? do you confirm $r=1$?}
% \jncomment{$N_min = 1$ added and I confirm $r=1$}
% Thanks!


\paragraph{Piecewise-linear norms.}
\label{sec:piec-line-norms}

We consider two simple piecewise linear test functions: the $\|\cdot\|_{1}$ and $\|\cdot\|_{\infty}$ norms, defined for $x\in\bbR^{\ndim}$ by
\begin{equation}
  \|x\|_{1} = \sum_{i=1}^{\ndim} |x_{i}|, \qquad \|x\|_{\infty} = \max_{i\in [\ndim]} |x_{i}|.
\end{equation}
% These functions are piecewise linear.
% Note that the linear pieces are known exactly and have a simple expression.
% The boundaries of the linear pieces are not aligned with the cartesian axes.
For both functions, we set depth $D=2$ and polynomial degree $\polyDeg=1$. We sample $\nSample=250$ points in the approximation space. We test both the axis-aligned \eqref{eq:OrtFormulation} and the general affine-hyperplane \eqref{eq:OrtFormulationHplane} formulations with a time limit of 5 minutes for each optimization.
For both norms, the axis-aligned formulation was solved to optimality and the affine-hyperplane formulation timed out.

\Cref{fig:l1norm} shows the results on the $\|\cdot\|_{1}$ norm. 
Note that the full-dimensional cells of the $\|\cdot\|_{1}$ are axis aligned, so the axis-aligned formulation \eqref{eq:OrtFormulation} (with $D=2$) recovers both the correct cells and the correct polynomial function on each piece.
The more general affine-hyperplane formulation \eqref{eq:OrtFormulationHplane} with performs equally well. 
% The inaccurate slope of the discontinuity line on the right is caused by a lack of points sampled near the true discontinuity line. 

% How would the pieces of the learned function compare with the actual pieces of these functions?

\Cref{fig:linfnorm} shows the results for the $\|\cdot\|_{\infty}$.
The axis-aligned formulation \eqref{eq:OrtFormulation} with depth yields a piecewise polynomial function that performs poorly at approximating the function.
This is reasonable, as the full-dimensional cells are not axis-aligned anymore.
The affine-hyperplane formulation \eqref{eq:OrtFormulationHplane} yields a piecewise polynomial function that matches the cells of the function, as well as the polynomial expression of the function on the cells.

Numerical results for both norms are in \Cref{table:expsappx}. They show a slight increase in error when using the affine-hyperplane formulation on the $\|\cdot\|_1$ norm. 
This can be attributed to the fact that the true partitioning is axis-aligned, which agrees with the main limitation of the axis-aligned formulation. 
And because the axis-aligned formulation is simpler to optimize, we obtain a provably optimal solution. Despite that, the solution of affine-hyperplane formulation has only slightly worse error. Additionally, looking at the errors for the $\|\cdot\|_{\infty}$ norm, we see order(s) of magnitude improvements in the error, when using the affine-hyperplane formulation. This underlines the increased expressivness of the more general formulation.

\begin{figure*}[p]
    \centering
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{figures/numres_plots/norm1.png}
        \caption{$\|\cdot\|_1$ norm.}
        \label{fig:l1norm}
    \end{subfigure}
    \begin{subfigure}{.6\textwidth}
    
% \end{figure*}
% \begin{figure*}[p]
        \includegraphics[width=\textwidth]{figures/numres_plots/norminf.png}
        \caption{$\|\cdot\|_\infty$ norm.}
        \label{fig:linfnorm}
    \end{subfigure}
    \caption{Results on norm functions. On the left are the original functions, middle column contains the results of the axis-aligned formulation \eqref{eq:OrtFormulation} and on the right are results of the more general affine-hyperplane formulation \eqref{eq:OrtFormulationHplane} from the main body of the paper. Red crosses are the coordinates of samples used in the optimization. White lines are the level lines of the function values. Black lines show the partitioning of the space.}
\end{figure*}

% \jncomment{Discuss the optimality, add the mention of the optimal affine-hyperplane result}

% Firstly, we consider $\|x\|_{\infty} = \max_{i \in [\ndim]} |x_i|$ as an example of a tame function with partitions unfit for trees with axis-aligned splits, \eg~recent works of \citet{NEURIPS2021_dba4c1a1,chatterjeeAdaptiveEstimationMultivariate2021}. When we fit a hundred points sampled from the $\|\cdot\|_{\infty}$, our approach \eqref{eq:OrtFormulationHplane} can return a nearly optimal solution in under 30 seconds and the optimal within one hour.

% An other possibility: the $2$-norm $\|x\|_{2} = \sqrt{x_{1}^{2} + \ldots + x_{n}^{2}}$.
% This function is not piecewise linear.

% A more involved function from $\bbR^n$, $n\ge 2$, is, for $r>0$ and $s>0$ small,
% \begin{equation}
%   f(x) =
%   \begin{cases}
%     -sx_{1} + \frac{1+s}{r}x_{i} & \text{ if } x_{1} > 0 \text{ and } \exists i \in \{2, \dots, n\}, \|\pi^{1, i}(x)\|_{\infty} < x_{i} < r x_{1} \\
%     -sx_{1} - \frac{1+s}{r}x_{i} & \text{ if } x_{1} > 0 \text{ and } \exists i \in \{2, \dots, n\}, \|\pi^{1, i}(x)\|_{\infty} < -x_{i} < r x_{1} \\
%     \|x\|_{\infty} & \text{ else}
%   \end{cases},
% \end{equation}
% where $\pi^{1, i}(x)$ denotes the vector of $\bbR^{n-2}$ obtained by removing coordinates $1$ and $i$ of vector $x\in\bbR^n$.

% \old{
% \subsubsection{Piecewise affine function}
% \label{sec:piec-affine-funct}

% We turn to a more challenging test function, mentioned in the introduction:
% \begin{align}
%   % \tag{\ref{eq:cone}}
%   \resizebox{0.40\textwidth}{!}{$
%   f(x) =
%   \begin{cases}
%     -sx_{1} + \frac{1+s}{r}x_{2} & \text{ if } x_{1} > 0 \text{ and } 0 < x_{2} < r x_{1} \\
%     -sx_{1} - \frac{1+s}{r}x_{2} & \text{ if } x_{1} > 0 \text{ and } -rx_1 < x_{2} < 0 \\
%     \|x\|_{\infty} & \text{ else}
%   \end{cases}$
% }
% \end{align}
% This function is piecewise-linear, continuous, and equals $\|\cdot\|_{\infty}$ on almost all the space, except on a small cone of radius $r$ that goes along the ray $x_{1}\cdot \bbR_{+}$
% On this ray, the function is decreasing and equals $-sx_{1}$; see \cref{fig:cone} for an illustration.
% % This is a nonconvex function that may be worst-case for most regression methods (in terms of the number of samples required to obtain a good model of the function).
% This is a nonconvex function for which estimating the correct full-dimensional cells with a sampling scheme gets harder as $r$ goes to zero, or as the dimension of the space $\ndim$ increases.
% % that may be worst-case for most regression methods (in terms of the number of samples required to obtain a good model of the function).
% % In two dimensions, this function boils down to \eqref{eq:cone2dintro} of the introduction.

% \Cref{fig:coneappx} shows the results of the regression of \eqref{eq:cone} with $\rCone=\sCone=0.5$.
% The affine-hyperplane fits the function better than the axis-aligned one, but still does not capture well the cells of the cone function with depth $D=2$.
% This is reasonable, as the function contains more than $4$ cells.

% \Cref{fig:cone_depth3} shows the output of the two regression models with depth $D=3$ (ran with a 10 minute time limit).
% The obtained functions are much more accurate, although the cell decomposition of the function is still not obtained (this would require $D=4$).
% % We ran this with 10 minute time limit and obtained very accurate results (see )
% % , although .
% % This function contains more than 4 full-dimensional cells, so we also tested trees of depth equal to 3.
% % because the ground truth contains more than 4 regions.

% }

% \begin{figure*}[p]
%     \centering
%     \includegraphics[width=\textwidth]{figures/numres_plots/cone_depth2.pdf}
%     \caption{Cone function \eqref{eq:cone}. Red x symbols are the coordinates of samples used in the training. Black lines show the partitioning of the space.}
%     \label{fig:coneappx}
% \end{figure*}
% \begin{figure*}[p]
%     \centering
%     \includegraphics[width=\textwidth]{figures/numres_plots/cone_depth3.pdf}
%     \caption{Cone function \eqref{eq:cone} for trees of depth 3. Red x symbols are the coordinates of samples used in the training. Black lines show the partitioning of the space.}
%     \label{fig:cone_depth3}
% \end{figure*}

% \old{
% \subsubsection{Denoising of a piecewise-constant 2d signal}

% Following the denoising scenarios of \cite{NEURIPS2021_dba4c1a1}, we generate four functions that are piecewise constant on a 25 by 25 grid; see column one of \Cref{fig:data_noisy}.
% The regression data is the function corrupted by a zero-mean additive gaussian noise of standard deviation $\sigma = 0.5$; see column two of \Cref{fig:data_noisy}.
% Column three of \Cref{fig:data_noisy} shows the output of the regression models with depth $D=4$ and polynomial degree $\polyDeg=0$, with a time limit of 1 hour per scenario.
% The recovered signals are comparable in quality to Figure 3 in \cite{NEURIPS2021_dba4c1a1}.
% % \jncomment{Is the above enough? In the implementation, it looks like this $y_{noisy} = y^* + 0.5 \cdot N(0,1)$ where $N(0,1)$ is the standard }
% % \gbcomment{Sorry, this was mostly a note to myself. What you wrote is great.}
% }

\paragraph{An additional Neural Network approximation}
% The last example we present is the case of approximation of a Neural Network (NN) using the affine-hyperplane MIP formulation.

Much as described in \cref{sec:numer-exper}, we consider a similar NN with $25$ parameters (2 hidden layers with 4 and 2 neurons respectively), but with \emph{only ReLU} activation functions.
The network is trained to minimize the mean squared error with the 2-dimensional function $x\mapsto  2\sin{x_1} + 2\cos{x_2} + x_2 / 2$ taken at $10$ random points from the input space.
All data is processed in a single batch, for 5000 epochs, using \texttt{AdamW} optimizer.

% This network
% We have a two-dimensional input space $(x_1, x_2) \in [-2,2]^2$, and we are estimating a function $2\sin{x_1} + 2\cos{x_2} + x_2 / 2$.
% We train two small NNs to approximate the function.
% A simpler one has 25 parameters (2 hidden layers with 4 and 2 neurons respectively) and is trained on 10 random samples from the input space.
% It contains only ReLU activations, which makes the approximation piecewise linear.
% \old{
% The other NN, with 27 parameters, is trained on 15 random samples from the input space.
% This second NN uses various activations and a 1D max-pooling layer.
% % Its strucure is shown in \Cref{fig:NN_vis}.
% }
% We train both NNs in the same way, with all data in a single batch, for 5000 epochs, using \texttt{AdamW} optimizer.
% \Cref{fig:nn_relu} (left pane) shows the ground truth function and the 10 sample points from which the network is trained.
% neural network is shown and sampled points are visualized in \Cref{fig:dl_example1} (left pane).
% The second plots, in the middle, show the approximation by the respective NNs.
% \Cref{fig:nn_relu} (right pane) shows 
The piecewise linear approximation is obtained from the affine-hyperplane formulation with depth $D=2$ and $\nSample=225=15^{2}$ sample points (shown as red crosses) placed on a regular grid of $15\times 15$ points. The degree of the polynomial pieces is $\polyDeg = 1$. The MIP optimization times out after 5 minutes.
% Finally, we approximate each NN output landscape by sampling a $15\times15$ grid over the input space and optimizing the tree.
% We use the affine-hyperplane regression tree formulation for trees of depth 2 to fit the data.
% The third plot in \Cref{fig:dl_example1} shows the result after a 5-minute timeout for the simpler and more complex NNs respectively.

\Cref{fig:nn_relu} shows the landscape of the network in the left pane and the piecewise linear approximation in the right pane.
The obtained piecewise polynomial function essentially recovers the cell decomposition of the network.
\Cref{fig:nn_errors} presents the difference between the NN oputut and the approximation.
The approximation recovers the slope of the network correctly on each of the cells.
The discrepancy between the two functions is caused by the slight mismatch between the cell decomposition of the network and its approximation, which could arguably be improved by taking samples from a denser grid.

Looking at the last row of \Cref{table:expsappx}, we notice that the median error is the lowest among all functions by orders of magnitude, pointing to the high quality of the fit of the polynomials in each partition. This might be due to the properties of taking points on a regular grid which might allow for better approximation.  

% \gbcomment{Simplify the above text to only mention the ReLU NN experiment, and make clear this is different from what is in the main text.}

%\jncomment{in progress by Jiri, also comparison to ORT of \cite{chatterjeeAdaptiveEstimationMultivariate2021}}


% \textcolor{red}{[Jiri] Thank you, I have implemented the correct function now, but I now struggle with the hyperplane formulation. It has significantly poorer performance, leading to inaccurate results due to time constraint on the solving. Will try to do something about it tomorrow and pos the updated figures here.
% Was not sure how to interpret the $\pi(\cdot)$ in the cone function and where $i$ comes from. So I implemented something similar, \begin{equation}
%   \hat{f}(x) =
%   \begin{cases}
%     -sx_{1} + \sum_{i=2}^n \frac{1+s}{r}|x_{i}| & \text{ if } x_{1} > 0 \text{ and } \|x\|_{\infty} \le r x_{1} \\
%     % -sx_{1} - \frac{1+s}{r}x_{i} & \text{ if } x_{1} > 0 \text{ and } \|\pi_{n}^{1, i}(x)\|_{\infty} < -x_{i} < r x_{1} \\
%     \|x\|_{\infty} & \text{ else}
%   \end{cases}
%   \label{eq:fhat}
% \end{equation}
% }

% \gbcomment{
% Sorry for the incomplete description. It should be fixed now. I also added the expression of the function in 2d.

% I'm curious about the $\hat{f}$ function: the figure 4 plot looks exactly like the function I had in mind. Is it obtained from the expression $(5.1)$? I would think the first condition cannot be met by any point if $r<1$. You are right, it was not correct, I missed in the linf of x should be all except $x_1$, my mistake.
% }

% \subsection{Experiments}
% We have tested all the above-mentioned functions.

% Axis-aligned trees return proven optimal solutions within the 5 minute time limit, while the rest reached the time limit before a solution was proven.
% Nevertheless, the best feasible solutions they provided were of good quality, as shown in Figures \ref{fig:l1norm},  \ref{fig:linfnorm} and \ref{fig:cone}.
% should I add numerical results?

% \gbcomment{The 2-norm will be delicate, perhaps the image denoising examples of \cite[Fig. 3]{NEURIPS2021_dba4c1a1} would be more interesting?}
% \jncomment{REPLY: going for those from the paper now, I thought that something 2+degree polynomial would be nice. If we are talking about polynomials in general, the functions in the paper are pw constant even}




\begin{table*}[p]
  \caption{Normalized absolute error between the functions and
their approximations by the axis-aligned \eqref{eq:OrtFormulation} and affine-hyperplane trees \eqref{eq:OrtFormulationHplane} of depth 2. The error is computed on a $1000 \times 1000$ grid of regularly
spaced points. We divide the absolute errors by the maximal absolute value of the underlying ground truth to improve comparability.\label{table:expsappx}}
  % \resizebox{0.48\textwidth}{!}{
  \centering
  \begin{tabular}{lcccccc}
    \toprule
    \multirow{2}{*}{Test function} & \multicolumn{3}{c}{Axis-aligned tree \eqref{eq:OrtFormulation}} & \multicolumn{3}{c}{Affine-hyperplane tree \eqref{eq:OrtFormulationHplane}} \\
                                   & max. err. & mean err. & median err. & max. err. & mean err. & median err. \\ \midrule
    $\|\cdot\|_{1}$ & \scinum{0.022015681793301364} & \scinum{9.788552931962676e-05} & \scinum{3.0399942109511357e-05} & \scinum{0.02604307023189877} & \scinum{0.00011084159563612121} & \scinum{3.0358761727917738e-05} \\
    $\|\cdot\|_{\infty}$ & \scinum{0.7186999470693821} & \scinum{0.08506899722574107} & \scinum{0.028201825194180685} & \scinum{0.06604959284704932} & \scinum{0.0004217446020628828} & \scinum{5.04670190798251e-05} \\
    % $f$ (cone) - depth 2 & \scinum{1.0490687762763655} & \scinum{0.10379329143739957} & \scinum{0.003929167425811357} & \scinum{1.9672187547679316} & \scinum{0.09526571450909092} & \scinum{7.357197365891466e-05} \\
    % $f$ (cone) - depth 3 & \scinum{0.5784551134624668} & \scinum{0.03756568102589815} & \scinum{0.00010145046652141332} & \scinum{0.3423133283548014} & \scinum{0.013404843145529395} & \scinum{6.324693143781701e-05} \\
    % Denoising 1 & \scinum{1.0935919250129167} & \scinum{0.17020131288580032} & \scinum{0.09359192501291669} & - & - & - \\
    % Denoising 2 & \scinum{1.2780367550387073} & \scinum{0.20302255169266256} & \scinum{0.13123372273163447} & - & - & - \\
    % Denoising 3 & \scinum{0.9270799634280893} & \scinum{0.1525090278362996} & \scinum{0.1316940385252443} & - & - & - \\
    % Denoising 4 & \scinum{0.8784205352258001} & \scinum{0.0387184639530068} & \scinum{0.014525874547933037} & - & - & - \\
    ReLU NN & - & - & - & \scinum{0.030146090044253088} & \scinum{0.00017965119982446732} & \scinum{2.686642996129214e-07} \\
    % \Ranexp{} net approx. & - & - & - & \scinum{0.31329737921872924} & \scinum{0.03478870115326954} & \scinum{0.02358915332408331} \\
    % \Ranexp{} - depth 3 & - & - & - & \scinum{0.20264276270604184} & \scinum{0.013826297079377634} & \scinum{0.007772890841115508} \\
    \bottomrule
  \end{tabular}
  % }
\end{table*}


\begin{figure}[p]
    \centering
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[height=0.5\textwidth]{figures/numres_plots/NN_relu.png}
        \caption{Approximation result. On the left is the NN output landscape and on the right is a resulting tree approximation. The black lines show the partitioning of the space, white lines are level lines, and red crosses show coordinates of sampled points.}
        \label{fig:nn_relu}
    \end{subfigure}
    \hspace{.03\textwidth}
    \begin{subfigure}{.45\textwidth}
        \centering
        % \includegraphics[height=\textwidth]{figures/numres_plots/NN_relu_err.png}
        \includegraphics[height=0.5\textwidth]{figures/numres_plots/NN_relu_err_bar.png}
        \caption{Normalized absolute error of the ReLU NN approximation by the tree model. The errors are divided by the maximum absolute value of the NN output. We can see that the majority of the error is due to slight inaccuracies in the partitioning. These errors are described numerically in \Cref{table:expsappx}.}
        \label{fig:nn_errors}
    \end{subfigure}
    \caption{Approximation of the Neural Network with only ReLU activations. On the left, in \Cref{fig:nn_relu}, is the original output and the approximation. \Cref{fig:nn_errors}, on the right, shows the normalized absolute error between the two functions.}
\end{figure}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figures/numres_plots/error.pdf}
%     \caption{Absolute error of the ReLU NN approximation by the tree model. We can see that the majority of the error is due to slight inaccuracies in the space partitioning.}
%     \label{fig:nn_errors}
% \end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\textwidth]{figures/NN_net.pdf}
%     \caption{A diagram of the used Neural Network. Green neurons represent the input, blue nodes are neurons in hidden layers, and the output neuron is red. Grey nodes represent the max-pooling layer. Above the connections are the names of activation functions used on the outputs of the layer to the left of each respective name. Output has no activation because we perform regression. The results using this architecture are in \Cref{fig:dl_example2}.}
%     \label{fig:NN_vis}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/numres_plots/NN_various_depth2.pdf}
%     \caption{Approximation of the more complex Neural Network. On the left is the original function, in the middle is the approximation of the function by a Neural Network, and finally, on the right is a resulting tree approximation of the Neural Network. The black lines show the partitioning of the space of the tree and the stratification of the NN output.}
%     \label{fig:dl_example2}
% \end{figure}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main_icml"
%%% End:
