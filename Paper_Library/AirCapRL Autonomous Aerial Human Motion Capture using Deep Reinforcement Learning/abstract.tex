\begin{abstract}  % put your abstract here!
%Goal
In this letter, we introduce a deep reinforcement learning (DRL) based multi-robot formation controller for the task of autonomous aerial human motion capture (MoCap). We focus on vision-based MoCap, where the objective is to estimate the trajectory of body pose and shape of a single moving person using multiple micro aerial vehicles.
%Problem
State-of-the-art solutions to this problem are based on classical control methods, which depend on hand-crafted system and observation models. Such models are difficult to derive and generalize across different systems. Moreover, the non-linearities and non-convexities of these models lead to sub-optimal controls.
%Nugget
In our work, we formulate this problem as a sequential decision making task to achieve the vision-based motion capture objectives, and solve it using a deep neural network-based RL method.
%Method
We leverage proximal policy optimization (PPO) to train a stochastic decentralized control policy for formation control.
The neural network is trained in a parallelized setup in synthetic environments.
%Results
We performed extensive simulation experiments to validate our approach. Finally, real-robot experiments demonstrate that our policies generalize to real world conditions. 
\end{abstract}


% \begin{abstract}  % put your abstract here!
% %Goal
% In this letter, we introduce a deep reinforcement learning (RL) based multi-robot formation controller for the task of autonomous aerial human motion capture.
% %Problem
% Ensuring smooth and consistent motion capture of a moving human subject in outdoor unstructured environments is a perceptually and navigationally challenging task.
% %Nugget
% In our work, the formation control problem is formulated as a model-free sequential decision making task to achieve perceptual outdoor motion capture objectives cooperatively.
% %Method
% Deep reinforcement learning with proximal policy optimization is leveraged to train a stochastic decentralized control policy for formation control.
% %Results
% Extensive simulation results and real-world experiments are performed to validate the proposed control algorithm.
% \end{abstract}
