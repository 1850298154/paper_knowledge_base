\section{Related Work}
% In this section we review literature related to developing a model-free multi-robot formation controller for aerial outdoor motion capture.\\

\textit{Aerial Motion Capture Methodologies:}
A marker-based multi-robot aerial motion capture system is presented in \cite{nageli2018flycon}. Here, pose of the person and the robots are jointly estimated and optimized online. A multi-robot model-predictive controller is used to compute trajectories which optimize the camera viewing angle and person visibility in the image. Marker-based methods suffer from tedious setup times, and optimal control methods for trajectory following can lead to sub-optimal policies for motion capture due to perceptual objectives.
A markerless aerial motion capture system using multiple aerial robots and depth cameras is proposed in \cite{xu2018flycap}. They use a non-rigid registration method to track and fuse the depth information from multiple flying cameras to jointly estimate the motion of a person and the cameras. Their approach works only indoors and the initial registration step can take a long time similar to other marker based method setups. In one of our previous works, \cite{saini2019markerless}, we introduced a vision-based (monocular RGB) markerless motion capture method using multiple aerial robots in outdoor scenarios. The pose and shape of the subject and the pose of the cameras are jointly estimated and optimized in \cite{saini2019markerless}. While our other previous work \cite{ActiveTallamraju19} introduces a front-end of our outdoor aerial MoCap system, \cite{saini2019markerless} describes the back-end. 
%Using the approach in  experiments and does not require any setup time for the person.

\textit{Perception-Aware Optimal Control Methods for Target Tracking:}
%In \cite{lima2015formation}, active perception based formation control is addressed  using a decentralized non-linear MPC. However, their method only identifies sub-optimal control inputs due to the non-convexity of optimization. 
In \cite{PampcFALANGA18}, a perception-aware MPC generates real-time motion plans which maximize the visibility of a desired static target. 
In \cite{lee2020aggressive} a  deep learned optical flow algorithm and non-linear MPC are jointly utilized to optimize a general task-specific objective. The optical flow dynamics are explicitly embedded into the MPC to generate policies which ensure the visibility of target features during navigation. 
An occlusion-aware moving target following controller is proposed in \cite{jeon2019online}. Here, metrics for target visibility are utilized to navigate towards a moving target and constrained optimization is leveraged to navigate safely through corridors.
In the above works, the motion plans are generated only for a single aerial robot to track a single generic target.  
In our previous work \cite{ActiveTallamraju19}, a non-linear MPC based formation controller for active target perception is introduced for target following. The controller assumes Gaussian observation models and linearizes system dynamics. Using these, it identifies a collision-free trajectory which minimizes the fused uncertainty in target position estimates.
In contrast to that, in our current work we learn a control policy to explicitly improve the quality of 3D reconstruction of human pose. An implicit perception-aware target following behavior evolves out of the controller for both single and multi-agent scenarios.



\textit{Learning based Control for Aerial Robots for Perception Driven Tasks:}
Optimal control methods are computationally expensive, require explicit estimation of the state of the system and world, and depend mostly on hand-crafted system and observation models. Thus, it can often lead to sub-optimal behaviors.
A model-predictive control guided policy search was proposed in \cite{zhang2016learning} where supervised learning is used to obtain policies which map the on-board aerial robot sensor observations to control actions. The method does not require explicit state estimation at test time and plans based on just input observations.
In \cite{bonatti2019autonomous} authors used a deep Q-learning based approach for cinematographic planning of an aerial robot (or MAV). A discrete action policy was trained on rewards that exploit aesthetic features in synthetic environments. User studies were performed to obtain the aesthetic criteria. In contrast to that, our current work proposes single and multi-agent MAV control policies that reward the  minimization of errors in body pose and shape estimation.
% \cite{fan2018fully},
A proximal policy optimization (PPO) based distributed collision avoidance policy was proposed in \cite{fan2018fully}. A centralized training and decentralized execution paradigm was leveraged to obtain a policy that maps laser range scans to non-holonomic control actions. 
%In contrast we propose aerial robots and mocap aware control policies.
%\cite{Ding2018},
In \cite{Everett2018} the authors propose an A3C actor-critic algorithm to develop reactive control actions in dynamic environments. Each agent's ego observations and an LSTM-encoded dynamic environmental observations are inputs to a fully connected network. Their goal is to obtain a fully distributed control policy.
%\cite{Hwangbo2017},
In contrast to the aforementioned works, we propose a model-free deep reinforcement learning approach to the MoCap-aware MAV formation control problem. In our work, a policy neural network directly maps observations of the target subject to control actions of each MAV without any underlying assumptions of the observation model or system dynamics. 
