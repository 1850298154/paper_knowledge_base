

\newpage

~

\newpage
% The agent's observation of the environment $o_t$ has four components, $o_t= \{o_{d,t}, o_{j,t},o_{\bar{j},t},o_{f,t}\}$. The first component $o_{d,t}\in\mathbb{R}^5$, defines observations which aid MAV navigation towards the subject. The AlphaPose \cite{cao2017realtime,xiu2018poseflow} network's inference is used in real-time to obtain the bounding box and joint detections of a subject in the image plane. The detections from alphapose are visualized as circles in Fig. \ref{f:mesh}. The bounding box position $\{I_x,~I_y\}$, ratio of its height w.r.t image height $\{h_r\}$, and its direction of motion w.r.t previous image frame, $\{\Delta I_x,~\Delta I_y\}$,  aid the MAV in estimating the heading of the subject motion.  The second and third observation components $o_{j,t}\in\mathbb{R}^{17}$ and $o_{\bar{j},t}\in\mathbb{R}^{17}$ represent the 2-D joint detections of the subject in the image plane obtained from the agent and communicated observation from the neighboring agent respectively. Finally, $o_{f,t}\in \mathbb{R}^4$ defines observations for formation parameters which consist of relative 2-D position of neighboring MAV agent $\Delta \xi_t \in \mathbb{R}^2$ and the heading angle of the MAV and its neighbor $\phi_t,~\bar{\phi_t}$. In summary, an MAV's observation vector at each time instant $o_t=\{I_x,~I_y,~h_r,~\Delta I_x,~\Delta I_y,~o_{j,t},~o_{\bar{j},t},~\Delta \xi_t,~\phi_t,~\bar{\phi_t}\}\in\mathbb{R}^{43}$. All observations are normalized in the range $\{0,1\}$. $~\Delta I_x,~\Delta I_y$ are normalized in the range $\{-1,1\}$.
% 


% 
% \subsubsection{Reward Function}
% The reward function is formulated to embed perceptual mocap objectives into the formation controller. We introduce the reward function in three parts.
% \begin{enumerate}
% \item Reward for single agent perception aware navigation.
% \item Reward for multi-agent triangulation.
% \item Reward for inter-agent collision avoidance.
% \end{enumerate}
% \textit{(1)~Single Agent Navigation Reward}:
% \begin{equation}
% 	r_{navigation} = r_{follow}+r_{center}+r_{\Delta center}+r_{forward}+r_{heading}
% \end{equation}
% For the task of motion capture it is ideal if the subject is of the same size relative to the size of the image. To enforce this, we propose a reward for ensuring that the ratio of the height of the subject, relative to the height of the image is close to a desired fixed value. Assuming an average height model for the subject \cite{DeepPrice18} and given the camera projection model, we can analytically compute [--show computation or cite--] a virtual circle of a certain radius $d_{des}$ and height $h_{des}$ around the subject from which a given image height ratio of the subject can be maintained. This circle defines an ideal virtual control surface from which the camera must ideally view the subject. Here we assume that the camera is fixed rigidly to the MAV frame. Therefore we define $r_{follow}$ as a reward for being close to a desired distance and height,
% \begin{equation}
% r_{follow} = \begin{cases}
% \Gamma k_{follow} ,& \text{if } \|d_t-d_{des}\|_2\leq d_{thresh}\\
% 0.01~\Gamma ,              & \text{otherwise} \\
% +\Gamma k_{follow} ,& \text{if } |h_t-h_{des}|\leq h_{thresh}\\
% +0.01~\Gamma ,              & \text{otherwise}
% \end{cases}.
% \end{equation}
% Here, $d_t$ is the euclidean distance between the ground truth 2-D positions of the robot and the subject measured along the plane of motion of the subject. $h_t$ is the ground truth altitude of the MAV measured from the subject. $d_{thresh},~h_{thresh}$ are custom defined performance thresholds. $\Gamma$ is a binary variable which indicates if the subject is in the camera field-of-view (fov). Therefore, if the subject is not in the camera fov, $r_{follow}=0$. $k_{follow}<1$ is a constant reward value.
% Next, we introduce perceptual rewards for centering the subject in the image and further for ensuring that this subject has low velocity in the image plane between frames. Normalizing image coordinates between $\{0,1\}$, the observed center of the subject's bounding box is denoted as $I_{c,t} = \{I_x,I_y\}$ and its desired position is given by $I_{des}=\{0.5,0.5\}$. A policy which ensures centering of the subject in the image provides more robustness from losing the subject from the fov. 
% \begin{equation}
% r_{center} = \begin{cases}
% \Gamma k_{center} ,& \text{if } \|I_{c,t}-I_{des}\|_2\leq I_{thresh}\\
% 0.01~\Gamma ,              & \text{otherwise}
% \end{cases}
% \end{equation}
% \begin{equation}
% r_{\Delta center} = \begin{cases}
% \Gamma k_{\Delta center} ,& \text{if } |I_{c,t}-I_{c,t-1}|\leq I_{vthresh}\\
% 0.01~\Gamma ,              & \text{otherwise}
% \end{cases}
% \end{equation}
% Here, $r_{center}, r_{\Delta center}$ are constant reward values and $I_{thresh}$ $,~I_{vthresh}$ are custom defined performance thresholds. We additionally reward the MAV agent to radially move towards the subject if the MAV is at a radial distance greater than $r_{thresh}+\epsilon$ away.
% \begin{equation}
% r_{forward} = \begin{cases}
% \Gamma k_{forward} ,& \text{if } r_t-r_{t-1}<0~\&~r_t>r_{thresh}+\epsilon\\
% 0.01~\Gamma ,              & \text{otherwise}
% \end{cases}
% \end{equation}
% Here, $r_t=\|[d_t,h_t]\|_2$. Finally, we reward the agent if its heading direction aligns with the subject's 3-D position.
% \begin{equation}
% r_{heading} = \begin{cases}
% k_{heading} ,& \text{if } |\phi_t-\phi_{des}|<\phi_{thresh}\\
% 0.01~\Gamma ,              & \text{otherwise}
% \end{cases}
% \end{equation}
% 
% \begin{figure*}
% 	\centering
% 	\includegraphics[scale=0.3]{schematics/MultiAgentRL2.pdf}
% 	\caption{Both the agents use the above reinforcement learning network architecture to train the common policy and value networks.}
% 	\label{f:training}
% \end{figure*}


\textit{(2)~Multi-Agent Triangulation}:
Using known camera intrinsics and extrinsics for both agents, a point in the image plane and its corresponding view from another camera, we can estimate the 3-D position of the point using a least squares formulation. We refer the reader to equation (14.42) in \cite{prince2012computer} for more details. 
Therefore, by using the observations of the subject's joints from the MAV agent, $o_j$, and communicated corresponding observations from the neighboring agent, $o_{\bar{j}}$, we estimate the 3-D joint positions of the subject $\hat{\xi}_{t,j}^P$ and compare it to ground-truth joint positions $\xi_{t,j}^P$.
\begin{equation}
r_{triangulate} = \begin{cases}
\Gamma k_{triangulate} ,& \text{if } \sum_{j}\|\hat{\xi}_{t,j}^P - \xi_{t,j}^P\|<\xi^P_{thresh}\\
0.01~\Gamma ,              & \text{otherwise}
\end{cases}
\end{equation}

\textit{(3)~Multi-Agent Collision Avoidance}:
It is important to ensure safety of the MAVs and the moving subject. In order to enforce collision avoidance we reward the agents if they maintain a distance threshold w.r.t neighbor and subject. If collision occurs during training, the policy is penalized with a negative reward.
\begin{equation}
r_{collision} = \begin{cases}
-k_{collision} ,& \text{if } \|\mathbf{x}_t-\bar{\mathbf{x}}_t\|_2\geq \mathbf{x}_{thresh}\\
-k_{collision} ,& \text{if } \|\mathbf{x}_t-\mathbf{x}^P_t\|_2\geq \mathbf{x}_{thresh}\\
k_{collision} ,              & \text{otherwise}
\end{cases}
\end{equation}
Here, $k_{collision}<0$. Finally, we also penalize agent with a small negative reward $r_{workspace} = k_{workspace}$ if the agent violates workspace bounding box constraints.
The overall reward $r_t$ is normalized ($-1 \leq r_t \leq 1$), therefore individual components of the reward are always less than one. The overall reward function at time $t$ is therefore defined as follows. 
\begin{equation}
	r_t =  r_{navigation}+r_{triangulate}+r_{collision}+r_{workspace}
\end{equation} 

\subsection{Training Methodology}
The policy is learned using a hierarchical training methodology. We first train a policy and value neural network for (1) single agent navigation towards a moving subject and, (2) subject collision avoidance using proximal policy optimization (PPO) \cite{schulman2017proximal} for deep reinforcement learning. Subsequently, we train another policy and value network for (1) multi-agent subject triangulation and, (2) multi-agent collision avoidance, using the single agent navigation policy. The actions accumulated from both policies provide sub-optimal multi-agent behaviors. Finally, the policies from both the networks are used to pretrain a common network for navigation, multi-agent triangulation and collision avoidance. The rewards for all behaviors are finally jointly optimized. This hierarchical training methodology aids reducing training time for the policies. The trained policies operate only on local observations obtained from each agent.

\subsubsection{Algorithmic Details}
We train the agents using a centralized training and decentralized execution paradigm. Specifically, a centralized fully observable critic and an actor with local observations are learned by collecting experiences of all robots simultaneously. The robots then execute this common shared policy to collect new data. The fully observable centralized critic and shared actor policy aid in maintaining a stationary environment for our two-agent navigation problem. This enables us to use conventional single-agent reinforcement learning algorithms to train the multi-agent policy network.
Algorithm \ref{alg:1} summarizes the training methodology used.
\begin{algorithm}[h]
	\caption{Pseudocode for centralized training} 
		\label{alg:1}
		\begin{algorithmic}
			\begin{footnotesize}
				\label{Alg:fc}
				\STATE \textbf{Input}: initial policy parameters $\theta_0$, initial value function parameters $\phi_0$
				\STATE \textbf{for} k in Total Episodes
				\STATE \hspace{3em} Collect trajectories $D_k=\{s_t,o_t,r_t,a_t\}, t = \{1\dots T_{episode}\}$  \\   
				 \hspace{3em} from parallel Gazebo runners, with 2 agents using policy $\pi_{k,\theta}$
				\STATE \hspace{3em} Estimate advantage $\hat{A}_{\pi_{k,\theta}}(s_t,a_t)$ \cite{schulman2015high} using the  value function,   \\
				 \hspace{3em} $V_{k-1,\phi}(s_t)$  and reward to go from $D_k$.
				\STATE \hspace{3em} Maximize PPO surrogate loss L \cite{schulman2017proximal} w.r.t $\theta$ via SGD with Adam.
				\STATE \hspace{3em} Fit value function $V_{k,\phi}(s_t)$ by regressing over current reward to go.
				\STATE end \textbf{for}
			\end{footnotesize}
		\end{algorithmic}
\end{algorithm} 

\subsubsection{Network Architecture}
Figure \ref{f:training}. showcases the policy network architecture. The navigation policy is a two layer $64\times64$ network and the triangulation policy is a two layer $256\times256$ network each with ReLu activations. For estimating the advantage $A_t$ the state-value network $V_\mu(s_t)$ is approximated with a neural network with parameters $\mu$. \todoRahul{The value network architecture is a clone of the policy network at each stage}. The current value function is fit by regressing over the reward to go estimate. We train the networks on tensorflow with Adam optimizer and a stable baselines software implementation of PPO\footnote{\url{https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html}}.


\subsubsection{Training Setup}
We utilize the Gazebo multi-body dynamics simulator with ROS, OpenAI-Gym\footnote{\url{http://wiki.ros.org/openai_ros}} and Tensorflow libraries to train the agents. We run 5 parallel instances of Gazebo and the alphapose network on multiple computers over a network to render the simulation. The policy network is trained on a dedicated PC which samples a batch of transition and reward tuples from the network of computers to update the networks.  

To obtain the joints of the mocap simulation subject we use a PyTorch implementation of Alphapose as a ros node running at 4 fps. We use the Gazebo actor\footnote{\url{http://gazebosim.org/tutorials?tut=actor&cat=build_robot}} as the mocap subject and generate a random trajectory using a custom plugin. 
\todoRahul{The agents are trained on different synthetic environments to generalize the following and triangulation policies.}

\subsection{Experiments}
\subsubsection{Single Agent Following}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textit{\textbf{Test 1 : States in Ego Frame: Only Centering Reward}}
All agent observations are in MAV local frame. The agent is rewarded if the Mocap subject is in the center of its camera field of view. The detections are emulated by projecting the ground truth joints in the MAV camera. 
Moreover, zero mean gaussian noise is added to the projected joints. The variance in the noise is high when the agent is too close (d<10 m) or far away from the subject (d>10 m).

% \textit{\textbf{Test 2 : States in Ego Frame: Only Centering Reward using alphapose}}
% The experimental setup is similar to Test 1. The agent however is now rewarded by running inference on the Alphapose network to determine the center of the person in the image.

% \textit{Test 3 : States in Ego Frame, Only Visibility Reward}
% The agent is rewarded based on the number of joints of the Mocap subject that are visible in the MAV field of view.

% \textit{Test 4 : States in Ego Frame,Centering and Visibility Reward    }

\textit{Test 2 : States in Ego Frame, Only 3D HMR error Reward}
SPIN+HMR network is used to infer 3-D pose of the Mocap subject using the MAV camera input. The agent is rewarded based on the average error in 3-D pose estimation of the person.

\textit{Test 3 : States in Ego Frame Centering + 3D HMR error Reward}

\textit{Test Extra 1 : Test 1 + mixture of person movements}
\textit{Test Extra 2 : Test 1 + additiona person movements}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Person is always in the same 3D position in this subsection
\subsubsection{Multi-Agent Formation Control and Comparison to Active Tracking}

\textit{\textbf{Test 1 : States in Ego Frame: Multi-Agent Triangulation Reward using GT + Obstacle Avoidance}}
Noisy ground truth joint detections are used to infer the 3-D joints of a person from the image. The 3-D pose of the person is estimated using a least squares formulation. The agent is rewarded based on the error in 3-D pose estimate of the person w.r.t GT. Additionally, the agent is penalized for collisions with the neighboring agent and rewarded positively otherwise.

%using alphapose here is a consequence of keeping something from Nitin's method
\textit{Test 2 : States in Ego Frame: Multi-Agent Triangulation Reward using Alphapose + Obstacle Avoidance}
The experimental setup is similar to experiment 1. The alphapose network is used to estimate the pose of the person in the image.


\textit{Test 3 : States in Ego Frame: Multi-Agent Triangulation Reward using Alphapose + Centering (also from alphapose) + Obstacle Avoidance} % centering helps converge faster?


\subsubsection{\textit{Combined Behaviors}}
Combine and train the best of single agent following and multi agent networks.

\textit{Test 1 -- S1 + M1 }

\textit{Test 2 -- S1 + M2 }


\subsubsection{E\textit{valuate DRL controller and Active Tracking controllers on accuracy of 3-D pose and shape estimation (Nitin's method)}}


\subsubsection{\textit{Unreal Engine Test Results}}

\subsubsection{\textit{Real Robot Experiments}}
 
