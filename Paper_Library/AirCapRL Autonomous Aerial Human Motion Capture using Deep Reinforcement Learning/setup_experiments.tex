\section{Experiments and Results}


\subsubsection{Training Setup in Simulation}

% \footnote{http://wiki.ros.org/openai\_ros}
% actor\footnote{http://gazebosim.org/tutorials?tut=actor\&cat=build\_robot}

We train and our networks in simulation. We use Gazebo multi-body dynamics simulator with ROS and OpenAI-Gym to train the MAV agents. For the MAV agent we use AscTec Firefly model with an on-board RGB camera facing down at 45$^\circ$ pitch angle w.r.t.\ the MAV body frame. We run 5 parallel instances of Gazebo and the Alphapose network on multiple computers over a network to render the simulation. The policy network is trained on a dedicated PC which samples a batch of transition and reward tuples from the network of computers to update the networks. We use a simulated human in Gazebo as the MoCap subject and generate random trajectories using a custom plugin. Details of the network architectures, training process, libraries, instructions on how to run the code, etc., are provided in the attached supplementary material.

\subsection{Simulation results}


In this sub-section we evaluate our trained policies in Gazebo simulation environment. We create a test trajectory for the simulated human actor for $120$s on which it walks with varying speeds. The best policy of each network variant, as described in subsection~\ref{subsec:proposed_method}, is run 20 times while the actor walks the trajectory. Thus, results from a total of $2400$s of evaluation run of each network variant is obtained. 

For single agent experiments, in addition to the DRL-based methods, we run 4 other methods: i) `Network 1.4 + AirCap', ii) Orbiting Strategy, iii) Frontal-view Strategy and iv) MPC-based approach \cite{ActiveTallamraju19}. For multi-agent experiments we run 2 additional methods: i) `Network 2.3 + AirCap' and ii) MPC-based approach \cite{ActiveTallamraju19}. All these were also run 20 times for 120s each to allow comparison with our DRL-based policies. 
`Network 1.4 + AirCap' and `Network 2.3 + AirCap' imply running the networks with `true observations' instead of directly using simulator-generated ground-truth observations. To this end, we ran the complete AirCap pipeline \cite{ActiveTallamraju19} during the test by replacing only the MPC-based high-level controller with the DRL policy in it. It executes an NN-based person detector, a Kalman filter-based estimator for person's 3D position estimation (not orientation), cooperative self-localization of the MAVs using simulated GPS measurements with noise as well as communication packet loss. More details regarding this are provided in the supplementary material associated with this article.
`Orbiting Strategy' is essentially a `model-free' approach in which a robot orbits around the person at a fixed distance in order to increase the coverage. In `Frontal-View Strategy' a robot maintains a fixed distance to the person and attempts to always keep the frontal view of the person in the camera image.
Below we discuss the results for single and multi-agent network variants and other aforementioned methods.


\begin{figure}[t]
 \includegraphics[width=\columnwidth]{single_agent_sim_experiments.eps}
 \caption{Simulation results of Single Agent Network variants.}
 \label{fig:sim_experiments_single_agent}
\end{figure}


% Thus, to compute MPE, the SPIN method \cite{kolotouros2019spin} is run on every image acquired and saved by the agents during the evaluation runs and then the equation for $d_{\mathrm{J}}$, as defined for the reward in (\ref{eqn:spinreward}), is used.


% \smallskip

\subsubsection{Single Agent Network Variants}

In order to compare the network variants, we use 2 metrics, i) centering performance error (CPE) and ii) MoCap performance error (MPE). CPE is computed as the pixel distance from the center of the bounding box around the person in the agent's camera image to the image center. MPE, for single agent networks is simply $d_{\mathrm{J}}$, as defined for the reward in (\ref{eqn:spinreward}). 
To compute this, the SPIN method \cite{kolotouros2019spin} is run on the images acquired by the agents during testing.

Note that the metric which quantifies the MoCap accuracy of any method in this paper is MPE (the right side box plots in Fig.~\ref{fig:sim_experiments_single_agent}  and \ref{fig:sim_experiments_multi_agent}). CPE is a metric that we plot only to make the policy performance intuitively explainable and understand `what' the learned RL policies are doing to achieve a good MPE.


 
Figure~\ref{fig:sim_experiments_single_agent} shows the error statistics of the aforementioned metrics. The grey background behind any box plot signifies that the method could not keep the person, even partially, in the MAV FOV, thereby completely losing him/her, for at least some duration of the experiment runs. In these cases, the box plot represents errors computed only for those timesteps when the person was at least partially in the FOV.


% The MPE performance of all approaches except the  Network variant 1.1 is reasonable and similar to each other.


MPE plots in Fig.~5 for single robot experiments show that for all methods the medians of the MPEs are very similar to each other. This is the most significant result, especially because we can demonstrate that in terms of accuracy our DRL-based approach is on par with the state-of-the-art MPC-based approach [2] (or fixed-strategy methods), without the need for hand-crafting observation models and system dynamics (or pre-specified robot trajectories). Furthermore, the MPE for network 1.4 and 1.2 also has significantly less variance of MPE compared to all other methods. Due to these reasons, Network 1.4 and Network 1.2 are the two most successful approaches for the MoCap task.

From Fig.~5 plots, we also see that Network 1.4 keeps the person centered much more than Network 1.2, 1.3 or MPC. This is expected because Network 1.4 is rewarded for centering the person in the image in addition to SPIN-based MoCap rewards. Network 1.2 or 1.3, on the other hand, only has SPIN-based MoCap rewards. Nevertheless, the MPE of Network 1.4 is only slightly better than that of Network 1.3. This signifies that centering the person in the image does not have a great impact on the accuracy of the motion capture estimates.


Network 1.1, which often lost the person in its FOV, outperforms all other methods in its CPE performance for the duration it could `see' the person. This is expected as it is trained with only centering reward. Even though its MPE mean for the person-visible duration is similar to other networks, the variance of its MPE is higher than the other networks. Moreover, the fact that it could keep the person in FOV only $76\%$ of the time as compared to $100\%$ for other networks (1.2--1.4) makes it less desirable even for the MoCap task. 
%The main reason why it loses the person is as follows.



% The next inference we make here is that rewarding the agent for both centering and MoCap performance allows for significantly better centering performance, than when these rewards are treated separately. 
% 
% The most important observation is that to obtain high MoCap performance the  centering rewards play little role.

%In fact, agents with only centering reward tend to lose the person from their FOVs so often that their overall MoCap performance becomes significantly poorer than other network variants. This is due the following reason.

% %########################################### Moved to sup mat
% When the only reward concerns centering (Network variant 1.1), there is only a single image point constraint for the MAV's to keep. In this case, due to the underactuation of the MAV agents and the the fact that the camera is rigidly attached to the MAV frame, the MAVs tend to lose the person completely while making fast and aggressive maneuvers while maintaining the single point constraint.
% 
% On the other hand, when rewarded for only MoCap-related objectives (Networks 1.2 and 1.3), the agents become constrained by many more points on the image plane. Hence, they are more likely to keep the person anywhere on the image plane, irrespective of how far from the image center. Clearly, then combining both these rewards helps to achieve best CPE, while the MPE remains similar to the agents that only got MoCap objective-related rewards.
% %###########################################



% Finally, we also observe that our DRL-based approach performs similar in MoCap accuracy to that of MPC-based approach, while keeping a much low variance in the MoCap errors. MPC-based approach requires hand-crafted observation models. Thus, the fact that our observation model-free DRL-based approach is on par with it in accuracy and better in precision, is a very significant result.


The median MPE of `Network 1.4 + AirCap' is very similar to all other methods. However, it should be noted that there is one drawback in `Network 1.4 + AirCap'. As the `ground truth observations' are not used in this method and the simulated person can rapidly make sudden direction changes, the person is much more susceptible to go out of the FOV of the MAV's camera. Since the network never learned to `search' for the person who is out of the FOV, the method has to `wait' until the person walks back in the FOV. The cooperative estimation method of the AirCap pipeline helps in this regard as the person might still be in another robot's FOV. For a single robot case this is also not possible. Thus, `Network 1.4 + AirCap' loses the person for 35\% of the time. 



The strategy-based methods struggle to keep the person, even partially, in the MAV camera's FOV. While the `Orbiting Strategy' was able to keep the person in the FOV for 73\% of the total time of all experiments combined, the `Frontal-View Strategy' managed to do that only 20\% of the total time. This is because when the person changes his direction or speed of motion, the robot could fly around to reposition itself in the front of the person, thus losing him during the transition. On the other hand, our successful DRL-based approaches, i.e., Network 1.2, 1.3 and 1.4, never lose the person from the camera FOV.
Based on this analysis, we can conclude that the strategy-based methods, while being `model-free', still have a major drawback of losing the person often, if not very carefully hand-crafted. Our DRL-based approaches `explore' the space of these strategies and finds the most suitable one in their policies. 





\begin{figure}[t]
 \includegraphics[width=\columnwidth]{multi_agent_sim_experiments.eps}
 \caption{Simulation results of Multi-Agent Network variants.}
 \label{fig:sim_experiments_multi_agent}
\end{figure}

\subsubsection{Multi-Agent Network Variants}

The MPE in the multi-agent case is also simply $d_{\mathrm{J}}$, as defined for the reward in (\ref{eqn:spinreward}), but instead of using SPIN as in the single agent case, here it is computed by running Multiview HMR \cite{liang2019shape} for pose and shape estimation on every simultaneous pair of images acquired by both the agents during the evaluation runs.
Network 2.1 and 2.2 were trained and tested on a static person. On the other hand, Network 2.3 and Network 2.4 + Potential Field were both trained and tested with a moving person (in the same way as for the single agent experiments). The remaining two methods in the multi-agent case were also tested with moving persons.

%Here, our key observation is that the Network 2.2, trained with MoCap-specific objectives (and centering), has no noticeable difference from the MPC-based approach in terms of MoCap performance. 

% On the other hand, we notice that Network 2.2 does not achieve good performance in centering the person in the MAV's camera FOV in comparison with the MPC-based approach. This again highlights that MoCap performance depends little on whether the person is in the center of the image or not. MPC, however, enforces these constraints directly. Finally, we observe that our multi agent network variant 2.3 shows poorer performance when the subject is moving. This is mainly attributed to the following issues. First, when the subject moves, the MAVs have to follow them. This increases the chances of inter-robot collisions. Hence, the agents might require a lot more training episodes to learn optimal policies.

Figure~\ref{fig:sim_experiments_multi_agent} shows the error statistics of multi-agent simulation experiments. The best performing network in multi-agent case is Network 2.3. It is very similar to the MPC-based method in terms of the MPE median value (See Fig.~6 right side) and has much less MPE variance than MPC. This is a very significant result as MPC required observation models of the subject and our DRL-based approach in Network 2.3 did not. In the MPC approach, the viewpoint configurations for the MAVs emerge out of the joint target perception models. In contrast, in the DRL-based approach the MAVs directly learn the viewpoint configurations from experience. We also notice that the rewards based on a triangulation method assist, to some extent, in achieving acceptable MoCap performance (see results of Network 2.1). However, they remain inferior to the Network 2.3, which used the sophisticated approach taken in Multiview HMR \cite{liang2019shape} for reward computation.


%Furthermore, for the multi-robot case with moving people we added two more methods in the revised manuscript: `Network 2.3 + AirCap' (as explained in \S 3) and `Network 2.4 + Potential field'. The latter is essentially similar to Network 2.3 with the difference that it is not rewarded for collision avoidance with teammate during training or testing. Instead, a potential-field based collision avoidance is run ad-hoc, both during training and testing. 

Furthermore, we find that in terms of MPE, `Network 2.3 + AirCap' is close to both Network 2.3 and MPC. Similar to `Network 1.4 + AirCap', the `Network 2.3 + AirCap' also loses the person from the robots' FOV. However, it is present in at least one robot's FOV for approx.~97\% of the total experiment duration. The increased visibility in the multi-robot case is due to the cooperative estimator module of AirCap pipeline. This assessment signifies the usability of our method in real robots with real observations.

Next, we find that the policy learned by `Network 2.4 + Potential field' was able to achieve MPE median value comparable to Network 2.3 but at the cost of slightly higher MPE variance and loss of person from at least one robot's FOV for several periods (13\% of total duration). This experiment further signifies the key benefit of our DRL-based approach in Network 2.3. It overcomes the need for knowing models, strategies as well as any ad-hoc collision avoidance techniques. In Network 2.3 the learned policy not only achieves good MoCap performance, but it also naturally learns to avoid collisions with the teammates. In the video associated to this paper (also available here -- \url{https://youtu.be/07KwNjc7Sy0}) we show how well Network 2.3 performs.
The networks for the moving person, however, did not ensure very good centering of the person in the image (see the left side of Fig.~6) as compared to the MPC-based approach. Despite this, their MPE performances are only slightly poorer than MPC (MPE median difference is approx.~0.05m only). This further signifies that centering the person on the image has a very low effect on MoCap performance.

%For multi robot experiments we evaluated the trained policy of the best performing network (Network 2.3)\footnote{In the revised manuscript we show that Network 2.3 now works well and results in MPE similar to that of MPC. Please refer \S 4 and \S 5} by running evaluation experiments with this policy and using the complete AirCap pipeline in simulation. These evaluation experiments (henceforth called `Network 2.3 + AirCap') were also run for 20 times, 120 seconds each time like other methods in the previous version of the paper. 

\begin{figure}[t]
\centering
 \includegraphics[width=0.9\columnwidth]{Overlay}
 \caption{A snapshot of the real robot experiment.}
 \label{fig:realrobotexpsfootage}
\end{figure}

Finally, for the multi-agent case, we find that the medians of the MPEs for all multi-agent networks were substantially lowered compared to the MPEs obtained by single-drone experiments (from $\sim$ 0.7m to 0.22m). This highlights the benefit of using multiple drones and hence multiple views to improve MoCap performance. 



\subsection{Real Robot results}




%MPC -- 1243.9/3 = 414
%RL -- 2322.7/3  = 741

% \footnote{https://www.ryzerobotics.com/tello}

In order to validate our approach in a real robot scenario, we used a DJI Ryze Tello drone. It consists of a forward looking camera capturing images at $30$ hz. The drone is controllable using an SDK with ROS interface. Tello has the functionality of vision-based localization, which is highly inaccurate. Hence, we performed experiments within a Vicon hall with markers on top of the drone to estimate its position and velocity. The tracked subject wore a helmet with Vicon markers. Vicon-based position estimate of the person was used to compute the observations for the neural network.

\begin{figure}[!t]
 \includegraphics[width=\columnwidth]{real_world_experiments}
 \caption{Real Robot Experiments: Comparison of single agent network variant 1.1 and MPC-based \cite{ActiveTallamraju19} approach.}
 \label{fig:realrobotexps}
\end{figure}



We performed experiments with $1$ Tello drone and compared our DRL-based approach using Network 1.1 with state-of-the-art MPC-based approach \cite{ActiveTallamraju19}. These were performed for approximately $400$s and $700$s, respectively. Figure~\ref{fig:realrobotexpsfootage} shows an external camera footage of the experiment and the on-board drone view with pose and shape overlay using SPIN. As the ground truth pose and shape of the human subject in real experiment is not available, we only compare the following criteria. We compare i) the length and breadth of the bounding box around the person in the drone images, and ii) proximity of the person to the center of those images, calculated as pixel distance from the image center to the center of the bounding box around the person. The bounding boxes are computed by running Alphapose \cite{cao2017realtime} method on the images recorded by the drone. Figure~\ref{fig:realrobotexps} presents the statistics of these evaluation criteria. We notice that the performance of both approaches is similar in terms of the person's proximity to the image center, with our DRL-based approach performing slightly better. However, we observe that the MPC-based approach is consistently able to keep a larger size (projected height) of the person in the images. This is due the fact that the MPC's objectives enforce it to keep a certain threshold distance to the person. As the DRL-based approach has no such incentive, it varies its distance to the person more, therefore causing a greater variance in the projected height of the person. On the other hand, this enables our DRL-based approach to change its relative orientation with respect to the person such that she/he is is observed from several possible sides. This is evident by the greater variance in the projected width of the person on the images. This property of our DRL-based approach will benefit pose and shape estimation methods, as demonstrated in the simulation experiments.






\section{Conclusions and Future Work}

% \setlength{\belowcaptionskip}{-5pt}
% 
% 
% \setlength{\belowcaptionskip}{-15pt}



In this letter, we presented the first deep reinforcement learning-based approach to human motion capture using aerial robots. 
%We introduce a novel solution to the formation control problem of the MoCap's robotic front-end that learns control policies directly from the MoCap's backend performance. 
Our solution does not depend on hand-crafted system or observation models. Formation control policies are directly learned through experience, which is obtained in synthetic training environments. 
%Through centralized learning and decentralized evaluation paradigm we enable 2 agents to concurrently learn in a parallelized training setup. 
%We proposed several network variants based on the number of agents and the reward structure and evaluate each of them through extensive simulation experiments. 
Through extensive experiments and comparisons we find that DRL-based agents learn extremely good policies, on par with carefully designed model-based (MPC) or model-free, fixed strategy-based methods. These policies even generalize to real robot scenarios. We also find that multiple agents learn even better policies and outperform single agents in performing MoCap.
\
The learning objective (MoCap accuracy) is far simpler to construct than deriving system or observation models \cite{ActiveTallamraju19}. Moreover, strategy based methods, as shown in our experiments, can have various drawbacks, such as losing the person from the field of view. To overcome that, each drawback must be identified and addressed within the fixed strategy. A DRL-based approach overcomes the need for fixing a strategy by `exploring' the space of such strategies. Thus, a major conclusion of our work is that DRL-based approaches are likely the ideal way forward for aerial MoCap systems. Eventually, an end-to-end approach of learning actions directly from images is needed to overcome the need for an additional person-detection method, that has been used so far (e.g., SSD multibox in AirCap \cite{DeepPrice18}). To this end, we are improving our training by using SMPL body models in richer, photorealistic simulated environments.

Our approach would also be applicable in a real robot setting with `real observations' while achieving accuracy similar to an MPC-based approach \cite{ActiveTallamraju19}. Nevertheless, this is valid only for those durations when the person is not lost from the FOV of all cameras. In order for the policy to `search' for the person, network training should be done with the AirCap pipeline's `real observations'. This would involve running several DNN-based detectors and keeping track of delayed measurements. Furthermore, our approach is limited in terms of scaling up to more agents. While addressing this will require more sophisticated network architecture, it should be noted that 2 to 3 aerial robots may be enough to achieve a good MoCap accuracy \cite{MarkerlessNitin19}.

% We have shown that our DRL-based approaches achieve accuracy, similar to MPC or fixed strategy-based methods, without the need for hand-crafted system/observation models. 





\section*{Acknowledgments}
The authors would like to thank Prof.\ Dr.\ Heinrich B\"ulthoff for his constant support and providing us the access to the Vicon tracking hall in MPI for Biological Cybernetics. The authors also thank Igor Martinovi\'c and the anonymous reviewers for extremely helpful suggestions.















