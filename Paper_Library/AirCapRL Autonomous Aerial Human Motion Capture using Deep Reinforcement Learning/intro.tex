\section{Introduction}


% Motivation and Goal
\IEEEPARstart{H}{uman} motion capture (MoCap) implies accurately estimating 3D pose and shape trajectory of a person. 3D pose, in our case, consists of the 3D positions of the major human body joints. Shape is usually parameterized by a large number (in thousands) of 3D vertices. In a laboratory setting MoCap is performed using a large number of precisely calibrated and high-resolution static cameras. To perform human MoCap in an outdoor setting or in an unstructured indoor environment, the use of multiple and autonomous micro aerial vehicles (MAVs) has recently gained attention \cite{MarkerlessNitin19,ActiveTallamraju19, DeepPrice18,nageli2018flycon,xu2018flycap}. Aerial MoCap of humans/animals facilitates several important applications, e.g., search and rescue using aerial vehicles, behavior estimation for endangered animal species, aerial cinematography and sports analysis.

Realizing an aerial MoCap system involves several challenges. The system's robotic front-end \cite{ActiveTallamraju19} must ensure that the subject i) is accurately and continuously followed by all aerial robots, and ii) is within the field of view (FOV) of the cameras of all robots. The back-end of the system estimates the 3D pose and shape of the subject, using the images and other data acquired by the front-end \cite{MarkerlessNitin19}. The front-end poses a formation control problem for multiple MAVs. In this letter, we propose a deep neural network-based reinforcement learning (DRL) method for this formation control problem.


\begin{figure}[!t]
 \includegraphics[width=\columnwidth]{cover.pdf}
 \caption{An illustration of an aerial MoCap system where MAV agents learn formation control policies based on MoCap performance rewards.}
 \label{fig:cover}
\end{figure}

% Problem
Below, we describe the drawbacks in state-of-the-art methods and highlight the novelties in our work to address them.

% \begin{itemize}
In existing solutions \cite{MarkerlessNitin19,ActiveTallamraju19, DeepPrice18} the front and back end are developed independently -- the formation control algorithms of the existing aerial MoCap front ends assume that the person should be centered in every MAV's camera image and she/he should be within a threshold distance to each MAV. These assumptions are intuitive and important. Also, experimentally it has been shown that it leads to a good MoCap estimate. However, it remains sub-optimal without any feedback from the estimation back-end of the MoCap system. The estimated 3D pose and shape are strongly dependent on the viewpoints of the MAVs. In the current work, we take a learning-based approach to map and embed this dependency within the formation control algorithm. This is our \textbf{first key novelty}.   

Existing approaches \cite{ActiveTallamraju19, DeepPrice18,nageli2018flycon,xu2018flycap} depend on tediously obtained system and observation models. State-of-the-art solutions to formation control problems, which involve perception-related objectives, derive observation models for the robot's camera and the desired subject to compute real-time robot trajectories \cite{ActiveTallamraju19, PampcFALANGA18, RealNageli2017}. Since these observation models are based on assumptions on the shape and motion of the subject, sensor noise and the system kinematics, the computed trajectories are sub-optimal. We overcome the aforementioned issue by addressing the formation control for aerial MoCap as a multi-agent reinforcement learning (RL) problem. This is the \textbf{second key novelty} of our approach. We let the MAVs learn the best control action given only the subject perception observable through the MAV's on-board camera images, without making any assumptions on the observation model. 
% \end{itemize}


The key insights which enable us to do this are i) the sequential decision making nature of the formation control problem with MoCap objectives, and ii) the feasibility of simulating control policies in synthetic environments. We leverage the actor-critic methodology of training an RL agent with a centralized training and decentralized execution paradigm. At test time, each agent runs a decentralized instance of the trained network in real-time. 
We showcase the performance of our method in several simulation experiments. We evaluate the quality of the generated robot trajectories using the pose and shape estimation algorithms in \cite{kolotouros2019spin}, \cite{liang2019shape} and \cite{MarkerlessNitin19}. Additionally, we compare our new approach with the state-of-the-art model-based controller from \cite{ActiveTallamraju19}. A demonstration and comparison with the method of \cite{ActiveTallamraju19} on a real MAV is also presented.
Code and implementation details of our method is provided in the supplementaty material. 

%here\footnote{Project Page for AirCapRL: \url{https://ps.is.tuebingen.mpg.de/research_projects/aircaprl/}}.


