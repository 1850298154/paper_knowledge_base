In the above analyses, we saw that the fast convergence of Bayesian learners is both a strength and a weakness. On the one hand, when a Bayesian converges to a correct model, it achieves constant regret and survives even against a fully informed competitor. On the other hand, if it converges to an incorrect model, it suffers linear regret (relative to the best fixed strategy in hindsight) and vanishes from the market. This strong convergence becomes especially problematic when the data-generating process undergoes distribution shifts. 

In this section, we propose a simple method that naturally retains the fast convergence of Bayesian learning at early stages while regularizing its update to ensure robustness under distribution shifts. The method provides provable regret guarantees and improves adaptability. We consider a setting in which the data-generating distribution may shift over time. We first show that, while standard Bayesian learners may eventually adapt, their regret against the true sequence of distributions after a distribution shift can be linear in the total time $T$.

\begin{proposition}\label{thm:dist-shift-linear-regret-proposition}
    Consider a market in which the data-generating distribution $q$ may shift over the course of $T$ steps among a finite set $Q$ of distributions. 
    A Bayesian learner with a finite prior that assigns positive probability to each model in $Q$ may incur regret linear in $T$ compared to a benchmark that knows the correct sequence of distributions in advance, even if there is only a single distribution shift during the $T$ steps. 
\end{proposition}

The proof is in Appendix~\ref{sec:appendix-proofs}. 
The Bayesian’s fast convergence is a feature if the Bayesian puts positive prior probability on a fixed, correct model of the data-generating process and for asymptotic results this all that matters; the prior is otherwise irrelevant. But if the data generating process shifts over time the Bayesian’s posterior at the (unknown) time of a shift becomes the prior. So intermediate-term properties matter. In the intermediate term, a Bayesian who has nearly converged to a past model learns slowly, as the log odds of the new model to the past one are extreme and the expected shift in these log odds is the fixed relative entropy between the two models. With repeated shifts, the Bayesian thus spends a large amount of time near models that are incorrect and accumulates regret that can be linear. 

In contrast, there are no-regret learning algorithms that are known to handle distribution shifts more gracefully \citep{hazan2009efficient,kozat2007universal}. In particular, for the portfolio selection problem we consider, such algorithms can achieve regret that is logarithmic in $T$ and linear in the number $n$ of distribution shifts (see also \cite{hazan2016introduction}, Sections 10.3 and 10.4). However, as we have seen, logarithmic regret is not sufficient for survival when competing against agents who invest according to the true distribution, or even against Bayesian learners. 

The main observation of this section is that, while the exponential convergence that Bayesians achieve is sufficient to obtain constant regret---and thus ensures long-run survival against fully informed competitors---it is not necessary. We propose a simple and natural method that combines advantages of both approaches, making a step towards a best-of-both-worlds learning strategy. It guarantees constant regret in stationary settings, like standard Bayesian learning, thus outperforming no-regret learners in such cases. At the same time, it guarantees logarithmic regret under distribution shifts, surviving against no-regret learners in settings where adaptability is essential. 

Technically, our method stays close to Bayesian learning. 
We use a regularized version of the Bayesian update rule that initially behaves like standard Bayesian learning, but slows convergence just enough to keep alternative hypotheses alive. This enables fast recovery after shifts. 

\vspace{8pt}
\noindent
\textbf{Robust Bayesian Update:}
The \emph{Robust Bayesian Update} process we propose uses a simple and efficient modification of standard Bayesian learning. Initially, the learner sets uniform prior weights $\lambda^1_0, \dots, \lambda^K_0$. At each time $t \geq 0$, the learner uses the current weights $\lambda_t$ as its investment strategy. Then, after observing the outcome, it updates beliefs with an added regularization term $\epsilon_t = t^{-2}$:
% 
\begin{equation}\label{eq:robust-Bayes-update}
\lambda_{t} \rightarrow \text{ (Bayes update) } \rightarrow \tilde{\lambda}_{t+1} \rightarrow \lambda_{t+1} = \frac{\tilde{\lambda}_{t+1} + \epsilon_t}{1 + K \epsilon_t}.    
\end{equation}
where $K$ is the number of hypotheses.  
That is, using the observed outcome at time~$t$, the learner applies the standard Bayesian update to obtain tentative weights~$\tilde{\lambda}_{t+1}$. It then adds regularization and normalizes to get weights $\lambda_{t+1}$ in the simplex.   
This simple process satisfies the following guarantee:

\begin{theorem}\label{thm:robust-Bayesian}
 Let $Q$ be the set of models in the learner's support. Suppose an adversary selects a sequence of intervals $\big(\{T_i, q_i\}\big)_{i=1}^n$, where $q_i \in Q$ and $T_i$ is the duration of the $i$-th interval during which $q_i$ is the data-generating distribution. Let $T = \sum_{i=1}^n T_i$ denote the total time. Then the regret of the Robust Bayesian Update, relative to a benchmark of fixed investment strategies in each interval, knowing the intervals in advance, is $O(n \log T)$ with high probability. In particular, when $n$ is constant, the regret is logarithmic in $T$. 

 Moreover, if there are no distribution shifts (i.e., when $n=1$), the regret remains bounded by a constant at all times with high probability.
\end{theorem} 
\vspace{3pt}

The intuition behind the proof is that when the weight of an incorrect model is large, it decays exponentially fast, as in the case of a standard Bayesian learner. When it is small---approaching the scale of the regularization term---it no longer contributes significantly to regret. Thus, the regret under a stationary process remains bounded. After a distribution shift, all model weights are of at least the size of the regularization level, which prevents suppressing the alternatives at an exponential rate and keeps them ``alive.'' This is then followed by exponential convergence, leading to regret that grows only logarithmically in $T$. The proof of the theorem is in Appendix~\ref{sec:appendix-proofs}.
