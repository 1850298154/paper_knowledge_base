In this section we further analyze the relationship between an agent's  regret  and long-term survival in market dynamics, and use this analysis to derive the regret level of Bayesian learners. This relationship depends on the competition. In particular, we saw in Theorem \ref{thm:survival-by-finite-regret-gap} that the learners who survive (have a positive expected wealth level in the limit) are only those who maintain a constant regret gap relative to the best competitor in the market. 

As mentioned in Section \ref{sec:compare}, using the state distribution $q$ as the investment rule (if $q$ were known)  maximizes the expected growth rate among constant strategies. However, even this strategy incurs some regret, as it generally differs from the best strategy in hindsight. We now ask:  what is the regret level of using the true state distribution $q$ as the investment rule? This regret level represents the best attainable expected regret. Thus, by Theorem \ref{thm:survival-by-finite-regret-gap}, evaluating this regret level would serve as the benchmark for determining which regret levels ensure survival (with high probability, see Definition \ref{def:survive-and-vanish}) against any competitors who do not have information about future state realizations. 
To set the ground, let us first consider the following definition:

\begin{definition}
Fix an arbitrary history of state realizations $s_1,\dots,s_T$ and denote the empirical distribution of this history by $\hat{q}$, where $\hat{q}_s = \frac{1}{T}\sum_{t=1}^T \ind{s_t=s}$. A {\em ``magic agent,''} indexed also by $\hat{q}$, is an agent playing the (eventual) empirical distribution in every step. That is,  $\alpha^{\hat{q}}_{st} = \hat{q}_s$ for all $t \in [T]$.     
\end{definition}

Note that this agent uses information regarding future realizations of random states, which is not available to real agents in a stochastic environment. Next, we derive the regret level of using the correct distribution of states $q$ as the investment strategy.
The proof is given in Appendix \ref{sec:appendix-proofs}; it uses our analysis from Section~\ref{sec:compare} and a result from information theory on the relative entropy between the underlying distribution $q$ and the empirical realization of states.  

\begin{theorem}\label{thm:regret-of-q}
    An agent using the state distribution $q$ as the investment strategy for all $t$ has a constant expected regret.
    The expected regret, denoted $R$, depends only on the number of states $S$. 
\end{theorem}
We can now also state the following corollary.
\begin{corollary}\label{cor:q-survives-against-zero-regret}
An agent indexed by $q$ using the correct distribution $q$ survives when competing against a magic agent. This is since by Theorem \ref{thm:regret-of-q}, the strategy $q$ yields constant expected regret $R$, and so the regret difference compared to $\hat{q}$---which  by definition yields zero regret---is constant as well. By Theorem \ref{thm:survival-by-finite-regret-gap}, this implies that agent $q$ survives.
\end{corollary}

Our analyses above and in the preceding sections lead to the following characterization of the regret of Bayesian learners and the survival conditions for other learning agents competing with them in the market. The proof is given in Appendix \ref{sec:appendix-proofs}.

\begin{theorem}\label{thm:regret-of-a-Bayesian}
    The following holds for any market parameters:
    \begin{enumerate}
        \item Bayesian learners with any finite-support prior that includes the correct state distribution have expected regret bounded by a constant at all times.
        \item A learning agent survives in competition with Bayesians if and only if it has constant regret.
    \end{enumerate}
\end{theorem}

The next lemma provides an expression for the regret of a player as a function of the entropy relative to strategy $q$, using our notation $R$ from Theorem \ref{thm:regret-of-q} for the expected regret of strategy $q$. This expression will be useful in the following Section on imperfect Bayesians.

\begin{lemma}\label{thm:expected-regret-lemma}
    The expected regret of a constant strategy sequence (that is, $\alpha^n_{1:T}$ such that $\alpha^n_t =  \alpha^n$ for all $t$) is given by 
    $
    \E[R^T(\alpha^n_{1:T})] = T \cdot I_q(\alpha^n) + R.
    $ 
\end{lemma}
