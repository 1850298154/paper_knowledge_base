In the preceding sections, we saw that a perfect Bayesian is optimal in the sense that it survives almost surely in the market and drives out any player with regret increasing over time. In this section, we explore a scenario where a Bayesian learner suffers small errors, either in its set of prior distributions or in its update rule. We find that, while perfect Bayesians are optimal, Bayesian learning is also very fragile; for example, it is key that the correct probability distribution is one of the models they consider. Specifically, we show that an imperfect Bayesian would eventually vanish from the market, either against the state distribution $q$ or in competition with any player with sub-linear regret. This holds even if the Bayesian's errors are very small and have zero mean.
 
\subsection{Bayesian Learning with Inaccurate Priors}\label{sec:noisy-priors-Bayesian}

\input{inaccurate-priors-and-survival-time}

\subsection{Bayesian Learning with Noisy Updates}\label{sec:noisy-Bayesian}
Next, we consider a different type of imperfect Bayesian learner that does have $q$ in its prior, but in every step performs slightly inaccurate ``trembling hand'' updates. Also here, we demonstrate that Bayesian learning is fragile, even when the correct distribution lies in the support. To model this, we define a noisy Bayesian learner as one who at each step, either slightly over-weights the current observation or slightly over-weights its current prior, such that, in expectation, both the data and the prior receive the correct weights in every step (i.e., the errors in weight have zero mean).

For concreteness, consider the following scenario of a learner attempting to learn a distribution of states. Suppose that there are two states, $s_t = 0$ with a fixed probability $q \in (0,1)$, and $s_t = 1$ otherwise. The learner considers two models: $\theta_a = q$, which is the correct model, and $\theta_b \neq q$, with $\theta_b < 1$. The log-likelihood is given by
\begin{align}
    L(s_t) = 
    (1 - s_t) \log\Big(\frac{\theta_a}{\theta_b}\Big) + 
    s_t \log\Big(\frac{1 - \theta_a}{1 - \theta_b}\Big).
\end{align}

Now suppose that in every step $t$ the Bayesian learner performs ``$\lmb$-noisy updates'' where it over- or under-weights the data compared to the prior with a small excess weight $\lmbt$, where $\lmbt$ has $0$ mean. Specifically, for a parameter $\lmb > 0$, $\lmbt= \lmb$ with probability $1/2$ and $\eta_t = -\lmb$ otherwise. We find that even a tiny zero-mean error has a significant impact, essentially breaking the learning process.

\begin{theorem}\label{thm:wrong-update-Bayesians-vanish}
    For any $\lmb > 0$, the Bayesian learner with $\lmb$-noisy updates does not converge, and therefore incurs regret linear in $T$. 
\end{theorem}

The idea of the proof is to show that, with high probability, the learner has a systematic drift toward over-weighing new observations, despite the symmetry of the errors around zero. As a result, the learnerâ€™s beliefs fail to converge and continue to fluctuate in response to recent random events, leading to linear regret. The full proof appears in Appendix~\ref{sec:appendix-proofs}.
