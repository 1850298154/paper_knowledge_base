We provide in the following the proofs for the theorems stated in the main text.


\vspace{5pt}
\begin{proof} (Theorem \ref{thm:survival-by-finite-regret-gap})
 This result follows from the analysis leading to Equation (\ref{eq:logshare-as-regret}). For the first part of the theorem, we have $R^n(T) - R^m(T) = -\log(r^{nm}) - \log(r_0^{nm}) = \log(r^{mn}) + const$. If the left-hand side diverges to $+\infty$, then $r^{mn} = w^m/w^n$ diverges as well, which is only possible if $w^n \rightarrow 0$. For the second part, clearly this is a necessary condition due to the first part of the theorem: if an agent vanishes, it does not survive. To see that this condition is sufficient, if there exists a constant $c > 0$ such that $R^n(T) - R^m(T) < c$ at all times and for all $m \neq n$, then the wealth ratio $r^{mn}$ with any other player is bounded, and thus $w^n$ is bounded from below. 
\end{proof}
\vspace{5pt}


\begin{proof} (Theorem \ref{thm:regret-of-q}) 
Consider an agent indexed by $q$ using the state distribution $q$ as its investment strategy; i.e, $\alpha^q_{t} = q$ for all $t$, and we denote this constant sequence by $q_{1:T}$. Agent $q$ competes with a magic agent. 
The regret of the agent playing strategy $q$  is denoted $R^T(q_{1:T})$, and denote its expectation  
by $R = \E[R^T(q_{1:T})]$. On the one hand, using the fact that $\hat{q}$ has zero regret by definition, from Equation \ref{eq:logshare-as-regret} we have   
\[
R^T(q_{1:T}) = \log(r^{\hat{q}q}_T) - \log(r^{\hat{q}q}_0).
\] 
On the other hand, taking expectation of Equation \ref{eq:logshares}, we have
\begin{align*}
     \E \big[ & \log(r^{\hat{q}q}_T)\big]\\ & = 
     \E \big[\sum_s \sum_t \mathbf{1}_{s_t = s} \log(\frac{\alpha^{\hat{q}}_{s}}{q_{s}})\big]+ \log(r^{\hat{q}q}_{0}) \\
     & = 
     T \E \big[\sum_s \frac{n^s_T}{T} \log(\frac{n^s_T/T}{q_s})\big]+ \log(r^{\hat{q}q}_{0}) \\
     & = 
     T  \cdot 
     \E[I_{\hat{q}}(q)] + \log(r^{\hat{q}q}_{0}).
\end{align*}
where $n^s_T$ is the empirical count of state $s$ until time $T$. And thus, we get
$ R =  T \cdot \E \big[I_{\hat{q}}(q)\big]$.     
That is, the expected regret equals the relative entropy (a.k.a. KL-divergence) between an empirical distribution and the state distribution scaled by $T$.  The question of survival of this agent now hinges on how does $I_{\hat{q}}(q)$ depend on time. 
Here, we use a result from information theory, showing that the expected KL-divergence of the empirical distribution decreases as the alphabet size divided by the sample size\footnote{We are interested in the expectation. \cite{mardia2020concentration} also provide concentration bounds for this result.} (see, e.g., Lemma 13.2 in \cite{polyanskiy2024information}). In our case, $\E[I_{\hat{q}}(q)] \equiv \E[D_{KL}(\hat{q}||q)] = \frac{S-1}{2T} + o(\frac{1}{T})$.
Thus, the expected regret $R$ is constant, depending only on the number of states. 
\end{proof}
\vspace{5pt}


\begin{proof} (Theorem \ref{thm:regret-of-a-Bayesian}) 
By Theorem \ref{thm:regret-of-q}, an agent $n$ using $\alpha^n_t = q$ for all $t$ incurs constant expected regret. By Proposition \ref{thm:Bayesians-suvive-against-q}, a Bayesian learner whose support includes the distribution $q$ almost surely survives against such a competitor. Next, by Theorem \ref{thm:survival-by-finite-regret-gap} if some agent survives against a competitor, then the difference between their regret levels must be bounded by a constant at all times. Therefore, the Bayesian competing against the state distribution $q$ almost surely incurs constant regret. 
% 
Having established the first point, the second follows directly from Theorem \ref{thm:survival-by-finite-regret-gap}: Since the Bayesian has constant regret, any competitor who survives must also have constant regret. 
\end{proof}
\vspace{5pt}


\begin{proof} (Theorem \ref{thm:wrong-Bayesians-vanish}) 
Let $n$ be the index of a Bayesian agent with $q$ not within its support, and let $q' \neq q$ be the strategy within the support that is closest to $q$ in relative entropy. By Proposition \ref{thm:bayesian-convergence}, the strategy used by the Bayesian agent, $\alpha^n_t$, converges almost surely to $q'$. Hence, intuitively, for sufficiently large $t$, the strategies remain bounded away from $q$, leading to linear regret from that point onward. In more detail, define $I_0 = \frac{1}{2} I_q(q') > 0$. In every sample path $i$ (i.e., an infinite sequence, indexed by $i$, of state realizations and the ensuing sequence of $\alpha^n_t$), except on a set of measure zero, there exists a time $T_i$ such that for all $t \geq T_i$, we have $I_q(\alpha^n_t) > I_0$. The regret accumulated up to time $T_i$ is constant (possibly positive or negative). The expected regret difference relative to strategy $q$ at $t > T_i$ is $\E[U^t(q) - U^t(\alpha^n_t)] \geq I_0$, and so the expected regret relative to $q$ until time $T$ is at least $I_0 \cdot (T - T_i) + const$. By Theorem \ref{thm:regret-of-q}, the strategy $q$ incurs only constant regret, so the regret of the Bayesian agent converging to $q'$ grows linearly in $T$. Now, consider a no-regret learner. Such a learner has sublinear worst-case regret and thus sublinear expected regret. By Theorem \ref{thm:survival-by-finite-regret-gap}, the imperfect Bayesian agent who has linear regret vanishes.    
\end{proof}
\vspace{5pt}


\begin{proof} (Theorem \ref{thm:wrong-update-Bayesians-vanish}) 
Let $\epsilon > 0$. 
The log-likelihood ratio under the noisy update rule takes the following form:
\begin{align}\label{eq:inacurate-update-log-ratio}
    \log\Big(
    \frac{P_t(\theta_a)}{P_t(\theta_b)}
    \Big) 
    &=  
    (1 + \lmbt) L(s_t) 
    \nonumber \\&+ 
    (1 - \lmbt) \log \Big(\frac{P_{t-1}(\theta_a)}{P_{t-1}(\theta_b)}\Big) \nonumber \\
    &= 
    (1 + \lmbt)\sum_{\tau=0}^{t-1} L(s_{t - \tau}) \prod_{k=0}^\tau (1 - \eta_k)\\ 
    \nonumber 
    &+ 
    \log\Big(
    \frac{P_0(\theta_a)}{P_0(\theta_b)}
    \Big)
    \prod_{\tau=0}^t
    (1 - \eta_{\tau}),
\end{align}
where the empty product equals one, and we define $\eta_0 = 0$.
The products can be simplified:
\begin{align*}
    \prod_{\tau=0}^t
    (1 - \eta_\tau)  = 
    (1 - \lmb)^{n_+(t)}
    (1 + \lmb)^{n_-(t)},
\end{align*}
where $n_+(t)$ is a binomial random variable counting the number of times $\eta_{\tau \leq t} = \lmb$, and $n_-(t) = t - n_+(t)$. 
The last term can be written as  
\begin{align*}
    &
    \log\Big(
    \frac{P_0(\theta_a)}{P_0(\theta_b)}
    \Big)
    \cdot 
    (1 - \lmb)^{n_+(t)}
    (1 + \lmb)^{n_-(t)} \\
    &= 
    \log\Big(
    \frac{P_0(\theta_a)}{P_0(\theta_b)}
    \Big)
    \cdot 
    \Big[
    \Big(
    \frac{1 - \lmb}
    {1 + \lmb}\Big)^{n_+(t)}
    \Big] \cdot
    (1 + \lmb)^t.
\end{align*}

Intuitively, since $\E[n_+] = T/2$, this should be close to $(1 - \lmb^2)^{\nicefrac{t}{2}}$ with high probability as $t \rightarrow \infty$, which converges to zero for $\eta < 1$. Formally, we have the following claim.
    \begin{claim}
        Let $\epsilon > 0$.
        $
        \lim_{t \rightarrow \infty} \Pr \Big[ \left(\frac{1 - \lmb}{1 + \lmb}\right)^{n_+(t)} \cdot (1 + \lmb)^t > \epsilon \Big] = 0.
        $
    \end{claim}
    
    \begin{proof}
        Let $\delta > 0$. We need to show that there exists $T$ such that for all $t > T$, the event
        \[
        \Big(\frac{1 - \lmb}{1 + \lmb}\Big)^{n_+(t)} \cdot (1 + \lmb)^t > \epsilon
        \]
        has probability less than $\delta$. 
        % We will call this the high-probability event. 
        Dividing by $(1 + \lmb)^t$ and taking the logarithm, we get 
        $
        n_+(t) \cdot \big(\log(1 - \lmb) - \log(1 + \lmb)\big) > \log(\epsilon) - t \log(1 + \lmb).
        $.
        Rearranging, we have
        \[
        n_+(t) < \frac{t \log(1 + \lmb) - \log(\epsilon)}{\log(1 + \lmb) - \log(1 - \lmb)}.
        \]
        Denote $c_{\epsilon} = \frac{\log(\epsilon)}{\log(1 + \lmb) - \log(1 - \lmb)}$,
        so
        $
        n_+(t) < \frac{\log(1 + \lmb)}{\log(1 + \lmb) - \log(1 - \lmb)} \cdot t - c_{\epsilon}.
        $ 
        Next, we observe that the coefficient of $t$ is strictly less than $1/2$. To see this, define
        \begin{align*}
        c &= \frac{1}{2} - \frac{\log(1 + \lmb)}{\log(1 + \lmb) - \log(1 - \lmb)} \\
        &= 
        -\frac{\log(1 + \lmb) + \log(1 - \lmb)}{2 \big(\log(1 + \lmb) - \log(1 - \lmb)\big)}\\ 
        &= 
        \frac{-\log(1 - \lmb^2)}{\text{Positive Number}} > 0.
        \end{align*}
        Thus, we obtain that the event we wish to bound is
        $
        n_+(t) < \Big(\frac{1}{2} - c\Big)t - c_{\epsilon}.
        $
        Using Hoeffding's inequality, using that $\E[n_+] = \nicefrac{t}{2}$, we bound the probability of this event:
        \[
        \Pr\Big[\big|n_+(t) - \nicefrac{t}{2}\big| > ct + c_{\epsilon}\Big] < 2e^{-\frac{2(ct + c_{\epsilon})^2}{4t \lmb^2}} < 2e^{-\frac{1}{2}ct}.
        \]
        Setting $T = \lceil \frac{2}{c} \ln\left(\frac{2}{\delta}\right)
        \rceil$, we conclude that for all $t > T$, the probability of the event
        \[
        (1 + \lmb)^t \cdot \Big(\frac{1 - \lmb}{1 + \lmb}\Big)^{n_+(t)} > \epsilon
        \]
        is less than $\delta$, as required. Hence, the product approaches zero for large $t$:
        $
        \lim_{t \rightarrow \infty} \prod_{\tau=1}^t (1 - \eta_\tau) = 0
        $ almost surely, proving the claim.
    \end{proof}

    \vspace{5pt}
    \noindent
    {\em Remark:} It is worth noting here that the almost-sure behavior of the product is very different from its expectation, which equals one: 
    $$\sum_{k=0}^T \binom{T}{k} p^k(1 - \lmb)^k \cdot (1 - p)^{T - k} (1 + \lmb)^{T - k} = \big(p (1 - \lmb) + (1 - p) (1 + \lmb)\big)^T = 1 \ \text{ (since $p = 1/2$)}.$$

    The above claim implies that the second term in Equation (\ref{eq:inacurate-update-log-ratio}) converges to zero almost surely. In other words, the learner forgets the prior, much like a perfect Bayesian learner (albeit at a somewhat faster rate in this case). For the first term of Equation (\ref{eq:inacurate-update-log-ratio})
    $$(1 + \lmbt)\sum_{\tau=0}^{t-1} L(s_{t - \tau}) \prod_{k=0}^\tau (1 - \eta_k),$$
    a similar argument applies: 
    with high probability the product for large values of $\tau$ contributes negligibly to the sum, but there remains a random contribution from the smaller $\tau$ terms.  
    To see why there is no convergence, observe that the coefficient $(1 + \lmbt)$ oscillates randomly around $1$. More formally, let 
    $Y_\tau = \prod_{k=0}^\tau (1 - \eta_k)$, noting that $Y_0 = 1$, and let $Z_t  = \sum_{\tau=0}^{t-1}L(s_{t-\tau})Y_\tau$. Expanding the sum, we have: $Z_t = L(s_t)Y_0 + \dots + L(s_1)Y_{t-1}$ and for the next time step $Z_{t+1} = L(s_t)Y_0 + \dots + L(s_1)Y_{t}$. Since $Y_t = (1-\lmbt)Y_{t-1}$, this simplifies to $Z_{t+1} = (1 - \lmbt)Z_t + L(s_{t+1})$. 
    Clearly, this sequence does not converge, as both $L(s_{t+1})$ and $\lmbt$ take independent random values at each step.
\end{proof}
\vspace{5pt}


\begin{proof} (Lemma \ref{thm:expected-regret-lemma}) 
    The result follows from the definition of regret as a utility difference, as discussed in Section \ref{sec:compare}. Specifically, the regret of strategy sequence $\alpha^n_{1:T}$ is $R^T(\alpha^n_{1:T}) = U(\alpha^{\hat{q}}_{1:T}) - U(\alpha^n_{1:T})$, also, $R = \E[U(\alpha^{\hat{q}}_{1:T}) - U(q_{1:T})]$, and denote the regret of $\alpha^n_{1:T}$ with respect to the benchmark $q_{1:T}$ as $R^\alpha_q = U(q_{1:T}) - U(\alpha^n_{1:T})$. We have 
    $
    \E[R^T(\alpha^n_{1:T})] = \E[R^\alpha_q] + R = \E\Big[ T \sum_s \frac{n^s_T}{T} \log(\frac{q_s}{\alpha^n_{s}}) \Big] + R = T \cdot I_q(\alpha^n) + R
    $.   
\end{proof}
\vspace{5pt}



% Section 7
% \subsection{TBD: Move these proofs to Appendix:}
\begin{proof} (Proposition~\ref{thm:dist-shift-linear-regret-proposition})
    Consider a market process with two states and $T$ time steps. Let $1/2 < p < 1$, and define $q_1 = (p, 1-p)$, $q_2 = (1-p, p)$, and $Q = \{q_1, q_2\}$. Suppose that for the first $T_1$ steps the data-generating distribution is $q_1$, and for the remaining $T_2 = T - T_1$ steps it is $q_2$. At any time $t \leq T_1$, taking expectation in Equation~(\ref{eq:log-odds-by-n}), we have for a Bayesian learner starting from a uniform prior:
    $$
    \E[\log \frac{\lambda^1_t}{\lambda^2_t}] = \E[n^1_t] \cdot \log \frac{p}{1-p} + \E[n^2_t] \cdot \log \frac{1-p}{p},
    $$
    where, recall, $n^s_t$ denotes the number of times state $s$ has occurred up to time $t$.
    At time $T_1$ we get:
    $$
    \E[\log \frac{\lambda^1_{T_1}}{\lambda^2_{T_1}}] = 
    pT_1 \cdot \log \frac{p}{1-p} + (1-p)T_1 \cdot \log \frac{1-p}{p}.
    $$

    After the shift, at $t = T_1 + \tau$, using the same formula and the learner's state at time $T_1$, we have:
    $$
    \E[\log \frac{\lambda^1_t}{\lambda^2_t}] 
    = 
    \Big(
    pT_1 \cdot \log \frac{p}{1-p} + (1-p)T_1 \cdot \log \frac{1-p}{p} 
    \Big)
    +
    \Big(
    (1-p)\tau \cdot \log \frac{1-p}{p} + p\tau \cdot \log \frac{p}{1-p}
    \Big).
    $$
    Grouping terms, we get:
    $$
    \E[\log \frac{\lambda^1_t}{\lambda^2_t}] 
    = 
    p(T_1+\tau) \cdot \log \frac{p}{1-p} + (1-p)(T_1+\tau) \cdot \log \frac{1-p}{p}.
    $$

    Intuitively, for any $\tau < T_1$, we have $\E[n^1_{T_1 + \tau}] > \E[n^2_{T_1 + \tau}]$, and so $\lambda^1_t > \lambda^2_t$ during steps $t = T_1,\dots,T_1 + \tau$. This implies that during the second phase, the learner still assigns probability greater than $1/2$ to state~1, despite state~1 occurring with probability less than $1/2$. This discrepancy leads to regret proportional to $\tau$.     Concretely, take $T_1 = 2T/3$, $p = 3/4$, and $\tau = T/3$. Then during steps $t = T_1 + 1,\dots,T$ we have:
    $$
    \E[\log \frac{\lambda^1_t}{\lambda^2_t}] 
    > 
    \E[\log \frac{\lambda^1_T}{\lambda^2_T}] 
    = 
    \frac{3T}{4} \cdot \log 3 
    -
    \frac{T}{4} \cdot \log 3
    =
    \frac{\log 3}{2} T > \frac{1}{2} T.
    $$
    So the empirical probability the learner assigns to state 1 is at least $1/2$, whereas the true probability during the last $T/3$ steps is $1/4$. Therefore, similarly to the proof of Theorem \ref{thm:wrong-Bayesians-vanish}, 
    since there is a gap between the assigned and true probabilities, the regret accumulated during these steps after $2T/3$ is linear; in expectation it is at least the regret of the strategy $(\tfrac{1}{2}, \tfrac{1}{2})$, which is
    \[
    \frac{T}{3}\cdot I_{q_2}\left(\tfrac{1}{2}, \tfrac{1}{2}\right) = \frac{3 \log 3 + 4 \log 2}{4} \cdot \frac{T}{3} > \frac{T}{23}.
    \]
    During the first stage, until $2T/3$, the learner incurs a constant regret, so the total regret is linear in $T$.
\end{proof}
\vspace{5pt}


\begin{proof} (Theorem~\ref{thm:robust-Bayesian}) Let us consider a phase of the process in which the data generating distribution is $q$. For notational convenience, we overload the symbol $q$ to also denote the index of this distribution, so that $\lambda^q$ denotes the weight the learner assigns to this correct model.  
By the update rule~(\ref{eq:robust-Bayes-update}), for any $k \neq q$:
$$
\log \frac{\lambda^k_{t+1}}{\lambda^q_{t+1}} 
= 
\log \frac{\tilde{\lambda}^k_{t+1} + \epsilon_t}{\tilde{\lambda}^q_{t+1} + \epsilon_t}
=
\log \frac{\tilde{\lambda}^k_{t+1}}{\tilde{\lambda}^q_{t+1}} 
+ 
\log \big(1 + \tfrac{\epsilon_t}{\tilde{\lambda}^k_{t+1}} \big) 
- 
\log \big(1 + \tfrac{\epsilon_t}{\tilde{\lambda}^q_{t+1}} \big).
$$
Since for any $x > 0$ it holds that $0 < \log(1 + x) \leq x$, we have
$$
\log \frac{\lambda^k_{t+1}}{\lambda^q_{t+1}} 
\leq
\log \frac{\tilde{\lambda}^k_{t+1}}{\tilde{\lambda}^q_{t+1}} 
+
\frac{\epsilon_t}{\tilde{\lambda}^k_{t+1}}.
$$
Applying the Bayesian update rule for the log-odds and taking expectations, we get
$$
\E\Big[\log \frac{\lambda^k_{t+1}}{\lambda^q_{t+1}} \Big]
\leq
\log \frac{\lambda^k_t}{\lambda^q_t}
-
I_q(\theta^k)
+
\E\Big[ \frac{\epsilon_t}{\tilde{\lambda}^k_{t+1}} \Big].
$$
Now consider two cases. First, suppose first that $\tilde{\lambda}^k_{t+1} < t^{-\tfrac{3}{2}}$. For $t$ large enough (e.g., $t > 6$), we have
$$
\lambda^k_{t+1} < \tilde{\lambda}^k_{t+1} + \epsilon_t \le t^{-\tfrac{3}{2}} + t^{-2} < t^{-\tfrac{4}{3}}.
$$
Intuitively, once $\tilde{\lambda}^k_{t+1}$ becomes small, it stays small.
In the second case, $\tilde{\lambda}^k_{t+1} \geq t^{-\tfrac{3}{2}}$. Then, we have
\begin{equation}\label{eq:robust-Bayes-update-convergence}
\E\Big[\log \frac{\lambda^k_{t+1}}{\lambda^q_{t+1}} \Big]
\leq
\log \frac{\lambda^k_t}{\lambda^q_t}
-
I_q(\theta^k)
+
O\big(t^{-\frac{1}{2}}\big).
\end{equation}
Since $I_q(\theta^k)$ is a positive constant for any $\theta^k \neq q$, we see that for large enough $t$, the log-odds of $k \neq q$ relative to $q$ decrease at a linear rate, implying that the posterior distribution approaches exponentially fast the true model $q$. Thus, after a number of steps depending logarithmically on the value of  $\lambda^q$ at the start of the phase,  
% (independent of $T$), 
we reach---with high probability over the randomness in the data-generating process---the first case where $\tilde{\lambda}^k_{t+1} < t^{-\tfrac{3}{2}}$. 
% \etdelete{Overall, we conclude that after some constant time $T_0$ (independent of $T$), it holds with high probability that for all $t > T_0$ and all $k \neq q$, we have $\lambda^k_t < t^{-\tfrac{4}{3}}$. \etcomment{this last sentence only true in the initial phase when the $\lambda$ values start out as constants. we need to say that this is what we need. Or maybe even better: move this sentence to the next paragraph}}

\vspace{8pt}
\noindent
\textbf{Constant regret for a stationary process:} 
Given that $\lambda_0^q$ is a constant, after some constant time $T_0$ (independent of $T$), it holds with high probability that for all $t > T_0$ and all $k \neq q$, we have $\lambda^k_t < t^{-\tfrac{4}{3}}$.
Using this, we can bound the regret of player $n$ under our strategy. Let $I$ denote the relative entropy of the worst model in the prior: $I = \max_k I_q(\theta^k)$. With high probability,
$$
R^T(\alpha_{1:T}^n) - R \leq 2\sum_{t=1}^{T} I_q(\alpha^n_t) \leq 2\sum_{t=1}^{T} I \cdot t^{-4/3} < 2\sum_{t=1}^{\infty} I \cdot t^{-4/3} = 2I \cdot \zeta(4/3) \approx 2I \cdot 3.601.
$$
Where $\zeta(x) = \sum_{n=1}^\infty \tfrac{1}{n^x}$ denotes the Riemann zeta function which has finite value for $x>1$.

\vspace{8pt}
\noindent
\textbf{Logarithmic regret after a distribution shift:} The above analysis shows one side of the regularization method: when the weights on models $k \neq q$ are large, they decay exponentially fast, similar to the standard Bayesian update. 

The other side is what happens after a distribution shift. The key point is that while the weights on incorrect models become small enough to ensure constant regret, the regularization prevents them from becoming exponentially small. Suppose that after some history of length $T_1$, the stochastic process shifts to a new distribution, which we denote by $q'$. 
A lower bound on $\lambda^{q'}_{T_1+1}$ is the amount of regularization added to it in the previous step, namely,
$$
\lambda^{q'}_{T_1+1} \geq \frac{\epsilon_{T_1}}{1 + K \epsilon_{T_1}} \geq \frac{1}{K {T_1}^2}.
$$
The last inequality holds since the shift occurs at $t > 1$.

So at time $T_1 + 1$, we can think about the situation as a new learning process with a prior weight of at least $\lambda^{q'}_{T_1+1} \geq \frac{1}{K {T_1}^2}$. 
Because of exponential convergence, the time to ``forget'' this prior is logarithmic in $T_1$. 
Specifically, the exponent for model $k$ is $I_{q'}(\theta^k)$. 
Let $I$ now be the minimal entropy over models $k \neq q'$. The expected time $\tau$ it takes to reach $\lambda_t^k\le \lambda_t^{q'}$ for all $k \neq q'$ and eliminate this prior is $\tau = \tfrac{2}{I} \log T_1 + \text{constant}$. 
After this time, we already know that the regret incurred until the next distribution shift, or reaching the time horizon $T$, is bounded by a constant independent of $T$. 
\end{proof}
\vspace{12pt}

